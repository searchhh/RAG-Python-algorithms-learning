Contents
Preface xiii
I Foundations
Introduction 3
1 TheRoleofAlgorithmsinComputing 5
1.1 Algorithms 5
1.2 Algorithmsasatechnology 11
2 GettingStarted 16
2.1 Insertion sort 16
2.2 Analyzing algorithms 23
2.3 Designing algorithms 29
3 GrowthofFunctions 43
3.1 Asymptoticnotation 43
3.2 Standardnotations andcommonfunctions 53
4 Divide-and-Conquer 65
4.1 Themaximum-subarray problem 68
4.2 Strassen’salgorithm formatrixmultiplication 75
4.3 Thesubstitution methodforsolving recurrences 83
4.4 Therecursion-tree methodforsolvingrecurrences 88
4.5 Themastermethodforsolving recurrences 93
?
4.6 Proofofthemastertheorem 97
5 ProbabilisticAnalysisandRandomizedAlgorithms 114
5.1 Thehiringproblem 114
5.2 Indicator randomvariables 118
5.3 Randomizedalgorithms 122
?
5.4 Probabilistic analysis andfurtherusesofindicator randomvariables
130
vi Contents
II Sorting and OrderStatistics
Introduction 147
6 Heapsort 151
6.1 Heaps 151
6.2 Maintaining theheapproperty 154
6.3 Buildingaheap 156
6.4 Theheapsort algorithm 159
6.5 Priorityqueues 162
7 Quicksort 170
7.1 Description ofquicksort 170
7.2 Performanceofquicksort 174
7.3 Arandomized versionofquicksort 179
7.4 Analysisofquicksort 180
8 SortinginLinearTime 191
8.1 Lowerbounds forsorting 191
8.2 Countingsort 194
8.3 Radixsort 197
8.4 Bucketsort 200
9 MediansandOrderStatistics 213
9.1 Minimumandmaximum 214
9.2 Selectioninexpected lineartime 215
9.3 Selectioninworst-case lineartime 220
III DataStructures
Introduction 229
10 ElementaryDataStructures 232
10.1 Stacksandqueues 232
10.2 Linkedlists 236
10.3 Implementing pointersandobjects 241
10.4 Representing rootedtrees 246
11 HashTables 253
11.1 Direct-address tables 254
11.2 Hashtables 256
11.3 Hashfunctions 262
11.4 Openaddressing 269
?
11.5 Perfecthashing 277
Contents vii
12 BinarySearchTrees 286
12.1 Whatisabinarysearchtree? 286
12.2 Queryingabinarysearchtree 289
12.3 Insertion anddeletion 294
?
12.4 Randomlybuiltbinarysearchtrees 299
13 Red-BlackTrees 308
13.1 Propertiesofred-black trees 308
13.2 Rotations 312
13.3 Insertion 315
13.4 Deletion 323
14 AugmentingDataStructures 339
14.1 Dynamicorderstatistics 339
14.2 Howtoaugmentadatastructure 345
14.3 Intervaltrees 348
IV Advanced Designand Analysis Techniques
Introduction 357
15 DynamicProgramming 359
15.1 Rodcutting 360
15.2 Matrix-chain multiplication 370
15.3 Elementsofdynamicprogramming 378
15.4 Longestcommonsubsequence 390
15.5 Optimalbinarysearchtrees 397
16 GreedyAlgorithms 414
16.1 Anactivity-selection problem 415
16.2 Elementsofthegreedy strategy 423
16.3 Huffmancodes 428
?
16.4 Matroidsandgreedymethods 437
?
16.5 Atask-scheduling problem asamatroid 443
17 AmortizedAnalysis 451
17.1 Aggregateanalysis 452
17.2 Theaccounting method 456
17.3 Thepotential method 459
17.4 Dynamictables 463
viii Contents
V Advanced DataStructures
Introduction 481
18 B-Trees 484
18.1 DefinitionofB-trees 488
18.2 Basicoperations onB-trees 491
18.3 DeletingakeyfromaB-tree 499
19 FibonacciHeaps 505
19.1 StructureofFibonacciheaps 507
19.2 Mergeable-heap operations 510
19.3 Decreasingakeyanddeleting anode 518
19.4 Boundingthemaximumdegree 523
20 vanEmdeBoasTrees 531
20.1 Preliminaryapproaches 532
20.2 Arecursivestructure 536
20.3 ThevanEmdeBoastree 545
21 DataStructuresforDisjointSets 561
21.1 Disjoint-set operations 561
21.2 Linked-list representation ofdisjointsets 564
21.3 Disjoint-set forests 568
?
21.4 Analysisofunionbyrankwithpathcompression 573
VI GraphAlgorithms
Introduction 587
22 ElementaryGraphAlgorithms 589
22.1 Representations ofgraphs 589
22.2 Breadth-firstsearch 594
22.3 Depth-firstsearch 603
22.4 Topological sort 612
22.5 Stronglyconnected components 615
23 MinimumSpanningTrees 624
23.1 Growingaminimumspanning tree 625
23.2 ThealgorithmsofKruskalandPrim 631
Contents ix
24 Single-SourceShortestPaths 643
24.1 TheBellman-Fordalgorithm 651
24.2 Single-source shortestpathsindirected acyclicgraphs 655
24.3 Dijkstra’salgorithm 658
24.4 Differenceconstraints andshortestpaths 664
24.5 Proofsofshortest-paths properties 671
25 All-PairsShortestPaths 684
25.1 Shortestpathsandmatrixmultiplication 686
25.2 TheFloyd-Warshall algorithm 693
25.3 Johnson’s algorithm forsparsegraphs 700
26 MaximumFlow 708
26.1 Flownetworks 709
26.2 TheFord-Fulkerson method 714
26.3 Maximumbipartitematching 732
?
26.4 Push-relabel algorithms 736
?
26.5 Therelabel-to-front algorithm 748
VII Selected Topics
Introduction 769
27 MultithreadedAlgorithms 772
27.1 Thebasicsofdynamicmultithreading 774
27.2 Multithreaded matrixmultiplication 792
27.3 Multithreaded mergesort 797
28 MatrixOperations 813
28.1 Solvingsystemsoflinearequations 813
28.2 Inverting matrices 827
28.3 Symmetricpositive-definite matricesandleast-squares approximation
832
29 LinearProgramming 843
29.1 Standardandslackforms 850
29.2 Formulatingproblemsaslinearprograms 859
29.3 Thesimplexalgorithm 864
29.4 Duality 879
29.5 Theinitialbasicfeasiblesolution 886
x Contents
30 PolynomialsandtheFFT 898
30.1 Representing polynomials 900
30.2 TheDFTandFFT 906
30.3 EfficientFFTimplementations 915
31 Number-TheoreticAlgorithms 926
31.1 Elementarynumber-theoretic notions 927
31.2 Greatestcommondivisor 933
31.3 Modulararithmetic 939
31.4 Solvingmodularlinearequations 946
31.5 TheChineseremainder theorem 950
31.6 Powersofanelement 954
31.7 TheRSApublic-key cryptosystem 958
?
31.8 Primalitytesting 965
?
31.9 Integerfactorization 975
32 StringMatching 985
32.1 Thenaivestring-matching algorithm 988
32.2 TheRabin-Karpalgorithm 990
32.3 Stringmatchingwithfiniteautomata 995
?
32.4 TheKnuth-Morris-Pratt algorithm 1002
33 ComputationalGeometry 1014
33.1 Line-segmentproperties 1015
33.2 Determiningwhetheranypairofsegmentsintersects 1021
33.3 Findingtheconvexhull 1029
33.4 Findingtheclosestpairofpoints 1039
34 NP-Completeness 1048
34.1 Polynomialtime 1053
34.2 Polynomial-time verification 1061
34.3 NP-completeness andreducibility 1067
34.4 NP-completeness proofs 1078
34.5 NP-completeproblems 1086
35 ApproximationAlgorithms 1106
35.1 Thevertex-coverproblem 1108
35.2 Thetraveling-salesman problem 1111
35.3 Theset-covering problem 1117
35.4 Randomization andlinearprogramming 1123
35.5 Thesubset-sum problem 1128
Contents xi
VIII Appendix: MathematicalBackground
Introduction 1143
A Summations 1145
A.1 Summationformulasandproperties 1145
A.2 Boundingsummations 1149
B Sets,Etc. 1158
B.1 Sets 1158
B.2 Relations 1163
B.3 Functions 1166
B.4 Graphs 1168
B.5 Trees 1173
C CountingandProbability 1183
C.1 Counting 1183
C.2 Probability 1189
C.3 Discreterandomvariables 1196
C.4 Thegeometricandbinomialdistributions 1201
?
C.5 Thetailsofthebinomialdistribution 1208
D Matrices 1217
D.1 Matricesandmatrixoperations 1217
D.2 Basicmatrixproperties 1222
Bibliography 1231
Index 1251
Preface
Before there werecomputers, there were algorithms. Butnow that there are com-
puters,thereareevenmorealgorithms,andalgorithmslieattheheartofcomputing.
This book provides a comprehensive introduction to the modern study of com-
puter algorithms. It presents many algorithms and covers them in considerable
depth, yet makes their design and analysis accessible to all levels of readers. We
have tried to keep explanations elementary without sacrificing depth of coverage
ormathematical rigor.
Eachchapterpresentsanalgorithm,adesigntechnique, anapplicationarea,ora
relatedtopic. AlgorithmsaredescribedinEnglishandinapseudocodedesignedto
bereadable byanyonewhohasdonealittleprogramming. Thebookcontains 244
figures—many with multiple parts—illustrating how the algorithms work. Since
we emphasize efficiency as a design criterion, we include careful analyses of the
runningtimesofallouralgorithms.
The text is intended primarily for use in undergraduate or graduate courses in
algorithmsordatastructures. Becauseitdiscusses engineering issuesinalgorithm
design, as well as mathematical aspects, it is equally well suited for self-study by
technical professionals.
In this, the third edition, we have once again updated the entire book. The
changes coverabroad spectrum, including newchapters, revisedpseudocode, and
amoreactivewritingstyle.
Totheteacher
Wehave designed this book to be both versatile and complete. You should find it
useful foravariety ofcourses, from anundergraduate course indata structures up
through a graduate course in algorithms. Because we have provided considerably
morematerialthancanfitinatypicalone-termcourse, youcanconsider thisbook
tobea“buffet”or“smorgasbord”fromwhichyoucanpickandchoosethematerial
thatbestsupports thecourseyouwishtoteach.
xiv Preface
You should find it easy to organize your course around just the chapters you
need. Wehavemadechaptersrelativelyself-contained, sothatyouneednotworry
aboutanunexpectedandunnecessary dependence ofonechapteronanother. Each
chapter presents the easier material first and the more difficult material later, with
section boundaries marking natural stopping points. In an undergraduate course,
you might use only the earlier sections from a chapter; in a graduate course, you
mightcovertheentirechapter.
Wehaveincluded957exercisesand158problems. Eachsectionendswithexer-
cises,andeachchapterendswithproblems. Theexercisesaregenerallyshortques-
tions that test basic mastery of the material. Some are simple self-check thought
exercises, whereas others are more substantial and are suitable as assigned home-
work. Theproblems are moreelaborate case studies that often introduce new ma-
terial;theyoftenconsistofseveralquestionsthatleadthestudentthroughthesteps
required toarriveatasolution.
Departing from our practice in previous editions of this book, we have made
publicly available solutions tosome, butbynomeans all, oftheproblems andex-
ercises. OurWebsite, http://mitpress.mit.edu/algorithms/, linkstothesesolutions.
Youwillwanttocheckthissitetomakesurethatitdoesnotcontainthesolutionto
anexercise orproblem thatyou plan toassign. Weexpect thesetofsolutions that
weposttogrowslowlyovertime,soyouwillneedtocheckiteachtimeyouteach
thecourse.
Wehavestarred(?)thesectionsandexercisesthataremoresuitableforgraduate
students than for undergraduates. A starred section is not necessarily more diffi-
cult than an unstarred one, but it may require an understanding of more advanced
mathematics. Likewise, starred exercises mayrequire anadvanced background or
morethanaveragecreativity.
Tothestudent
We hope that this textbook provides you with an enjoyable introduction to the
field of algorithms. We have attempted to make every algorithm accessible and
interesting. Tohelpyouwhenyouencounter unfamiliarordifficultalgorithms, we
describe each one in a step-by-step manner. We also provide careful explanations
of the mathematics needed to understand the analysis of the algorithms. If you
already havesomefamiliarity withatopic, youwillfindthechapters organized so
thatyoucanskimintroductory sectionsandproceedquicklytothemoreadvanced
material.
This is a large book, and your class will probably cover only a portion of its
material. We have tried, however, to make this a book that will be useful to you
now as a course textbook and also later in your career as a mathematical desk
reference oranengineering handbook.
Preface xv
Whataretheprerequisites forreadingthisbook?
 You should have some programming experience. In particular, you should un-
derstand recursive procedures and simple data structures such as arrays and
linkedlists.
 Youshouldhavesomefacility withmathematicalproofs,andespecially proofs
bymathematicalinduction. Afewportionsofthebookrelyonsomeknowledge
ofelementarycalculus. Beyondthat,PartsIandVIIIofthisbookteachyouall
themathematical techniques youwillneed.
We have heard, loud and clear, the call to supply solutions to problems and
exercises. OurWeb site, http://mitpress.mit.edu/algorithms/, links to solutions for
afewoftheproblemsandexercises. Feelfreetocheckyoursolutionsagainstours.
Weask,however,thatyoudonotsendyoursolutions tous.
Totheprofessional
The wide range of topics in this book makes it an excellent handbook on algo-
rithms. Because each chapter is relatively self-contained, you can focus in on the
topicsthatmostinterest you.
Most of the algorithms we discuss have great practical utility. We therefore
address implementation concerns and other engineering issues. We often provide
practicalalternativestothefewalgorithmsthatareprimarilyoftheoreticalinterest.
If you wish to implement any of the algorithms, you should find the transla-
tion of our pseudocode into your favorite programming language to be a fairly
straightforward task. Wehave designed thepseudocode to present each algorithm
clearly and succinctly. Consequently, we do not address error-handling and other
software-engineering issuesthatrequirespecificassumptionsaboutyourprogram-
mingenvironment. Weattempttopresenteachalgorithmsimplyanddirectlywith-
out allowing the idiosyncrasies of a particular programming language to obscure
itsessence.
We understand that if you are using this book outside of a course, then you
mightbeunabletocheckyoursolutionstoproblemsandexercisesagainstsolutions
provided by aninstructor. OurWebsite, http://mitpress.mit.edu/algorithms/, links
to solutions for some of the problems and exercises so that you can check your
work. Pleasedonotsendyoursolutions tous.
Toourcolleagues
We have supplied an extensive bibliography and pointers to the current literature.
Each chapter ends with a set of chapter notes that give historical details and ref-
erences. The chapter notes donot provide a complete reference to the whole field
xvi Preface
of algorithms, however. Though it may be hard to believe for a book of this size,
spaceconstraints preventedusfromincluding manyinteresting algorithms.
Despite myriad requests from students for solutions to problems and exercises,
we have chosen as a matter of policy not to supply references for problems and
exercises,toremovethetemptationforstudentstolookupasolutionratherthanto
finditthemselves.
Changesforthethirdedition
What has changed between the second and third editions of this book? The mag-
nitude of the changes is on a par with the changes between the first and second
editions. As we said about the second-edition changes, depending on how you
lookatit,thebookchanged eithernotmuchorquiteabit.
Aquicklookatthetableofcontentsshowsthatmostofthesecond-editionchap-
ters and sections appear in the third edition. We removed two chapters and one
section, but we have added three new chapters and two new sections apart from
thesenewchapters.
Wekeptthehybridorganizationfromthefirsttwoeditions. Ratherthanorganiz-
ing chapters by only problem domains or according only to techniques, this book
haselements ofboth. Itcontains technique-based chaptersondivide-and-conquer,
dynamic programming, greedy algorithms, amortized analysis, NP-Completeness,
and approximation algorithms. But it also has entire parts on sorting, on data
structures for dynamic sets, and on algorithms for graph problems. We find that
althoughyouneedtoknowhowtoapplytechniquesfordesigningandanalyzingal-
gorithms, problems seldom announce toyouwhichtechniques aremostamenable
tosolvingthem.
Hereisasummaryofthemostsignificantchanges forthethirdedition:
 WeaddednewchaptersonvanEmdeBoastreesandmultithreaded algorithms,
andwehavebrokenoutmaterialonmatrixbasicsintoitsownappendixchapter.
 We revised the chapter on recurrences to more broadly cover the divide-and-
conquer technique, anditsfirsttwosections applydivide-and-conquer tosolve
twoproblems. ThesecondsectionofthischapterpresentsStrassen’salgorithm
for matrix multiplication, which we have moved from the chapter on matrix
operations.
 We removed two chapters that were rarely taught: binomial heaps and sorting
networks. One key idea in the sorting networks chapter, the 0-1 principle, ap-
pearsinthiseditionwithinProblem8-7asthe0-1sortinglemmaforcompare-
exchange algorithms. The treatment of Fibonacci heaps no longer relies on
binomialheapsasaprecursor.
Preface xvii
 Werevisedourtreatmentofdynamicprogrammingandgreedyalgorithms. Dy-
namicprogrammingnowleadsoffwithamoreinterestingproblem,rodcutting,
than the assembly-line scheduling problem from the second edition. Further-
more,weemphasizememoizationabitmorethanwedidinthesecondedition,
and we introduce the notion of the subproblem graph as a way to understand
the running timeof a dynamic-programming algorithm. In our opening exam-
ple of greedy algorithms, the activity-selection problem, we get to the greedy
algorithm moredirectly thanwedidinthesecondedition.
 The way we delete a node from binary search trees (which includes red-black
trees) now guarantees that the node requested for deletion is the node that is
actually deleted. In the first two editions, in certain cases, some other node
wouldbedeleted,withitscontents movingintothenodepassedtothedeletion
procedure. Withournewwaytodeletenodes,ifothercomponentsofaprogram
maintainpointerstonodesinthetree,theywillnotmistakenlyendupwithstale
pointers tonodesthathavebeendeleted.
 The material on flow networks now bases flows entirely on edges. This ap-
proachismoreintuitivethanthenetflowusedinthefirsttwoeditions.
 With the material on matrix basics and Strassen’s algorithm moved to other
chapters, thechapteronmatrixoperationsissmallerthaninthesecondedition.
 Wehave modified our treatment ofthe Knuth-Morris-Pratt string-matching al-
gorithm.
 We corrected several errors. Most of these errors were posted on our Website
ofsecond-edition errata,butafewwerenot.
 Basedonmanyrequests,wechangedthesyntax(asitwere)ofourpseudocode.
Wenowuse“ ”toindicateassignmentand“==”totestforequality, justasC,
D
C++,Java, andPython do. Likewise, wehaveeliminated thekeywords doand
thenandadopted“//”asourcomment-to-end-of-linesymbol. Wealsonowuse
dot-notation toindicate object attributes. Ourpseudocode remains procedural,
rather than object-oriented. In other words, rather than running methods on
objects, wesimplycallprocedures, passing objectsasparameters.
 We added 100 new exercises and 28 new problems. We also updated many
bibliography entriesandaddedseveralnewones.
 Finally, we went through the entire book and rewrote sentences, paragraphs,
andsections tomakethewritingclearerandmoreactive.
xviii Preface
Website
You can use our Web site, http://mitpress.mit.edu/algorithms/, to obtain supple-
mentary information and to communicate with us. The Web site links to a list of
known errors, solutions to selected exercises and problems, and (of course) a list
explaining the corny professor jokes, as well as other content that we might add.
TheWebsitealsotellsyouhowtoreporterrorsormakesuggestions.
Howweproducedthisbook
Like the second edition, the third edition was produced in LATEX2". We used the
Times font with mathematics typeset using the MathTime Pro 2 fonts. We thank
Michael Spivak from Publish or Perish, Inc., Lance Carnes from Personal TeX,
Inc., and Tim Tregubov from Dartmouth College for technical support. As in the
previous twoeditions, wecompiled the index using Windex, aCprogram that we
wrote, and the bibliography was produced with BIBTEX. The PDF files for this
bookwerecreatedonaMacBookrunning OS10.5.
We drew the illustrations for the third edition using MacDraw Pro, with some
of the mathematical expressions in illustrations laid in with the psfrag package
for LATEX2". Unfortunately, MacDraw Pro is legacy software, having not been
marketed for over a decade now. Happily, we still have a couple of Macintoshes
thatcanruntheClassic environment under OS10.4, andhencetheycanrunMac-
DrawPro—mostly. EvenundertheClassicenvironment, wefindMacDrawProto
be far easier to use than any other drawing software for the types of illustrations
that accompany computer-science text, and it produces beautiful output.1 Who
knows howlong our pre-Intel Macswillcontinue torun, soifanyone from Apple
islistening: PleasecreateanOSX-compatible versionofMacDrawPro!
Acknowledgmentsforthethirdedition
We have been working with the MIT Press for over two decades now, and what a
terrific relationship it has been! We thank Ellen Faran, Bob Prior, Ada Brunstein,
andMaryReillyfortheirhelpandsupport.
We were geographically distributed while producing the third edition, working
in the Dartmouth College Department of Computer Science, the MIT Computer
1WeinvestigatedseveraldrawingprogramsthatrununderMacOSX,butallhadsignificantshort-
comings compared with MacDraw Pro. We briefly attempted to produce the illustrations for this
bookwithadifferent,wellknowndrawingprogram.Wefoundthatittookatleastfivetimesaslong
toproduceeachillustrationasittookwithMacDrawPro,andtheresultingillustrationsdidnotlook
asgood.HencethedecisiontoreverttoMacDrawProrunningonolderMacintoshes.
Preface xix
Science and Artificial Intelligence Laboratory, and the Columbia University De-
partment of Industrial Engineering and Operations Research. We thank our re-
spective universities and colleagues forproviding such supportive andstimulating
environments.
JulieSussman,P.P.A.,onceagainbailedusoutasthetechnicalcopyeditor. Time
and again, wewere amazed at theerrors that eluded us, but that Julie caught. She
alsohelpedusimproveourpresentationinseveralplaces. IfthereisaHallofFame
for technical copyeditors, Julie is a sure-fire, first-ballot inductee. She is nothing
shortofphenomenal. Thankyou,thankyou,thankyou,Julie! PriyaNatarajanalso
foundsomeerrorsthatwewereabletocorrectbeforethisbookwenttopress. Any
errorsthatremain(andundoubtedly, somedo)aretheresponsibility oftheauthors
(andprobably wereinserted afterJuliereadthematerial).
The treatment for van Emde Boas trees derives from Erik Demaine’s notes,
which were in turn influenced by Michael Bender. We also incorporated ideas
fromJavedAslam,BradleyKuszmaul,andHuiZhaintothisedition.
Thechapteronmultithreadingwasbasedonnotesoriginallywrittenjointlywith
HaraldProkop. ThematerialwasinfluencedbyseveralothersworkingontheCilk
project at MIT, including Bradley Kuszmaul and Matteo Frigo. The design of the
multithreaded pseudocode took its inspiration from the MIT Cilk extensions to C
andbyCilkArts’sCilk++extensions toC++.
We also thank the many readers of the first and second editions who reported
errorsorsubmittedsuggestions forhowtoimprovethisbook. Wecorrected allthe
bona fide errors that were reported, and we incorporated as many suggestions as
wecould. Werejoice thatthenumber ofsuchcontributors hasgrownsogreatthat
wemustregretthatithasbecomeimpractical tolistthemall.
Finally, we thank our wives—Nicole Cormen, Wendy Leiserson, Gail Rivest,
and Rebecca Ivry—and our children—Ricky, Will, Debby, and Katie Leiserson;
AlexandChristopherRivest;andMolly,Noah,andBenjaminStein—fortheirlove
andsupport whileweprepared this book. Thepatience andencouragement ofour
familiesmadethisprojectpossible. Weaffectionately dedicate thisbooktothem.
THOMAS H. CORMEN Lebanon, NewHampshire
CHARLES E. LEISERSON Cambridge, Massachusetts
RONALD L. RIVEST Cambridge, Massachusetts
CLIFFORD STEIN NewYork,NewYork
February2009
Introduction to Algorithms
Third Edition
I Foundations
Introduction
This part will start you thinking about designing and analyzing algorithms. It is
intended to be a gentle introduction to how we specify algorithms, some of the
design strategies we will use throughout this book, and many of the fundamental
ideasusedinalgorithmanalysis. Laterpartsofthisbookwillbuilduponthisbase.
Chapter 1 provides an overview of algorithms and their place in modern com-
putingsystems. Thischapterdefineswhatanalgorithmisandlistssomeexamples.
It also makes a case that we should consider algorithms as a technology, along-
side technologies such as fast hardware, graphical user interfaces, object-oriented
systems,andnetworks.
In Chapter 2, we see our first algorithms, which solve the problem of sorting
a sequence of n numbers. They are written in a pseudocode which, although not
directlytranslatabletoanyconventionalprogramminglanguage,conveysthestruc-
tureofthealgorithm clearly enoughthatyoushouldbeabletoimplementitinthe
language of your choice. The sorting algorithms we examine are insertion sort,
whichuses anincremental approach, and merge sort, whichuses arecursive tech-
nique known as “divide-and-conquer.” Although the time each requires increases
with the value of n, the rate of increase differs between the two algorithms. We
determine these running times in Chapter 2, and we develop a useful notation to
expressthem.
Chapter 3 precisely defines this notation, which we call asymptotic notation. It
starts by defining several asymptotic notations, which we use for bounding algo-
rithm running times from above and/or below. The rest of Chapter 3 is primarily
a presentation of mathematical notation, more to ensure that your use of notation
matchesthatinthisbookthantoteachyounewmathematicalconcepts.
4 PartI Foundations
Chapter 4 delves further into the divide-and-conquer method introduced in
Chapter 2. It provides additional examples of divide-and-conquer algorithms, in-
cluding Strassen’s surprising method for multiplying two square matrices. Chap-
ter 4 contains methods for solving recurrences, which are useful for describing
the running times of recursive algorithms. One powerful technique is the “mas-
ter method,” which we often use to solve recurrences that arise from divide-and-
conquer algorithms. Although much of Chapter 4 is devoted to proving the cor-
rectness ofthemastermethod, youmayskipthisproofyetstillemploythemaster
method.
Chapter5introducesprobabilistic analysisandrandomizedalgorithms. Wetyp-
ically use probabilistic analysis to determine the running time of an algorithm in
cases in which, due to the presence of an inherent probability distribution, the
running time may differ on different inputs of the same size. In some cases, we
assume that theinputs conform toaknownprobability distribution, sothat weare
averaging therunning timeoverallpossible inputs. Inother cases, theprobability
distribution comes not from the inputs but from random choices made during the
courseofthealgorithm. Analgorithmwhosebehaviorisdeterminednotonlybyits
input but by the values produced by a random-number generator is a randomized
algorithm. Wecanuserandomizedalgorithmstoenforceaprobability distribution
ontheinputs—therebyensuringthatnoparticularinputalwayscausespoorperfor-
mance—or even to bound the error rate of algorithms that are allowed to produce
incorrect resultsonalimitedbasis.
Appendices A–Dcontain othermathematical materialthatyouwillfindhelpful
as you read this book. You are likely to have seen much of the material in the
appendix chapters before having read this book (although the specific definitions
and notational conventions we use may differ in some cases from what you have
seeninthepast),andsoyoushould thinkoftheAppendices asreferencematerial.
On the other hand, you probably have not already seen most of the material in
Part I. All the chapters in Part I and the Appendices are written with a tutorial
flavor.
1 The Role of Algorithms in Computing
Whatarealgorithms? Whyisthestudyofalgorithmsworthwhile? Whatistherole
ofalgorithms relativetoothertechnologies usedincomputers? Inthischapter, we
willanswerthesequestions.
1.1 Algorithms
Informally, an algorithm is any well-defined computational procedure that takes
somevalue,orsetofvalues,asinputandproducessomevalue,orsetofvalues,as
output. Analgorithm isthusasequence ofcomputational steps thattransform the
inputintotheoutput.
We can also view an algorithm as a tool for solving a well-specified computa-
tionalproblem. Thestatementoftheproblemspecifiesingeneraltermsthedesired
input/output relationship. Thealgorithm describes aspecificcomputational proce-
dureforachieving thatinput/output relationship.
For example, we might need to sort a sequence of numbers into nondecreasing
order. This problem arises frequently in practice and provides fertile ground for
introducing many standard design techniques and analysis tools. Here is how we
formallydefinethesortingproblem:
Input: Asequence ofnnumbers a ;a ;:::;a .
1 2 n
h i
Output: A permutation (reordering) a ;a ;:::;a of the input sequence such
h
10 20 n0
i
thata a a .
10

20
  
n0
Forexample, giventheinput sequence 31;41;59;26;41;58 ,asorting algorithm
h i
returns as output the sequence 26;31;41;41;58;59 . Such an input sequence is
h i
called an instance of the sorting problem. In general, an instance of a problem
consists of the input (satisfying whatever constraints are imposed in the problem
statement)neededtocomputeasolutiontotheproblem.
6 Chapter1 TheRoleofAlgorithmsinComputing
Because manyprograms useitasanintermediate step, sorting isafundamental
operationincomputerscience. Asaresult,wehavealargenumberofgoodsorting
algorithmsatourdisposal. Whichalgorithmisbestforagivenapplicationdepends
on—among other factors—the number of items to be sorted, the extent to which
the items are already somewhat sorted, possible restrictions on the item values,
the architecture of the computer, and the kind of storage devices tobe used: main
memory,disks,oreventapes.
An algorithm is said to be correct if, for every input instance, it halts with the
correct output. We say that a correct algorithm solves the given computational
problem. Anincorrectalgorithmmightnothaltatallonsomeinputinstances, orit
mighthaltwithanincorrect answer. Contrary towhatyoumightexpect, incorrect
algorithmscansometimesbeuseful,ifwecancontroltheirerrorrate. Weshallsee
an example of an algorithm with a controllable error rate in Chapter 31 when we
study algorithms for finding large prime numbers. Ordinarily, however, we shall
beconcerned onlywithcorrectalgorithms.
An algorithm can be specified in English, as a computer program, or even as
a hardware design. The only requirement is that the specification must provide a
precisedescription ofthecomputational procedure tobefollowed.
Whatkindsofproblemsaresolvedbyalgorithms?
Sortingisbynomeanstheonlycomputational problemforwhichalgorithmshave
been developed. (You probably suspected as much when you saw the size of this
book.) Practical applications of algorithms are ubiquitous and include the follow-
ingexamples:
 TheHumanGenomeProjecthasmadegreatprogresstowardthegoalsofiden-
tifyingallthe100,000genesinhumanDNA,determining thesequences ofthe
3 billion chemical base pairs that make up human DNA, storing this informa-
tion in databases, and developing tools for data analysis. Each of these steps
requires sophisticated algorithms. Although the solutions to the various prob-
lemsinvolved arebeyond thescope ofthisbook, manymethods tosolve these
biologicalproblemsuseideasfromseveralofthechaptersinthisbook,thereby
enabling scientists to accomplish tasks while using resources efficiently. The
savings areintime,both humanandmachine, andinmoney, asmoreinforma-
tioncanbeextracted fromlaboratory techniques.
 TheInternet enables people allaround theworldtoquickly access andretrieve
large amounts of information. With the aid of clever algorithms, sites on the
Internetareabletomanageandmanipulatethislargevolumeofdata. Examples
of problems that make essential use of algorithms include finding good routes
on which the data will travel (techniques for solving such problems appear in
1.1 Algorithms 7
Chapter24),andusingasearchenginetoquicklyfindpagesonwhichparticular
information resides(related techniques areinChapters11and32).
 Electronic commerce enables goods and services to be negotiated and ex-
changed electronically, and it depends on the privacy of personal informa-
tion such as credit card numbers, passwords, and bank statements. The core
technologiesusedinelectroniccommerceincludepublic-keycryptographyand
digital signatures (covered inChapter 31), whicharebased onnumerical algo-
rithmsandnumbertheory.
 Manufacturing and other commercial enterprises often need to allocate scarce
resourcesinthemostbeneficialway. Anoilcompanymaywishtoknowwhere
toplaceitswellsinordertomaximizeitsexpectedprofit. Apoliticalcandidate
maywant todetermine wheretospend money buying campaign advertising in
order to maximize the chances of winning an election. An airline may wish
to assign crews toflights inthe least expensive waypossible, making sure that
eachflightiscoveredandthatgovernmentregulations regardingcrewschedul-
ingaremet. AnInternetserviceprovidermaywishtodeterminewheretoplace
additional resources in order to serve its customers more effectively. All of
these are examples of problems that can be solved using linear programming,
whichweshallstudyinChapter29.
Although some of the details of these examples are beyond the scope of this
book, wedogiveunderlying techniques thatapply tothese problems andproblem
areas. Wealsoshowhowtosolvemanyspecificproblems,includingthefollowing:
 We are given a road map on which the distance between each pair of adjacent
intersections is marked, and we wish to determine the shortest route from one
intersection toanother. Thenumber ofpossible routes can behuge, evenifwe
disallow routes that cross over themselves. How do we choose which of all
possible routes is the shortest? Here, we model the road map (which is itself
a model of the actual roads) as a graph (which we will meet in Part VI and
Appendix B), and wewish tofindthe shortest path from one vertex toanother
inthegraph. Weshallseehowtosolvethisproblem efficientlyinChapter24.
 We are given two ordered sequences of symbols, X x ;x ;:::;x and
1 2 m
D h i
Y y ;y ;:::;y , and we wish to find a longest common subsequence of
1 2 n
D h i
X andY. Asubsequence ofX isjustX withsome(orpossibly allornone)of
itselementsremoved. Forexample,onesubsequence of A;B;C;D;E;F;G
h i
would be B;C;E;G . The length of a longest common subsequence of X
h i
andY givesonemeasureofhowsimilarthesetwosequencesare. Forexample,
if the two sequences are base pairs in DNA strands, then we might consider
them similar if they have a long common subsequence. If X has m symbols
and Y has n symbols, then X and Y have 2m and 2n possible subsequences,
8 Chapter1 TheRoleofAlgorithmsinComputing
respectively. Selecting all possible subsequences of X and Y and matching
them up could take a prohibitively long time unless m and n are very small.
We shall see in Chapter 15 how to use a general technique known as dynamic
programmingtosolvethisproblem muchmoreefficiently.
 Wearegivenamechanicaldesignintermsofalibraryofparts,whereeachpart
may include instances of other parts, and we need to list the parts in order so
that each part appears before any part that uses it. If the design comprises n
parts,thentherearenŠpossible orders, wherenŠdenotes thefactorialfunction.
Because the factorial function grows faster than even an exponential function,
we cannot feasibly generate each possible order and then verify that, within
that order, each part appears before the parts using it (unless we have only a
fewparts). Thisproblem isaninstance oftopological sorting, andweshallsee
inChapter22howtosolvethisproblem efficiently.
 We are given n points in the plane, and we wish to find the convex hull of
these points. The convex hull is the smallest convex polygon containing the
points. Intuitively, we can think of each point as being represented by a nail
sticking out from a board. The convex hull would be represented by a tight
rubber band that surrounds all the nails. Each nail around which the rubber
bandmakesaturnisavertexoftheconvexhull. (SeeFigure33.6onpage1029
for an example.) Any of the 2n subsets of the points might be the vertices
of the convex hull. Knowing which points are vertices of the convex hull is
not quite enough, either, since we also need to know the order in which they
appear. There are many choices, therefore, for the vertices of the convex hull.
Chapter33givestwogoodmethodsforfindingtheconvexhull.
These lists are far from exhaustive (as you again have probably surmised from
thisbook’sheft),butexhibittwocharacteristics thatarecommontomanyinterest-
ingalgorithmic problems:
1. They have many candidate solutions, the overwhelming majority of which do
notsolvetheproblem athand. Finding onethatdoes, oronethatis“best,” can
presentquiteachallenge.
2. Theyhavepractical applications. Oftheproblems intheabovelist, findingthe
shortest path provides the easiest examples. A transportation firm, such as a
trucking or railroad company, has a financial interest in finding shortest paths
through a road or rail network because taking shorter paths results in lower
labor and fuel costs. Or a routing node on the Internet may need to find the
shortest path through the network in order to route a message quickly. Or a
person wishing to drive from New York to Boston may want to find driving
directionsfromanappropriateWebsite,orshemayuseherGPSwhiledriving.
1.1 Algorithms 9
Noteveryproblemsolvedbyalgorithmshasaneasilyidentifiedsetofcandidate
solutions. Forexample, suppose wearegiven aset ofnumerical values represent-
ingsamples ofasignal, andwewanttocompute thediscrete Fouriertransform of
thesesamples. Thediscrete Fouriertransform converts thetimedomaintothefre-
quencydomain,producingasetofnumericalcoefficients,sothatwecandetermine
the strength of various frequencies in the sampled signal. In addition to lying at
theheartofsignalprocessing,discreteFouriertransformshaveapplicationsindata
compression and multiplying large polynomials and integers. Chapter 30 gives
an efficient algorithm, the fast Fourier transform (commonly called the FFT), for
this problem, and the chapter also sketches out the design of ahardware circuit to
computetheFFT.
Datastructures
Thisbook also contains several data structures. Adata structure isawaytostore
and organize data in order to facilitate access and modifications. No single data
structure works well for all purposes, and so it is important to know the strengths
andlimitationsofseveralofthem.
Technique
Althoughyoucanusethisbookasa“cookbook”foralgorithms,youmaysomeday
encounteraproblemforwhichyoucannotreadilyfindapublishedalgorithm(many
oftheexercisesandproblemsinthisbook,forexample). Thisbookwillteachyou
techniques ofalgorithm designandanalysissothatyoucandevelopalgorithmson
your own, show that they give the correct answer, and understand their efficiency.
Differentchapters addressdifferent aspects ofalgorithmic problem solving. Some
chapters address specific problems, such asfindingmedians and order statistics in
Chapter 9, computing minimum spanning trees in Chapter 23, and determining a
maximum flow in a network in Chapter 26. Other chapters address techniques,
such as divide-and-conquer in Chapter 4, dynamic programming in Chapter 15,
andamortizedanalysis inChapter17.
Hardproblems
Most of this book is about efficient algorithms. Our usual measure of efficiency
is speed, i.e., how long an algorithm takes to produce its result. There are some
problems, however, for which no efficient solution is known. Chapter 34 studies
aninteresting subsetoftheseproblems, whichareknownasNP-complete.
Why are NP-complete problems interesting? First, although no efficient algo-
rithm for an NP-complete problem has ever been found, nobody has ever proven
10 Chapter1 TheRoleofAlgorithmsinComputing
that an efficient algorithm for one cannot exist. In other words, no one knows
whether or not efficient algorithms exist for NP-complete problems. Second, the
setofNP-complete problems hastheremarkable property thatifanefficientalgo-
rithmexistsforanyoneofthem,thenefficientalgorithmsexistforallofthem. This
relationshipamongtheNP-completeproblemsmakesthelackofefficientsolutions
all the more tantalizing. Third, several NP-complete problems are similar, but not
identical, to problems for which we do know of efficient algorithms. Computer
scientists areintrigued byhowasmallchange totheproblem statement cancause
abigchangetotheefficiencyofthebestknownalgorithm.
YoushouldknowaboutNP-completeproblemsbecausesomeofthemarisesur-
prisingly often in real applications. If you are called upon to produce an efficient
algorithm for an NP-complete problem, you are likely to spend a lot of time in a
fruitlesssearch. IfyoucanshowthattheproblemisNP-complete,youcaninstead
spend your time developing an efficient algorithm that gives a good, but not the
bestpossible, solution.
Asaconcrete example, consider adelivery companywithacentral depot. Each
day,itloadsupeachdeliverytruckatthedepotandsendsitaroundtodelivergoods
toseveraladdresses. Attheendoftheday,eachtruckmustendupbackatthedepot
sothatitisreadytobeloadedforthenextday. Toreducecosts,thecompanywants
to select an order of delivery stops that yields the lowest overall distance traveled
byeachtruck. Thisproblem isthewell-known “traveling-salesman problem,”and
itisNP-complete. Ithasnoknownefficientalgorithm. Undercertainassumptions,
however, we know of efficient algorithms that give an overall distance which is
nottoofarabovethesmallestpossible. Chapter35discusses such“approximation
algorithms.”
Parallelism
For many years, we could count on processor clock speeds increasing at a steady
rate. Physicallimitationspresentafundamentalroadblocktoever-increasingclock
speeds, however: because powerdensity increases superlinearly withclockspeed,
chipsruntheriskofmeltingoncetheirclockspeedsbecomehighenough. Inorder
to perform more computations per second, therefore, chips are being designed to
contain not just one but several processing “cores.” We can liken these multicore
computerstoseveralsequentialcomputersonasinglechip;inotherwords,theyare
atypeof“parallelcomputer.” Inordertoelicitthebestperformancefrommulticore
computers, we need to design algorithms with parallelism in mind. Chapter 27
presentsamodelfor“multithreaded”algorithms,whichtakeadvantageofmultiple
cores. This model has advantages from a theoretical standpoint, and it forms the
basis of several successful computer programs, including a championship chess
program.
1.2 Algorithmsasatechnology 11
Exercises
1.1-1
Give a real-world example that requires sorting or a real-world example that re-
quirescomputingaconvexhull.
1.1-2
Otherthanspeed, whatother measures ofefficiency mightoneuseinareal-world
setting?
1.1-3
Selectadata structure that youhave seen previously, and discuss its strengths and
limitations.
1.1-4
How are the shortest-path and traveling-salesman problems given above similar?
Howaretheydifferent?
1.1-5
Comeup withareal-world problem inwhich only the bestsolution willdo. Then
come up with one in which a solution that is “approximately” the best is good
enough.
1.2 Algorithmsas a technology
Suppose computers were infinitely fast and computer memory was free. Would
youhaveanyreasontostudyalgorithms? Theanswerisyes,iffornootherreason
than that you would still like todemonstrate that your solution method terminates
anddoessowiththecorrectanswer.
If computers were infinitely fast, any correct method for solving a problem
woulddo. Youwouldprobablywantyourimplementation tobewithinthebounds
of good software engineering practice (for example, your implementation should
be well designed and documented), but you would most often use whichever
methodwastheeasiesttoimplement.
Ofcourse, computers maybefast, but they are notinfinitely fast. And memory
may be inexpensive, but it is not free. Computing time is therefore a bounded
resource, and so is space in memory. You should use these resources wisely, and
algorithmsthatareefficientintermsoftimeorspacewillhelpyoudoso.
12 Chapter1 TheRoleofAlgorithmsinComputing
Efficiency
Differentalgorithmsdevisedtosolvethesameproblemoftendifferdramaticallyin
their efficiency. These differences can be much more significant than differences
duetohardwareandsoftware.
As an example, in Chapter 2, we will see two algorithms for sorting. The first,
knownasinsertionsort,takestimeroughlyequaltoc n2 tosortnitems,wherec
1 1
isaconstant thatdoesnotdepend onn. Thatis,ittakestimeroughly proportional
to n2. The second, merge sort, takes time roughly equal to c nlgn, where lgn
2
stands for log n and c is another constant that also does not depend on n. Inser-
2 2
tion sort typically has a smaller constant factor than merge sort, so that c < c .
1 2
Weshallseethattheconstant factorscanhavefarlessofanimpactontherunning
time than the dependence on the input size n. Let’s write insertion sort’s running
timeasc n nandmergesort’srunningtimeasc n lgn. Thenweseethatwhere
1 2
 
insertion sort has a factor of n in its running time, merge sort has a factor of lgn,
which is much smaller. (For example, when n 1000, lgn is approximately 10,
D
andwhennequalsonemillion,lgnisapproximately only20.) Althoughinsertion
sort usually runsfaster than merge sortforsmall input sizes, oncethe input sizen
becomes large enough, merge sort’s advantage of lgn vs. n will more than com-
pensate for the difference in constant factors. No matter how much smaller c is
1
thanc ,therewillalwaysbeacrossoverpointbeyondwhichmergesortisfaster.
2
Foraconcreteexample,letuspitafastercomputer(computerA)runninginser-
tion sort against a slower computer (computer B) running merge sort. They each
must sort an array of 10 million numbers. (Although 10 million numbers might
seem like a lot, if the numbers are eight-byte integers, then the input occupies
about80megabytes,whichfitsinthememoryofevenaninexpensive laptopcom-
puter manytimes over.) Suppose that computer Aexecutes 10 billion instructions
per second (faster than any single sequential computer at the time of this writing)
and computer B executes only 10 million instructions per second, so that com-
puter A is 1000 times faster than computer B in raw computing power. To make
the difference even more dramatic, suppose that the world’s craftiest programmer
codes insertion sort in machine language for computer A, and the resulting code
requires 2n2 instructions to sort n numbers. Suppose further that just an average
programmerimplementsmergesort,usingahigh-levellanguagewithaninefficient
compiler, with the resulting code taking 50nlgn instructions. To sort 10 million
numbers, computerAtakes
2 .107/2 instructions
 20,000seconds (morethan5.5hours);
1010 instructions/second D
whilecomputerBtakes
1.2 Algorithmsasatechnology 13
50 107lg107 instructions
 1163seconds(lessthan20minutes):
107 instructions/second 
By using an algorithm whose running time grows more slowly, even with a poor
compiler,computerBrunsmorethan17timesfasterthancomputerA! Theadvan-
tage of merge sort is even more pronounced when we sort 100 million numbers:
where insertion sort takes more than 23 days, merge sort takes under four hours.
In general, as the problem size increases, so does the relative advantage of merge
sort.
Algorithmsandothertechnologies
Theexampleaboveshowsthatweshouldconsideralgorithms,likecomputerhard-
ware, as a technology. Total system performance depends on choosing efficient
algorithmsasmuchasonchoosingfasthardware. Justasrapidadvancesarebeing
madeinothercomputertechnologies, theyarebeingmadeinalgorithmsaswell.
Youmightwonderwhetheralgorithmsaretrulythatimportantoncontemporary
computersinlightofotheradvanced technologies, suchas
 advanced computerarchitectures andfabrication technologies,
 easy-to-use, intuitive, graphical userinterfaces (GUIs),
 object-oriented systems,
 integrated Webtechnologies, and
 fastnetworking, bothwiredandwireless.
The answer is yes. Although some applications do not explicitly require algorith-
miccontentattheapplicationlevel(suchassomesimple,Web-basedapplications),
manydo. Forexample,consideraWeb-basedservicethatdetermineshowtotravel
from one location to another. Its implementation would rely on fast hardware, a
graphical user interface, wide-area networking, and also possibly on object ori-
entation. However, it would also require algorithms for certain operations, such
as finding routes (probably using a shortest-path algorithm), rendering maps, and
interpolating addresses.
Moreover, even an application that does not require algorithmic content at the
application level relies heavily upon algorithms. Does the application rely on fast
hardware? The hardware design used algorithms. Does the application rely on
graphical user interfaces? The design of any GUI relies on algorithms. Does the
application relyonnetworking? Routinginnetworks reliesheavily onalgorithms.
Was the application written in a language other than machine code? Then it was
processedbyacompiler,interpreter, orassembler, allofwhichmakeextensiveuse
14 Chapter1 TheRoleofAlgorithmsinComputing
of algorithms. Algorithms are atthe core ofmosttechnologies used incontempo-
rarycomputers.
Furthermore, with the ever-increasing capacities of computers, we use them to
solve larger problems than ever before. As we saw in the above comparison be-
tweeninsertionsortandmergesort,itisatlargerproblemsizesthatthedifferences
inefficiencybetweenalgorithms becomeparticularly prominent.
Havingasolidbaseofalgorithmicknowledgeandtechniqueisonecharacteristic
that separates the truly skilled programmers from thenovices. With modern com-
puting technology, you can accomplish some tasks without knowing much about
algorithms, but with a good background in algorithms, you can do much, much
more.
Exercises
1.2-1
Giveanexampleofanapplication thatrequiresalgorithmic contentattheapplica-
tionlevel,anddiscussthefunction ofthealgorithms involved.
1.2-2
Supposewearecomparingimplementationsofinsertionsortandmergesortonthe
same machine. For inputs of size n, insertion sort runs in 8n2 steps, while merge
sort runs in 64nlgn steps. For which values of n does insertion sort beat merge
sort?
1.2-3
Whatisthesmallestvalueofnsuchthatanalgorithmwhoserunningtimeis100n2
runsfasterthananalgorithm whoserunning timeis2n onthesamemachine?
Problems
1-1 Comparison ofrunningtimes
For each function f.n/ and time t in the following table, determine the largest
size n of a problem that can be solved in time t, assuming that the algorithm to
solvetheproblem takesf.n/microseconds.
NotesforChapter1 15
1 1 1 1 1 1 1
second minute hour day month year century
lgn
pn
n
nlgn
n2
n3
2n
nŠ
Chapter notes
Thereare manyexcellent texts onthe general topic ofalgorithms, including those
by Aho, Hopcroft, and Ullman [5, 6]; Baase and Van Gelder [28]; Brassard and
Bratley[54];Dasgupta, Papadimitriou, and Vazirani [82]; Goodrich andTamassia
[148]; Hofri [175]; Horowitz, Sahni, and Rajasekaran [181]; Johnsonbaugh and
Schaefer [193]; Kingston [205]; Kleinberg and Tardos [208]; Knuth [209, 210,
211]; Kozen [220]; Levitin [235]; Manber [242]; Mehlhorn [249, 250, 251]; Pur-
dom and Brown [287]; Reingold, Nievergelt, and Deo [293]; Sedgewick [306];
Sedgewick and Flajolet [307]; Skiena [318]; and Wilf [356]. Some of the more
practicalaspectsofalgorithmdesignarediscussedbyBentley[42,43]andGonnet
[145]. SurveysofthefieldofalgorithmscanalsobefoundintheHandbookofThe-
oreticalComputerScience,VolumeA[342]andtheCRCAlgorithmsandTheoryof
Computation Handbook [25]. Overviews of the algorithms used incomputational
biology can be found in textbooks by Gusfield [156], Pevzner [275], Setubal and
Meidanis[310],andWaterman[350].
2 Getting Started
This chapter will familiarize you with the framework we shall use throughout the
book tothink aboutthedesign andanalysis ofalgorithms. Itisself-contained, but
itdoesincludeseveralreferencestomaterialthatweintroduceinChapters3and4.
(Italsocontains severalsummations, whichAppendixAshowshowtosolve.)
Webeginbyexaminingtheinsertionsortalgorithmtosolvethesortingproblem
introducedinChapter1. Wedefinea“pseudocode”thatshouldbefamiliartoyouif
youhavedonecomputerprogramming,andweuseittoshowhowweshallspecify
ouralgorithms. Havingspecifiedtheinsertion sortalgorithm,wethenarguethatit
correctlysorts,andweanalyzeitsrunningtime. Theanalysisintroducesanotation
that focuses on how that time increases with the number of items to be sorted.
Following our discussion of insertion sort, we introduce the divide-and-conquer
approach to the design of algorithms and use it to develop an algorithm called
mergesort. Weendwithananalysis ofmergesort’srunning time.
2.1 Insertionsort
Ourfirstalgorithm, insertion sort, solves thesorting problem introduced inChap-
ter1:
Input: Asequence ofnnumbers a ;a ;:::;a .
1 2 n
h i
Output: A permutation (reordering) a ;a ;:::;a of the input sequence such
h
10 20 n0
i
thata a a .
10

20
  
n0
Thenumbers thatwewishtosortarealsoknownasthekeys. Although conceptu-
allywearesortingasequence,theinputcomestousintheformofanarraywithn
elements.
In this book, we shall typically describe algorithms as programs written in a
pseudocode that issimilarinmanyrespects toC,C++, Java, Python, orPascal. If
youhavebeenintroduced toanyoftheselanguages, youshould havelittletrouble
2.1 Insertionsort 17
§
§7 §
§
§
§
§
§4§ §5§§ § 1§0 §§
§
§
§2 § § §§ 7§
§ § §
§§
§2 §4 §§ §5§§
§01§
Figure2.1 Sortingahandofcardsusinginsertionsort.
reading our algorithms. What separates pseudocode from “real” code is that in
pseudocode, we employ whatever expressive method is most clear and concise to
specify a given algorithm. Sometimes, the clearest method is English, so do not
be surprised if you come across an English phrase or sentence embedded within
a section of “real” code. Another difference between pseudocode and real code
isthat pseudocode is not typically concerned withissues ofsoftware engineering.
Issuesofdataabstraction,modularity,anderrorhandlingareoftenignoredinorder
toconveytheessenceofthealgorithm moreconcisely.
We start with insertion sort, which is an efficient algorithm for sorting a small
number of elements. Insertion sort works the way many people sort a hand of
playing cards. We start with an empty left hand and the cards face down on the
table. We then remove one card at a time from the table and insert it into the
correctpositioninthelefthand. Tofindthecorrectpositionforacard,wecompare
it with each of the cards already in the hand, from right to left, as illustrated in
Figure 2.1. Atalltimes, thecards held inthe left hand are sorted, and these cards
wereoriginally thetopcardsofthepileonthetable.
Wepresentourpseudocode forinsertion sortasaprocedure called INSERTION-
SORT, which takes as a parameter an array AŒ1::n containing a sequence of
lengthnthatistobesorted. (Inthecode,thenumbernofelementsinAisdenoted
by A:length.) The algorithm sorts the input numbers in place: it rearranges the
numberswithinthearrayA,withatmostaconstantnumberofthemstoredoutside
thearrayatanytime. TheinputarrayAcontains thesorted outputsequence when
the INSERTION-SORT procedure isfinished.
18 Chapter2 GettingStarted
1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6
(a) 5 2 4 6 1 3 (b) 2 5 4 6 1 3 (c) 2 4 5 6 1 3
1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6
(d) 2 4 5 6 1 3 (e) 1 2 4 5 6 3 (f) 1 2 3 4 5 6
Figure2.2 Theoperationof INSERTION-SORTonthearrayA 5;2;4;6;1;3 . Arrayindices
D h i
appear above therectangles, and valuesstoredinthearraypositionsappear withintherectangles.
(a)–(e) Theiterationsof theforloopof lines1–8. Ineachiteration, theblackrectangleholdsthe
keytakenfromAŒj,whichiscomparedwiththevaluesinshadedrectanglestoitsleftinthetestof
line5. Shadedarrowsshowarrayvaluesmovedonepositiontotherightinline6,andblackarrows
indicatewherethekeymovestoinline8.(f)Thefinalsortedarray.
INSERTION-SORT.A/
1 forj 2toA:length
D
2 key AŒj
D
3 //InsertAŒjintothesortedsequence AŒ1::j 1.
(cid:0)
4 i j 1
D (cid:0)
5 whilei > 0andAŒi> key
6 AŒi 1 AŒi
C D
7 i i 1
D (cid:0)
8 AŒi 1 key
C D
Loopinvariantsandthecorrectness ofinsertionsort
Figure 2.2 shows how this algorithm works for A 5; 2; 4; 6; 1; 3 . The in-
D h i
dex j indicates the “current card” being inserted into the hand. At the beginning
of each iteration of the for loop, which is indexed by j, the subarray consisting
of elements AŒ1::j 1 constitutes the currently sorted hand, and the remaining
(cid:0)
subarray AŒj 1::n corresponds to the pile of cards still on the table. In fact,
C
elementsAŒ1::j 1aretheelementsoriginallyinpositions1throughj 1,but
(cid:0) (cid:0)
now in sorted order. We state these properties of AŒ1::j 1 formally as a loop
(cid:0)
invariant:
At the start of each iteration of the for loop of lines 1–8, the subarray
AŒ1::j 1consistsoftheelementsoriginallyinAŒ1::j 1,butinsorted
(cid:0) (cid:0)
order.
We use loop invariants to help us understand why an algorithm is correct. We
mustshowthreethingsaboutaloopinvariant:
2.1 Insertionsort 19
Initialization: Itistruepriortothefirstiteration oftheloop.
Maintenance: Ifitistruebeforeaniterationoftheloop,itremainstruebeforethe
nextiteration.
Termination: When the loop terminates, the invariant gives us a useful property
thathelpsshowthatthealgorithm iscorrect.
Whenthefirsttwopropertieshold,theloopinvariantistruepriortoeveryiteration
of the loop. (Of course, we are free to use established facts other than the loop
invariant itself to prove that the loop invariant remains true before each iteration.)
Notethesimilaritytomathematicalinduction,wheretoprovethatapropertyholds,
youproveabasecaseandaninductivestep. Here,showingthattheinvariantholds
beforethefirstiterationcorrespondstothebasecase,andshowingthattheinvariant
holdsfromiteration toiteration corresponds totheinductivestep.
Thethirdpropertyisperhapsthemostimportantone,sinceweareusingtheloop
invariant to show correctness. Typically, we use the loop invariant along with the
condition thatcaused theloop toterminate. Thetermination property differs from
how weusually use mathematical induction, inwhich weapply the inductive step
infinitely;here,westopthe“induction” whentheloopterminates.
Letusseehowtheseproperties holdforinsertion sort.
Initialization: We start by showing that the loop invariant holds before the first
loop iteration, when j 2.1 The subarray AŒ1::j 1, therefore, consists
D (cid:0)
of just the single element AŒ1, which is in fact the original element in AŒ1.
Moreover, this subarray is sorted (trivially, of course), which shows that the
loopinvariant holdspriortothefirstiteration oftheloop.
Maintenance: Next, we tackle the second property: showing that each iteration
maintains the loop invariant. Informally, the body of the for loop works by
moving AŒj 1, AŒj 2, AŒj 3, and so on by one position to the right
(cid:0) (cid:0) (cid:0)
until it finds the proper position for AŒj (lines 4–7), at which point it inserts
thevalue ofAŒj(line 8). Thesubarray AŒ1::jthenconsists oftheelements
originally inAŒ1::j,butinsortedorder. Incrementing j forthenextiteration
oftheforloopthenpreservestheloopinvariant.
A more formal treatment of the second property would require us to state and
show a loop invariant for the while loop of lines 5–7. At this point, however,
1Whentheloopisaforloop,themomentatwhichwechecktheloopinvariantjustpriortothefirst
iterationisimmediatelyaftertheinitialassignmenttotheloop-countervariableandjustbeforethe
firsttestintheloopheader. Inthecaseof INSERTION-SORT,thistimeisafterassigning 2tothe
variablej butbeforethefirsttestofwhetherj A:length.

20 Chapter2 GettingStarted
we prefer not to get bogged down in such formalism, and so we rely on our
informalanalysistoshowthatthesecondproperty holdsfortheouterloop.
Termination: Finally, we examine what happens when the loop terminates. The
condition causing theforlooptoterminateisthatj > A:length n. Because
D
each loop iteration increases j by 1, we must have j n 1 at that time.
D C
Substituting n 1 for j in the wording of loop invariant, we have that the
C
subarray AŒ1::n consists of the elements originally in AŒ1::n, but in sorted
order. ObservingthatthesubarrayAŒ1::nistheentirearray,weconcludethat
theentirearrayissorted. Hence,thealgorithm iscorrect.
We shall use this method of loop invariants to show correctness later in this
chapter andinotherchaptersaswell.
Pseudocodeconventions
Weusethefollowingconventions inourpseudocode.
 Indentationindicatesblockstructure. Forexample,thebodyoftheforloopthat
beginsonline1consistsoflines2–8,andthebodyofthewhileloopthatbegins
on line 5 contains lines 6–7 but not line 8. Our indentation style applies to
if-elsestatements2 aswell. Usingindentationinsteadofconventionalindicators
of block structure, such as begin and end statements, greatly reduces clutter
whilepreserving, orevenenhancing, clarity.3
 Thelooping constructs while,for, and repeat-until andthe if-else conditional
construct have interpretations similar to those in C, C++, Java, Python, and
Pascal.4 In this book, the loop counter retains its value after exiting the loop,
unlike some situations that arise in C++, Java, and Pascal. Thus, immediately
afteraforloop, theloopcounter’s valueisthevalue thatfirstexceeded thefor
loop bound. We used this property in our correctness argument for insertion
sort. The for loop header in line 1 is for j 2 to A:length, and so when
D
this loop terminates, j A:length 1 (or, equivalently, j n 1, since
D C D C
n A:length). We use the keyword to when a for loop increments its loop
D
2Inanif-elsestatement,weindentelseatthesamelevelasitsmatchingif. Althoughweomitthe
keywordthen,weoccasionallyrefertotheportionexecutedwhenthetestfollowingifistrueasa
thenclause.Formultiwaytests,weuseelseiffortestsafterthefirstone.
3Eachpseudocodeprocedureinthisbookappearsononepagesothatyouwillnothavetodiscern
levelsofindentationincodethatissplitacrosspages.
4Most block-structured languages have equivalent constructs, though theexact syntax may differ.
Pythonlacksrepeat-untilloops, anditsforloopsoperatealittledifferentlyfromtheforloopsin
thisbook.
2.1 Insertionsort 21
counter in each iteration, and we use the keyword downto when a for loop
decrements its loop counter. When the loop counter changes by an amount
greaterthan1,theamountofchangefollowstheoptional keywordby.
 Thesymbol“//”indicates thattheremainderofthelineisacomment.
 Amultipleassignmentoftheformi j eassignstobothvariablesi andj
D D
the value of expression e; it should be treated as equivalent to the assignment
j e followedbytheassignment i j.
D D
 Variables (such asi, j,andkey)are local tothegiven procedure. Weshall not
useglobalvariables withoutexplicitindication.
 We access array elements by specifying the array name followed by the in-
dex in square brackets. For example, AŒi indicates the ith element of the
array A. The notation “::” is used to indicate a range of values within an ar-
ray. Thus, AŒ1::j indicates the subarray of A consisting of the j elements
AŒ1;AŒ2;:::;AŒj.
 We typically organize compound data into objects, which are composed of
attributes. We access a particular attribute using the syntax found in many
object-oriented programming languages: the object name, followed by a dot,
followed by the attribute name. For example, we treat an array as an object
with the attribute length indicating how many elements it contains. Tospecify
thenumberofelementsinanarrayA,wewriteA:length.
Wetreat a variable representing an array or object as a pointer to the data rep-
resenting thearrayorobject. Forallattributes f ofanobjectx,setting y x
D
causes y:f to equal x:f. Moreover, ifwenow setx:f 3, then afterward not
D
only does x:f equal 3, but y:f equals 3as well. Inother words, x and y point
tothesameobjectaftertheassignment y x.
D
Ourattributenotationcan“cascade.” Forexample,supposethattheattributef
isitselfapointertosometypeofobjectthathasanattributeg. Thenthenotation
x:f:gisimplicitlyparenthesized as.x:f/:g. Inotherwords,ifwehadassigned
y x:f,thenx:f:gisthesameasy:g.
D
Sometimes, a pointer will refer to no object at all. In this case, we give it the
specialvalue NIL.
 We pass parameters to a procedure by value: the called procedure receives its
owncopyoftheparameters,andifitassignsavaluetoaparameter, thechange
is not seen by the calling procedure. When objects are passed, the pointer to
thedatarepresentingtheobjectiscopied,buttheobject’sattributesarenot. For
example,ifxisaparameterofacalledprocedure,theassignmentx ywithin
D
the called procedure is not visible to the calling procedure. The assignment
x:f 3, however, is visible. Similarly, arrays are passed by pointer, so that
D
22 Chapter2 GettingStarted
a pointer to the array is passed, rather than the entire array, and changes to
individual arrayelementsarevisibletothecallingprocedure.
 A return statement immediately transfers control back to the point of call in
thecallingprocedure. Mostreturnstatementsalsotakeavaluetopassbackto
the caller. Our pseudocode differs from many programming languages in that
weallowmultiplevaluestobereturnedinasinglereturnstatement.
 The boolean operators “and” and “or” are short circuiting. That is, when we
evaluate theexpression “x andy”wefirstevaluate x. Ifx evaluates to FALSE,
thentheentireexpressioncannotevaluatetoTRUE,andsowedonotevaluatey.
If,ontheotherhand,x evaluatestoTRUE,wemustevaluatey todeterminethe
value of the entire expression. Similarly, in the expression “x ory” we eval-
uate the expression y only if x evaluates to FALSE. Short-circuiting operators
allowustowriteboolean expressions suchas“x NIL andx:f y”without
¤ D
worryingaboutwhathappens whenwetrytoevaluatex:f whenx is NIL.
 The keyword error indicates that an error occurred because conditions were
wrong for the procedure to have been called. The calling procedure is respon-
sibleforhandling theerror,andsowedonotspecifywhatactiontotake.
Exercises
2.1-1
Using Figure 2.2 as a model, illustrate the operation of INSERTION-SORT on the
arrayA 31;41;59;26;41;58 .
Dh i
2.1-2
RewritetheINSERTION-SORT proceduretosortintononincreasinginsteadofnon-
decreasing order.
2.1-3
Considerthesearchingproblem:
Input: Asequence ofnnumbersA a ;a ;:::;a andavalue.
1 2 n
D h i
Output: An index i such that  AŒi or the special value NIL if  does not
D
appearinA.
Write pseudocode for linear search, which scans through the sequence, looking
for . Usingaloop invariant, prove that your algorithm iscorrect. Make sure that
yourloopinvariant fulfillsthethreenecessary properties.
2.1-4
Considertheproblem ofaddingtwon-bitbinaryintegers, storedintwon-element
arrays A and B. The sum of the two integers should be stored in binary form in
2.2 Analyzingalgorithms 23
an.n 1/-elementarrayC. Statetheproblem formallyandwritepseudocode for
C
addingthetwointegers.
2.2 Analyzing algorithms
Analyzing an algorithm has come to mean predicting the resources that the algo-
rithm requires. Occasionally, resources such as memory, communication band-
width, or computer hardware are of primary concern, but most often it is compu-
tational time that we want to measure. Generally, by analyzing several candidate
algorithms foraproblem, wecanidentify amostefficientone. Suchanalysis may
indicate more than one viable candidate, but we can often discard several inferior
algorithmsintheprocess.
Before we can analyze an algorithm, we must have a model of the implemen-
tation technology that we will use, including a model for the resources of that
technology and their costs. Formost ofthis book, weshall assume ageneric one-
processor, random-access machine (RAM) model of computation as our imple-
mentation technology and understand that our algorithms will be implemented as
computer programs. In the RAM model, instructions are executed one after an-
other,withnoconcurrent operations.
Strictlyspeaking,weshouldpreciselydefinetheinstructionsoftheRAMmodel
andtheircosts. Todoso, however, would betedious andwould yieldlittle insight
into algorithm design and analysis. Yetwemust becareful not to abuse theRAM
model. Forexample, whatifaRAMhadaninstruction that sorts? Thenwecould
sortinjustoneinstruction. SuchaRAMwouldbeunrealistic,sincerealcomputers
donothave such instructions. Ourguide, therefore, ishowreal computers arede-
signed. TheRAMmodelcontainsinstructionscommonlyfoundinrealcomputers:
arithmetic (such as add, subtract, multiply, divide, remainder, floor, ceiling), data
movement (load, store, copy), and control (conditional and unconditional branch,
subroutine callandreturn). Eachsuchinstruction takesaconstantamountoftime.
ThedatatypesintheRAMmodelareintegerandfloatingpoint(forstoringreal
numbers). Although we typically do not concern ourselves with precision in this
book,insomeapplications precision iscrucial. Wealsoassumealimitonthesize
of each word of data. For example, when working with inputs of size n, we typ-
ically assume that integers are represented by clgn bits for some constant c 1.

Werequirec 1sothateachwordcanholdthevalueofn,enablingustoindexthe

individual input elements, and we restrict c to be a constant so that the word size
does not grow arbitrarily. (If the word size could grow arbitrarily, we could store
huge amounts of data in one word and operate on it all in constant time—clearly
anunrealistic scenario.)
24 Chapter2 GettingStarted
Realcomputers contain instructions notlisted above, and suchinstructions rep-
resent a gray area in the RAM model. For example, is exponentiation a constant-
timeinstruction? Inthegeneralcase,no;ittakesseveralinstructionstocomputexy
whenx andy arerealnumbers. Inrestrictedsituations,however,exponentiationis
a constant-time operation. Many computers have a “shift left” instruction, which
in constant time shifts the bits of an integer by k positions to the left. In most
computers, shifting the bits of an integer by one position to the left is equivalent
to multiplication by 2, so that shifting the bits by k positions to the left is equiv-
alent to multiplication by 2k. Therefore, such computers can compute 2k in one
constant-timeinstructionbyshiftingtheinteger1byk positionstotheleft,aslong
as k is no more than the number of bits in a computer word. Wewill endeavor to
avoid such grayareasintheRAMmodel, butwewilltreatcomputation of2k asa
constant-time operation whenk isasmallenough positiveinteger.
In the RAM model, we do not attempt to model the memory hierarchy that is
common in contemporary computers. That is, we do not model caches or virtual
memory. Several computational models attempt toaccount for memory-hierarchy
effects, which are sometimes significant in real programs on real machines. A
handful of problems in this book examine memory-hierarchy effects, but for the
most part, the analyses in this book will not consider them. Models that include
the memory hierarchy are quite a bit more complex than the RAM model, and so
they can be difficult to work with. Moreover, RAM-model analyses are usually
excellent predictors ofperformance onactualmachines.
Analyzing even asimple algorithm in the RAMmodel can be achallenge. The
mathematical tools required may include combinatorics, probability theory, alge-
braic dexterity, and the ability to identify the most significant terms in a formula.
Because thebehavior ofanalgorithm maybedifferent foreachpossible input, we
needameansforsummarizingthatbehaviorinsimple,easilyunderstoodformulas.
Eventhough wetypically select only onemachine modelto analyze agiven al-
gorithm, we still face many choices in deciding how to express our analysis. We
wouldlikeawaythatissimpletowriteandmanipulate, showstheimportantchar-
acteristics ofanalgorithm’sresourcerequirements, andsuppresses tediousdetails.
Analysisofinsertionsort
ThetimetakenbytheINSERTION-SORT proceduredependsontheinput: sortinga
thousandnumberstakeslongerthansortingthreenumbers. Moreover,INSERTION-
SORT can take different amounts of time to sort two input sequences of the same
size depending on how nearly sorted they already are. In general, the time taken
byanalgorithm growswiththesizeoftheinput, soitistraditional todescribe the
running timeofaprogram asafunction ofthesizeofitsinput. Todoso, weneed
todefinetheterms“running time”and“sizeofinput”morecarefully.
2.2 Analyzingalgorithms 25
Thebest notion forinputsizedepends onthe problem being studied. Formany
problems, such as sorting or computing discrete Fourier transforms, the most nat-
ural measure is the number of items in the input—for example, the array size n
for sorting. For many other problems, such as multiplying two integers, the best
measure of input size is the total number of bits needed to represent the input in
ordinary binary notation. Sometimes, itismoreappropriate todescribe thesizeof
the input with two numbers rather than one. For instance, if the input to an algo-
rithm is a graph, the input size can be described by the numbers of vertices and
edges inthegraph. Weshall indicate whichinput size measure isbeing used with
eachproblem westudy.
Therunningtimeofanalgorithmonaparticularinputisthenumberofprimitive
operations or “steps” executed. It is convenient to define the notion of step so
that it is as machine-independent as possible. For the moment, let us adopt the
following view. Aconstant amount of timeis required toexecute each line ofour
pseudocode. One line may take a different amount of time than another line, but
we shall assume that each execution of the ith line takes time c , where c is a
i i
constant. This viewpoint is in keeping with the RAM model, and it also reflects
howthepseudocode wouldbeimplemented onmostactualcomputers.5
Inthefollowingdiscussion, ourexpression fortherunning timeof INSERTION-
SORT will evolve from a messy formula that uses all the statement costs c
i
to a
much simpler notation that is more concise and more easily manipulated. This
simplernotationwillalsomakeiteasytodeterminewhetheronealgorithmismore
efficientthananother.
We start by presenting the INSERTION-SORT procedure with the time “cost”
of each statement and the number of times each statement is executed. For each
j 2;3;:::;n, where n A:length, we let t denote the number of times the
j
D D
whileloop test inline 5isexecuted for that value ofj. Whenafor orwhileloop
exits in the usual way(i.e., due to the test in the loop header), the test is executed
one time more than the loop body. We assume that comments are not executable
statements, andsotheytakenotime.
5Therearesomesubtletieshere. ComputationalstepsthatwespecifyinEnglishareoftenvariants
of a procedure that requires more than just a constant amount of time. For example, later in this
book we might say “sort the points by x-coordinate,” which, as we shall see, takes more than a
constant amount of time. Also, note that a statement that calls a subroutine takes constant time,
thoughthesubroutine,onceinvoked,maytakemore. Thatis,weseparatetheprocessofcallingthe
subroutine—passingparameterstoit,etc.—fromtheprocessofexecutingthesubroutine.
26 Chapter2 GettingStarted
INSERTION-SORT.A/ cost times
1 forj 2toA:length c n
1
D
2 key AŒj c n 1
2
D (cid:0)
3 //InsertAŒjintothesorted
sequence AŒ1::j 1. 0 n 1
(cid:0) (cid:0)
4 i j 1 c n 1
4
5
whD ilei(cid:0)
> 0andAŒi> key c
(cid:0)n
t
5 j 2 j
6 AŒi 1 AŒi c nD .t 1/
C D 6 Pj 2 j (cid:0)
7 i i 1 c nD .t 1/
D (cid:0) 7 Pj 2 j (cid:0)
8 AŒi 1 key c n D1
8
C D P(cid:0)
The running time of the algorithm is the sum of running times for each state-
mentexecuted; astatement thattakesc stepstoexecuteandexecutesntimeswill
i
contribute c n to the total running time.6 To compute T.n/, the running time of
i
INSERTION-SORT on an input of n values, we sum the products of the cost and
timescolumns,obtaining
n n
T.n/ c n c .n 1/ c .n 1/ c t c .t 1/
1 2 4 5 j 6 j
D C (cid:0) C (cid:0) C C (cid:0)
j 2 j 2
XD XD
n
c .t 1/ c .n 1/:
7 j 8
C (cid:0) C (cid:0)
j 2
XD
Even for inputs of a given size, an algorithm’s running time may depend on
which input of that size is given. For example, in INSERTION-SORT, the best
case occurs if the array is already sorted. For each j 2;3;:::;n, we then find
D
that AŒi key in line 5 when i has its initial value of j 1. Thus t 1 for
j
 (cid:0) D
j 2;3;:::;n,andthebest-caserunning timeis
D
T.n/ c n c .n 1/ c .n 1/ c .n 1/ c .n 1/
1 2 4 5 8
D C (cid:0) C (cid:0) C (cid:0) C (cid:0)
.c c c c c /n .c c c c /:
1 2 4 5 8 2 4 5 8
D C C C C (cid:0) C C C
Wecan express this running time asan b for constants a and b that depend on
C
thestatementcostsc ;itisthusalinearfunctionofn.
i
If the array is in reverse sorted order—that is, in decreasing order—the worst
case results. Wemust compare each element AŒjwitheach element inthe entire
sortedsubarray AŒ1::j 1,andsot j forj 2;3;:::;n. Notingthat
j
(cid:0) D D
6This characteristic does not necessarily hold for a resource such as memory. A statement that
referencesmwordsofmemoryandisexecutedntimesdoesnotnecessarilyreferencemndistinct
wordsofmemory.
2.2 Analyzingalgorithms 27
n
n.n 1/
j C 1
D 2 (cid:0)
j 2
XD
and
n
n.n 1/
.j 1/ (cid:0)
(cid:0) D 2
j 2
XD
(see Appendix A for a review of how to solve these summations), we find that in
theworstcase,therunning timeofINSERTION-SORT is
n.n 1/
T.n/ c n c .n 1/ c .n 1/ c C 1
1 2 4 5
D C (cid:0) C (cid:0) C 2 (cid:0)
 
n.n 1/ n.n 1/
c (cid:0) c (cid:0) c .n 1/
6 7 8
C 2 C 2 C (cid:0)
   
c c c c c c
5 6 7 n2 c c c 5 6 7 c n
1 2 4 8
D 2 C 2 C 2 C C C C 2 (cid:0) 2 (cid:0) 2 C
 .c c c c /: 
2 4 5 8
(cid:0) C C C
Wecan express this worst-case running time as an2 bn c for constants a, b,
C C
and c that again depend on the statement costs c ; it is thus a quadratic function
i
ofn.
Typically, as in insertion sort, the running time of an algorithm is fixed for a
giveninput,although inlaterchaptersweshallseesomeinteresting “randomized”
algorithmswhosebehavior canvaryevenforafixedinput.
Worst-case andaverage-case analysis
Inouranalysisofinsertionsort,welookedatboththebestcase,inwhichtheinput
array was already sorted, and the worst case, in which the input array was reverse
sorted. For the remainder of this book, though, we shall usually concentrate on
findingonlytheworst-caserunningtime,thatis,thelongest running timeforany
inputofsizen. Wegivethreereasons forthisorientation.
 The worst-case running time of an algorithm gives us an upper bound on the
running timeforanyinput. Knowingitprovides aguarantee thatthealgorithm
will never take any longer. We need not make some educated guess about the
running timeandhopethatitnevergetsmuchworse.
 Forsomealgorithms,theworstcaseoccursfairlyoften. Forexample,insearch-
ing a database for a particular piece of information, the searching algorithm’s
worstcasewilloftenoccurwhentheinformationisnotpresentinthedatabase.
Insomeapplications, searches forabsentinformation maybefrequent.
28 Chapter2 GettingStarted
 The“average case” isoften roughly asbad asthe worstcase. Suppose that we
randomly choose nnumbers andapply insertion sort. Howlongdoesittaketo
determine whereinsubarray AŒ1::j 1toinsertelementAŒj? Onaverage,
(cid:0)
half the elements in AŒ1::j 1 are less than AŒj, and half the elements are
(cid:0)
greater. Onaverage,therefore, wecheckhalfofthesubarray AŒ1::j 1,and
(cid:0)
so t is about j=2. The resulting average-case running time turns out to be a
j
quadratic function oftheinputsize,justliketheworst-case runningtime.
Insomeparticularcases,weshallbeinterestedintheaverage-caserunningtime
of an algorithm; we shall see the technique of probabilistic analysis applied to
various algorithms throughout this book. The scope of average-case analysis is
limited, because it may not be apparent what constitutes an “average” input for
a particular problem. Often, we shall assume that all inputs of a given size are
equallylikely. Inpractice, thisassumptionmaybeviolated, butwecansometimes
usearandomizedalgorithm,whichmakesrandomchoices,toallowaprobabilistic
analysis and yield an expected running time. We explore randomized algorithms
moreinChapter5andinseveralothersubsequent chapters.
Orderofgrowth
We used some simplifying abstractions to ease our analysis of the INSERTION-
SORT procedure. First, we ignored the actual cost of each statement, using the
constants c to represent these costs. Then, weobserved that even these constants
i
giveusmoredetailthanwereallyneed: weexpressedtheworst-caserunningtime
as an2 bn c for some constants a, b, and c that depend on the statement
C C
costs c . Wethus ignored notonly the actual statement costs, but also theabstract
i
costsc .
i
We shall now make one more simplifying abstraction: it is the rate of growth,
or order of growth, of the running time that really interests us. Wetherefore con-
sideronlytheleadingtermofaformula(e.g.,an2),sincethelower-ordertermsare
relativelyinsignificantforlargevaluesofn. Wealsoignoretheleadingterm’scon-
stant coefficient, since constant factors are less significant than the rate of growth
in determining computational efficiency for large inputs. For insertion sort, when
weignorethelower-ordertermsandtheleadingterm’sconstantcoefficient,weare
left with the factor of n2 from the leading term. Wewrite that insertion sort has a
worst-caserunningtimeof‚.n2/(pronounced“thetaofn-squared”). Weshalluse
‚-notation informally inthischapter, andwewilldefineitprecisely inChapter3.
Weusuallyconsideronealgorithmtobemoreefficientthananotherifitsworst-
caserunning timehasalowerorder ofgrowth. Duetoconstant factors andlower-
order terms, an algorithm whose running timehas ahigher order ofgrowth might
take less time for small inputs than an algorithm whose running time has a lower
2.3 Designingalgorithms 29
orderofgrowth. Butforlargeenoughinputs,a‚.n2/algorithm,forexample,will
runmorequickly intheworstcasethana‚.n3/algorithm.
Exercises
2.2-1
Expressthefunction n3=1000 100n2 100n 3intermsof‚-notation.
(cid:0) (cid:0) C
2.2-2
Consider sorting nnumbers stored inarray Abyfirstfinding thesmallest element
of A and exchanging it with the element in AŒ1. Then find the second smallest
elementofA,andexchangeitwithAŒ2. Continueinthismannerforthefirstn 1
(cid:0)
elements of A. Write pseudocode for this algorithm, which is known as selection
sort. What loop invariant does this algorithm maintain? Why does it need to run
foronlythefirstn 1elements,ratherthanforallnelements? Givethebest-case
(cid:0)
andworst-caserunning timesofselection sortin‚-notation.
2.2-3
Consider linear search again (see Exercise 2.1-3). How many elements of the in-
put sequence need to be checked on the average, assuming that the element being
searched for is equally likely to be any element in the array? How about in the
worst case? What are the average-case and worst-case running times of linear
searchin‚-notation? Justifyyouranswers.
2.2-4
Howcanwemodifyalmostanyalgorithm tohaveagoodbest-case running time?
2.3 Designing algorithms
We can choose from a wide range of algorithm design techniques. For insertion
sort, we used an incremental approach: having sorted the subarray AŒ1::j 1,
(cid:0)
we inserted the single element AŒj into its proper place, yielding the sorted
subarrayAŒ1::j.
In this section, we examine an alternative design approach, known as “divide-
and-conquer,”whichweshallexploreinmoredetailinChapter4. We’llusedivide-
and-conquer todesignasorting algorithm whoseworst-caserunning timeismuch
lessthanthatofinsertion sort. Oneadvantage ofdivide-and-conquer algorithms is
that their running times are often easily determined using techniques that we will
seeinChapter4.
30 Chapter2 GettingStarted
2.3.1 Thedivide-and-conquerapproach
Many useful algorithms are recursive in structure: to solve a given problem, they
call themselves recursively one or more times to deal with closely related sub-
problems. Thesealgorithmstypicallyfollowadivide-and-conquerapproach: they
break the problem into several subproblems that are similar to the original prob-
lembutsmallerinsize,solvethesubproblems recursively, andthencombinethese
solutions tocreateasolutiontotheoriginalproblem.
Thedivide-and-conquer paradigminvolvesthreestepsateachleveloftherecur-
sion:
Dividetheproblemintoanumberofsubproblemsthataresmallerinstancesofthe
sameproblem.
Conquerthesubproblemsbysolvingthemrecursively. Ifthesubproblemsizesare
smallenough,however,justsolvethesubproblemsinastraightforwardmanner.
Combinethe solutions to the subproblems into the solution for the original prob-
lem.
Themergesortalgorithm closelyfollowsthedivide-and-conquer paradigm. In-
tuitively, itoperatesasfollows.
Divide: Dividethen-elementsequencetobesortedintotwosubsequences ofn=2
elementseach.
Conquer: Sortthetwosubsequences recursively usingmergesort.
Combine: Mergethetwosortedsubsequences toproducethesortedanswer.
Therecursion“bottomsout”whenthesequencetobesortedhaslength1,inwhich
case there is no work to be done, since every sequence of length 1 is already in
sortedorder.
The key operation of the merge sort algorithm is the merging of two sorted
sequences in the “combine” step. We merge by calling an auxiliary procedure
MERGE.A;p;q;r/, where Ais anarray and p,q, and r are indices into thearray
such that p q < r. The procedure assumes that the subarrays AŒp::q and

AŒq 1::r are in sorted order. It merges them to form a single sorted subarray
C
thatreplaces thecurrentsubarray AŒp::r.
Our MERGE procedure takes time ‚.n/, where n r p 1 is the total
D (cid:0) C
numberofelementsbeingmerged, anditworksasfollows. Returning toourcard-
playing motif, suppose wehave twopiles ofcards face upon atable. Eachpile is
sorted,withthesmallestcardsontop. Wewishtomergethetwopilesintoasingle
sorted output pile, which is to be face down on the table. Our basic step consists
of choosing the smaller of the two cards on top of the face-up piles, removing it
fromitspile(whichexposesanewtopcard),andplacing thiscardfacedownonto
2.3 Designingalgorithms 31
the output pile. We repeat this step until one input pile is empty, at which time
we just take the remaining input pile and place it face down onto the output pile.
Computationally, eachbasic steptakes constant time, since wearecomparing just
the two top cards. Since we perform at most n basic steps, merging takes ‚.n/
time.
The following pseudocode implements the above idea, but with an additional
twist that avoids having to check whether either pile is empty in each basic step.
Weplaceonthebottomofeachpileasentinelcard,whichcontainsaspecialvalue
that we use to simplify our code. Here, we use as the sentinel value, so that
1
wheneveracardwith isexposed,itcannotbethesmallercardunlessbothpiles
1
havetheirsentinel cardsexposed. Butoncethathappens, allthenonsentinel cards
have already been placed onto the output pile. Since we know in advance that
exactly r p 1 cards will be placed onto the output pile, we can stop once we
(cid:0) C
haveperformedthatmanybasicsteps.
MERGE.A;p;q;r/
1 n q p 1
1
D (cid:0) C
2 n r q
2
D (cid:0)
3 letLŒ1::n 1andRŒ1::n 1benewarrays
1 2
C C
4 fori 1ton
1
D
5 LŒi AŒp i 1
D C (cid:0)
6 forj 1ton
2
D
7 RŒj AŒq j
D C
8 LŒn 1
1
C D 1
9 RŒn 1
2
C D 1
10 i 1
D
11 j 1
D
12 fork p tor
D
13 ifLŒi RŒj

14 AŒk LŒi
D
15 i i 1
D C
16 elseAŒk RŒj
D
17 j j 1
D C
Indetail,theMERGEprocedureworksasfollows. Line1computesthelengthn
1
of the subarray AŒp::q, and line 2 computes the length n of the subarray
2
AŒq 1::r. We create arrays L and R (“left” and “right”), of lengths n 1
1
C C
and n 1, respectively, in line 3; the extra position in each array will hold the
2
C
sentinel. The for loop of lines 4–5 copies the subarray AŒp::q into LŒ1::n ,
1
and the for loop of lines 6–7 copies the subarray AŒq 1::r into RŒ1::n .
2
C
Lines8–9 put the sentinels at the ends of the arrays L and R. Lines 10–17, illus-
32 Chapter2 GettingStarted
8 9 10 11 12 13 14 15 16 17 8 9 10 11 12 13 14 15 16 17
A … 2 4 5 7 1 2 3 6 … A … 1 4 5 7 1 2 3 6 …
k k
1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
L 2 4 5 7 ¥ R 1 2 3 6 ¥ L 2 4 5 7 ¥ R 1 2 3 6 ¥
i j i j
(a) (b)
8 9 10 11 12 13 14 15 16 17 8 9 10 11 12 13 14 15 16 17
A … 1 2 5 7 1 2 3 6 … A … 1 2 2 7 1 2 3 6 …
k k
1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
L 2 4 5 7 ¥ R 1 2 3 6 ¥ L 2 4 5 7 ¥ R 1 2 3 6 ¥
i j i j
(c) (d)
Figure 2.3 The operation of lines 10–17 in the call MERGE.A;9;12;16/, when the subarray
AŒ9::16 contains the sequence 2;4;5;7;1;2;3;6 . After copying and inserting sentinels, the
h i
arrayLcontains 2;4;5;7; ,andthearrayRcontains 1;2;3;6; . Lightlyshadedpositions
h 1i h 1i
inAcontaintheirfinalvalues,andlightlyshadedpositionsinLandRcontainvaluesthathaveyet
tobecopiedbackintoA. Takentogether, thelightlyshadedpositionsalwayscomprisethevalues
originallyinAŒ9::16,alongwiththetwosentinels. HeavilyshadedpositionsinAcontainvalues
thatwillbecopiedover,andheavilyshadedpositionsinLandRcontainvaluesthathavealready
beencopiedbackintoA. (a)–(h)ThearraysA,L,andR,andtheirrespectiveindicesk,i,andj
priortoeachiterationoftheloopoflines12–17.
tratedinFigure2.3,performther p 1basicstepsbymaintainingthefollowing
(cid:0) C
loopinvariant:
At the start of each iteration of the for loop of lines 12–17, the subarray
AŒp::k 1 contains the k p smallest elements of LŒ1::n 1 and
1
(cid:0) (cid:0) C
RŒ1::n 1, in sorted order. Moreover, LŒi and RŒj are the smallest
2
C
elementsoftheirarraysthathavenotbeencopiedbackintoA.
Wemust show thatthis loop invariant holds prior tothe firstiteration ofthefor
loop of lines 12–17, that each iteration of the loop maintains the invariant, and
that the invariant provides a useful property to show correctness when the loop
terminates.
Initialization: Prior to the first iteration of the loop, we have k p, so that the
D
subarray AŒp::k 1 is empty. This empty subarray contains the k p 0
(cid:0) (cid:0) D
smallestelementsofLandR,andsincei j 1,bothLŒiandRŒjarethe
D D
smallestelementsoftheirarraysthathavenotbeencopiedbackintoA.
2.3 Designingalgorithms 33
8 9 10 11 12 13 14 15 16 17 8 9 10 11 12 13 14 15 16 17
A … 1 2 2 3 1 2 3 6 … A … 1 2 2 3 4 2 3 6 …
k k
1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
L 2 4 5 7 ¥ R 1 2 3 6 ¥ L 2 4 5 7 ¥ R 1 2 3 6 ¥
i j i j
(e) (f)
8 9 10 11 12 13 14 15 16 17 8 9 10 11 12 13 14 15 16 17
A … 1 2 2 3 4 5 3 6 … A … 1 2 2 3 4 5 6 6 …
k k
1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
L 2 4 5 7 ¥ R 1 2 3 6 ¥ L 2 4 5 7 ¥ R 1 2 3 6 ¥
i j i j
(g) (h)
8 9 10 11 12 13 14 15 16 17
A … 1 2 2 3 4 5 6 7 …
k
1 2 3 4 5 1 2 3 4 5
L 2 4 5 7 ¥ R 1 2 3 6 ¥
i j
(i)
Figure 2.3, continued (i) The arrays and indices at termination. At this point, the subarray in
AŒ9::16issorted,andthetwosentinelsinLandRaretheonlytwoelementsinthesearraysthat
havenotbeencopiedintoA.
Maintenance: To see that each iteration maintains the loop invariant, let us first
suppose that LŒi RŒj. Then LŒi is the smallest element not yet copied

backintoA. BecauseAŒp::k 1contains thek p smallestelements, after
(cid:0) (cid:0)
line14copiesLŒiintoAŒk,thesubarrayAŒp::kwillcontainthek p 1
(cid:0) C
smallest elements. Incrementing k (in the for loop update) and i (in line 15)
reestablishes the loop invariant for the next iteration. If instead LŒi > RŒj,
thenlines16–17perform theappropriate actiontomaintaintheloopinvariant.
Termination: At termination, k r 1. By the loop invariant, the subarray
D C
AŒp::k 1, which is AŒp::r, contains the k p r p 1 smallest
(cid:0) (cid:0) D (cid:0) C
elements of LŒ1::n 1 and RŒ1::n 1, in sorted order. The arrays L
1 2
C C
and R together contain n n 2 r p 3 elements. All but the two
1 2
C C D (cid:0) C
largest have been copied back into A, and these two largest elements are the
sentinels.
34 Chapter2 GettingStarted
To see that the MERGE procedure runs in ‚.n/ time, where n r p 1,
D (cid:0) C
observe that each of lines 1–3 and 8–11 takes constant time, the for loops of
lines 4–7 take ‚.n n / ‚.n/ time,7 and there are n iterations of the for
1 2
C D
loopoflines12–17,eachofwhichtakesconstanttime.
We can now use the MERGE procedure as a subroutine in the merge sort al-
gorithm. The procedure MERGE-SORT.A;p;r/ sorts the elements in the subar-
ray AŒp::r. If p r, the subarray has at most one element and is therefore

already sorted. Otherwise, the divide step simply computes an index q that par-
titions AŒp::r into two subarrays: AŒp::q, containing n=2 elements, and
d e
AŒq 1::r,containing n=2 elements.8
C b c
MERGE-SORT.A;p;r/
1 ifp <r
2 q .p r/=2
D b C c
3 MERGE-SORT.A;p;q/
4 MERGE-SORT.A;q 1;r/
C
5 MERGE.A;p;q;r/
To sort the entire sequence A AŒ1;AŒ2;:::;AŒn , we make the initial call
D h i
MERGE-SORT.A;1;A:length/, where once again A:length n. Figure 2.4 il-
D
lustrates the operation of the procedure bottom-up when n is a power of 2. The
algorithm consists ofmerging pairs of 1-item sequences to form sorted sequences
of length 2, merging pairs of sequences of length 2 to form sorted sequences of
length4,andsoon,untiltwosequencesoflengthn=2aremergedtoformthefinal
sortedsequence oflengthn.
2.3.2 Analyzingdivide-and-conqueralgorithms
When an algorithm contains a recursive call to itself, we can often describe its
running timebyarecurrenceequationorrecurrence,whichdescribes theoverall
runningtimeonaproblemofsizenintermsoftherunningtimeonsmallerinputs.
Wecanthenusemathematicaltoolstosolvetherecurrenceandprovideboundson
theperformance ofthealgorithm.
7WeshallseeinChapter3howtoformallyinterpretequationscontaining‚-notation.
8Theexpression x denotestheleastintegergreaterthanorequaltox,and x denotesthegreatest
d e b c
integerlessthanorequaltox. ThesenotationsaredefinedinChapter3. Theeasiestwaytoverify
thatsettingqto .p r/=2 yieldssubarraysAŒp::qandAŒq 1::rofsizes n=2 and n=2 ,
b C c C d e b c
respectively,istoexaminethefourcasesthatarisedependingonwhethereachofpandr isoddor
even.
2.3 Designingalgorithms 35
sorted sequence
1 2 2 3 4 5 6 7
merge
2 4 5 7 1 2 3 6
merge merge
2 5 4 7 1 3 2 6
merge merge merge merge
5 2 4 7 1 3 2 6
initial sequence
Figure2.4 TheoperationofmergesortonthearrayA 5;2;4;7;1;3;2;6 . Thelengthsofthe
Dh i
sortedsequencesbeingmergedincreaseasthealgorithmprogressesfrombottomtotop.
A recurrence for the running time of a divide-and-conquer algorithm falls out
from the three steps ofthe basic paradigm. Asbefore, welet T.n/be the running
time on a problem of size n. If the problem size is small enough, say n c

for some constant c, the straightforward solution takes constant time, which we
write as ‚.1/. Suppose that our division of the problem yields a subproblems,
each of which is 1=b the size of the original. (For merge sort, both a and b are 2,
but we shall see many divide-and-conquer algorithms in which a b.) It takes
¤
time T.n=b/ to solve one subproblem of size n=b, and so it takes time aT.n=b/
to solve a of them. If we take D.n/ time to divide the problem into subproblems
andC.n/timetocombinethesolutions tothesubproblems intothesolutiontothe
originalproblem, wegettherecurrence
‚.1/ ifn c ;
T.n/ 
D
(
aT.n=b/ D.n/ C.n/ otherwise:
C C
InChapter4,weshallseehowtosolvecommonrecurrences ofthisform.
Analysisofmergesort
Although the pseudocode for MERGE-SORT works correctly when the number of
elementsisnoteven,ourrecurrence-based analysis issimplifiedifweassumethat
36 Chapter2 GettingStarted
the original problem size is apower of 2. Each divide step then yields two subse-
quences of size exactly n=2. In Chapter 4, we shall see that this assumption does
notaffecttheorderofgrowthofthesolutiontotherecurrence.
We reason as follows to set up the recurrence for T.n/, the worst-case running
time of merge sort on n numbers. Merge sort on just one element takes constant
time. Whenwehaven > 1elements,webreakdowntherunning timeasfollows.
Divide: The divide step just computes the middle of the subarray, which takes
constant time. Thus,D.n/ ‚.1/.
D
Conquer: We recursively solve two subproblems, each of size n=2, which con-
tributes2T.n=2/totherunningtime.
Combine: We have already noted that the MERGE procedure on an n-element
subarray takestime‚.n/,andsoC.n/ ‚.n/.
D
When we add the functions D.n/ and C.n/ for the merge sort analysis, we are
adding a function that is ‚.n/ and a function that is ‚.1/. This sum is a linear
function of n, that is, ‚.n/. Adding it to the 2T.n=2/ term from the “conquer”
stepgivestherecurrence fortheworst-case runningtimeT.n/ofmergesort:
‚.1/ ifn 1;
T.n/ D (2.1)
D
(
2T.n=2/ ‚.n/ ifn > 1:
C
In Chapter 4, we shall see the “master theorem,” which we can use to show
thatT.n/ is ‚.nlgn/, where lgn stands for log n. Because the logarithm func-
2
tion grows more slowly than any linear function, for large enough inputs, merge
sort, with its ‚.nlgn/ running time, outperforms insertion sort, whose running
timeis‚.n2/,intheworstcase.
Wedonotneedthemastertheoremtointuitivelyunderstandwhythesolutionto
therecurrence (2.1)isT.n/ ‚.nlgn/. Letusrewriterecurrence (2.1)as
D
c ifn 1;
T.n/ D (2.2)
D
(
2T.n=2/ cn ifn >1;
C
where the constant c represents the time required to solve problems of size 1 as
wellasthetimeperarrayelementofthedivideandcombinesteps.9
9It isunlikely thatthe sameconstant exactlyrepresents both thetimetosolve problems of size1
andthetimeperarrayelementofthedivideandcombinesteps. Wecangetaroundthisproblemby
lettingcbethelargerofthesetimesandunderstandingthatourrecurrencegivesanupperboundon
therunningtime,orbylettingc bethelesserofthesetimesandunderstandingthatourrecurrence
givesalowerboundontherunningtime.Bothboundsareontheorderofnlgnand,takentogether,
givea‚.nlgn/runningtime.
2.3 Designingalgorithms 37
Figure 2.5 shows how we can solve recurrence (2.2). For convenience, we as-
sume that n is an exact power of 2. Part (a) of the figure shows T.n/, which we
expandinpart(b)intoanequivalent treerepresenting therecurrence. Thecnterm
isthe root (the cost incurred atthe top levelof recursion), and the twosubtrees of
therootarethetwosmallerrecurrencesT.n=2/. Part(c)showsthisprocesscarried
one step further by expanding T.n=2/. The cost incurred at each of the two sub-
nodes atthe second level of recursion is cn=2. Wecontinue expanding each node
inthetreebybreakingitintoitsconstituent partsasdeterminedbytherecurrence,
until the problem sizes get down to 1, each with a cost of c. Part (d) shows the
resulting recursiontree.
Next, we add the costs across each level of the tree. The top level has total
cost cn, the next level down has total cost c.n=2/ c.n=2/ cn, the level after
C D
thathastotalcostc.n=4/ c.n=4/ c.n=4/ c.n=4/ cn,andsoon. Ingeneral,
C C C D
thelevel i below the top has2i nodes, each contributing acost ofc.n=2i/, sothat
theithlevelbelow thetophas total cost2i c.n=2i/ cn. Thebottom levelhas n
D
nodes,eachcontributing acostofc,foratotalcostofcn.
Thetotal number oflevels ofthe recursion tree inFigure 2.5is lgn 1, where
C
n is the number of leaves, corresponding to the input size. An informal inductive
argument justifiesthisclaim. Thebasecaseoccurswhenn 1,inwhichcasethe
D
tree has only one level. Since lg1 0, we have that lgn 1 gives the correct
D C
numberoflevels. Nowassumeasaninductivehypothesisthatthenumberoflevels
of a recursion tree with 2i leaves is lg2i 1 i 1 (since for any value of i,
C D C
we have that lg2i i). Because we are assuming that the input size is a power
D
of 2, the next input size to consider is 2i 1. A tree with n 2i 1 leaves has
C C
D
one more level than a tree with 2i leaves, and so the total number of levels is
.i 1/ 1 lg2i 1 1.
C
C C D C
Tocomputethetotalcostrepresented bytherecurrence (2.2),wesimplyaddup
the costs of all the levels. The recursion tree has lgn 1 levels, each costing cn,
C
for a total cost of cn.lgn 1/ cnlgn cn. Ignoring the low-order term and
C D C
theconstantc givesthedesired resultof‚.nlgn/.
Exercises
2.3-1
Using Figure 2.4 as a model, illustrate the operation of merge sort on the array
A 3;41;52;26;38;57;9;49 .
Dh i
2.3-2
Rewrite the MERGE procedure so that it does not use sentinels, instead stopping
onceeitherarrayLorRhashadallitselementscopiedbacktoAandthencopying
theremainderoftheotherarraybackintoA.
38 Chapter2 GettingStarted
cn
cn
lg n
cn
…
c c c c c c c cn
n
Total: cn lg n + cn
…
T(n) cn cn
T(n/2) T(n/2) cn/2 cn/2
T(n/4) T(n/4) T(n/4) T(n/4)
(a) (b) (c)
cn
cn/2 cn/2
cn/4 cn/4 cn/4 cn/4
(d)
Figure 2.5 How to construct a recursion tree for the recurrence T.n/ 2T.n=2/ cn.
D C
Part(a)showsT.n/,whichprogressivelyexpandsin(b)–(d)toformtherecursiontree. Thefully
expanded tree in part (d) has lgn 1 levels (i.e., it has height lgn, as indicated), and each level
C
contributesatotalcostofcn.Thetotalcost,therefore,iscnlgn cn,whichis‚.nlgn/.
C
ProblemsforChapter2 39
2.3-3
Usemathematical induction to show that when nis anexact power of 2, the solu-
tionoftherecurrence
2 ifn 2;
T.n/ D
D
(
2T.n=2/ n ifn 2k,fork > 1
C D
isT.n/ nlgn.
D
2.3-4
Wecan express insertion sort as a recursive procedure as follows. In order to sort
AŒ1::n,werecursivelysortAŒ1::n 1andtheninsertAŒnintothesortedarray
(cid:0)
AŒ1::n 1. Write arecurrence for the worst-case running time of this recursive
(cid:0)
versionofinsertion sort.
2.3-5
Referring back to the searching problem (see Exercise 2.1-3), observe that if the
sequence A is sorted, we can check the midpoint of the sequence against  and
eliminate half of the sequence from further consideration. The binary search al-
gorithm repeats this procedure, halving the size of the remaining portion of the
sequence each time. Write pseudocode, either iterative or recursive, for binary
search. Arguethattheworst-case runningtimeofbinarysearchis‚.lgn/.
2.3-6
Observe that the while loop of lines 5–7 of the INSERTION-SORT procedure in
Section 2.1 uses a linear search to scan (backward) through the sorted subarray
AŒ1::j 1. Can weuse a binary search (see Exercise 2.3-5) instead to improve
(cid:0)
theoverallworst-case runningtimeofinsertion sortto‚.nlgn/?
2.3-7 ?
Describe a ‚.nlgn/-time algorithm that, given a set S of n integers and another
integer x, determines whether or not there exist two elements in S whose sum is
exactlyx.
Problems
2-1 Insertionsortonsmallarraysinmergesort
Although merge sort runs in ‚.nlgn/ worst-case time and insertion sort runs
in ‚.n2/ worst-case time, the constant factors in insertion sort can make it faster
in practice for small problem sizes on many machines. Thus, it makes sense to
coarsen theleavesoftherecursion byusing insertion sortwithinmergesortwhen
40 Chapter2 GettingStarted
subproblems become sufficiently small. Consider a modification to merge sort in
which n=k sublists of length k are sorted using insertion sort and then merged
usingthestandard mergingmechanism,wherek isavaluetobedetermined.
a. Show that insertion sort can sort the n=k sublists, each of length k, in ‚.nk/
worst-casetime.
b. Showhowtomergethesublistsin‚.nlg.n=k//worst-case time.
c. Giventhatthemodifiedalgorithmrunsin‚.nk nlg.n=k//worst-casetime,
C
whatisthelargestvalueofkasafunctionofnforwhichthemodifiedalgorithm
hasthesamerunning timeasstandard mergesort,intermsof‚-notation?
d. Howshouldwechoosek inpractice?
2-2 Correctness ofbubblesort
Bubblesort is a popular, but inefficient, sorting algorithm. It works by repeatedly
swapping adjacentelementsthatareoutoforder.
BUBBLESORT.A/
1 fori 1toA:length 1
D (cid:0)
2 forj A:lengthdowntoi 1
D C
3 ifAŒj< AŒj 1
(cid:0)
4 exchange AŒjwithAŒj 1
(cid:0)
a. Let A
0
denote the output of BUBBLESORT.A/. Toprove that BUBBLESORT is
correct, weneedtoprovethatitterminatesandthat
AŒ1 AŒ2 AŒn; (2.3)
0 0 0
   
where n A:length. In order to show that BUBBLESORT actually sorts, what
D
elsedoweneedtoprove?
Thenexttwopartswillproveinequality (2.3).
b. Stateprecisely aloopinvariant fortheforloopinlines2–4,andprovethatthis
loop invariant holds. Your proof should use the structure of the loop invariant
proofpresented inthischapter.
c. Using the termination condition of the loop invariant proved in part (b), state
a loop invariant for the for loop in lines 1–4 that will allow you to prove in-
equality (2.3). Your proof should use the structure of the loop invariant proof
presented inthischapter.
ProblemsforChapter2 41
d. Whatistheworst-caserunningtimeofbubblesort? Howdoesitcomparetothe
running timeofinsertion sort?
2-3 CorrectnessofHorner’srule
ThefollowingcodefragmentimplementsHorner’sruleforevaluatingapolynomial
n
P.x/ a xk
k
D
k 0
XD
a x.a x.a x.a xa / //;
0 1 2 n 1 n
D C C CC (cid:0) C 
giventhecoefficientsa ;a ;:::;a andavalueforx:
0 1 n
1 y 0
D
2 fori ndownto0
D
3 y a x y
i
D C 
a. In terms of ‚-notation, what is the running time of this code fragment for
Horner’srule?
b. Writepseudocodetoimplementthenaivepolynomial-evaluation algorithmthat
computes each term of the polynomial from scratch. What is the running time
ofthisalgorithm? HowdoesitcomparetoHorner’srule?
c. Considerthefollowingloopinvariant:
Atthestartofeachiteration oftheforloopoflines2–3,
n .i 1/
(cid:0) C
y a xk :
k i 1
D C C
k 0
XD
Interpret a summation with no terms as equaling 0. Following the structure of
theloopinvariantproofpresentedinthischapter,usethisloopinvarianttoshow
that,attermination, y n a xk.
D k 0 k
D
P
d. Conclude by arguing that the given code fragment correctly evaluates a poly-
nomialcharacterized bythecoefficients a ;a ;:::;a .
0 1 n
2-4 Inversions
LetAŒ1::nbeanarrayofndistinctnumbers. Ifi < j andAŒi > AŒj,thenthe
pair.i;j/iscalledaninversionofA.
a. Listthefiveinversions ofthearray 2;3;8;6;1 .
h i
42 Chapter2 GettingStarted
b. What array with elements from the set 1;2;:::;n has the most inversions?
f g
Howmanydoesithave?
c. What is the relationship between the running time of insertion sort and the
numberofinversions intheinputarray? Justify youranswer.
d. Giveanalgorithm thatdeterminesthenumberofinversions inanypermutation
onnelementsin‚.nlgn/worst-casetime. (Hint:Modifymergesort.)
Chapter notes
In1968,KnuthpublishedthefirstofthreevolumeswiththegeneraltitleTheArtof
Computer Programming [209, 210, 211]. Thefirst volume ushered in the modern
studyofcomputeralgorithmswithafocusontheanalysisofrunningtime,andthe
full series remains an engaging and worthwhile reference for many of the topics
presented here. According to Knuth, the word “algorithm” is derived from the
name“al-Khowaˆrizmˆı,”aninth-century Persianmathematician.
Aho, Hopcroft, and Ullman [5] advocated the asymptotic analysis of algo-
rithms—using notations that Chapter 3 introduces, including ‚-notation—as a
means of comparing relative performance. They also popularized the use of re-
currence relations todescribe therunningtimesofrecursive algorithms.
Knuth[211]providesanencyclopedictreatmentofmanysortingalgorithms. His
comparisonofsortingalgorithms(page381)includesexactstep-countinganalyses,
like the one we performed here for insertion sort. Knuth’s discussion of insertion
sort encompasses several variations ofthealgorithm. Themostimportant ofthese
is Shell’s sort, introduced by D. L. Shell, which uses insertion sort on periodic
subsequences oftheinputtoproduce afastersortingalgorithm.
Merge sort is also described by Knuth. He mentions that a mechanical colla-
tor capable of merging two decks of punched cards in a single pass was invented
in 1938. J. von Neumann, one of the pioneers of computer science, apparently
wroteaprogram formergesortontheEDVACcomputerin1945.
Theearly history ofproving programs correct is described byGries [153], who
credits P.Naur withthe firstarticle inthis field. Griesattributes loop invariants to
R. W. Floyd. The textbook by Mitchell [256] describes more recent progress in
provingprograms correct.
3 Growth of Functions
The order of growth of the running time of an algorithm, defined in Chapter 2,
gives a simple characterization of the algorithm’s efficiency and also allows us to
compare the relative performance of alternative algorithms. Once the input size n
becomes large enough, merge sort, with its ‚.nlgn/ worst-case running time,
beats insertion sort, whose worst-case running time is ‚.n2/. Although we can
sometimesdeterminetheexactrunningtimeofanalgorithm,aswedidforinsertion
sort in Chapter 2, the extra precision is not usually worth the effort of computing
it. For large enough inputs, the multiplicative constants and lower-order terms of
anexactrunning timearedominated bytheeffectsoftheinputsizeitself.
When we look at input sizes large enough to make only the order of growth of
therunningtimerelevant,wearestudyingtheasymptoticefficiencyofalgorithms.
Thatis,weareconcernedwithhowtherunningtimeofanalgorithmincreaseswith
the size of the input in the limit, as the size of the input increases without bound.
Usually, an algorithm that is asymptotically more efficient will be the best choice
forallbutverysmallinputs.
Thischaptergivesseveralstandardmethodsforsimplifyingtheasymptoticanal-
ysis of algorithms. The next section begins by defining several types of “asymp-
toticnotation,” ofwhichwehavealready seenanexamplein‚-notation. Wethen
present several notational conventions used throughout this book, and finally we
reviewthebehavioroffunctionsthatcommonlyariseintheanalysisofalgorithms.
3.1 Asymptoticnotation
The notations we use to describe the asymptotic running time of an algorithm
are defined in terms of functions whose domains are the set of natural numbers
N 0;1;2;::: . Such notations are convenient for describing the worst-case
D f g
running-time function T.n/, which usually is defined only on integer input sizes.
We sometimes find it convenient, however, to abuse asymptotic notation in a va-
44 Chapter3 GrowthofFunctions
riety of ways. For example, we might extend the notation to the domain of real
numbers or, alternatively, restrict it toa subset of the natural numbers. Weshould
makesure,however,tounderstandtheprecisemeaningofthenotationsothatwhen
weabuse, wedonotmisuse it. Thissection definesthebasicasymptotic notations
andalsointroduces somecommonabuses.
Asymptoticnotation,functions,andrunningtimes
We will use asymptotic notation primarily to describe the running times of algo-
rithms, as when we wrote that insertion sort’s worst-case running time is ‚.n2/.
Asymptoticnotationactuallyappliestofunctions, however. Recallthatwecharac-
terizedinsertionsort’sworst-caserunningtimeasan2 bn c,forsomeconstants
C C
a, b, and c. By writing that insertion sort’s running time is ‚.n2/, we abstracted
away some details of this function. Because asymptotic notation applies to func-
tions, what we were writing as ‚.n2/ was the function an2 bn c, which in
C C
thatcasehappened tocharacterize theworst-caserunning timeofinsertion sort.
In this book, the functions to which we apply asymptotic notation will usually
characterize therunningtimesofalgorithms. Butasymptoticnotationcanapplyto
functions that characterize some other aspect of algorithms (the amount of space
they use, for example), or even to functions that have nothing whatsoever to do
withalgorithms.
Even when we use asymptotic notation to apply to the running time of an al-
gorithm, we need to understand which running time we mean. Sometimes we are
interested intheworst-caserunning time. Often,however,wewishtocharacterize
the running timenomatter whattheinput. Inother words, weoften wishtomake
a blanket statement that covers all inputs, not just the worst case. We shall see
asymptotic notations thatarewellsuitedtocharacterizing runningtimesnomatter
whattheinput.
‚-notation
In Chapter 2, we found that the worst-case running time of insertion sort is
T.n/ ‚.n2/. Letusdefinewhatthisnotationmeans. Foragivenfunction g.n/,
D
wedenoteby‚.g.n//thesetoffunctions
‚.g.n// f.n/ thereexistpositiveconstants c ,c ,andn suchthat
1 2 0
Df W
0 c g.n/ f.n/ c g.n/foralln n :1
1 2 0
    g
1Withinsetnotation,acolonmeans“suchthat.”
3.1 Asymptoticnotation 45
c g.n/ cg.n/
2
f.n/
f.n/
f.n/
cg.n/
c g.n/
1
n n n
n n n
0 0 0
f.n/ ‚.g.n// f.n/ O.g.n// f.n/ .g.n//
D D D
(a) (b) (c)
Figure3.1 Graphicexamplesofthe‚,O,andnotations. Ineachpart,thevalueofn0 shown
istheminimumpossiblevalue; anygreatervaluewouldalsowork. (a)‚-notationboundsafunc-
tiontowithinconstantfactors. Wewritef.n/ ‚.g.n//ifthereexistpositiveconstantsn0,c1,
D
andc2suchthatatandtotherightofn0,thevalueoff.n/alwaysliesbetweenc1g.n/andc2g.n/
inclusive. (b)O-notationgivesanupperboundforafunctiontowithinaconstantfactor. Wewrite
f.n/ O.g.n//iftherearepositiveconstantsn0andcsuchthatatandtotherightofn0,thevalue
D
off.n/alwaysliesonorbelowcg.n/. (c)-notationgivesalowerboundforafunctiontowithin
aconstantfactor.Wewritef.n/ .g.n//iftherearepositiveconstantsn0andcsuchthatatand
D
totherightofn0,thevalueoff.n/alwaysliesonorabovecg.n/.
A function f.n/ belongs to the set ‚.g.n// if there exist positive constants c
1
and c such that it can be “sandwiched” between c g.n/ and c g.n/, for suffi-
2 1 2
ciently large n. Because ‚.g.n// is a set, we could write “f.n/ ‚.g.n//”
2
to indicate that f.n/ is a member of ‚.g.n//. Instead, we will usually write
“f.n/ ‚.g.n//” to express the same notion. You might be confused because
D
we abuse equality in this way, but we shall see later in this section that doing so
hasitsadvantages.
Figure 3.1(a) gives an intuitive picture of functions f.n/ and g.n/, where
f.n/ ‚.g.n//. For all values of n at and to the right of n , the value of f.n/
0
D
liesatorabovec g.n/andatorbelow c g.n/. Inotherwords, foralln n ,the
1 2 0

function f.n/ is equal to g.n/ to within a constant factor. We say that g.n/ is an
asymptotically tightboundforf.n/.
The definition of ‚.g.n// requires that every member f.n/ ‚.g.n// be
2
asymptotically nonnegative, that is, that f.n/ be nonnegative whenever n is suf-
ficiently large. (An asymptotically positive function is one that is positive for all
sufficientlylargen.) Consequently, thefunctiong.n/itselfmustbeasymptotically
nonnegative,orelsetheset‚.g.n//isempty. Weshallthereforeassumethatevery
function used within ‚-notation is asymptotically nonnegative. This assumption
holdsfortheotherasymptoticnotations definedinthischapteraswell.
46 Chapter3 GrowthofFunctions
In Chapter 2, we introduced an informal notion of ‚-notation that amounted
to throwing away lower-order terms and ignoring the leading coefficient of the
highest-order term. Let us briefly justify this intuition by using the formal defi-
nition to show that 1n2 3n ‚.n2/. To do so, we must determine positive
2 (cid:0) D
constants c ,c ,andn suchthat
1 2 0
1
c n2 n2 3n c n2
1 2
 2 (cid:0) 
foralln n . Dividingbyn2 yields
0

1 3
c c :
1 2
 2 (cid:0) n 
Wecanmaketheright-handinequalityholdforanyvalueofn 1bychoosingany

constant c 1=2. Likewise, we can make the left-hand inequality hold for any
2

valueofn 7bychoosinganyconstantc 1=14. Thus,bychoosingc 1=14,
1 1
  D
c 1=2, and n 7, we can verify that 1n2 3n ‚.n2/. Certainly, other
2 D 0 D 2 (cid:0) D
choices for the constants exist, but the important thing is that some choice exists.
Note that these constants depend on the function 1n2 3n; a different function
2 (cid:0)
belonging to‚.n2/wouldusuallyrequiredifferent constants.
We can also use the formal definition to verify that 6n3 ‚.n2/. Suppose
¤
for the purpose of contradiction that c and n exist such that 6n3 c n2 for
2 0 2

all n n . But then dividing by n2 yields n c =6, which cannot possibly hold
0 2
 
forarbitrarily largen,sincec isconstant.
2
Intuitively, the lower-order terms of an asymptotically positive function can be
ignored in determining asymptotically tight bounds because they are insignificant
for large n. When n is large, even a tiny fraction of the highest-order term suf-
ficestodominate the lower-order terms. Thus, setting c toavalue that isslightly
1
smaller than thecoefficient ofthehighest-order term andsetting c toavalue that
2
is slightly larger permits the inequalities in the definition of ‚-notation to be sat-
isfied. The coefficient of the highest-order term can likewise be ignored, since it
onlychangesc andc byaconstant factorequaltothecoefficient.
1 2
As an example, consider any quadratic function f.n/ an2 bn c, where
D C C
a, b, and c are constants and a > 0. Throwing away the lower-order terms and
ignoring theconstant yields f.n/ ‚.n2/. Formally,toshowthesamething, we
D
take the constants c a=4, c 7a=4, and n 2 max. b =a; c =a/. You
1 2 0
D D D  j j j j
may verify that 0 c n2 an2 bn c c n2 for all n n . In general,

1
 C C 
2

p0
foranypolynomial p.n/ d a ni,wherethea areconstants anda > 0,we
D i 0 i i d
havep.n/ ‚.nd/(seeProbleDm3-1).
D P
Sinceanyconstant isadegree-0 polynomial, wecanexpress anyconstant func-
tionas‚.n0/,or‚.1/. Thislatternotation isaminorabuse, however,becausethe
3.1 Asymptoticnotation 47
expression does not indicate what variable is tending to infinity.2 We shall often
usethenotation‚.1/tomeaneitheraconstantoraconstant functionwithrespect
tosomevariable.
O-notation
The ‚-notation asymptotically bounds a function from above and below. When
we have only an asymptotic upper bound, we use O-notation. For a given func-
tion g.n/, we denote by O.g.n// (pronounced “big-oh of g of n” or sometimes
just“ohofg ofn”)thesetoffunctions
O.g.n// f.n/ thereexistpositiveconstants c andn suchthat
0
Df W
0 f.n/ cg.n/foralln n :
0
   g
We use O-notation to give an upper bound on a function, to within a constant
factor. Figure3.1(b)showstheintuitionbehindO-notation. Forallvaluesnatand
totherightofn ,thevalueofthefunction f.n/isonorbelowcg.n/.
0
We write f.n/ O.g.n// to indicate that a function f.n/ is a member of the
D
set O.g.n//. Note that f.n/ ‚.g.n// implies f.n/ O.g.n//, since ‚-
D D
notation is a stronger notion than O-notation. Written set-theoretically, we have
‚.g.n// O.g.n//. Thus, our proof that any quadratic function an2 bn c,
 C C
wherea > 0,isin‚.n2/alsoshowsthatanysuchquadratic function isinO.n2/.
What may be more surprising is that when a > 0, any linear function an b is
C
inO.n2/,whichiseasilyverifiedbytakingc a b andn max.1; b=a/.
0
D Cj j D (cid:0)
If you have seen O-notation before, you might find it strange that we should
write, for example, n O.n2/. In the literature, we sometimes find O-notation
D
informally describing asymptotically tight bounds, that is, what we have defined
using ‚-notation. In this book, however, when we write f.n/ O.g.n//, we
D
are merely claiming that some constant multiple of g.n/ is an asymptotic upper
bound on f.n/, with no claim about how tight an upper bound it is. Distinguish-
ing asymptotic upper bounds from asymptotically tight bounds is standard in the
algorithmsliterature.
Using O-notation, we can often describe the running time of an algorithm
merely by inspecting the algorithm’s overall structure. For example, the doubly
nested loop structure of the insertion sort algorithm from Chapter 2 immediately
yields an O.n2/ upper bound on the worst-case running time: the cost of each it-
eration of the inner loop is bounded from above by O.1/ (constant), the indices i
2The real problem is that our ordinary notation for functions does not distinguish functions from
values. In-calculus, theparameterstoafunction areclearlyspecified: thefunction n2 couldbe
writtenas n:n2, or even r:r2. Adopting amore rigorous notation, however, would complicate
algebraicmanipulations,andsowechoosetotoleratetheabuse.
48 Chapter3 GrowthofFunctions
and j are both at most n, and the inner loop is executed at most once for each of
then2 pairsofvaluesfori andj.
SinceO-notation describes anupperbound,whenweuseittoboundtheworst-
caserunningtimeofanalgorithm,wehaveaboundontherunningtimeofthealgo-
rithmoneveryinput—theblanketstatementwediscussed earlier. Thus,theO.n2/
bound onworst-case running timeofinsertion sortalsoapplies toitsrunning time
oneveryinput. The‚.n2/boundontheworst-case running timeofinsertion sort,
however, does not imply a ‚.n2/ bound on the running time of insertion sort on
every input. For example, we saw in Chapter 2 that when the input is already
sorted, insertion sortrunsin‚.n/time.
Technically, itisanabusetosaythattherunningtimeofinsertionsortisO.n2/,
since for a given n, the actual running time varies, depending on the particular
inputofsizen. Whenwesay“therunning timeisO.n2/,”wemeanthatthereisa
function f.n/thatisO.n2/suchthatforanyvalueofn,nomatterwhatparticular
inputofsizenischosen, therunning timeonthatinputisbounded fromaboveby
thevaluef.n/. Equivalently, wemeanthattheworst-case runningtimeisO.n2/.
-notation
Just asO-notation provides anasymptotic upper bound onafunction, -notation
provides an asymptotic lower bound. For a given function g.n/, we denote
by .g.n// (pronounced “big-omega of g of n” or sometimes just “omega of g
ofn”)thesetoffunctions
.g.n// f.n/ thereexistpositiveconstants c andn suchthat
0
Df W
0 cg.n/ f.n/foralln n :
0
   g
Figure 3.1(c) shows the intuition behind -notation. For all values n at or to the
rightofn ,thevalueoff.n/isonorabovecg.n/.
0
Fromthedefinitionsoftheasymptoticnotationswehaveseenthusfar,itiseasy
toprovethefollowingimportanttheorem(seeExercise3.1-5).
Theorem3.1
For any two functions f.n/ and g.n/, we have f.n/ ‚.g.n// if and only if
D
f.n/ O.g.n//andf.n/ .g.n//.
D D
Asanexampleoftheapplicationofthistheorem,ourproofthatan2 bn c
C C D
‚.n2/ for any constants a, b, and c, where a > 0, immediately implies that
an2 bn c .n2/andan2 bn c O.n2/. Inpractice,ratherthanusing
C C D C C D
Theorem 3.1 to obtain asymptotic upper and lower bounds from asymptotically
tightbounds, aswedidforthisexample, weusuallyuseittoproveasymptotically
tightboundsfromasymptoticupperandlowerbounds.
3.1 Asymptoticnotation 49
When we say that the running time (no modifier) of an algorithm is .g.n//,
we mean that no matter what particular input of size n is chosen for each value
ofn,therunningtimeonthatinputisatleastaconstanttimesg.n/,forsufficiently
large n. Equivalently, we are giving a lower bound on the best-case running time
ofanalgorithm. Forexample,thebest-case running timeofinsertion sortis.n/,
whichimpliesthattherunning timeofinsertion sortis.n/.
The running time of insertion sort therefore belongs to both .n/ and O.n2/,
sinceitfallsanywherebetweenalinearfunctionofnandaquadraticfunctionofn.
Moreover, these bounds are asymptotically as tight as possible: for instance, the
running time of insertion sort is not .n2/, since there exists an input for which
insertion sort runs in ‚.n/ time (e.g., when the input is already sorted). It is not
contradictory, however, to say that the worst-case running time of insertion sort
is.n2/,sincethereexistsaninputthatcausesthealgorithm totake.n2/time.
Asymptoticnotationinequationsandinequalities
We have already seen how asymptotic notation can be used within mathematical
formulas. For example, in introducing O-notation, we wrote “n O.n2/.” We
D
mightalsowrite2n2 3n 1 2n2 ‚.n/. Howdoweinterpretsuchformulas?
C C D C
Whentheasymptotic notation stands alone(that is,notwithinalargerformula)
on the right-hand side of an equation (or inequality), as in n O.n2/, we have
D
already defined the equal sign to mean set membership: n O.n2/. In general,
2
however, when asymptotic notation appears in aformula, we interpret it as stand-
ing for some anonymous function that we do not care to name. For example, the
formula 2n2 3n 1 2n2 ‚.n/ means that 2n2 3n 1 2n2 f.n/,
C C D C C C D C
wheref.n/issomefunction inthe set‚.n/. Inthiscase, weletf.n/ 3n 1,
D C
whichindeedisin‚.n/.
Using asymptotic notation in this manner can help eliminate inessential detail
andclutter inanequation. Forexample, inChapter2weexpressed theworst-case
runningtimeofmergesortastherecurrence
T.n/ 2T.n=2/ ‚.n/:
D C
If we are interested only in the asymptotic behavior of T.n/, there is no point in
specifyingallthelower-ordertermsexactly;theyareallunderstood tobeincluded
intheanonymous functiondenoted bytheterm‚.n/.
Thenumberofanonymous functions inanexpression isunderstood tobeequal
to the number of times the asymptotic notation appears. For example, in the ex-
pression
n
O.i/;
i 1
XD
50 Chapter3 GrowthofFunctions
thereisonlyasingleanonymousfunction(afunctionofi). Thisexpressionisthus
not the same as O.1/ O.2/ O.n/, which doesn’t really have a clean
C C  C
interpretation.
Insomecases,asymptoticnotation appearsontheleft-hand sideofanequation,
asin
2n2 ‚.n/ ‚.n2/:
C D
We interpret such equations using the following rule: No matter how the anony-
mous functions are chosen on the left of the equal sign, there is a way to choose
theanonymous functions ontherightoftheequalsigntomaketheequation valid.
Thus, our example means that for any function f.n/ ‚.n/, there is some func-
2
tion g.n/ ‚.n2/ such that 2n2 f.n/ g.n/ for all n. In other words, the
2 C D
right-hand side of an equation provides a coarser level of detail than the left-hand
side.
Wecanchaintogether anumberofsuchrelationships, asin
2n2 3n 1 2n2 ‚.n/
C C D C
‚.n2/:
D
We can interpret each equation separately by the rules above. The first equa-
tion says that there is some function f.n/ ‚.n/ such that 2n2 3n 1
2 C C D
2n2 f.n/foralln. Thesecondequationsaysthatforanyfunctiong.n/ ‚.n/
C 2
(such as the f.n/ just mentioned), there is some function h.n/ ‚.n2/ such
2
that 2n2 g.n/ h.n/ for all n. Note that this interpretation implies that
C D
2n2 3n 1 ‚.n2/, which iswhat thechaining of equations intuitively gives
C C D
us.
o-notation
The asymptotic upper bound provided by O-notation may or may not be asymp-
totically tight. The bound 2n2 O.n2/ is asymptotically tight, but the bound
D
2n O.n2/isnot. Weuseo-notationtodenoteanupperboundthatisnotasymp-
D
totically tight. Weformallydefineo.g.n//(“little-oh ofg ofn”)astheset
o.g.n// f.n/ foranypositiveconstant c >0,thereexistsaconstant
Df W
n > 0suchthat0 f.n/< cg.n/foralln n :
0 0
  g
Forexample,2n o.n2/,but2n2 o.n2/.
D ¤
The definitions of O-notation and o-notation are similar. The main difference
is that in f.n/ O.g.n//, the bound 0 f.n/ cg.n/ holds for some con-
D  
stant c >0, but in f.n/ o.g.n//, the bound 0 f.n/ < cg.n/ holds for all
D 
constantsc > 0. Intuitively,ino-notation,thefunctionf.n/becomesinsignificant
relativetog.n/asnapproaches infinity;thatis,
3.1 Asymptoticnotation 51
f.n/
lim 0: (3.1)
n g.n/ D
!1
Some authors use this limit as a definition of the o-notation; the definition in this
bookalsorestricts theanonymousfunctions tobeasymptotically nonnegative.
!-notation
By analogy, !-notation is to -notation as o-notation is to O-notation. We use
!-notation to denote a lower bound that is not asymptotically tight. One way to
defineitisby
f.n/ !.g.n//ifandonlyifg.n/ o.f.n//:
2 2
Formally,however,wedefine!.g.n//(“little-omega ofg ofn”)astheset
!.g.n// f.n/ foranypositiveconstant c > 0,thereexistsaconstant
Df W
n > 0suchthat0 cg.n/ < f.n/foralln n :
0 0
  g
For example, n2=2 !.n/, but n2=2 !.n2/. The relation f.n/ !.g.n//
D ¤ D
impliesthat
f.n/
lim ;
n g.n/ D1
!1
if the limit exists. That is, f.n/ becomes arbitrarily large relative to g.n/ as n
approaches infinity.
Comparingfunctions
Manyoftherelationalpropertiesofrealnumbersapplytoasymptoticcomparisons
aswell. Forthefollowing, assumethatf.n/andg.n/areasymptotically positive.
Transitivity:
f.n/ ‚.g.n// and g.n/ ‚.h.n// imply f.n/ ‚.h.n//;
D D D
f.n/ O.g.n// and g.n/ O.h.n// imply f.n/ O.h.n//;
D D D
f.n/ .g.n// and g.n/ .h.n// imply f.n/ .h.n//;
D D D
f.n/ o.g.n// and g.n/ o.h.n// imply f.n/ o.h.n//;
D D D
f.n/ !.g.n// and g.n/ !.h.n// imply f.n/ !.h.n//:
D D D
Reflexivity:
f.n/ ‚.f.n//;
D
f.n/ O.f.n//;
D
f.n/ .f.n//:
D
52 Chapter3 GrowthofFunctions
Symmetry:
f.n/ ‚.g.n// ifandonlyif g.n/ ‚.f.n//:
D D
Transposesymmetry:
f.n/ O.g.n// ifandonlyif g.n/ .f.n//;
D D
f.n/ o.g.n// ifandonlyif g.n/ !.f.n//:
D D
Becausethesepropertiesholdforasymptoticnotations,wecandrawananalogy
betweentheasymptotic comparison oftwofunctions f andg andthecomparison
oftworealnumbersaandb:
f.n/ O.g.n// islike a b ;
D 
f.n/ .g.n// islike a b ;
D 
f.n/ ‚.g.n// islike a b ;
D D
f.n/ o.g.n// islike a <b ;
D
f.n/ !.g.n// islike a >b :
D
Wesaythatf.n/isasymptoticallysmallerthang.n/iff.n/ o.g.n//,andf.n/
D
isasymptotically largerthang.n/iff.n/ !.g.n//.
D
Oneproperty ofrealnumbers, however, doesnotcarryovertoasymptotic nota-
tion:
Trichotomy: Foranytworealnumbersaandb,exactlyoneofthefollowingmust
hold: a < b,a b,ora > b.
D
Although any two real numbers can be compared, not all functions are asymptot-
ically comparable. That is, for two functions f.n/ and g.n/, it may be the case
thatneitherf.n/ O.g.n//norf.n/ .g.n//holds. Forexample,wecannot
D D
compare the functions n and n1 sinn using asymptotic notation, since the value of
C
theexponentinn1 sinn oscillatesbetween0and2,takingonallvaluesinbetween.
C
Exercises
3.1-1
Letf.n/and g.n/beasymptotically nonnegative functions. Using thebasic defi-
nitionof‚-notation, provethatmax.f.n/;g.n// ‚.f.n/ g.n//.
D C
3.1-2
Showthatforanyrealconstants a andb,whereb > 0,
.n a/b ‚.nb/: (3.2)
C D
3.2 Standardnotationsandcommonfunctions 53
3.1-3
Explainwhythestatement,“TherunningtimeofalgorithmAisatleastO.n2/,”is
meaningless.
3.1-4
Is2n 1 O.2n/? Is22n O.2n/?
C
D D
3.1-5
ProveTheorem3.1.
3.1-6
Provethattherunningtimeofanalgorithmis‚.g.n//ifandonlyifitsworst-case
runningtimeisO.g.n//anditsbest-case runningtimeis.g.n//.
3.1-7
Provethato.g.n// !.g.n//istheemptyset.
\
3.1-8
We can extend our notation to the case of two parameters n and m that can go to
infinity independently at different rates. For a given function g.n;m/, we denote
byO.g.n;m//thesetoffunctions
O.g.n;m// f.n;m/ thereexistpositiveconstants c,n ,andm
0 0
D f W
suchthat0 f.n;m/ cg.n;m/
 
foralln n orm m :
0 0
  g
Givecorresponding definitions for.g.n;m//and‚.g.n;m//.
3.2 Standard notations and commonfunctions
This section reviews some standard mathematical functions and notations and ex-
plores the relationships among them. It also illustrates the use of the asymptotic
notations.
Monotonicity
A function f.n/ is monotonically increasing if m n implies f.m/ f.n/.
 
Similarly, it is monotonically decreasing if m n implies f.m/ f.n/. A
 
function f.n/ is strictly increasing if m < n implies f.m/ < f.n/ and strictly
decreasingifm < nimpliesf.m/>f.n/.
54 Chapter3 GrowthofFunctions
Floorsandceilings
Foranyrealnumberx,wedenotethegreatestintegerlessthanorequaltoxby x
b c
(read“thefloorofx”)andtheleast integergreater thanorequal tox by x (read
d e
“theceiling ofx”). Forallrealx,
x 1 < x x x < x 1: (3.3)
(cid:0) b c   d e C
Foranyintegern,
n=2 n=2 n;
d eCb c D
andforanyrealnumberx 0andintegersa;b > 0,

x=a x
d e ; (3.4)
b D ab
 
l m
x=a x
b c ; (3.5)
b D ab
 
j k
a a .b 1/
C (cid:0) ; (3.6)
b  b
lam a .b 1/
(cid:0) (cid:0) : (3.7)
b  b
j k
Thefloorfunction f.x/ x ismonotonically increasing, asistheceilingfunc-
D b c
tionf.x/ x .
D d e
Modulararithmetic
For any integer a and any positive integer n, the value a mod n is the remainder
(orresidue)ofthequotient a=n:
a mod n a n a=n : (3.8)
D (cid:0) b c
Itfollowsthat
0 a modn <n: (3.9)

Givenawell-definednotionoftheremainderofoneintegerwhendividedbyan-
other,itisconvenienttoprovidespecialnotationtoindicateequalityofremainders.
If.a mod n/ .b mod n/,wewritea b .mod n/andsaythata isequivalent
D 
tob,modulon. Inotherwords, a b .mod n/ifa andb havethesameremain-

der when divided by n. Equivalently, a b .mod n/ ifand only if n isa divisor

ofb a. Wewritea b .mod n/ifa isnotequivalent tob,modulon.
(cid:0) 6
3.2 Standardnotationsandcommonfunctions 55
Polynomials
Given a nonnegative integer d, a polynomial in n of degree d is a function p.n/
oftheform
d
p.n/ a ni ;
i
D
i 0
XD
where the constants a ;a ;:::;a are the coefficients of the polynomial and
0 1 d
a 0. A polynomial is asymptotically positive if and only if a > 0. For an
d d
¤
asymptoticallypositivepolynomialp.n/ofdegreed,wehavep.n/ ‚.nd/. For
D
any real constant a 0, the function na is monotonically increasing, and for any

real constant a 0, the function na is monotonically decreasing. We say that a

functionf.n/ispolynomially bounded iff.n/ O.nk/forsomeconstant k.
D
Exponentials
Forallreala > 0,m,andn,wehavethefollowingidentities:
a0 1;
D
a1 a;
D
a 1 1=a ;
(cid:0)
D
.am/n amn ;
D
.am/n .an/m ;
D
aman am n :
C
D
For all n and a 1, the function an is monotonically increasing in n. When

convenient, weshallassume00 1.
D
We can relate the rates of growth of polynomials and exponentials by the fol-
lowingfact. Forallrealconstants a andb suchthata > 1,
nb
lim 0; (3.10)
n an D
!1
fromwhichwecanconclude that
nb o.an/:
D
Thus,anyexponentialfunctionwithabasestrictlygreaterthan1growsfasterthan
anypolynomial function.
Using e to denote 2:71828:::, the base of the natural logarithm function, we
haveforallrealx,
x2 x3
1
xi
ex 1 x ; (3.11)
D C C 2Š C 3Š C D iŠ
i 0
XD
56 Chapter3 GrowthofFunctions
where“Š”denotes thefactorial function definedlaterinthissection. Forallrealx,
wehavetheinequality
ex 1 x ; (3.12)
 C
whereequalityholdsonlywhenx 0. When x 1,wehavetheapproximation
D j j
1 x ex 1 x x2 : (3.13)
C   C C
Whenx 0,theapproximation ofex by1 x isquitegood:
! C
ex 1 x ‚.x2/:
D C C
(In thisequation, theasymptotic notation isused todescribe the limiting behavior
asx 0ratherthanasx .) Wehaveforallx,
! ! 1
x n
lim 1 ex : (3.14)
n C n D
!1 
Logarithms
Weshallusethefollowingnotations:
lgn log n (binary logarithm) ,
D 2
lnn log n (natural logarithm) ,
D e
lgkn .lgn/k (exponentiation) ,
D
lglgn lg.lgn/ (composition) .
D
Animportant notational convention weshall adoptisthatlogarithm functions will
apply only to the next term in the formula, so that lgn k will mean .lgn/ k
C C
and not lg.n k/. If wehold b > 1 constant, then for n > 0, the function log n
C b
isstrictlyincreasing.
Forallreala > 0,b > 0,c >0,andn,
a blogba ;
D
log .ab/ log a log b ;
c D c C c
log an nlog a ;
b D b
log a
log a c ; (3.15)
b D log b
c
log .1=a/ log a ;
b D (cid:0) b
1
log a ;
b D log b
a
alogbc clogba ; (3.16)
D
where,ineachequationabove,logarithm basesarenot1.
3.2 Standardnotationsandcommonfunctions 57
By equation (3.15), changing the base of a logarithm from one constant to an-
otherchanges thevalueofthelogarithm byonlyaconstant factor, andsoweshall
oftenusethenotation “lgn”whenwedon’t careaboutconstant factors, suchasin
O-notation. Computer scientists find 2 to be the most natural base for logarithms
because so many algorithms and data structures involve splitting a problem into
twoparts.
Thereisasimpleseriesexpansion forln.1 x/when x < 1:
C j j
x2 x3 x4 x5
ln.1 x/ x :
C D (cid:0) 2 C 3 (cid:0) 4 C 5 (cid:0)
Wealsohavethefollowinginequalities forx > 1:
(cid:0)
x
ln.1 x/ x ; (3.17)
1 x  C 
C
whereequalityholdsonlyforx 0.
D
Wesaythatafunctionf.n/ispolylogarithmicallyboundediff.n/ O.lgkn/
D
forsomeconstant k. Wecanrelate thegrowthofpolynomials andpolylogarithms
bysubstituting lgnfornand2a forainequation(3.10),yielding
lgbn lgbn
lim lim 0:
n .2a/lgn D n na D
!1 !1
Fromthislimit,wecanconclude that
lgbn o.na/
D
for any constant a > 0. Thus, any positive polynomial function grows faster than
anypolylogarithmic function.
Factorials
Thenotation nŠ(read“nfactorial”) isdefinedforintegersn 0as

1 ifn 0;
nŠ D
D
(
n .n 1/Š ifn > 0:
 (cid:0)
Thus,nŠ 1 2 3 n.
D   
A weak upper bound on the factorial function is nŠ nn, since each of the n

termsinthefactorialproduct isatmostn. Stirling’sapproximation,
n n 1
nŠ p2n 1 ‚ ; (3.18)
D e C n
  
 
58 Chapter3 GrowthofFunctions
where e isthe base ofthe natural logarithm, gives us atighter upper bound, and a
lowerboundaswell. AsExercise3.2-3asksyoutoprove,
nŠ o.nn/;
D
nŠ !.2n/;
D
lg.nŠ/ ‚.nlgn/; (3.19)
D
whereStirling’sapproximationishelpfulinprovingequation(3.19). Thefollowing
equation alsoholdsforalln 1:

n n
nŠ p2n e˛n (3.20)
D e
 
where
1 1
< ˛ < : (3.21)
n
12n 1 12n
C
Functionaliteration
Weusethenotation f.i/.n/todenotethefunctionf.n/iterativelyappliedi times
to an initial value of n. Formally, let f.n/ be a function over the reals. For non-
negativeintegersi,werecursively define
n ifi 0;
f.i/.n/ D
D
(
f.f.i (cid:0)1/.n// ifi >0:
Forexample,iff.n/ 2n,thenf.i/.n/ 2in.
D D
Theiteratedlogarithm function
Weusethenotationlgn(read“logstarofn”)todenotetheiteratedlogarithm,de-
finedasfollows. Letlg.i/nbeasdefinedabove,withf.n/ lgn. Becausethelog-
D
arithmofanonpositivenumberisundefined,lg.i/nisdefinedonlyiflg.i (cid:0)1/n > 0.
Besuretodistinguish lg.i/n(thelogarithmfunction appliedi timesinsuccession,
starting with argument n) from lgin (the logarithm of n raised to the ith power).
Thenwedefinetheiterated logarithmfunction as
lgn min i 0 lg.i/n 1 :
D  W 
Theiterated logarithm isaveryslowlygrowingfunction:
˚ (cid:9)
lg 2 1;

D
lg4 2;
D
lg16 3;
D
lg65536 4;
D
lg.265536/ 5:
D
3.2 Standardnotationsandcommonfunctions 59
Sincethenumberofatomsintheobservableuniverseisestimatedtobeabout1080,
which is much less than 265536, we rarely encounter an input size n such that
lgn > 5.
Fibonaccinumbers
WedefinetheFibonaccinumbersbythefollowingrecurrence:
F 0;
0
D
F 1; (3.22)
1
D
F F F fori 2:
i i 1 i 2
D (cid:0) C (cid:0) 
Thus, each Fibonacci number is the sum of the two previous ones, yielding the
sequence
0; 1; 1; 2; 3; 5; 8; 13; 21; 34; 55; ::: :
Fibonacci numbers arerelated to the golden ratio and toits conjugate , which
y
arethetworootsoftheequation
x2 x 1 (3.23)
D C
andaregivenbythefollowingformulas(seeExercise3.2-6):
1 p5
C (3.24)
D 2
1:61803::: ;
D
1 p5
(cid:0)
y D 2
:61803::: :
D (cid:0)
Specifically, wehave
i i
F (cid:0) y ;
i
D p5
whichwecanprovebyinduction (Exercise3.2-7). Since < 1,wehave
y
i 1 ˇ ˇ
y < ˇ ˇ
p5 p5
ˇ ˇ
ˇ ˇ 1
< ;
2
whichimpliesthat
60 Chapter3 GrowthofFunctions
i 1
F ; (3.25)
i
D p5 C 2
 
whichistosaythattheithFibonacci numberF isequalto i=p5roundedtothe
i
nearestinteger. Thus,Fibonaccinumbersgrowexponentially.
Exercises
3.2-1
Show that if f.n/ and g.n/ are monotonically increasing functions, then so are
the functions f.n/ g.n/ and f.g.n//, and if f.n/ and g.n/ are in addition
C
nonnegative, thenf.n/ g.n/ismonotonically increasing.

3.2-2
Proveequation (3.16).
3.2-3
Proveequation (3.19). AlsoprovethatnŠ !.2n/andnŠ o.nn/.
D D
3.2-4 ?
Isthefunction lgn Špolynomially bounded? Isthefunction lglgn Špolynomi-
d e d e
allybounded?
3.2-5 ?
Whichisasymptotically larger: lg.lgn/orlg.lgn/?
3.2-6
Show that the golden ratio and its conjugate both satisfy the equation
y
x2 x 1.
D C
3.2-7
Provebyinduction thattheithFibonaccinumbersatisfiestheequality
i i
F (cid:0) y ;
i
D p5
where isthegoldenratioand isitsconjugate.
y
3.2-8
Showthatklnk ‚.n/impliesk ‚.n=lnn/.
D D
ProblemsforChapter3 61
Problems
3-1 Asymptoticbehaviorofpolynomials
Let
d
p.n/ a ni ;
i
D
i 0
XD
where a > 0, be a degree-d polynomial in n, and let k be a constant. Use the
d
definitionsoftheasymptoticnotations toprovethefollowingproperties.
a. Ifk d,thenp.n/ O.nk/.
 D
b. Ifk d,thenp.n/ .nk/.
 D
c. Ifk d,thenp.n/ ‚.nk/.
D D
d. Ifk >d,thenp.n/ o.nk/.
D
e. Ifk <d,thenp.n/ !.nk/.
D
3-2 Relativeasymptotic growths
Indicate,foreachpairofexpressions.A;B/inthetablebelow,whetherAisO,o,
,!,or‚ofB. Assumethatk 1, > 0,andc > 1areconstants. Youranswer

shouldbeintheformofthetablewith“yes”or“no”writtenineachbox.
A B O o  ! ‚
a. lgkn n
b. nk cn
c. pn nsinn
d. 2n 2n=2
e. nlgc clgn
f. lg.nŠ/ lg.nn/
3-3 Orderingbyasymptoticgrowthrates
a. Rank the following functions by order of growth; that is, find an arrangement
g ;g ;:::;g of the functions satisfying g .g /, g .g /, ...,
1 2 30 1 2 2 3
D D
g .g /. Partition your list into equivalence classes such that functions
29 30
D
f.n/andg.n/areinthesameclassifandonlyiff.n/ ‚.g.n//.
D
62 Chapter3 GrowthofFunctions
lg.lg n/ 2lgn .p2/lgn n2 nŠ .lgn/Š

.3/n n3 lg2n lg.nŠ/ 22n n1=lgn
2
lnlnn lgn n 2n nlglgn lnn 1

2lgn .lgn/lgn en 4lgn .n 1/Š lgn
C
lg.lgn/ 2p2lgn n 2n nlgn p22nC1
b. Give an example of a single nonnegative function f.n/ such that for all func-
tionsg .n/inpart(a),f.n/isneitherO.g .n//nor.g .n//.
i i i
3-4 Asymptoticnotationproperties
Letf.n/andg.n/beasymptotically positivefunctions. Proveordisproveeachof
thefollowingconjectures.
a. f.n/ O.g.n//impliesg.n/ O.f.n//.
D D
b. f.n/ g.n/ ‚.min.f.n/;g.n///.
C D
c. f.n/ O.g.n// implies lg.f.n// O.lg.g.n///, where lg.g.n// 1 and
D D 
f.n/ 1forallsufficiently largen.

d. f.n/ O.g.n//implies2f.n/ O 2g.n/ .
D D
e. f.n/ O..f.n//2/. (cid:0) 
D
f. f.n/ O.g.n//impliesg.n/ .f.n//.
D D
g. f.n/ ‚.f.n=2//.
D
h. f.n/ o.f.n// ‚.f.n//.
C D
3-5 VariationsonO and˝
Some authors define  in a slightly different way than we do; let’s use 1 (read
“omega infinity”) for this alternative definition. We say that f.n/ 1.g.n// if
D
there exists a positive constant c such that f.n/ cg.n/ 0 for infinitely many
 
integersn.
a. Showthatforanytwofunctionsf.n/andg.n/thatareasymptoticallynonneg-
ative, either f.n/ O.g.n// orf.n/ 1.g.n// orboth, whereas this is not
D D
trueifweuseinplaceof1.
ProblemsforChapter3 63
b. Describethepotentialadvantagesanddisadvantagesofusing1 insteadofto
characterize therunningtimesofprograms.
Some authors also define O in a slightly different manner; let’s use O for the
0
alternative definition. We say that f.n/ O .g.n// if and only if f.n/
0
D j j D
O.g.n//.
c. What happens to each direction of the “if and only if” in Theorem 3.1 if we
substitute O forO butstilluse?
0
Some authors define O (read “soft-oh”) to mean O with logarithmic factors ig-
nored:
e
O.g.n// f.n/ thereexistpositiveconstants c,k,andn suchthat
0
Df W
0 f.n/ cg.n/lgk.n/foralln n :
0
   g
e
d. Defineand‚inasimilarmanner. Provethecorresponding analog toTheo-
rem3.1.
e e
3-6 Iteratedfunctions
Wecanapplytheiterationoperator usedinthelg functiontoanymonotonically
 
increasing function f.n/overthereals. Foragivenconstant c R ,wedefinethe
2
iteratedfunction f by
c
f .n/ min i 0 f.i/.n/ c ;
c
D  W 
which need no˚t be well defined in a(cid:9)ll cases. In other words, the quantity f .n/ is
c
the number of iterated applications of the function f required to reduce its argu-
mentdowntoc orless.
Foreach of thefollowing functions f.n/and constants c,give astight abound
aspossible onf .n/.
c
f.n/ c f .n/
c
a. n 1 0
(cid:0)
b. lgn 1
c. n=2 1
d. n=2 2
e. pn 2
f. pn 1
g. n1=3 2
h. n=lgn 2
64 Chapter3 GrowthofFunctions
Chapter notes
Knuth[209]tracestheoriginoftheO-notationtoanumber-theorytextbyP.Bach-
mannin1892. Theo-notationwasinventedbyE.Landauin1909forhisdiscussion
of the distribution of prime numbers. The  and ‚ notations were advocated by
Knuth[213]tocorrectthepopular, buttechnically sloppy,practiceintheliterature
of using O-notation for both upper and lower bounds. Many people continue to
use the O-notation where the ‚-notation ismore technically precise. Further dis-
cussion of the history and development of asymptotic notations appears in works
byKnuth[209,213]andBrassardandBratley[54].
Not all authors define the asymptotic notations in the same way, although the
various definitions agree in most common situations. Someofthe alternative def-
initions encompass functions that are not asymptotically nonnegative, as long as
theirabsolutevaluesareappropriately bounded.
Equation (3.20) is due to Robbins [297]. Other properties of elementary math-
ematical functions can be found in any good mathematical reference, such as
Abramowitz and Stegun [1] or Zwillinger [362], or in a calculus book, such as
Apostol[18]orThomasetal.[334]. Knuth[209]andGraham,Knuth,andPatash-
nik[152]containawealthofmaterialondiscretemathematicsasusedincomputer
science.
4 Divide-and-Conquer
In Section 2.3.1, we saw how merge sort serves as an example of the divide-and-
conquer paradigm. Recall that in divide-and-conquer, we solve a problem recur-
sively,applying threestepsateachleveloftherecursion:
Dividetheproblemintoanumberofsubproblems thataresmallerinstances ofthe
sameproblem.
Conquerthesubproblemsbysolvingthemrecursively. Ifthesubproblemsizesare
smallenough,however,justsolvethesubproblemsinastraightforwardmanner.
Combinethe solutions to the subproblems into the solution for the original prob-
lem.
Whenthesubproblemsarelargeenoughtosolverecursively,wecallthattherecur-
sivecase. Oncethesubproblems become smallenough that wenolonger recurse,
wesay that the recursion “bottoms out” and that wehave gotten downtothe base
case. Sometimes,inadditiontosubproblemsthataresmallerinstancesofthesame
problem, wehave to solve subproblems that are not quite the same as the original
problem. Weconsider solvingsuchsubproblems aspartofthecombinestep.
Inthischapter, weshall seemorealgorithms basedondivide-and-conquer. The
firstonesolvesthemaximum-subarray problem: ittakesasinputanarrayofnum-
bers,anditdeterminesthecontiguoussubarraywhosevalueshavethegreatestsum.
Thenweshallseetwodivide-and-conquer algorithmsformultiplying n nmatri-

ces. Onerunsin‚.n3/time,whichisnobetterthanthestraightforwardmethodof
multiplying square matrices. Butthe other, Strassen’s algorithm, runs in O.n2:81/
time,whichbeatsthestraightforward methodasymptotically.
Recurrences
Recurrences gohandinhandwiththedivide-and-conquer paradigm, because they
giveusanaturalwaytocharacterizetherunningtimesofdivide-and-conquer algo-
rithms. Arecurrenceisanequationorinequalitythatdescribesafunctioninterms
66 Chapter4 Divide-and-Conquer
of its value on smaller inputs. For example, in Section 2.3.2 we described the
worst-case runningtimeT.n/ofthe MERGE-SORT procedure bytherecurrence
‚.1/ ifn 1;
T.n/ D (4.1)
D
(
2T.n=2/ ‚.n/ ifn > 1;
C
whosesolution weclaimedtobeT.n/ ‚.nlgn/.
D
Recurrences can take many forms. For example, a recursive algorithm might
dividesubproblemsintounequalsizes,suchasa2=3-to-1=3split. Ifthedivideand
combinestepstakelineartime,suchanalgorithmwouldgiverisetotherecurrence
T.n/ T.2n=3/ T.n=3/ ‚.n/.
D C C
Subproblems are not necessarily constrained to being a constant fraction of
the original problem size. For example, a recursive version of linear search
(see Exercise 2.1-3) would create just one subproblem containing only one el-
ement fewer than the original problem. Each recursive call would take con-
stant time plus the time for the recursive calls it makes, yielding the recurrence
T.n/ T.n 1/ ‚.1/.
D (cid:0) C
Thischapteroffersthreemethodsforsolvingrecurrences—that is,forobtaining
asymptotic “‚”or“O”boundsonthesolution:
 In the substitution method, we guess a bound and then use mathematical in-
ductiontoproveourguesscorrect.
 The recursion-tree method converts the recurrence into a tree whose nodes
representthecostsincurredatvariouslevelsoftherecursion. Weusetechniques
forbounding summationstosolvetherecurrence.
 Themastermethodprovides boundsforrecurrences oftheform
T.n/ aT.n=b/ f.n/; (4.2)
D C
where a 1, b > 1, and f.n/ is a given function. Such recurrences arise

frequently. A recurrence of the form in equation (4.2) characterizes a divide-
and-conquer algorithm that creates a subproblems, each of which is 1=b the
sizeoftheoriginalproblem,andinwhichthedivideandcombinestepstogether
takef.n/time.
To use the master method, you will need to memorize three cases, but once
you do that, you will easily be able to determine asymptotic bounds for many
simple recurrences. We will use the master method to determine the running
timesofthedivide-and-conqueralgorithmsforthemaximum-subarrayproblem
and for matrix multiplication, as well as for other algorithms based on divide-
and-conquer elsewhereinthisbook.
Chapter4 Divide-and-Conquer 67
Occasionally, weshallseerecurrences thatarenotequalities butrather inequal-
ities, such as T.n/ 2T.n=2/ ‚.n/. Because such a recurrence states only
 C
an upper bound on T.n/, we will couch its solution using O-notation rather than
‚-notation. Similarly,iftheinequalitywerereversedtoT.n/ 2T.n=2/ ‚.n/,
 C
then because the recurrence gives only a lower bound on T.n/, we would use
-notation initssolution.
Technicalitiesinrecurrences
In practice, we neglect certain technical details when we state and solve recur-
rences. For example, if we call MERGE-SORT on n elements when n is odd, we
end up with subproblems of size n=2 and n=2 . Neither size is actually n=2,
b c d e
becausen=2isnotanintegerwhennisodd. Technically,therecurrencedescribing
theworst-caserunning timeofMERGE-SORT isreally
‚.1/ ifn 1;
T.n/ D (4.3)
D
(
T. n=2 / T. n=2 / ‚.n/ ifn > 1:
d e C b c C
Boundary conditions represent another class of details that wetypically ignore.
Since the running time of an algorithm on a constant-sized input is a constant,
the recurrences that arise from the running times of algorithms generally have
T.n/ ‚.1/ for sufficiently small n. Consequently, for convenience, we shall
D
generally omit statements of the boundary conditions of recurrences and assume
thatT.n/isconstant forsmalln. Forexample, wenormally staterecurrence (4.1)
as
T.n/ 2T.n=2/ ‚.n/; (4.4)
D C
without explicitly giving values for small n. Thereason isthat although changing
the value of T.1/ changes the exact solution to the recurrence, the solution typi-
cally doesn’t change bymore than aconstant factor, and sothe order of growth is
unchanged.
Whenwestateandsolverecurrences, weoftenomitfloors,ceilings,andbound-
ary conditions. We forge ahead without these details and later determine whether
or not they matter. They usually do not, but you should know when they do. Ex-
periencehelps,andsodosometheoremsstatingthatthesedetailsdonotaffectthe
asymptotic bounds of many recurrences characterizing divide-and-conquer algo-
rithms(seeTheorem4.1). Inthischapter,however,weshalladdresssomeofthese
detailsandillustrate thefinepointsofrecurrence solution methods.
68 Chapter4 Divide-and-Conquer
4.1 Themaximum-subarray problem
SupposethatyouhavebeenofferedtheopportunitytoinvestintheVolatileChem-
icalCorporation. Likethechemicals thecompany produces, thestockprice ofthe
Volatile Chemical Corporation is rather volatile. You are allowed to buy one unit
of stock only one time and then sell it at a later date, buying and selling after the
close oftrading forthe day. Tocompensate forthis restriction, youare allowedto
learn what the price of the stock will be in the future. Your goal is to maximize
your profit. Figure 4.1 shows the price of the stock over a 17-day period. You
maybuythestock atanyonetime,starting afterday0,whenthepriceis$100per
share. Ofcourse, you would want to “buy low, sell high”—buy atthe lowest pos-
siblepriceandlateronsellatthehighest possible price—tomaximizeyourprofit.
Unfortunately, youmightnotbeabletobuyatthelowestpriceandthensellatthe
highest price within a given period. In Figure 4.1, the lowest price occurs after
day7,whichoccursafterthehighest price,afterday1.
You might think that you can always maximize profit by either buying at the
lowest price or selling at the highest price. For example, in Figure 4.1, we would
maximize profit by buying at the lowest price, after day 7. If this strategy always
worked,thenitwouldbeeasytodeterminehowtomaximizeprofit: findthehighest
andlowestprices,andthenworkleftfromthehighestpricetofindthelowestprior
price, work right from the lowest price to find the highest later price, and take
the pair with the greater difference. Figure 4.2 shows a simple counterexample,
120
110
100
90
80
70
60
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Day 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Price 100 113 110 85 105 102 86 63 81 101 94 106 101 79 94 90 97
Change 13 3 25 20 3 16 23 18 20 7 12 5 22 15 4 7
(cid:0) (cid:0) (cid:0) (cid:0) (cid:0) (cid:0) (cid:0) (cid:0) (cid:0)
Figure4.1 InformationaboutthepriceofstockintheVolatileChemicalCorporationaftertheclose
oftradingoveraperiodof17days.Thehorizontalaxisofthechartindicatestheday,andthevertical
axisshowstheprice.Thebottomrowofthetablegivesthechangeinpricefromthepreviousday.
4.1 Themaximum-subarrayproblem 69
11
10
9
Day 0 1 2 3 4
8 Price 10 11 7 10 6
7 Change 1 4 3 4
(cid:0) (cid:0)
6
0 1 2 3 4
Figure4.2 Anexampleshowingthatthemaximumprofitdoesnotalwaysstartatthelowestprice
orendatthehighestprice. Again,thehorizontalaxisindicatestheday,andtheverticalaxisshows
the price. Here, the maximum profit of $3 per share would be earned by buying after day 2 and
sellingafterday3. Thepriceof$7afterday2isnotthelowestpriceoverall,andthepriceof$10
afterday3isnotthehighestpriceoverall.
demonstrating thatthemaximum profitsometimes comesneither bybuying atthe
lowestpricenorbysellingatthehighestprice.
Abrute-forcesolution
Wecaneasilydeviseabrute-force solution tothisproblem: justtryeverypossible
pairofbuyandselldatesinwhichthebuydateprecedestheselldate. Aperiodofn
days has n such pairs of dates. Since n is ‚.n2/, and the best wecan hope for
2 2
is to evaluate each pair of dates in constant time, this approach would take .n2/
(cid:0)  (cid:0) 
time. Canwedobetter?
Atransformation
In order to design an algorithm with an o.n2/ running time, we will look at the
input in a slightly different way. We want to find a sequence of days over which
thenetchange from thefirstdaytothe lastismaximum. Instead oflooking atthe
dailyprices, letusinstead consider thedailychangeinprice, wherethechange on
dayi isthedifferencebetweenthepricesafterdayi 1andafterdayi. Thetable
(cid:0)
inFigure 4.1 shows these daily changes in the bottom row. If wetreat this row as
an array A, shown in Figure 4.3, we now want to find the nonempty, contiguous
subarrayofAwhosevalueshavethelargestsum. Wecallthiscontiguous subarray
the maximum subarray. For example, in the array of Figure 4.3, the maximum
subarray ofAŒ1::16isAŒ8::11,withthesum43. Thus,youwouldwanttobuy
the stock just before day 8 (that is, after day 7) and sell it after day 11, earning a
profitof$43pershare.
At first glance, this transformation does not help. We still need to check
n (cid:0)1 ‚.n2/ subarrays for a period of n days. Exercise 4.1-2 asks you to show
2 D
(cid:0) 
70 Chapter4 Divide-and-Conquer
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
A 13 –3 –25 20 –3 –16 –23 18 20 –7 12 –5 –22 15 –4 7
maximum subarray
Figure 4.3 The change in stock prices as a maximum-subarray problem. Here, the subar-
rayAŒ8::11,withsum43,hasthegreatestsumofanycontiguoussubarrayofarrayA.
that although computing the cost of one subarray might take time proportional to
thelengthofthesubarray, whencomputingall‚.n2/subarraysums,wecanorga-
nize the computation so that each subarray sum takes O.1/ time, given the values
ofpreviouslycomputedsubarraysums,sothatthebrute-forcesolutiontakes‚.n2/
time.
So let us seek a more efficient solution to the maximum-subarray problem.
Whendoingso, wewillusually speakof“a”maximumsubarray ratherthan“the”
maximum subarray, since there could bemorethan onesubarray that achieves the
maximumsum.
The maximum-subarray problem is interesting only when the array contains
some negative numbers. If all the array entries were nonnegative, then the
maximum-subarray problem would present no challenge, since the entire array
wouldgivethegreatestsum.
Asolutionusingdivide-and-conquer
Let’s think about how we might solve the maximum-subarray problem using
the divide-and-conquer technique. Suppose we want to find a maximum subar-
ray of the subarray AŒlow::high. Divide-and-conquer suggests that we divide
the subarray into two subarrays of as equal size as possible. That is, we find
the midpoint, say mid, of the subarray, and consider the subarrays AŒlow::mid
andAŒmid 1::high. AsFigure4.4(a)shows,anycontiguous subarray AŒi::j
C
ofAŒlow::highmustlieinexactlyoneofthefollowingplaces:
 entirelyinthesubarrayAŒlow::mid,sothatlow i j mid,
  
 entirelyinthesubarrayAŒmid 1::high,sothatmid <i j high,or
C  
 crossing themidpoint, sothatlow i mid < j high.
  
Therefore, a maximum subarray of AŒlow::high must lie in exactly one of these
places. In fact, a maximum subarray of AŒlow::high must have the greatest
sum over all subarrays entirely in AŒlow::mid, entirely in AŒmid 1::high,
C
or crossing the midpoint. We can find maximum subarrays of AŒlow::mid and
AŒmid 1::highrecursively,becausethesetwosubproblemsaresmallerinstances
C
oftheproblem offinding amaximumsubarray. Thus,allthatislefttodoisfinda
4.1 Themaximum-subarrayproblem 71
crossesthemidpoint
AŒmid 1::j
C
low mid high low i mid high
mid 1 mid 1 j
C C
entirelyinAŒlow::mid entirelyinAŒmid 1::high AŒi::mid
C
(a) (b)
Figure4.4 (a)PossiblelocationsofsubarraysofAŒlow::high: entirelyinAŒlow::mid,entirely
in AŒmid 1::high, or crossing the midpoint mid. (b) Any subarray of AŒlow::high crossing
C
themidpointcomprisestwosubarrays AŒi::midandAŒmid 1::j, wherelow i mid and
C  
mid<j high.

maximum subarray that crosses the midpoint, and take asubarray withthe largest
sumofthethree.
We can easily find a maximum subarray crossing the midpoint in time linear
in the size of the subarray AŒlow::high. This problem is not a smaller instance
of our original problem, because it has the added restriction that the subarray it
chooses must cross the midpoint. As Figure 4.4(b) shows, any subarray crossing
themidpointisitselfmadeoftwosubarraysAŒi::midandAŒmid 1::j,where
C
low i mid and mid < j high. Therefore, we just need to find maximum
  
subarraysoftheformAŒi::midandAŒmid 1::jandthencombinethem. The
C
procedure FIND-MAX-CROSSING-SUBARRAY takes as input the array A and the
indiceslow,mid,andhigh,anditreturnsatuplecontainingtheindicesdemarcating
amaximumsubarraythatcrossesthemidpoint,alongwiththesumofthevaluesin
amaximumsubarray.
FIND-MAX-CROSSING-SUBARRAY.A;low;mid;high/
1 left-sum
D (cid:0)1
2 sum 0
D
3 fori mid downtolow
D
4 sum sum AŒi
D C
5 ifsum >left-sum
6 left-sum sum
D
7 max-left i
D
8 right-sum
D (cid:0)1
9 sum 0
D
10 forj mid 1tohigh
D C
11 sum sum AŒj
D C
12 ifsum >right-sum
13 right-sum sum
D
14 max-right j
D
15 return.max-left;max-right;left-sum right-sum/
C
72 Chapter4 Divide-and-Conquer
This procedure works as follows. Lines 1–7 find a maximum subarray of the
left half, AŒlow::mid. Since this subarray must contain AŒmid, the for loop of
lines 3–7 starts the index i at mid and works down to low, so that every subarray
it considers is of the form AŒi::mid. Lines 1–2 initialize the variables left-sum,
whichholdsthegreatest sumfoundsofar,andsum,holding thesumoftheentries
in AŒi::mid. Whenever we find, in line 5, a subarray AŒi::mid with a sum of
valuesgreaterthanleft-sum,weupdateleft-sumtothissubarray’ssuminline6,and
in line 7 we update the variable max-left to record this index i. Lines 8–14 work
analogouslyfortherighthalf,AŒmid 1::high. Here,theforloopoflines10–14
C
startstheindexj atmid 1andworksuptohigh,sothateverysubarrayitconsiders
C
is of the form AŒmid 1::j. Finally, line 15 returns the indices max-left and
C
max-right that demarcate a maximum subarray crossing the midpoint, along with
thesumleft-sum right-sumofthevaluesinthesubarrayAŒmax-left::max-right.
C
If the subarray AŒlow::high contains n entries (so that n high low 1),
D (cid:0) C
we claim that the call FIND-MAX-CROSSING-SUBARRAY.A;low;mid;high/
takes ‚.n/ time. Since each iteration of each of the two for loops takes ‚.1/
time, we just need to count up how many iterations there are altogether. The for
loop oflines 3–7 makesmid low 1iterations, and theforloop oflines 10–14
(cid:0) C
makeshigh mid iterations, andsothetotalnumberofiterations is
(cid:0)
.mid low 1/ .high mid/ high low 1
(cid:0) C C (cid:0) D (cid:0) C
n:
D
With a linear-time FIND-MAX-CROSSING-SUBARRAY procedure in hand, we
can write pseudocode for a divide-and-conquer algorithm to solve the maximum-
subarray problem:
FIND-MAXIMUM-SUBARRAY.A;low;high/
1 ifhigh ==low
2 return.low;high;AŒlow/ //basecase: onlyoneelement
3 elsemid .low high/=2
D b C c
4 .left-low;left-high;left-sum/
D
FIND-MAXIMUM-SUBARRAY.A;low;mid/
5 .right-low;right-high;right-sum/
D
FIND-MAXIMUM-SUBARRAY.A;mid 1;high/
C
6 .cross-low;cross-high;cross-sum/
D
FIND-MAX-CROSSING-SUBARRAY.A;low;mid;high/
7 ifleft-sum right-sum andleft-sum cross-sum
 
8 return.left-low;left-high;left-sum/
9 elseifright-sum left-sumandright-sum cross-sum
 
10 return.right-low;right-high;right-sum/
11 elsereturn.cross-low;cross-high;cross-sum/
4.1 Themaximum-subarrayproblem 73
The initial call FIND-MAXIMUM-SUBARRAY.A;1;A:length/ will find a maxi-
mumsubarray ofAŒ1::n.
Similar to FIND-MAX-CROSSING-SUBARRAY, the recursive procedure FIND-
MAXIMUM-SUBARRAY returns a tuple containing the indices that demarcate a
maximum subarray, along with the sum of the values in a maximum subarray.
Line 1 tests for the base case, where the subarray has just one element. A subar-
ray with just one element has only one subarray—itself—and so line 2 returns a
tuple with the starting and ending indices of just the one element, along with its
value. Lines 3–11 handle the recursive case. Line3 does the divide part, comput-
ing the index mid of the midpoint. Let’s refer to the subarray AŒlow::mid as the
left subarray and to AŒmid 1::high as the right subarray. Because we know
C
thatthesubarray AŒlow::highcontains atleasttwoelements, eachoftheleftand
right subarrays must have at least one element. Lines 4 and 5 conquer by recur-
sivelyfindingmaximumsubarrayswithintheleftandrightsubarrays,respectively.
Lines6–11 form the combine part. Line6 findsamaximum subarray that crosses
themidpoint. (Recall thatbecause line 6solves asubproblem thatisnotasmaller
instance oftheoriginal problem, weconsider ittobeinthe combine part.) Line7
tests whether the left subarray contains a subarray with the maximum sum, and
line 8 returns that maximum subarray. Otherwise, line 9 tests whether the right
subarraycontainsasubarraywiththemaximumsum,andline10returnsthatmax-
imumsubarray. Ifneither theleftnorrightsubarrays contain asubarray achieving
themaximumsum,thenamaximumsubarraymustcrossthemidpoint,andline11
returnsit.
Analyzingthedivide-and-conqueralgorithm
Nextwesetuparecurrence thatdescribes therunning timeoftherecursive FIND-
MAXIMUM-SUBARRAY procedure. As we did when we analyzed merge sort in
Section 2.3.2, we make the simplifying assumption that the original problem size
is a power of 2, so that all subproblem sizes are integers. We denote by T.n/ the
running time of FIND-MAXIMUM-SUBARRAY on a subarray of n elements. For
starters, line 1 takes constant time. The base case, when n 1, is easy: line 2
D
takesconstant time,andso
T.1/ ‚.1/: (4.5)
D
Therecursive case occurs when n > 1. Lines 1 and 3 take constant time. Each
of the subproblems solved in lines 4 and 5 is on a subarray of n=2 elements (our
assumption that the original problem size is a power of 2 ensures that n=2 is an
integer), and so we spend T.n=2/ time solving each of them. Because we have
to solve two subproblems—for the left subarray and for the right subarray—the
contribution totherunningtimefromlines4and5comesto2T.n=2/. Aswehave
74 Chapter4 Divide-and-Conquer
already seen, the call to FIND-MAX-CROSSING-SUBARRAY in line 6 takes ‚.n/
time. Lines7–11takeonly‚.1/time. Fortherecursivecase,therefore, wehave
T.n/ ‚.1/ 2T.n=2/ ‚.n/ ‚.1/
D C C C
2T.n=2/ ‚.n/: (4.6)
D C
Combining equations (4.5) and (4.6) gives us a recurrence for the running
timeT.n/of FIND-MAXIMUM-SUBARRAY:
‚.1/ ifn 1;
T.n/ D (4.7)
D
(
2T.n=2/ ‚.n/ ifn > 1:
C
This recurrence is the same as recurrence (4.1) for merge sort. As we shall
see from the master method in Section 4.5, this recurrence has the solution
T.n/ ‚.nlgn/. You might also revisit the recursion tree in Figure 2.5 to un-
D
derstand whythesolutionshould beT.n/ ‚.nlgn/.
D
Thus, we see that the divide-and-conquer method yields an algorithm that is
asymptotically faster than the brute-force method. With merge sort and now the
maximum-subarray problem, webegin to get an idea of how powerful the divide-
and-conquer method can be. Sometimes it will yield the asymptotically fastest
algorithm foraproblem, andothertimeswecandoevenbetter. AsExercise4.1-5
shows,thereisinfactalinear-time algorithm forthemaximum-subarray problem,
anditdoesnotusedivide-and-conquer.
Exercises
4.1-1
Whatdoes FIND-MAXIMUM-SUBARRAY returnwhenallelementsofAarenega-
tive?
4.1-2
Write pseudocode for the brute-force method of solving the maximum-subarray
problem. Yourprocedure shouldrunin‚.n2/time.
4.1-3
Implement both the brute-force and recursive algorithms for the maximum-
subarrayproblemonyourowncomputer. Whatproblemsizen givesthecrossover
0
point at which the recursive algorithm beats the brute-force algorithm? Then,
change the base case of the recursive algorithm to use the brute-force algorithm
whenevertheproblemsizeislessthann . Doesthatchangethecrossoverpoint?
0
4.1-4
Suppose wechange the definition of the maximum-subarray problem toallow the
result to be an empty subarray, where the sum of the values of an empty subar-
4.2 Strassen’salgorithmformatrixmultiplication 75
ray is 0. How would you change any of the algorithms that do not allow empty
subarrays topermitanemptysubarray tobetheresult?
4.1-5
Use the following ideas to develop a nonrecursive, linear-time algorithm for the
maximum-subarray problem. Startattheleftendofthearray,andprogresstoward
theright,keepingtrackofthemaximumsubarrayseensofar. Knowingamaximum
subarray ofAŒ1::j,extendtheanswertofindamaximumsubarray ending atin-
dexj 1byusingthefollowingobservation: amaximumsubarrayofAŒ1::j 1
C C
is either a maximum subarray of AŒ1::j or a subarray AŒi::j 1, for some
C
1 i j 1. Determine a maximum subarray of the form AŒi::j 1 in
  C C
constanttimebasedonknowingamaximumsubarray endingatindexj.
4.2 Strassen’s algorithmformatrix multiplication
If you have seen matrices before, then you probably know how to multiply them.
(Otherwise, you should read Section D.1 in Appendix D.) If A .a / and
ij
D
B .b /aresquaren nmatrices,thenintheproductC A B,wedefinethe
ij
D  D 
entryc ,fori;j 1;2;:::;n,by
ij
D
n
c a b : (4.8)
ij ik kj
D 
k 1
XD
Wemustcomputen2matrixentries,andeachisthesumofnvalues. Thefollowing
proceduretakesn nmatricesAandB andmultipliesthem,returningtheirn n
 
product C. We assume that each matrix has an attribute rows, giving the number
ofrowsinthematrix.
SQUARE-MATRIX-MULTIPLY.A;B/
1 n A:rows
D
2 letC beanewn nmatrix

3 fori 1ton
D
4 forj 1ton
D
5 c 0
ij
D
6 fork 1ton
D
7 c c a b
ij ij ik kj
D C 
8 returnC
The SQUARE-MATRIX-MULTIPLY procedure works as follows. The for loop
of lines 3–7 computes the entries of each row i, and within a given row i, the
76 Chapter4 Divide-and-Conquer
for loop of lines 4–7 computes each of the entries c , for each column j. Line 5
ij
initializes c to0aswestartcomputing thesumgiveninequation (4.8), andeach
ij
iteration oftheforloopoflines6–7addsinonemoretermofequation (4.8).
Because each of the triply-nested for loops runs exactly n iterations, and each
execution of line 7 takes constant time, the SQUARE-MATRIX-MULTIPLY proce-
duretakes‚.n3/time.
Youmightatfirstthinkthatanymatrixmultiplicationalgorithmmusttake.n3/
time, since the natural definition of matrix multiplication requires that many mul-
tiplications. Youwouldbeincorrect, however: wehaveawaytomultiplymatrices
in o.n3/ time. In this section, we shall see Strassen’s remarkable recursive algo-
rithmformultiplyingn nmatrices. Itrunsin‚.nlg7/time,whichweshallshow

inSection 4.5. Sincelg7liesbetween 2:80and2:81, Strassen’s algorithm runsin
O.n2:81/ time, which is asymptotically better than the simple SQUARE-MATRIX-
MULTIPLY procedure.
Asimpledivide-and-conqueralgorithm
To keep things simple, when we use a divide-and-conquer algorithm to compute
thematrixproduct C A B,weassumethatnisanexactpowerof2ineachof
D 
then nmatrices. Wemakethisassumption because ineach dividestep, wewill

divide n n matrices into four n=2 n=2 matrices, and by assuming that n is an
 
exactpowerof2,weareguaranteed thataslongasn 2,thedimensionn=2isan

integer.
Supposethatwepartition eachofA,B,andC intofourn=2 n=2matrices

A A B B C C
A 11 12 ; B 11 12 ; C 11 12 ; (4.9)
D A 21 A 22 D B 21 B 22 D C 21 C 22
     
sothatwerewritetheequation C A B as
D 
C C A A B B
11 12 11 12 11 12 : (4.10)
C 21 C 22 D A 21 A 22  B 21 B 22
     
Equation(4.10)corresponds tothefourequations
C A B A B ; (4.11)
11 11 11 12 21
D  C 
C A B A B ; (4.12)
12 11 12 12 22
D  C 
C A B A B ; (4.13)
21 21 11 22 21
D  C 
C A B A B : (4.14)
22 21 12 22 22
D  C 
Each of these four equations specifies two multiplications of n=2 n=2 matrices

andtheadditionoftheirn=2 n=2products. Wecanusetheseequationstocreate

astraightforward, recursive, divide-and-conquer algorithm:
4.2 Strassen’salgorithmformatrixmultiplication 77
SQUARE-MATRIX-MULTIPLY-RECURSIVE.A;B/
1 n A:rows
D
2 letC beanewn nmatrix

3 ifn == 1
4 c a b
11 11 11
D 
5 elsepartitionA,B,andC asinequations (4.9)
6 C
11
SQUARE-MATRIX-MULTIPLY-RECURSIVE.A 11;B 11/
D
SQUARE-MATRIX-MULTIPLY-RECURSIVE.A 12;B 21/
C
7 C
12
SQUARE-MATRIX-MULTIPLY-RECURSIVE.A 11;B 12/
D
SQUARE-MATRIX-MULTIPLY-RECURSIVE.A 12;B 22/
C
8 C
21
SQUARE-MATRIX-MULTIPLY-RECURSIVE.A 21;B 11/
D
SQUARE-MATRIX-MULTIPLY-RECURSIVE.A 22;B 21/
C
9 C
22
SQUARE-MATRIX-MULTIPLY-RECURSIVE.A 21;B 12/
D
SQUARE-MATRIX-MULTIPLY-RECURSIVE.A 22;B 22/
C
10 returnC
This pseudocode glosses over one subtle but important implementation detail.
Howdowepartitionthematricesinline5? Ifweweretocreate12newn=2 n=2

matrices, we would spend ‚.n2/ time copying entries. In fact, we can partition
the matrices without copying entries. The trick is to use index calculations. We
identify a submatrix by a range of row indices and a range of column indices of
the original matrix. We end up representing a submatrix a little differently from
how we represent the original matrix, which is the subtlety we are glossing over.
The advantage is that, since we can specify submatrices by index calculations,
executing line 5 takes only ‚.1/ time (although we shall see that it makes no
difference asymptotically totheoverallrunning timewhetherwecopyorpartition
inplace).
Now, we derive a recurrence to characterize the running time of SQUARE-
MATRIX-MULTIPLY-RECURSIVE. Let T.n/ be the time to multiply two n n

matrices using this procedure. In the base case, when n 1, weperform just the
D
onescalarmultiplication inline4,andso
T.1/ ‚.1/: (4.15)
D
Therecursivecaseoccurswhenn > 1. Asdiscussed,partitioningthematricesin
line 5 takes ‚.1/ time, using index calculations. In lines 6–9, we recursively call
SQUARE-MATRIX-MULTIPLY-RECURSIVE a total of eight times. Because each
recursive call multiplies two n=2 n=2 matrices, thereby contributing T.n=2/ to

theoverallrunningtime,thetimetakenbyalleightrecursivecallsis8T.n=2/. We
alsomustaccountforthefourmatrixadditionsinlines6–9. Eachofthesematrices
contains n2=4 entries, and so each of the four matrix additions takes ‚.n2/ time.
Sincethenumberofmatrixadditions isaconstant, thetotaltimespentaddingma-
78 Chapter4 Divide-and-Conquer
trices inlines 6–9 is‚.n2/. (Again, weuse index calculations toplace the results
of the matrix additions into the correct positions of matrix C, with an overhead
of‚.1/timeperentry.) Thetotaltimefortherecursive case,therefore, isthesum
ofthepartitioning time,thetimeforalltherecursive calls, andthetimetoaddthe
matricesresulting fromtherecursivecalls:
T.n/ ‚.1/ 8T.n=2/ ‚.n2/
D C C
8T.n=2/ ‚.n2/: (4.16)
D C
Noticethatifweimplementedpartitioning bycopyingmatrices,whichwouldcost
‚.n2/ time, the recurrence would not change, and hence the overall running time
wouldincrease byonlyaconstant factor.
Combining equations (4.15) and (4.16) gives us the recurrence for the running
timeofSQUARE-MATRIX-MULTIPLY-RECURSIVE:
‚.1/ ifn 1;
T.n/ D (4.17)
D
(
8T.n=2/ ‚.n2/ ifn > 1:
C
As we shall see from the master method in Section 4.5, recurrence (4.17) has the
solution T.n/ ‚.n3/. Thus, this simple divide-and-conquer approach is no
D
fasterthanthestraightforward SQUARE-MATRIX-MULTIPLY procedure.
Before we continue on to examining Strassen’s algorithm, let us review where
the components of equation (4.16) came from. Partitioning each n n matrix by

indexcalculationtakes‚.1/time,butwehavetwomatricestopartition. Although
youcouldsaythatpartitioning thetwomatricestakes‚.2/time,theconstant of2
is subsumed by the ‚-notation. Adding two matrices, each with, say, k entries,
takes ‚.k/ time. Since the matrices we add each have n2=4 entries, you could
say that adding each pair takes ‚.n2=4/ time. Again, however, the ‚-notation
subsumes the constant factor of 1=4, and we say that adding two n=2 n=2

matrices takes ‚.n2/ time. We have four such matrix additions, and once again,
instead of saying that they take ‚.4n2/ time, we say that they take ‚.n2/ time.
(Of course, you might observe that we could say that the four matrix additions
take ‚.4n2=4/ time, and that 4n2=4 n2, but the point here is that ‚-notation
D
subsumes constant factors, whatever they are.) Thus, we end up with two terms
of‚.n2/,whichwecancombineintoone.
When we account for the eight recursive calls, however, we cannot just sub-
sume theconstant factor of 8. In other words, wemustsay thattogether they take
8T.n=2/time,ratherthanjustT.n=2/time. Youcangetafeelforwhybylooking
back at the recursion tree in Figure 2.5, for recurrence (2.1) (which is identical to
recurrence(4.7)),withtherecursivecaseT.n/ 2T.n=2/ ‚.n/. Thefactorof2
D C
determined how manychildren each treenode had, whichinturn determined how
many terms contributed to the sum at each level of the tree. If we were to ignore
4.2 Strassen’salgorithmformatrixmultiplication 79
thefactorof8inequation(4.16)orthefactorof2inrecurrence(4.1),therecursion
treewouldjustbelinear,ratherthan“bushy,”andeachlevelwouldcontribute only
onetermtothesum.
Bear in mind, therefore, that although asymptotic notation subsumes constant
multiplicative factors, recursivenotation suchasT.n=2/doesnot.
Strassen’smethod
ThekeytoStrassen’smethodistomaketherecursiontreeslightlylessbushy. That
is, instead of performing eight recursive multiplications of n=2 n=2 matrices,

it performs only seven. The cost of eliminating one matrix multiplication will be
several new additions of n=2 n=2 matrices, but still only a constant number of

additions. As before, the constant number of matrix additions will be subsumed
by‚-notation whenwesetuptherecurrence equation tocharacterize the running
time.
Strassen’s method is not at all obvious. (This might be the biggest understate-
mentinthisbook.) Ithasfoursteps:
1. DividetheinputmatricesAandB andoutputmatrixC inton=2 n=2subma-

trices, asinequation (4.9). Thissteptakes‚.1/timebyindexcalculation, just
asinSQUARE-MATRIX-MULTIPLY-RECURSIVE.
2. Create10matrices S ;S ;:::;S ,eachofwhichisn=2 n=2andisthesum
1 2 10

ordifferenceoftwomatricescreatedinstep1. Wecancreateall10matricesin
‚.n2/time.
3. Using the submatrices created in step 1 and the 10 matrices created in step 2,
recursively compute seven matrix products P ;P ;:::;P . Each matrix P is
1 2 7 i
n=2 n=2.

4. Compute the desired submatrices C ;C ;C ;C of the result matrix C by
11 12 21 22
adding and subtracting various combinations of the P matrices. Wecan com-
i
puteallfoursubmatrices in‚.n2/time.
We shall see the details of steps 2–4 in a moment, but we already have enough
informationtosetuparecurrencefortherunningtimeofStrassen’smethod. Letus
assumethatoncethematrixsizengetsdownto1,weperformasimplescalarmul-
tiplication, just as in line 4 of SQUARE-MATRIX-MULTIPLY-RECURSIVE. When
n > 1, steps 1, 2, and 4 take a total of ‚.n2/ time, and step 3 requires us to per-
formsevenmultiplications ofn=2 n=2matrices. Hence,weobtainthefollowing

recurrence fortherunningtimeT.n/ofStrassen’salgorithm:
‚.1/ ifn 1;
T.n/ D (4.18)
D
(
7T.n=2/ ‚.n2/ ifn > 1:
C
80 Chapter4 Divide-and-Conquer
We have traded off one matrix multiplication for a constant number of matrix ad-
ditions. Onceweunderstand recurrences andtheir solutions, weshall seethatthis
tradeoff actually leads to a lower asymptotic running time. Bythe master method
inSection4.5,recurrence (4.18)hasthesolutionT.n/ ‚.nlg7/.
D
We now proceed to describe the details. In step 2, we create the following 10
matrices:
S B B ;
1 12 22
D (cid:0)
S A A ;
2 11 12
D C
S A A ;
3 21 22
D C
S B B ;
4 21 11
D (cid:0)
S A A ;
5 11 22
D C
S B B ;
6 11 22
D C
S A A ;
7 12 22
D (cid:0)
S B B ;
8 21 22
D C
S A A ;
9 11 21
D (cid:0)
S B B :
10 11 12
D C
Sincewemustaddorsubtract n=2 n=2matrices 10times, this stepdoes indeed

take‚.n2/time.
Instep3,werecursivelymultiplyn=2 n=2matricesseventimestocomputethe

following n=2 n=2matrices, each ofwhich isthe sum ordifference ofproducts

ofAandB submatrices:
P A S A B A B ;
1 11 1 11 12 11 22
D  D  (cid:0) 
P S B A B A B ;
2 2 22 11 22 12 22
D  D  C 
P S B A B A B ;
3 3 11 21 11 22 11
D  D  C 
P A S A B A B ;
4 22 4 22 21 22 11
D  D  (cid:0) 
P S S A B A B A B A B ;
5 5 6 11 11 11 22 22 11 22 22
D  D  C  C  C 
P S S A B A B A B A B ;
6 7 8 12 21 12 22 22 21 22 22
D  D  C  (cid:0)  (cid:0) 
P S S A B A B A B A B :
7 9 10 11 11 11 12 21 11 21 12
D  D  C  (cid:0)  (cid:0) 
Notethat theonly multiplications weneed toperform arethose inthemiddle col-
umnoftheaboveequations. Theright-handcolumnjustshowswhattheseproducts
equalintermsoftheoriginal submatrices createdinstep1.
Step4adds and subtracts the P matrices created instep 3toconstruct the four
i
n=2 n=2submatrices oftheproductC. Westartwith

C P P P P :
11 5 4 2 6
D C (cid:0) C
4.2 Strassen’salgorithmformatrixmultiplication 81
Expanding out the right-hand side, with the expansion of each P on its own line
i
andverticallyaligning termsthatcancelout,weseethatC equals
11
A B A B A B A B
11 11 11 22 22 11 22 22
 C  C  C 
A B A B
22 11 22 21
(cid:0)  C 
A B A B
11 22 12 22
(cid:0)  (cid:0) 
A B A B A B A B
22 22 22 21 12 22 12 21
(cid:0)  (cid:0)  C  C 
A B A B ;
11 11 12 21
 C 
whichcorresponds toequation (4.11).
Similarly,weset
C P P ;
12 1 2
D C
andsoC equals
12
A B A B
11 12 11 22
 (cid:0) 
A B A B
11 22 12 22
C  C 
A B A B ;
11 12 12 22
 C 
corresponding toequation (4.12).
Setting
C P P
21 3 4
D C
makesC equal
21
A B A B
21 11 22 11
 C 
A B A B
22 11 22 21
(cid:0)  C 
A B A B ;
21 11 22 21
 C 
corresponding toequation (4.13).
Finally,weset
C P P P P ;
22 5 1 3 7
D C (cid:0) (cid:0)
sothatC equals
22
A B A B A B A B
11 11 11 22 22 11 22 22
 C  C  C 
A B A B
11 22 11 12
(cid:0)  C 
A B A B
22 11 21 11
(cid:0)  (cid:0) 
A B A B A B A B
11 11 11 12 21 11 21 12
(cid:0)  (cid:0)  C  C 
A B A B ;
22 22 21 12
 C 
82 Chapter4 Divide-and-Conquer
which corresponds to equation (4.14). Altogether, we add or subtract n=2 n=2

matriceseighttimesinstep4,andsothisstepindeedtakes‚.n2/time.
Thus, weseethatStrassen’s algorithm, comprising steps 1–4, produces thecor-
rectmatrixproductandthatrecurrence(4.18)characterizes itsrunningtime. Since
we shall see in Section 4.5 that this recurrence has the solution T.n/ ‚.nlg7/,
D
Strassen’s method is asymptotically faster than the straightforward SQUARE-
MATRIX-MULTIPLY procedure. Thenotes attheendofthischapter discuss some
ofthepracticalaspects ofStrassen’salgorithm.
Exercises
Note: Although Exercises 4.2-3, 4.2-4, and 4.2-5 are about variants on Strassen’s
algorithm, youshould readSection4.5beforetryingtosolvethem.
4.2-1
UseStrassen’s algorithm tocomputethematrixproduct
1 3 6 8
:
7 5 4 2
  
Showyourwork.
4.2-2
Writepseudocode forStrassen’salgorithm.
4.2-3
HowwouldyoumodifyStrassen’salgorithmtomultiplyn nmatricesinwhichn

isnotanexactpowerof2? Showthattheresultingalgorithmrunsintime‚.nlg7/.
4.2-4
What is the largest k such that if you can multiply 3 3 matrices using k multi-

plications (not assuming commutativity of multiplication), then you can multiply
n nmatricesintimeo.nlg7/? Whatwouldtherunningtimeofthisalgorithmbe?

4.2-5
V. Pan has discovered a way of multiplying 68 68 matrices using 132,464 mul-

tiplications, awayofmultiplying 70 70matrices using143,640 multiplications,

and a way of multiplying 72 72 matrices using 155,424 multiplications. Which

methodyieldsthebestasymptoticrunningtimewhenusedinadivide-and-conquer
matrix-multiplication algorithm? HowdoesitcomparetoStrassen’salgorithm?
4.3 Thesubstitutionmethodforsolvingrecurrences 83
4.2-6
Howquicklycanyoumultiplyakn nmatrixbyann knmatrix,usingStrassen’s
 
algorithm as a subroutine? Answer the same question with the order of the input
matricesreversed.
4.2-7
Show how to multiply the complex numbers a bi and c di using only three
C C
multiplications ofrealnumbers. Thealgorithm should takea,b,c,andd asinput
and produce the real component ac bd and the imaginary component ad bc
(cid:0) C
separately.
4.3 The substitution method forsolvingrecurrences
Now that wehave seen how recurrences characterize the running times of divide-
and-conquer algorithms, we will learn how to solve recurrences. We start in this
sectionwiththe“substitution” method.
Thesubstitutionmethodforsolvingrecurrences comprisestwosteps:
1. Guesstheformofthesolution.
2. Use mathematical induction to find the constants and show that the solution
works.
We substitute the guessed solution for the function when applying the inductive
hypothesis tosmaller values; hence thename “substitution method.” This method
ispowerful,butwemustbeabletoguesstheformoftheanswerinordertoapplyit.
Wecanusethesubstitution methodtoestablisheitherupperorlowerboundson
arecurrence. Asanexample,letusdetermineanupperboundontherecurrence
T.n/ 2T. n=2 / n; (4.19)
D b c C
which is similar to recurrences (4.3) and (4.4). We guess that the solution is
T.n/ O.nlgn/. The substitution method requires us to prove that T.n/
D 
cnlgn for an appropriate choice of the constant c > 0. We start by assuming
thatthisboundholdsforallpositive m < n,inparticular form n=2 ,yielding
D b c
T. n=2 / c n=2 lg. n=2 /. Substituting intotherecurrence yields
b c  b c b c
T.n/ 2.c n=2 lg. n=2 // n
 b c b c C
cnlg.n=2/ n
 C
cnlgn cnlg2 n
D (cid:0) C
cnlgn cn n
D (cid:0) C
cnlgn;

84 Chapter4 Divide-and-Conquer
wherethelaststepholdsaslongasc 1.

Mathematical induction now requires us to show that our solution holds for the
boundary conditions. Typically, we do so by showing that the boundary condi-
tions are suitable as base cases for the inductive proof. For the recurrence (4.19),
we must show that we can choose the constant c large enough so that the bound
T.n/ cnlgn works for the boundary conditions as well. This requirement

can sometimes lead to problems. Let us assume, for the sake of argument, that
T.1/ 1 is the sole boundary condition of the recurrence. Then for n 1, the
D D
bound T.n/ cnlgnyieldsT.1/ c1lg1 0,whichisatoddswithT.1/ 1.
  D D
Consequently, thebasecaseofourinductive prooffailstohold.
We can overcome this obstacle in proving an inductive hypothesis for a spe-
cific boundary condition with only a little more effort. In the recurrence (4.19),
for example, we take advantage of asymptotic notation requiring us only to prove
T.n/ cnlgn for n n , where n is a constant that we get to choose. We
0 0
 
keep the troublesome boundary condition T.1/ 1, but remove it from consid-
D
eration in the inductive proof. We do so by first observing that for n > 3, the
recurrence does not depend directly on T.1/. Thus, wecan replace T.1/ by T.2/
and T.3/ as the base cases in the inductive proof, letting n 2. Note that we
0
D
make a distinction between the base case of the recurrence (n 1) and the base
D
cases of the inductive proof (n 2 and n 3). With T.1/ 1, we derive from
D D D
the recurrence that T.2/ 4 and T.3/ 5. Now we can complete the inductive
D D
proof that T.n/ cnlgn for some constant c 1 by choosing c large enough
 
so that T.2/ c2lg2 and T.3/ c3lg3. As it turns out, any choice of c 2
  
sufficesforthebasecasesofn 2andn 3tohold. Formostoftherecurrences
D D
weshall examine, itis straightforward to extend boundary conditions to make the
inductiveassumptionworkforsmalln,andweshallnotalwaysexplicitlyworkout
thedetails.
Makingagoodguess
Unfortunately,thereisnogeneralwaytoguessthecorrectsolutionstorecurrences.
Guessing a solution takes experience and, occasionally, creativity. Fortunately,
though, you can use some heuristics to help you become a good guesser. You
can also use recursion trees, which we shall see in Section 4.4, to generate good
guesses.
If a recurrence is similar to one you have seen before, then guessing a similar
solution isreasonable. Asanexample,consider therecurrence
T.n/ 2T. n=2 17/ n;
D b cC C
whichlooks difficultbecause oftheadded “17”intheargument toT ontheright-
hand side. Intuitively, however, thisadditional termcannot substantially affectthe
4.3 Thesubstitutionmethodforsolvingrecurrences 85
solution to the recurrence. When n is large, the difference between n=2 and
b c
n=2 17 is not that large: both cut n nearly evenly in half. Consequently, we
b cC
make the guess that T.n/ O.nlgn/, which you can verify as correct by using
D
thesubstitution method(seeExercise4.3-6).
Anotherwaytomakeagoodguessistoprovelooseupperandlowerboundson
the recurrence and then reduce the range of uncertainty. For example, we might
start with a lower bound of T.n/ .n/ for the recurrence (4.19), since we
D
have the term n in the recurrence, and we can prove an initial upper bound of
T.n/ O.n2/. Then, we can gradually lower the upper bound and raise the
D
lower bound until we converge on the correct, asymptotically tight solution of
T.n/ ‚.nlgn/.
D
Subtleties
Sometimes you might correctly guess an asymptotic bound on the solution of a
recurrence, butsomehowthemathfailstoworkoutintheinduction. Theproblem
frequently turns out to be that the inductive assumption is not strong enough to
provethedetailedbound. Ifyourevisetheguessbysubtracting alower-orderterm
whenyouhitsuchasnag,themathoftengoesthrough.
Considertherecurrence
T.n/ T. n=2 / T. n=2 / 1:
D b c C d e C
WeguessthatthesolutionisT.n/ O.n/,andwetrytoshowthatT.n/ cnfor
D 
an appropriate choice of the constant c. Substituting our guess in the recurrence,
weobtain
T.n/ c n=2 c n=2 1
 b cC d eC
cn 1;
D C
whichdoesnot implyT.n/ cnforanychoice ofc. Wemightbetempted totry

alarger guess, sayT.n/ O.n2/. Although wecanmakethis larger guess work,
D
our original guess of T.n/ O.n/ is correct. In order to show that it is correct,
D
however,wemustmakeastrongerinductive hypothesis.
Intuitively, our guess is nearly right: we are off only by the constant 1, a
lower-order term. Nevertheless, mathematical induction does not work unless we
prove the exact form of the inductive hypothesis. We overcome our difficulty
by subtracting a lower-order term from our previous guess. Our new guess is
T.n/ cn d,whered 0isaconstant. Wenowhave
 (cid:0) 
T.n/ .c n=2 d/ .c n=2 d/ 1
 b c(cid:0) C d e(cid:0) C
cn 2d 1
D (cid:0) C
cn d ;
 (cid:0)
86 Chapter4 Divide-and-Conquer
aslongasd 1. Asbefore,wemustchoosetheconstantc largeenoughtohandle

theboundary conditions.
You might find the idea of subtracting a lower-order term counterintuitive. Af-
ter all, if the math does not work out, we should increase our guess, right?
Notnecessarily! When proving an upper bound by induction, it may actually be
moredifficulttoprovethataweakerupperboundholds, because inordertoprove
the weaker bound, we must use the same weaker bound inductively in the proof.
Inourcurrentexample,whentherecurrence hasmorethanonerecursiveterm,we
get to subtract out the lower-order term of the proposed bound once per recursive
term. In the above example, we subtracted out the constant d twice, once for the
T. n=2 / term and once for theT. n=2 / term. Weended up with theinequality
b c d e
T.n/ cn 2d 1,anditwaseasytofindvaluesofd tomakecn 2d 1be
 (cid:0) C (cid:0) C
lessthanorequaltocn d.
(cid:0)
Avoidingpitfalls
It is easy to err in the use of asymptotic notation. For example, in the recur-
rence (4.19) we can falsely “prove” T.n/ O.n/ by guessing T.n/ cn and
D 
thenarguing
T.n/ 2.c n=2 / n
 b c C
cn n
 C
O.n/; wrong!!
D 
since c is a constant. The error is that we have not proved the exact form of the
inductive hypothesis, that is, that T.n/ cn. We therefore will explicitly prove

thatT.n/ cnwhenwewanttoshowthatT.n/ O.n/.
 D
Changingvariables
Sometimes,alittlealgebraic manipulation canmakeanunknownrecurrence simi-
lartooneyouhaveseenbefore. Asanexample,consider therecurrence
T.n/ 2T pn lgn;
D C
which looks(cid:0)diffic˘ult. We can simplify this recurrence, though, with a change of
variables. For convenience, we shall not worry about rounding off values, such
aspn,tobeintegers. Renamingm lgnyields
D
T.2m/ 2T.2m=2/ m:
D C
WecannowrenameS.m/ T.2m/toproduce thenewrecurrence
D
S.m/ 2S.m=2/ m;
D C
4.3 Thesubstitutionmethodforsolvingrecurrences 87
which is very much like recurrence (4.19). Indeed, this new recurrence has the
samesolution: S.m/ O.mlgm/. ChangingbackfromS.m/toT.n/,weobtain
D
T.n/ T.2m/ S.m/ O.mlgm/ O.lgnlglgn/:
D D D D
Exercises
4.3-1
Showthatthesolution ofT.n/ T.n 1/ nisO.n2/.
D (cid:0) C
4.3-2
Showthatthesolution ofT.n/ T. n=2 / 1isO.lgn/.
D d e C
4.3-3
WesawthatthesolutionofT.n/ 2T. n=2 / nisO.nlgn/. Showthattheso-
D b c C
lutionofthisrecurrence isalso.nlgn/. Concludethatthesolution is‚.nlgn/.
4.3-4
Show that by making a different inductive hypothesis, we can overcome the diffi-
cultywiththeboundaryconditionT.1/ 1forrecurrence(4.19)withoutadjusting
D
theboundary conditions fortheinductiveproof.
4.3-5
Showthat‚.nlgn/isthesolution tothe“exact”recurrence (4.3)formergesort.
4.3-6
Showthatthesolution toT.n/ 2T. n=2 17/ nisO.nlgn/.
D b cC C
4.3-7
Using the master method in Section 4.5, you can show that the solution to the
recurrence T.n/ 4T.n=3/ n is T.n/ ‚.nlog34/. Show that a substitution
D C D
proof with the assumption T.n/ cnlog34 fails. Then show how to subtract off a

lower-ordertermtomakeasubstitution proofwork.
4.3-8
Using the master method in Section 4.5, you can show that the solution to the
recurrenceT.n/ 4T.n=2/ nisT.n/ ‚.n2/. Showthatasubstitution proof
D C D
withtheassumptionT.n/ cn2fails. Thenshowhowtosubtractoffalower-order

termtomakeasubstitution proofwork.
88 Chapter4 Divide-and-Conquer
4.3-9
Solve the recurrence T.n/ 3T.pn/ logn by making a change of variables.
D C
Your solution should be asymptotically tight. Do not worry about whether values
areintegral.
4.4 Therecursion-tree method forsolvingrecurrences
Although you can use the substitution method to provide a succinct proof that
a solution to a recurrence is correct, you might have trouble coming up with a
good guess. Drawing out a recursion tree, as we did in our analysis of the merge
sort recurrence in Section 2.3.2, serves as a straightforward way to devise a good
guess. In a recursion tree, each node represents the cost of a single subproblem
somewhere in the set of recursive function invocations. We sum the costs within
each level of the tree to obtain a set of per-level costs, and then we sum all the
per-level coststodeterminethetotalcostofalllevelsoftherecursion.
Arecursiontreeisbestusedtogenerateagoodguess,whichyoucanthenverify
bythesubstitution method. Whenusingarecursion treetogenerate agoodguess,
you can often tolerate a smallamount of“sloppiness,” since you will beverifying
your guess later on. Ifyouarevery careful whendrawing outarecursion tree and
summing the costs, however, you can use a recursion tree as a direct proof of a
solution to a recurrence. In this section, we will use recursion trees to generate
good guesses, and in Section 4.6, wewilluse recursion trees directly to prove the
theorem thatformsthebasisofthemastermethod.
For example, let us see how a recursion tree would provide a good guess for
the recurrence T.n/ 3T. n=4 / ‚.n2/. We start by focusing on finding an
D b c C
upperboundforthesolution. Becauseweknowthatfloorsandceilings usuallydo
not matter whensolving recurrences (here’s anexample ofsloppiness thatwecan
tolerate), we create a recursion tree for the recurrence T.n/ 3T.n=4/ cn2,
D C
having writtenouttheimpliedconstant coefficientc > 0.
Figure4.5showshowwederivetherecursion treeforT.n/ 3T.n=4/ cn2.
D C
For convenience, we assume that n is an exact power of 4 (another example of
tolerablesloppiness)sothatallsubproblemsizesareintegers. Part(a)ofthefigure
shows T.n/, which we expand in part (b) into an equivalent tree representing the
recurrence. Thecn2termattherootrepresentsthecostatthetoplevelofrecursion,
and the three subtrees of the root represent the costs incurred by the subproblems
ofsizen=4. Part(c)showsthisprocesscarriedonestepfurtherbyexpanding each
nodewithcostT.n=4/frompart(b). Thecostforeachofthethreechildren ofthe
root is c.n=4/2. We continue expanding each node in the tree by breaking it into
itsconstituent partsasdeterminedbytherecurrence.
4.4 Therecursion-treemethodforsolvingrecurrences 89
…
…
T.n/ cn2 cn2
T n T n T n c n 2 c n 2 c n 2
4 4 4 4 4 4
(cid:0)  (cid:0)  (cid:0)  (cid:0)  (cid:0)  (cid:0) 
T n T n T n T n T n T n T n T n T n
16 16 16 16 16 16 16 16 16
(cid:0)  (cid:0)  (cid:0)  (cid:0)  (cid:0)  (cid:0)  (cid:0)  (cid:0)  (cid:0) 
(a) (b) (c)
cn2 cn2
c n 2 c n 2 c n 2 3 cn2
4 4 4 16
(cid:0)  (cid:0)  (cid:0) 
log n
4
c n 2 c n 2 c n 2 c n 2 c n 2 c n 2 c n 2 c n 2 c n 2 3 2 cn2
16 16 16 16 16 16 16 16 16 16
(cid:0)  (cid:0)  (cid:0)  (cid:0)  (cid:0)  (cid:0)  (cid:0)  (cid:0)  (cid:0)  (cid:0) 
T.1/ T.1/ T.1/ T.1/ T.1/ T.1/ T.1/ T.1/ T.1/ T.1/ T.1/ T.1/ T.1/ ‚.nlog43/
nlog43
Total:O.n2/
(d)
Figure 4.5 Constructing a recursion tree for the recurrence T.n/ 3T.n=4/ cn2. Part (a)
D C
showsT.n/,whichprogressivelyexpandsin(b)–(d)toformtherecursiontree. Thefullyexpanded
treeinpart(d)hasheightlog4n(ithaslog4n C1levels).
90 Chapter4 Divide-and-Conquer
Because subproblem sizes decrease by a factor of 4 each time wego down one
level, we eventually must reach a boundary condition. How far from the root do
we reach one? The subproblem size for a node at depth i is n=4i. Thus, the
subproblem size hits n 1 when n=4i 1 or, equivalently, when i log n.
D D D 4
Thus,thetreehaslog n 1levels(atdepths0;1;2;:::;log n).
4 C 4
Next wedetermine the cost at each level of the tree. Each level has three times
more nodes than the level above, and so the number of nodes at depth i is 3i.
Because subproblem sizes reduce by a factor of 4 for each level we go down
from the root, each node at depth i, for i 0;1;2;:::;log n 1, has a cost
D 4 (cid:0)
of c.n=4i/2. Multiplying, we see that the total cost over all nodes at depth i, for
i 0;1;2;:::;log n 1, is 3ic.n=4i/2 .3=16/icn2. The bottom level, at
D 4 (cid:0) D
depth log n, has 3log4n nlog43 nodes, each contributing cost T.1/, for a total
4 D
costofnlog43T.1/,whichis‚.nlog43/,sinceweassumethatT.1/isaconstant.
Nowweaddupthecostsoveralllevelstodeterminethecostfortheentiretree:
3 3 2 3 log4n 1
T.n/ cn2 cn2 cn2 (cid:0) cn2 ‚.nlog43/
D C 16 C 16 CC 16 C
   
log4n (cid:0)1 3 i
cn2 ‚.nlog43/
D 16 C
i 0  
XD
.3=16/log4n 1
(cid:0) cn2 ‚.nlog43/ (byequation (A.5)):
D .3=16/ 1 C
(cid:0)
This last formula looks somewhat messy until we realize that we can again take
advantageofsmallamountsofsloppinessanduseaninfinitedecreasing geometric
series as an upper bound. Backing up one step and applying equation (A.6), we
have
log4n (cid:0)1 3 i
T.n/ cn2 ‚.nlog43/
D 16 C
i 0  
XD
1 3 i
< cn2 ‚.nlog43/
16 C
i 0 
XD
1
cn2 ‚.nlog43/
D 1 .3=16/ C
(cid:0)
16
cn2 ‚.nlog43/
D 13 C
O.n2/:
D
Thus, we have derived a guess of T.n/ O.n2/ for our original recurrence
D
T.n/ 3T. n=4 / ‚.n2/. In this example, the coefficients of cn2 form a
D b c C
decreasing geometric series and, by equation (A.6), the sum of these coefficients
4.4 Therecursion-treemethodforsolvingrecurrences 91
… …
cn cn
c n c 2n cn
3 3
(cid:0)  (cid:0) 
log n
3=2
c n c 2n c 2n c 4n cn
9 9 9 9
(cid:0)  (cid:0)  (cid:0)  (cid:0) 
Total:O.nlgn/
Figure4.6 ArecursiontreefortherecurrenceT.n/ T.n=3/ T.2n=3/ cn.
D C C
isbounded from above by the constant 16=13. Sincethe root’s contribution tothe
total cost is cn2, the root contributes a constant fraction of the total cost. In other
words,thecostoftherootdominatesthetotalcostofthetree.
Infact,ifO.n2/isindeedanupperboundfortherecurrence(asweshallverifyin
amoment),thenitmustbeatightbound. Why? Thefirstrecursivecallcontributes
acostof‚.n2/,andso.n2/mustbealowerboundfortherecurrence.
Now we can use the substitution method to verify that our guess was cor-
rect, that is, T.n/ O.n2/ is an upper bound for the recurrence T.n/
D D
3T. n=4 / ‚.n2/. WewanttoshowthatT.n/ dn2 forsomeconstantd >0.
b c C 
Usingthesameconstant c > 0asbefore, wehave
T.n/ 3T. n=4 / cn2
 b c C
3d n=4 2 cn2
 b c C
3d.n=4/2 cn2
 C
3
dn2 cn2
D 16 C
dn2 ;

wherethelaststepholdsaslongasd .16=13/c.

Inanother, moreintricate, example, Figure4.6showstherecursion treefor
T.n/ T.n=3/ T.2n=3/ O.n/:
D C C
(Again, we omit floor and ceiling functions for simplicity.) As before, we let c
represent theconstant factor intheO.n/term. Whenweaddthevaluesacrossthe
levelsoftherecursiontreeshowninthefigure,wegetavalueofcnforeverylevel.
92 Chapter4 Divide-and-Conquer
The longest simple path from the root to a leaf is n .2=3/n .2=3/2n
! ! !
1. Since.2=3/kn 1whenk log n,theheightofthetreeislog n.
 ! D D 3=2 3=2
Intuitively, we expect the solution to the recurrence to be at most the number
of levels times the cost of each level, or O.cnlog n/ O.nlgn/. Figure 4.6
3=2 D
showsonlythetoplevelsoftherecursion tree, however, andnoteverylevelinthe
tree contributes acost ofcn. Consider thecost ofthe leaves. Ifthis recursion tree
were a complete binary tree of height log n, there would be 2log3=2n nlog3=22
3=2 D
leaves. Since the cost of each leaf is a constant, the total cost of all leaves would
then be ‚.nlog3=22/ which, since log 2 is a constant strictly greater than 1,
3=2
is !.nlgn/. This recursion tree is not a complete binary tree, however, and so
it has fewer than nlog3=22 leaves. Moreover, as we go down from the root, more
andmoreinternal nodesareabsent. Consequently, levelstowardthebottom ofthe
recursion treecontribute lessthancntothetotalcost. Wecouldworkoutanaccu-
rateaccountingofallcosts,butrememberthatwearejusttryingtocomeupwitha
guess touseinthe substitution method. Letustolerate the sloppiness andattempt
toshowthataguessofO.nlgn/fortheupperboundiscorrect.
Indeed, we can use the substitution method to verify that O.nlgn/ is an upper
boundforthesolutiontotherecurrence. WeshowthatT.n/ dnlgn,whered is

asuitable positiveconstant. Wehave
T.n/ T.n=3/ T.2n=3/ cn
 C C
d.n=3/lg.n=3/ d.2n=3/lg.2n=3/ cn
 C C
.d.n=3/lgn d.n=3/lg3/
D (cid:0)
.d.2n=3/lgn d.2n=3/lg.3=2// cn
C (cid:0) C
dnlgn d..n=3/lg3 .2n=3/lg.3=2// cn
D (cid:0) C C
dnlgn d..n=3/lg3 .2n=3/lg3 .2n=3/lg2/ cn
D (cid:0) C (cid:0) C
dnlgn dn.lg3 2=3/ cn
D (cid:0) (cid:0) C
dnlgn;

aslongasd c=.lg3 .2=3//. Thus,wedidnotneedtoperformamoreaccurate
 (cid:0)
accounting ofcostsintherecursion tree.
Exercises
4.4-1
Usearecursiontreetodetermineagoodasymptoticupperboundontherecurrence
T.n/ 3T. n=2 / n. Usethesubstitution methodtoverifyyouranswer.
D b c C
4.4-2
Usearecursiontreetodetermineagoodasymptoticupperboundontherecurrence
T.n/ T.n=2/ n2. Usethesubstitution methodtoverifyyouranswer.
D C
4.5 Themastermethodforsolvingrecurrences 93
4.4-3
Usearecursiontreetodetermineagoodasymptoticupperboundontherecurrence
T.n/ 4T.n=2 2/ n. Usethesubstitution methodtoverifyyouranswer.
D C C
4.4-4
Usearecursiontreetodetermineagoodasymptoticupperboundontherecurrence
T.n/ 2T.n 1/ 1. Usethesubstitution methodtoverifyyouranswer.
D (cid:0) C
4.4-5
Usearecursiontreetodetermineagoodasymptoticupperboundontherecurrence
T.n/ T.n 1/ T.n=2/ n. Usethesubstitutionmethodtoverifyyouranswer.
D (cid:0) C C
4.4-6
ArguethatthesolutiontotherecurrenceT.n/ T.n=3/ T.2n=3/ cn,wherec
D C C
isaconstant, is.nlgn/byappealing toarecursion tree.
4.4-7
Draw the recursion tree for T.n/ 4T. n=2 / cn, where c is a constant, and
D b c C
provide atightasymptotic bound onitssolution. Verifyyour bound bythesubsti-
tutionmethod.
4.4-8
Use a recursion tree to give an asymptotically tight solution to the recurrence
T.n/ T.n a/ T.a/ cn,wherea 1andc > 0areconstants.
D (cid:0) C C 
4.4-9
Use a recursion tree to give an asymptotically tight solution to the recurrence
T.n/ T.˛n/ T..1 ˛/n/ cn,where˛ isaconstantintherange0 <˛ < 1
D C (cid:0) C
andc >0isalsoaconstant.
4.5 The mastermethod forsolvingrecurrences
The master method provides a “cookbook” method for solving recurrences of the
form
T.n/ aT.n=b/ f.n/; (4.20)
D C
where a 1 and b > 1 are constants and f.n/ is an asymptotically positive

function. To use the master method, you will need to memorize three cases, but
then you will be able to solve many recurrences quite easily, often without pencil
andpaper.
94 Chapter4 Divide-and-Conquer
The recurrence (4.20) describes the running time of an algorithm that divides a
problemofsizenintoasubproblems, eachofsizen=b,whereaandb arepositive
constants. The a subproblems are solved recursively, each in time T.n=b/. The
function f.n/ encompasses the cost of dividing the problem and combining the
results of the subproblems. For example, the recurrence arising from Strassen’s
algorithm hasa 7,b 2,andf.n/ ‚.n2/.
D D D
Asamatteroftechnical correctness, therecurrence isnotactually welldefined,
because n=b might not be an integer. Replacing each of the a terms T.n=b/ with
either T. n=b /orT. n=b /willnotaffect theasymptotic behavior ofthe recur-
b c d e
rence, however. (We will prove this assertion in the next section.) We normally
find it convenient, therefore, to omit the floor and ceiling functions when writing
divide-and-conquer recurrences ofthisform.
Themastertheorem
Themastermethoddepends onthefollowingtheorem.
Theorem4.1(Mastertheorem)
Leta 1andb > 1beconstants, letf.n/beafunction, andletT.n/ bedefined

onthenonnegativeintegers bytherecurrence
T.n/ aT.n=b/ f.n/;
D C
whereweinterpretn=b tomeaneither n=b or n=b . ThenT.n/hasthefollow-
b c d e
ingasymptotic bounds:
1. Iff.n/ O.nlogba /forsomeconstant  > 0,thenT.n/ ‚.nlogba/.
(cid:0)
D D
2. Iff.n/ ‚.nlogba/,thenT.n/ ‚.nlogbalgn/.
D D
3. If f.n/ .nlogba / for some constant  > 0, and if af.n=b/ cf.n/ for
C
D 
someconstant c < 1andallsufficientlylargen,thenT.n/ ‚.f.n//.
D
Before applying the master theorem to some examples, let’s spend a moment
trying to understand what it says. In each of the three cases, we compare the
function f.n/with the function nlogba. Intuitively, the larger of the two functions
determines thesolutiontotherecurrence. If,asincase1,thefunctionnlogba isthe
larger, then the solution is T.n/ ‚.nlogba/. If, as in case 3, the function f.n/
D
is the larger, then the solution is T.n/ ‚.f.n//. If, as in case 2, the two func-
D
tions are the same size, we multiply by a logarithmic factor, and the solution is
T.n/ ‚.nlogbalgn/ ‚.f.n/lgn/.
D D
Beyond this intuition, you need to be aware of some technicalities. In the first
case, not only must f.n/ be smaller than nlogba, it must be polynomially smaller.
4.5 Themastermethodforsolvingrecurrences 95
Thatis,f.n/mustbeasymptoticallysmallerthannlogba byafactorofn forsome
constant  > 0. In the third case, not only must f.n/be larger than nlogba, italso
mustbepolynomially larger and inaddition satisfy the “regularity” condition that
af.n=b/ cf.n/. Thisconditionissatisfiedbymostofthepolynomiallybounded

functions thatweshallencounter.
Note that the three cases do not cover all the possibilities for f.n/. There is
a gap between cases 1 and 2 when f.n/ is smaller than nlogba but not polynomi-
ally smaller. Similarly, there is a gap between cases 2 and 3 when f.n/ is larger
thannlogba butnotpolynomially larger. Ifthefunction f.n/fallsintooneofthese
gaps,oriftheregularityconditionincase3failstohold,youcannotusethemaster
methodtosolvetherecurrence.
Usingthemastermethod
Touse the master method, we simply determine which case (if any) of the master
theoremapplies andwritedowntheanswer.
Asafirstexample,consider
T.n/ 9T.n=3/ n:
D C
For this recurrence, we have a 9, b 3, f.n/ n, and thus we have that
D D D
nlogba nlog39 ‚.n2). Sincef.n/ O.nlog39 /, where 1, wecan apply
(cid:0)
D D D D
case1ofthemastertheorem andconclude thatthesolution isT.n/ ‚.n2/.
D
Nowconsider
T.n/ T.2n=3/ 1;
D C
in which a 1, b 3=2, f.n/ 1, and nlogba nlog3=21 n0 1. Case 2
D D D D D D
applies, since f.n/ ‚.nlogba/ ‚.1/, and thus the solution to the recurrence
D D
isT.n/ ‚.lgn/.
D
Fortherecurrence
T.n/ 3T.n=4/ nlgn;
D C
we have a 3, b 4, f.n/ nlgn, and nlogba nlog43 O.n0:793/.
D D D D D
Since f.n/ .nlog43 /, where  0:2, case 3 applies if we can show that
C
D 
the regularity condition holds for f.n/. For sufficiently large n, we have that
af.n=b/ 3.n=4/lg.n=4/ .3=4/nlgn cf.n/ for c 3=4. Consequently,
D  D D
bycase3,thesolution totherecurrence isT.n/ ‚.nlgn/.
D
Themastermethoddoesnotapplytotherecurrence
T.n/ 2T.n=2/ nlgn;
D C
even though it appears to have the proper form: a 2, b 2, f.n/ nlgn,
D D D
and nlogba n. You might mistakenly think that case 3 should apply, since
D
96 Chapter4 Divide-and-Conquer
f.n/ nlgn is asymptotically larger than nlogba n. The problem is that it
D D
is not polynomially larger. The ratio f.n/=nlogba .nlgn/=n lgn is asymp-
D D
toticallylessthann foranypositiveconstant. Consequently, therecurrencefalls
intothegapbetweencase2andcase3. (SeeExercise4.6-2forasolution.)
Let’s use the master method to solve the recurrences we saw in Sections 4.1
and4.2. Recurrence(4.7),
T.n/ 2T.n=2/ ‚.n/;
D C
characterizes the running times of the divide-and-conquer algorithm for both the
maximum-subarray problem and merge sort. (As is our practice, we omit stating
the base case intherecurrence.) Here, wehavea 2, b 2, f.n/ ‚.n/,and
D D D
thuswehavethatnlogba nlog22 n. Case2applies, sincef.n/ ‚.n/,andso
D D D
wehavethesolution T.n/ ‚.nlgn/.
D
Recurrence (4.17),
T.n/ 8T.n=2/ ‚.n2/;
D C
describes the running time of the first divide-and-conquer algorithm that we saw
for matrix multiplication. Now we have a 8, b 2, and f.n/ ‚.n2/,
D D D
and so nlogba nlog28 n3. Since n3 is polynomially larger than f.n/ (that is,
D D
f.n/ O.n3 /for 1),case1applies, andT.n/ ‚.n3/.
(cid:0)
D D D
Finally,consider recurrence (4.18),
T.n/ 7T.n=2/ ‚.n2/;
D C
which describes the running time of Strassen’s algorithm. Here, we have a 7,
D
b 2, f.n/ ‚.n2/, and thus nlogba nlog27. Rewriting log 7 as lg7 and
D D D 2
recalling that 2:80 < lg7 < 2:81, we see that f.n/ O.nlg7 / for  0:8.
(cid:0)
D D
Again,case1applies, andwehavethesolutionT.n/ ‚.nlg7/.
D
Exercises
4.5-1
Use the master method to give tight asymptotic bounds for the following recur-
rences.
a. T.n/ 2T.n=4/ 1.
D C
b. T.n/ 2T.n=4/ pn.
D C
c. T.n/ 2T.n=4/ n.
D C
d. T.n/ 2T.n=4/ n2.
D C
4.6 Proofofthemastertheorem 97
4.5-2
Professor Caesar wishes to develop a matrix-multiplication algorithm that is
asymptotically faster thanStrassen’s algorithm. Hisalgorithm willusethedivide-
and-conquer method, dividing each matrix into pieces of size n=4 n=4, and the

divide and combine steps together will take ‚.n2/ time. He needs to determine
howmanysubproblemshisalgorithmhastocreateinordertobeatStrassen’salgo-
rithm. Ifhis algorithm creates a subproblems, then the recurrence for the running
time T.n/ becomes T.n/ aT.n=4/ ‚.n2/. What is the largest integer value
D C
of a for which Professor Caesar’s algorithm would be asymptotically faster than
Strassen’salgorithm?
4.5-3
Use the master method to show that the solution to the binary-search recurrence
T.n/ T.n=2/ ‚.1/isT.n/ ‚.lgn/. (SeeExercise 2.3-5foradescription
D C D
ofbinarysearch.)
4.5-4
Can the master method be applied to the recurrence T.n/ 4T.n=2/ n2lgn?
D C
Whyorwhynot? Giveanasymptotic upperboundforthisrecurrence.
4.5-5 ?
Consider the regularity condition af.n=b/ cf.n/ for some constant c <1,

whichispartofcase3ofthemastertheorem. Giveanexampleofconstantsa 1

and b > 1 and a function f.n/ that satisfies all the conditions in case 3 of the
mastertheoremexcepttheregularity condition.
? 4.6 Proofofthemastertheorem
This section contains a proof of the master theorem (Theorem 4.1). You do not
needtounderstand theproofinordertoapplythemastertheorem.
The proof appears in two parts. The first part analyzes the master recur-
rence (4.20), under the simplifying assumption that T.n/ is defined only on ex-
act powers of b >1, that is, for n 1;b;b2;:::. This part gives all the intuition
D
needed tounderstand whythemastertheorem istrue. Thesecond partshowshow
to extend the analysis to all positive integers n; it applies mathematical technique
totheproblem ofhandling floorsandceilings.
In this section, we shall sometimes abuse our asymptotic notation slightly by
using it to describe the behavior of functions that are defined only over exact
powers of b. Recall that the definitions of asymptotic notations require that
98 Chapter4 Divide-and-Conquer
bounds be proved for all sufficiently large numbers, not just those that are pow-
ersofb. Sincewecould makenewasymptotic notations that applyonly totheset
bi i 0;1;2;::: ,instead oftothenonnegative numbers,thisabuseisminor.
f W D g
Nevertheless,wemustalwaysbeonguardwhenweuseasymptoticnotationover
a limited domain lest we draw improper conclusions. For example, proving that
T.n/ O.n/whennisanexactpowerof2doesnotguaranteethatT.n/ O.n/.
D D
Thefunction T.n/couldbedefinedas
n ifn 1;2;4;8;::: ;
T.n/ D
D
(
n2 otherwise;
inwhichcasethebestupperboundthatappliestoallvaluesofnisT.n/ O.n2/.
D
Becauseofthissortofdrasticconsequence, weshallneveruseasymptoticnotation
overalimited domain withoutmaking itabsolutely clear from thecontext thatwe
aredoingso.
4.6.1 Theproofforexactpowers
Thefirstpartoftheproofofthemastertheorem analyzestherecurrence (4.20)
T.n/ aT.n=b/ f.n/;
D C
for the master method, under the assumption that n is an exact power of b > 1,
whereb neednotbeaninteger. Webreaktheanalysis intothreelemmas. Thefirst
reduces theproblem ofsolving themasterrecurrence totheproblem ofevaluating
an expression that contains a summation. The second determines bounds on this
summation. The third lemma puts the first two together to prove a version of the
mastertheorem forthecaseinwhichnisanexactpowerofb.
Lemma4.2
Leta 1andb > 1beconstants, andletf.n/beanonnegative function defined

onexactpowersofb. DefineT.n/onexactpowersofb bytherecurrence
‚.1/ ifn 1;
T.n/ D
D
(
aT.n=b/ f.n/ ifn bi ;
C D
wherei isapositiveinteger. Then
logbn 1
(cid:0)
T.n/ ‚.nlogba/ ajf.n=bj/: (4.21)
D C
j 0
XD
Proof WeusetherecursiontreeinFigure4.7. Therootofthetreehascostf.n/,
andithasachildren,eachwithcostf.n=b/. (Itisconvenienttothinkofaasbeing
4.6 Proofofthemastertheorem 99
…
… … …
… … … … … … … … …
…
…
f.n/ f.n/
a
f.n=b/ f.n=b/ f.n=b/ af.n=b/
a a a
log n
b
f.n=b2/f.n=b2/ f.n=b2/ f.n=b2/f.n=b2/ f.n=b2/ f.n=b2/f.n=b2/ f.n=b2/ a2f.n=b2/
a a a a a a a a a
‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.nlogba/
nlogba
logbn (cid:0)1
Total:‚.nlogba/ ajf.n=bj/
C
j 0
XD
Figure4.7 TherecursiontreegeneratedbyT.n/ aT.n=b/ f.n/.Thetreeisacompletea-ary
treewithnlogba leavesandheightlogbn. ThecostD ofthenodeC sateachdepthisshownattheright,
andtheirsumisgiveninequation(4.21).
aninteger,especiallywhenvisualizingtherecursiontree,butthemathematicsdoes
notrequire it.) Eachofthese children hasa children, making a2 nodes atdepth 2,
and each of the a children has cost f.n=b2/. In general, there are aj nodes at
depth j, and each has cost f.n=bj/. The cost of each leaf is T.1/ ‚.1/, and
D
each leaf is at depth log n, since n=blogbn 1. There are alogbn nlogba leaves
b D D
inthetree.
Wecanobtain equation (4.21) bysumming thecosts ofthenodes ateach depth
in the tree, as shown in the figure. The cost for all internal nodes at depth j is
ajf.n=bj/,andsothetotalcostofallinternal nodesis
logbn 1
(cid:0)
ajf.n=bj/:
j 0
XD
In the underlying divide-and-conquer algorithm, this sum represents the costs of
dividing problems into subproblems and then recombining the subproblems. The
100 Chapter4 Divide-and-Conquer
cost of all the leaves, which is the cost of doing all nlogba subproblems of size 1,
is‚.nlogba/.
In termsofthe recursion tree, the three cases ofthe master theorem correspond
to cases in which the total cost of the tree is (1) dominated by the costs in the
leaves, (2)evenly distributed amongthelevelsofthetree,or(3)dominated bythe
costoftheroot.
The summation in equation (4.21) describes the cost of the dividing and com-
biningstepsintheunderlyingdivide-and-conquer algorithm. Thenextlemmapro-
videsasymptotic boundsonthesummation’s growth.
Lemma4.3
Leta 1andb > 1beconstants, andletf.n/beanonnegative function defined

onexactpowersofb. Afunctiong.n/definedoverexactpowersofb by
logbn 1
(cid:0)
g.n/ ajf.n=bj/ (4.22)
D
j 0
XD
hasthefollowingasymptotic boundsforexactpowersofb:
1. Iff.n/ O.nlogba /forsomeconstant  > 0,theng.n/ O.nlogba/.
(cid:0)
D D
2. Iff.n/ ‚.nlogba/,theng.n/ ‚.nlogbalgn/.
D D
3. If af.n=b/ cf.n/ for some constant c < 1 and for all sufficiently large n,

theng.n/ ‚.f.n//.
D
Proof For case 1, wehave f.n/ O.nlogba /, which implies that f.n=bj/
(cid:0)
D D
O..n=bj/logba /. Substituting intoequation (4.22)yields
(cid:0)
g.n/ O
logbn (cid:0)1
aj
n logba (cid:0)
: (4.23)
D bj
!
j XD0  
WeboundthesummationwithintheO-notationbyfactoringouttermsandsimpli-
fying, whichleavesanincreasing geometricseries:
logbn (cid:0)1
aj
n logba (cid:0)
nlogba
logbn (cid:0)1 ab j
(cid:0)
bj D blogba
j XD0   j XD0  
logbn 1
(cid:0)
nlogba  .b/j
(cid:0)
D
j 0
XD
blogbn 1
nlogba  (cid:0)
(cid:0)
D b 1
 (cid:0) 
4.6 Proofofthemastertheorem 101
n 1
nlogba  (cid:0) :
(cid:0)
D b 1
 (cid:0) 
Sinceb and areconstants, wecanrewritethelastexpressionasnlogba O.n/
(cid:0)
D
O.nlogba/. Substitutingthisexpressionforthesummationinequation(4.23)yields
g.n/ O.nlogba/;
D
therebyprovingcase1.
Because case 2 assumes that f.n/ ‚.nlogba/, we have that f.n=bj/
D D
‚..n=bj/logba/. Substituting intoequation (4.22)yields
g.n/ ‚
logbn (cid:0)1
aj
n logba
: (4.24)
D bj
!
j XD0  
Weboundthesummationwithinthe‚-notationasincase1,butthistimewedonot
obtain a geometric series. Instead, we discover that every term of the summation
isthesame:
logbn (cid:0)1
aj
n logba
nlogbalogbn (cid:0)1
a j
bj D blogba
j XD0   j XD0  
logbn 1
(cid:0)
nlogba 1
D
j 0
XD
nlogbalog n:
D b
Substituting thisexpression forthesummationinequation (4.24)yields
g.n/ ‚.nlogbalog n/
D b
‚.nlogbalgn/;
D
provingcase2.
We prove case 3 similarly. Since f.n/ appears in the definition (4.22) of g.n/
and all terms of g.n/ are nonnegative, we can conclude that g.n/ .f.n// for
D
exactpowersofb. Weassumeinthestatementofthelemmathataf.n=b/ cf.n/

for some constant c < 1 and all sufficiently large n. We rewrite this assumption
as f.n=b/ .c=a/f.n/and iterate j times, yielding f.n=bj/ .c=a/jf.n/or,
 
equivalently,ajf.n=bj/ cjf.n/,whereweassumethatthevaluesweiterateon

are sufficiently large. This inequality holds for all but at most a constant number
oftermswiththesmallestsuchvaluesn=bj,forwhichajf.n=bj/ O.1/.
D
Substituting into equation (4.22) and simplifying yields a geometric series, but
unlike the series incase 1, this onehas decreasing terms. Weusean O.1/ term to
102 Chapter4 Divide-and-Conquer
capturethetermsthatarenotcoveredbyourassumptionthatnissufficientlylarge:
logbn 1
(cid:0)
g.n/ ajf.n=bj/
D
j 0
XD
logbn 1
(cid:0)
cjf.n/ O.1/
 C
j 0
XD
1
f.n/ cj O.1/
 C
j 0
XD
1
f.n/ O.1/
D 1 c C
 (cid:0) 
O.f.n//;
D
sincecisaconstant. Thus,wecanconcludethatg.n/ ‚.f.n//forexactpowers
D
ofb. Withcase3proved,theproofofthelemmaiscomplete.
Wecannowproveaversionofthemastertheorem forthecaseinwhichnisan
exactpowerofb.
Lemma4.4
Leta 1andb > 1beconstants, andletf.n/beanonnegative function defined

onexactpowersofb. DefineT.n/onexactpowersofb bytherecurrence
‚.1/ ifn 1;
T.n/ D
D
(
aT.n=b/ f.n/ ifn bi ;
C D
where i is a positive integer. Then T.n/ has the following asymptotic bounds for
exactpowersofb:
1. Iff.n/ O.nlogba /forsomeconstant  > 0,thenT.n/ ‚.nlogba/.
(cid:0)
D D
2. Iff.n/ ‚.nlogba/,thenT.n/ ‚.nlogbalgn/.
D D
3. If f.n/ .nlogba / for some constant  > 0, and if af.n=b/ cf.n/ for
C
D 
someconstant c < 1andallsufficientlylargen,thenT.n/ ‚.f.n//.
D
Proof We use the bounds in Lemma 4.3 to evaluate the summation (4.21) from
Lemma4.2. Forcase1,wehave
T.n/ ‚.nlogba/ O.nlogba/
D C
‚.nlogba/;
D
4.6 Proofofthemastertheorem 103
andforcase2,
T.n/ ‚.nlogba/ ‚.nlogbalgn/
D C
‚.nlogbalgn/:
D
Forcase3,
T.n/ ‚.nlogba/ ‚.f.n//
D C
‚.f.n//;
D
becausef.n/ .nlogba /.
C
D
4.6.2 Floorsandceilings
Tocomplete the proof ofthe mastertheorem, wemustnow extend our analysis to
the situation in which floors and ceilings appear in the master recurrence, so that
therecurrence isdefinedforallintegers, notforjustexact powersofb. Obtaining
alowerboundon
T.n/ aT. n=b / f.n/ (4.25)
D d e C
andanupperboundon
T.n/ aT. n=b / f.n/ (4.26)
D b c C
is routine, since we can push through the bound n=b n=b in the first case to
d e 
yield the desired result, and we can push through the bound n=b n=b in the
b c 
secondcase. Weusemuchthesametechniquetolower-boundtherecurrence(4.26)
as to upper-bound the recurrence (4.25), and so we shall present only this latter
bound.
Wemodify therecursion treeofFigure4.7toproduce therecursion treeinFig-
ure 4.8. As we go down in the recursion tree, we obtain a sequence of recursive
invocations onthearguments
n;
n=b ;
d e
n=b =b ;
dd e e
n=b =b =b ;
ddd e e e
:
:
:
Letusdenotethejthelementinthesequence byn ,where
j
n ifj 0;
n D (4.27)
j
D
(
n
j
1=b ifj > 0:
d (cid:0) e
104 Chapter4 Divide-and-Conquer
…
… … …
… … … … … … … … …
…
…
f.n/ f.n/
a
f.n / f.n / f.n / af.n /
1 1 1 1
a a a
log n
b b c
f.n / f.n / f.n / f.n / f.n / f.n / f.n / f.n / f.n / a2f.n /
2 2 2 2 2 2 2 2 2 2
a a a a a a a a a
‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.1/ ‚.nlogba/
‚.nlogba/
blogbn c(cid:0)1
Total:‚.nlogba/
C
ajf.n j/
j 0
XD
Figure4.8 TherecursiontreegeneratedbyT.n/ aT. n=b / f.n/.Therecursiveargumentn j
D d e C
isgivenbyequation(4.27).
Our first goal is to determine the depth k such that n is a constant. Using the
k
inequality x x 1,weobtain
d e C
n n;
0
 n
n 1;
1
 b C
n 1
n 1;
2  b2 C b C
n 1 1
n 1;
3  b3 C b2 C b C
:
:
:
Ingeneral, wehave
4.6 Proofofthemastertheorem 105
j 1
n (cid:0) 1
n
j  bj C bi
i 0
XD
n 1 1
<
bj C bi
i 0
XD
n b
:
D bj C b 1
(cid:0)
Lettingj log n ,weobtain
D b b c
n b
n <
blogbn c b blogbn c C b (cid:0)1
n b
<
blogbn 1 C b 1
(cid:0) (cid:0)
n b
D n=b C b 1
(cid:0)
b
b
D C b 1
(cid:0)
O.1/;
D
andthusweseethatatdepth log n ,theproblem sizeisatmostaconstant.
b b c
FromFigure4.8,weseethat
logbn 1
b c(cid:0)
T.n/ ‚.nlogba/ ajf.n /; (4.28)
j
D C
j 0
XD
whichismuchthesameasequation(4.21),exceptthatnisanarbitraryintegerand
notrestricted tobeanexactpowerofb.
Wecannowevaluatethesummation
logbn 1
b c(cid:0)
g.n/ ajf.n / (4.29)
j
D
j 0
XD
fromequation(4.28)inamanneranalogoustotheproofofLemma4.3. Beginning
withcase3,ifaf. n=b / cf.n/forn > b b=.b 1/,wherec < 1isaconstant,
d e  C (cid:0)
then it follows that ajf.n / cjf.n/. Therefore, we can evaluate the sum in
j

equation(4.29)justasinLemma4.3. Forcase2,wehavef.n/ ‚.nlogba/. Ifwe
D
canshowthatf.n j/ O.nlogba=aj/ O..n=bj/logba/,thentheproofforcase2
D D
ofLemma4.3willgothrough. Observethatj log n impliesbj=n 1. The
 b b c 
boundf.n/ O.nlogba/impliesthatthereexistsaconstantc > 0suchthatforall
D
sufficientlylargen ,
j
106 Chapter4 Divide-and-Conquer
n b logba
f.n / c
j  bj C b 1
 (cid:0) 
n bj b logba
c 1
D bj C n  b 1
  (cid:0) 
nlogba bj b logba
c 1
D aj C n  b 1
   (cid:0) 
nlogba b logba
c 1
 aj C b 1
  (cid:0) 
nlogba
O ;
D aj
 
since c.1 b=.b 1//logba is a constant. Thus, we have proved case 2. The
C (cid:0)
proof of case 1 is almost identical. The key is to prove the bound f.n /
j
D
O..n=bj/logba /, which is similar to the corresponding proof of case 2, though
(cid:0)
thealgebraismoreintricate.
Wehave now proved theupper bounds inthe master theorem for allintegers n.
Theproofofthelowerboundsissimilar.
Exercises
4.6-1 ?
Giveasimpleandexactexpressionforn inequation(4.27)forthecaseinwhichb
j
isapositiveintegerinsteadofanarbitrary realnumber.
4.6-2 ?
Showthatiff.n/ ‚.nlogbalgkn/,wherek 0,thenthemasterrecurrence has
D 
solution T.n/ ‚.nlogbalgk C1n/. For simplicity, confine your analysis to exact
D
powersofb.
4.6-3 ?
Showthatcase3ofthemastertheoremisoverstated,inthesensethattheregularity
condition af.n=b/ cf.n/ for some constant c < 1 implies that there exists a

constant  >0suchthatf.n/ .nlogba /.
C
D
ProblemsforChapter4 107
Problems
4-1 Recurrenceexamples
Giveasymptotic upper and lower bounds for T.n/ in each of the following recur-
rences. Assume that T.n/ is constant for n 2. Make your bounds as tight as

possible, andjustifyyouranswers.
a. T.n/ 2T.n=2/ n4.
D C
b. T.n/ T.7n=10/ n.
D C
c. T.n/ 16T.n=4/ n2.
D C
d. T.n/ 7T.n=3/ n2.
D C
e. T.n/ 7T.n=2/ n2.
D C
f. T.n/ 2T.n=4/ pn.
D C
g. T.n/ T.n 2/ n2.
D (cid:0) C
4-2 Parameter-passing costs
Throughout this book, we assume that parameter passing during procedure calls
takes constant time, even if an N-element array is being passed. This assumption
isvalidinmostsystemsbecauseapointertothearrayispassed,notthearrayitself.
Thisproblem examinestheimplications ofthreeparameter-passing strategies:
1. Anarrayispassedbypointer. Time ‚.1/.
D
2. Anarrayispassedbycopying. Time ‚.N/,whereN isthesizeofthearray.
D
3. Anarrayispassed bycopying onlythesubrange thatmightbeaccessed bythe
calledprocedure. Time ‚.q p 1/ifthesubarray AŒp::qispassed.
D (cid:0) C
a. Considertherecursivebinarysearchalgorithmforfindinganumberinasorted
array (see Exercise 2.3-5). Give recurrences for the worst-case running times
ofbinarysearchwhenarraysarepassedusingeachofthethreemethodsabove,
and give good upper bounds on the solutions of the recurrences. LetN be the
sizeoftheoriginalproblem andnbethesizeofasubproblem.
b. Redopart(a)forthe MERGE-SORT algorithm fromSection2.3.1.
108 Chapter4 Divide-and-Conquer
4-3 Morerecurrenceexamples
Give asymptotic upper and lower bounds for T.n/ in each of the following recur-
rences. Assume that T.n/ is constant for sufficiently small n. Make your bounds
astightaspossible, andjustifyyouranswers.
a. T.n/ 4T.n=3/ nlgn.
D C
b. T.n/ 3T.n=3/ n=lgn.
D C
c. T.n/ 4T.n=2/ n2pn.
D C
d. T.n/ 3T.n=3 2/ n=2.
D (cid:0) C
e. T.n/ 2T.n=2/ n=lgn.
D C
f. T.n/ T.n=2/ T.n=4/ T.n=8/ n.
D C C C
g. T.n/ T.n 1/ 1=n.
D (cid:0) C
h. T.n/ T.n 1/ lgn.
D (cid:0) C
i. T.n/ T.n 2/ 1=lgn.
D (cid:0) C
j. T.n/ pnT.pn/ n.
D C
4-4 Fibonaccinumbers
This problem develops properties of the Fibonacci numbers, which are defined
by recurrence (3.22). We shall use the technique of generating functions to solve
the Fibonacci recurrence. Define the generating function (or formal power se-
F
ries) as
F.´/ 1 F ´i
i
D
i 0
XD
0 ´ ´2 2´3 3´4 5´5 8´6 13´7 21´8 ;
D C C C C C C C C C
whereF istheithFibonacci number.
i
a. ShowthatF.´/ ´ ´F.´/ ´2F.´/.
D C C
ProblemsforChapter4 109
b. Showthat
´
F.´/
D 1 ´ ´2
(cid:0) (cid:0)´
D .1 ´/.1 ´/
(cid:0) (cid:0) y
1 1 1
;
D p5 1 ´ (cid:0) 1 ´
 (cid:0) (cid:0) y 
where
1 p5
C 1:61803:::
D 2 D
and
1 p5
(cid:0) 0:61803::: :
y D 2 D(cid:0)
c. Showthat
F.´/ 1 1 . i i/´i :
D p5 (cid:0) y
i 0
XD
d. Usepart(c)toprovethatF i=p5fori > 0,roundedtothenearestinteger.
i
D
(Hint:Observethat < 1.)
y
ˇ ˇ
4-5 Chiptesting ˇ ˇ
ProfessorDiogeneshasnsupposedlyidenticalintegrated-circuit chipsthatinprin-
ciplearecapable oftesting each other. Theprofessor’s testjigaccommodates two
chipsatatime. Whenthejigisloaded,eachchipteststheotherandreportswhether
itisgood or bad. A good chip always reports accurately whether theother chip is
goodorbad,buttheprofessorcannottrusttheanswerofabadchip. Thus,thefour
possibleoutcomes ofatestareasfollows:
ChipAsays ChipB says Conclusion
B isgood Aisgood botharegood,orbotharebad
B isgood Aisbad atleastoneisbad
B isbad Aisgood atleastoneisbad
B isbad Aisbad atleastoneisbad
a. Show that if at least n=2 chips are bad, the professor cannot necessarily deter-
mine which chips are good using any strategy based on this kind of pairwise
test. Assumethatthebadchipscanconspire tofooltheprofessor.
110 Chapter4 Divide-and-Conquer
b. Consider the problem of finding a single good chip from among n chips, as-
suming that more than n=2 of the chips are good. Show that n=2 pairwise
b c
testsaresufficienttoreducetheproblem tooneofnearlyhalfthesize.
c. Showthat thegood chips can beidentified with‚.n/pairwise tests, assuming
that more than n=2 of the chips are good. Give and solve the recurrence that
describes thenumberoftests.
4-6 Mongearrays
An m n array A of real numbers is a Monge array if for all i, j, k, and l such

that1 i < k mand1 j < l n,wehave
   
AŒi;j AŒk;l AŒi;l AŒk;j:
C  C
Inotherwords,wheneverwepicktworowsandtwocolumnsofaMongearrayand
considerthefourelementsattheintersectionsoftherowsandthecolumns,thesum
of the upper-left and lower-right elements is less than or equal to the sum of the
lower-leftandupper-right elements. Forexample,thefollowingarrayisMonge:
10 17 13 28 23
17 22 16 29 23
24 28 22 34 24
11 13 6 17 7
45 44 32 37 23
36 33 19 21 6
75 66 51 53 34
a. Prove that an array is Monge if and only if for all i 1;2;:::;m 1 and
D (cid:0)
j 1;2;:::;n 1,wehave
D (cid:0)
AŒi;j AŒi 1;j 1 AŒi;j 1 AŒi 1;j:
C C C  C C C
(Hint:Forthe“if”part,useinduction separately onrowsandcolumns.)
b. The following array is not Monge. Change one element in order to make it
Monge. (Hint:Usepart(a).)
37 23 22 32
21 6 7 10
53 34 30 31
32 13 9 6
43 21 15 8
NotesforChapter4 111
c. Letf.i/betheindexofthecolumncontaining theleftmost minimumelement
ofrowi. Provethatf.1/ f.2/ f.m/foranym nMongearray.
    
d. Hereisadescription ofadivide-and-conquer algorithm thatcomputes theleft-
mostminimumelementineachrowofanm nMongearrayA:

ConstructasubmatrixA ofAconsistingoftheeven-numberedrowsofA.
0
Recursively determine the leftmost minimum for each row of A. Then
0
computetheleftmostminimumintheodd-numbered rowsofA.
Explainhowtocomputetheleftmostminimumintheodd-numberedrowsofA
(given that the leftmost minimum of the even-numbered rows is known) in
O.m n/time.
C
e. Write therecurrence describing the running timeofthe algorithm described in
part(d). ShowthatitssolutionisO.m nlogm/.
C
Chapter notes
Divide-and-conquer as a technique for designing algorithms dates back to at least
1962inanarticlebyKaratsubaandOfman[194]. Itmighthavebeenusedwellbe-
forethen,however;accordingtoHeideman,Johnson,andBurrus[163],C.F.Gauss
devisedthefirstfastFouriertransformalgorithmin1805,andGauss’sformulation
breakstheproblemintosmallersubproblems whosesolutions arecombined.
Themaximum-subarrayprobleminSection4.1isaminorvariationonaproblem
studiedbyBentley[43,Chapter7].
Strassen’s algorithm [325] caused much excitement when it was published
in1969. Beforethen, fewimagined the possibility ofanalgorithm asymptotically
faster than the basic SQUARE-MATRIX-MULTIPLY procedure. The asymptotic
upper bound for matrix multiplication has been improved since then. The most
asymptotically efficient algorithm for multiplying n n matrices to date, due to

Coppersmith andWinograd [78],has arunning timeofO.n2:376/. Thebest lower
boundknownisjusttheobvious .n2/bound(obvious because wemustfillinn2
elementsoftheproductmatrix).
From a practical point of view, Strassen’s algorithm is often not the method of
choiceformatrixmultiplication, forfourreasons:
1. The constant factor hidden in the ‚.nlg7/ running time of Strassen’s algo-
rithm is larger than the constant factor in the ‚.n3/-time SQUARE-MATRIX-
MULTIPLY procedure.
2. Whenthematricesaresparse, methodstailoredforsparsematricesarefaster.
112 Chapter4 Divide-and-Conquer
3. Strassen’s algorithm is not quite as numerically stable as SQUARE-MATRIX-
MULTIPLY. Inotherwords,becauseofthelimitedprecisionofcomputerarith-
metic on noninteger values, larger errors accumulate in Strassen’s algorithm
thanin SQUARE-MATRIX-MULTIPLY.
4. Thesubmatrices formedatthelevelsofrecursion consumespace.
The latter two reasons were mitigated around 1990. Higham [167] demonstrated
that the difference in numerical stability had been overemphasized; although
Strassen’s algorithm istoonumerically unstable forsomeapplications, itiswithin
acceptable limits for others. Bailey, Lee, and Simon [32] discuss techniques for
reducing thememoryrequirements forStrassen’salgorithm.
In practice, fast matrix-multiplication implementations for dense matrices use
Strassen’s algorithm for matrix sizes above a “crossover point,” and they switch
to a simpler method once the subproblem size reduces to below the crossover
point. Theexactvalueofthecrossoverpointishighlysystemdependent. Analyses
thatcount operations butignore effectsfrom caches andpipelining haveproduced
crossoverpointsaslowasn 8(byHigham[167])orn 12(byHuss-Lederman
D D
et al. [186]). D’Alberto and Nicolau [81] developed an adaptive scheme, which
determines the crossover point by benchmarking when their software package is
installed. They found crossover points on various systems ranging from n 400
D
ton 2150,andtheycouldnotfindacrossover pointonacoupleofsystems.
D
Recurrences were studied as early as 1202 by L. Fibonacci, for whom the Fi-
bonacci numbers are named. A. De Moivre introduced the method of generating
functions(seeProblem4-4)forsolvingrecurrences. Themastermethodisadapted
fromBentley,Haken,andSaxe[44],whichprovidestheextendedmethodjustified
byExercise4.6-2. Knuth[209]andLiu[237]showhowtosolvelinearrecurrences
using the method ofgenerating functions. Purdom andBrown[287]and Graham,
Knuth,andPatashnik[152]containextendeddiscussions ofrecurrence solving.
Several researchers, including Akra and Bazzi [13], Roura [299], Verma [346],
and Yap [360], have given methods for solving more general divide-and-conquer
recurrences than are solved bythe master method. Wedescribe the result ofAkra
andBazzihere,asmodifiedbyLeighton[228]. TheAkra-Bazzimethodworksfor
recurrences oftheform
‚.1/ if1 x x ;
T.x/   0 (4.30)
D
(
k
i
1a iT.b ix/ Cf.x/ ifx > x
0
;
D
where P
 x 1isarealnumber,

 x isaconstant suchthatx 1=b andx 1=.1 b /fori 1;2;:::;k,
0 0 i 0 i
  (cid:0) D
 a isapositiveconstant fori 1;2;:::;k,
i
D
NotesforChapter4 113
 b isaconstantintherange0 < b < 1fori 1;2;:::;k,
i i
D
 k 1isanintegerconstant, and

 f.x/ is a nonnegative function that satisfies the polynomial-growth condi-
tion: there exist positive constants c and c such that for all x 1, for
1 2

i 1;2;:::;k, and for all u such that b x u x, we have c f.x/
i 1
D   
f.u/ c f.x/. (If f .x/ is upper-bounded by some polynomial in x, then
2 0
f.x/s atisfiesthepolyj nomiaj l-growthcondition. Forexample,f.x/ x˛lgˇ x
D
satisfiesthiscondition foranyrealconstants ˛ andˇ.)
Although the master method does not apply to a recurrence such as T.n/
D
T. n=3 / T. 2n=3 / O.n/, the Akra-Bazzi method does. To solve the re-
b c C b c C
currence (4.30), wefirst findthe unique real number p such that k a bp 1.
i 1 i i D
(Suchap alwaysexists.) Thesolution totherecurrence isthen D
P
x f.u/
T.n/ ‚ xp 1 du :
D C up 1
  Z1 C 
The Akra-Bazzi method can be somewhat difficult to use, but it serves in solving
recurrences that model division of the problem into substantially unequally sized
subproblems. The master method is simpler to use, but it applies only when sub-
problemsizesareequal.
5 Probabilistic Analysis and Randomized
Algorithms
This chapter introduces probabilistic analysis and randomized algorithms. If you
are unfamiliar with the basics of probability theory, you should read Appendix C,
whichreviewsthismaterial. Weshallrevisitprobabilistic analysisandrandomized
algorithms severaltimesthroughout thisbook.
5.1 Thehiring problem
Suppose that you need to hire a new office assistant. Your previous attempts at
hiring have beenunsuccessful, andyou decide touseanemployment agency. The
employment agency sends you one candidate each day. Youinterview that person
and then decide either to hire that person or not. You must pay the employment
agency asmallfeetointerview anapplicant. Toactually hireanapplicant ismore
costly,however,sinceyoumustfireyourcurrentofficeassistantandpayasubstan-
tialhiringfeetotheemploymentagency. Youarecommittedtohaving,atalltimes,
the bestpossible person for thejob. Therefore, you decide that, after interviewing
each applicant, ifthat applicant isbetter qualified than the current officeassistant,
youwillfirethecurrentofficeassistantandhirethenewapplicant. Youarewilling
to pay the resulting price of this strategy, but you wish toestimate what that price
willbe.
TheprocedureHIRE-ASSISTANT, givenbelow,expressesthisstrategyforhiring
in pseudocode. Itassumes that the candidates for theofficeassistant job arenum-
bered 1through n. Theprocedure assumes thatyou areable to, after interviewing
candidate i, determine whether candidate i isthe best candidate you have seen so
far. To initialize, the procedure creates a dummy candidate, numbered 0, who is
lessqualifiedthaneachoftheothercandidates.
5.1 Thehiringproblem 115
HIRE-ASSISTANT.n/
1 best 0 //candidate 0isaleast-qualified dummycandidate
D
2 fori 1ton
D
3 interview candidate i
4 ifcandidate i isbetterthancandidate best
5 best i
D
6 hirecandidate i
Thecost model forthis problem differs from the modeldescribed inChapter 2.
We focus not on the running time of HIRE-ASSISTANT, but instead on the costs
incurredbyinterviewingandhiring. Onthesurface,analyzingthecostofthisalgo-
rithmmayseemverydifferentfromanalyzingtherunningtimeof,say,mergesort.
The analytical techniques used, however, are identical whether we are analyzing
cost or running time. In either case, we are counting the number of times certain
basicoperations areexecuted.
Interviewinghasalowcost,sayc ,whereashiringisexpensive,costingc . Let-
i h
tingmbethenumberofpeople hired, thetotal costassociated withthisalgorithm
is O.c n c m/. No matter how many people we hire, we always interview n
i h
C
candidates and thus always incur the cost c n associated with interviewing. We
i
therefore concentrate on analyzing c m, the hiring cost. This quantity varies with
h
eachrunofthealgorithm.
Thisscenario servesasamodelforacommoncomputational paradigm. Weof-
tenneedtofindthemaximumorminimumvalueinasequencebyexaminingeach
element of the sequence and maintaining a current “winner.” The hiring problem
modelshowoftenweupdateournotionofwhichelementiscurrently winning.
Worst-case analysis
Intheworstcase,weactuallyhireeverycandidatethatweinterview. Thissituation
occurs ifthe candidates come instrictly increasing order ofquality, inwhich case
wehirentimes,foratotalhiringcostofO.c n/.
h
Ofcourse, the candidates do not always come inincreasing order ofquality. In
fact, we have no idea about the order in which they arrive, nor do we have any
control over thisorder. Therefore, itisnatural toask whatweexpect tohappen in
atypicaloraverage case.
Probabilisticanalysis
Probabilistic analysis is the use of probability in the analysis of problems. Most
commonly, we use probabilistic analysis to analyze the running time of an algo-
rithm. Sometimes we use it to analyze other quantities, such as the hiring cost
116 Chapter5 ProbabilisticAnalysisandRandomizedAlgorithms
in procedure HIRE-ASSISTANT. In order to perform a probabilistic analysis, we
mustuseknowledge of, ormakeassumptions about, the distribution oftheinputs.
Then we analyze our algorithm, computing an average-case running time, where
we take the average over the distribution of the possible inputs. Thus we are, in
effect, averaging therunning timeoverall possible inputs. Whenreporting such a
running time,wewillrefertoitastheaverage-case runningtime.
We must be very careful in deciding on the distribution of inputs. For some
problems, we may reasonably assume something about the set of all possible in-
puts, and then we can use probabilistic analysis as a technique for designing an
efficient algorithm and as a means for gaining insight into a problem. For other
problems, we cannot describe a reasonable input distribution, and in these cases
wecannotuseprobabilistic analysis.
For the hiring problem, we can assume that the applicants come in a random
order. What does that mean for this problem? We assume that we can compare
any two candidates and decide which one is better qualified; that is, there is a
total order on the candidates. (See Appendix B for the definition of a total or-
der.) Thus, we can rank each candidate with a unique number from 1 through n,
using rank.i/ to denote the rank of applicant i, and adopt the convention that a
higher rank corresponds to a better qualified applicant. The ordered list rank.1/;
h
rank.2/;:::;rank.n/ is a permutation of the list 1;2; :::; n . Saying that the
i h i
applicants come in a random order is equivalent to saying that this list of ranks is
equally likely to be any one of the nŠ permutations of the numbers 1 through n.
Alternatively, wesay that the ranks form a uniform random permutation; that is,
eachofthepossible nŠpermutations appearswithequalprobability.
Section5.2contains aprobabilistic analysis ofthehiringproblem.
Randomizedalgorithms
Inordertouseprobabilistic analysis, weneedtoknowsomething about thedistri-
butionoftheinputs. Inmanycases,weknowverylittleabouttheinputdistribution.
Evenifwedoknowsomethingaboutthedistribution, wemaynotbeabletomodel
thisknowledgecomputationally. Yetweoftencanuseprobability andrandomness
as a tool for algorithm design and analysis, by making the behavior of part of the
algorithm random.
Inthehiringproblem,itmayseemasifthecandidates arebeingpresented tous
inarandom order, butwehavenowayofknowing whether ornottheyreally are.
Thus,inordertodeveloparandomized algorithm forthehiringproblem, wemust
havegreater controlovertheorderinwhichweinterview thecandidates. Wewill,
therefore, change the model slightly. We say that the employment agency has n
candidates, and they send us a list of the candidates in advance. On each day, we
choose,randomly,whichcandidatetointerview. Althoughweknownothingabout
5.1 Thehiringproblem 117
the candidates (besides their names), we have made a significant change. Instead
of relying on a guess that the candidates come to us in a random order, we have
insteadgainedcontroloftheprocess andenforcedarandom order.
More generally, we call an algorithm randomized if its behavior is determined
not only by its input but also by values produced by a random-number gener-
ator. We shall assume that we have at our disposal a random-number generator
RANDOM. A call to RANDOM.a;b/ returns an integer between a and b, inclu-
sive, with each such integer being equally likely. For example, RANDOM.0;1/
produces 0withprobability 1=2, anditproduces 1withprobability 1=2. Acall to
RANDOM.3;7/returnseither3,4,5,6,or7,eachwithprobability 1=5. Eachinte-
gerreturnedbyRANDOMisindependentoftheintegersreturnedonpreviouscalls.
You may imagine RANDOM as rolling a .b a 1/-sided die to obtain its out-
(cid:0) C
put. (Inpractice,mostprogrammingenvironmentsofferapseudorandom-number
generator: a deterministic algorithm returning numbers that “look” statistically
random.)
Whenanalyzingtherunningtimeofarandomizedalgorithm,wetaketheexpec-
tation of the running time over the distribution of values returned by the random
number generator. We distinguish these algorithms from those in which the input
is random by referring to the running time of a randomized algorithm as an ex-
pected running time. In general, we discuss the average-case running time when
the probability distribution is over the inputs to the algorithm, and we discuss the
expectedrunning timewhenthealgorithm itselfmakesrandom choices.
Exercises
5.1-1
Showthattheassumption thatwearealwaysabletodetermine whichcandidate is
best, inline 4of procedure HIRE-ASSISTANT, implies that weknow atotal order
ontheranksofthecandidates.
5.1-2 ?
DescribeanimplementationoftheprocedureRANDOM.a;b/thatonlymakescalls
to RANDOM.0;1/. What is the expected running time of your procedure, as a
functionofaandb?
5.1-3 ?
Supposethatyouwanttooutput0withprobability1=2and1withprobability1=2.
At your disposal is a procedure BIASED-RANDOM, that outputs either 0 or 1. It
outputs1withsomeprobability p and0withprobability 1 p,where0 < p <1,
(cid:0)
but you do not know what p is. Give an algorithm that uses BIASED-RANDOM
as a subroutine, and returns an unbiased answer, returning 0 with probability 1=2
118 Chapter5 ProbabilisticAnalysisandRandomizedAlgorithms
and 1 with probability 1=2. What is the expected running time of your algorithm
asafunction ofp?
5.2 Indicatorrandom variables
Inordertoanalyzemanyalgorithms,includingthehiringproblem,weuseindicator
random variables. Indicator random variables provide a convenient method for
convertingbetweenprobabilitiesandexpectations. Supposewearegivenasample
spaceS andaneventA. ThentheindicatorrandomvariableI A associatedwith
f g
eventAisdefinedas
1 ifAoccurs;
I A (5.1)
f g D
(
0 ifAdoesnotoccur:
As a simple example, let us determine the expected number of heads that we
obtainwhenflippingafaircoin. OursamplespaceisS H;T ,withPr H
D f g f g D
Pr T 1=2. We can then define an indicator random variable X , associated
H
f g D
with the coin coming up heads, which is the event H. This variable counts the
number of heads obtained in this flip, and it is1 ifthe coin comes up heads and 0
otherwise. Wewrite
X I H
H
D f g
1 ifH occurs;
D
(
0 ifT occurs:
The expected number of heads obtained in one flip of the coin is simply the ex-
pectedvalueofourindicator variableX :
H
EŒX  EŒI H 
H
D f g
1 Pr H 0 Pr T
D  f gC  f g
1 .1=2/ 0 .1=2/
D  C 
1=2:
D
Thus the expected number of heads obtained by one flip of a fair coin is 1=2. As
the following lemma shows, the expected value of an indicator random variable
associated withaneventAisequaltotheprobability thatAoccurs.
Lemma5.1
Given a sample space S and an event A in the sample space S, let X I A .
A
D f g
ThenEŒX  Pr A .
A
D f g
5.2 Indicatorrandomvariables 119
Proof Bythedefinition ofan indicator random variable from equation (5.1) and
thedefinitionofexpected value,wehave
EŒX  EŒI A 
A
D f g
1 Pr A 0 Pr A
D  f gC 
Pr A ;
D f g ˚ (cid:9)
whereAdenotesS A,thecomplementofA.
(cid:0)
Although indicator random variables may seem cumbersome for an application
such ascounting the expected number of heads on a flipof a single coin, they are
useful for analyzing situations in which we perform repeated random trials. For
example, indicator random variables give us a simple way to arrive at the result
of equation (C.37). In this equation, we compute the number of heads in n coin
flipsbyconsideringseparatelytheprobabilityofobtaining0heads,1head,2heads,
etc. Thesimplermethodproposedinequation(C.38)insteadusesindicatorrandom
variablesimplicitly. Makingthisargumentmoreexplicit,weletX betheindicator
i
random variable associated with the event in which the ith flip comes up heads:
X I theithflipresultsintheeventH . LetX betherandomvariabledenoting
i
D f g
thetotalnumberofheadsinthencoinflips,sothat
n
X X :
i
D
i 1
XD
Wewishtocomputetheexpectednumberofheads,andsowetaketheexpectation
ofbothsidesoftheaboveequation toobtain
n
EŒX E X :
i
D
" #
i 1
XD
The above equation gives the expectation of the sum of n indicator random vari-
ables. ByLemma5.1,wecaneasilycomputetheexpectationofeachoftherandom
variables. Byequation (C.21)—linearity ofexpectation—it iseasytocomputethe
expectation of the sum: it equals the sum of the expectations of the n random
variables. Linearity of expectation makes the use of indicator random variables a
powerfulanalyticaltechnique;itappliesevenwhenthereisdependenceamongthe
randomvariables. Wenowcaneasilycomputetheexpectednumberofheads:
120 Chapter5 ProbabilisticAnalysisandRandomizedAlgorithms
n
EŒX E X
i
D
" #
i 1
XD
n
EŒX 
i
D
i 1
XD
n
1=2
D
i 1
XD
n=2:
D
Thus,comparedtothemethodusedinequation(C.37),indicatorrandomvariables
greatly simplify thecalculation. Weshall useindicator random variables through-
outthisbook.
Analysisofthehiringproblemusingindicatorrandomvariables
Returning tothehiringproblem,wenowwishtocomputetheexpected numberof
timesthatwehireanewofficeassistant. Inordertouseaprobabilisticanalysis,we
assume that the candidates arrive in a random order, as discussed in the previous
section. (WeshallseeinSection5.3howtoremovethisassumption.) LetX bethe
random variable whose value equals the number of times wehire a new office as-
sistant. Wecouldthenapplythedefinitionofexpected valuefromequation (C.20)
toobtain
n
EŒX x Pr X x ;
D f D g
x 1
XD
but thiscalculation would becumbersome. Weshall instead useindicator random
variables togreatly simplifythecalculation.
To use indicator random variables, instead of computing EŒX by defining one
variable associated with the number of times we hire a new office assistant, we
define n variables related to whether or not each particular candidate is hired. In
particular, welet X be the indicator random variable associated with the event in
i
whichtheithcandidate ishired. Thus,
X I candidate i ishired
i
D f g
1 ifcandidate i ishired;
D ( 0 ifcandidate i isnothired;
and
X X X X : (5.2)
1 2 n
D C CC
5.2 Indicatorrandomvariables 121
ByLemma5.1,wehavethat
EŒX  Pr candidate i ishired ;
i
D f g
andwemustthereforecomputetheprobabilitythatlines5–6ofHIRE-ASSISTANT
areexecuted.
Candidate i is hired, in line 6, exactly when candidate i is better than each of
candidates1throughi 1. Becausewehaveassumedthatthecandidatesarrivein
(cid:0)
arandomorder,thefirsti candidateshaveappearedinarandomorder. Anyoneof
thesefirsti candidates isequally likely tobethebest-qualified sofar. Candidate i
has a probability of 1=i of being better qualified than candidates 1 through i 1
(cid:0)
andthusaprobability of1=i ofbeinghired. ByLemma5.1,weconclude that
EŒX  1=i : (5.3)
i
D
NowwecancomputeEŒX:
n
EŒX E X (byequation(5.2)) (5.4)
i
D
" #
i 1
XD
n
EŒX  (bylinearityofexpectation)
i
D
i 1
XD
n
1=i (byequation(5.3))
D
i 1
XD
lnn O.1/ (byequation(A.7)) . (5.5)
D C
Even though we interview n people, we actually hire only approximately lnn of
them,onaverage. Wesummarizethisresultinthefollowinglemma.
Lemma5.2
Assuming that the candidates are presented in a random order, algorithm HIRE-
ASSISTANT hasanaverage-case totalhiringcostofO.c hlnn/.
Proof The bound follows immediately from our definition of the hiring cost
and equation (5.5), which shows that the expected number of hires is approxi-
matelylnn.
The average-case hiring cost is a significant improvement over the worst-case
hiringcostofO.c n/.
h
122 Chapter5 ProbabilisticAnalysisandRandomizedAlgorithms
Exercises
5.2-1
In HIRE-ASSISTANT, assuming that the candidates are presented in a random or-
der, whatistheprobability thatyouhireexactlyonetime? Whatistheprobability
thatyouhireexactlyntimes?
5.2-2
In HIRE-ASSISTANT, assuming that the candidates are presented in a random or-
der,whatistheprobability thatyouhireexactlytwice?
5.2-3
Useindicatorrandomvariablestocomputetheexpectedvalueofthesumofndice.
5.2-4
Useindicatorrandomvariables tosolvethefollowingproblem,whichisknownas
thehat-checkproblem. Eachofncustomers givesahattoahat-check personata
restaurant. Thehat-check person givesthehatsbacktothecustomers inarandom
order. Whatistheexpectednumberofcustomers whogetbacktheirownhat?
5.2-5
Let AŒ1::n be an array of n distinct numbers. If i < j and AŒi > AŒj, then
the pair .i;j/ is called an inversion of A. (See Problem 2-4 for more on inver-
sions.) Suppose that the elements of A form a uniform random permutation of
1;2;:::;n . Use indicator random variables to compute the expected number of
h i
inversions.
5.3 Randomizedalgorithms
In the previous section, we showed how knowing a distribution on the inputs can
help us to analyze the average-case behavior of an algorithm. Many times, wedo
nothavesuchknowledge, thusprecluding anaverage-case analysis. Asmentioned
inSection5.1,wemaybeabletousearandomized algorithm.
For a problem such as the hiring problem, in which it is helpful to assume that
all permutations of the input are equally likely, a probabilistic analysis can guide
the development of a randomized algorithm. Instead of assuming a distribution
of inputs, we impose a distribution. In particular, before running the algorithm,
we randomly permute the candidates in order to enforce the property that every
permutation is equally likely. Although we have modified the algorithm, we still
expect tohire anew officeassistant approximately lnntimes. Butnow weexpect
5.3 Randomizedalgorithms 123
this to be the case for any input, rather than for inputs drawn from a particular
distribution.
Letusfurtherexplorethedistinctionbetweenprobabilisticanalysisandrandom-
ized algorithms. In Section 5.2, weclaimed that, assuming that the candidates ar-
riveinarandomorder,theexpectednumberoftimeswehireanewofficeassistant
isaboutlnn. Notethatthealgorithmhereisdeterministic;foranyparticularinput,
thenumberoftimesanewofficeassistantishiredisalwaysthesame. Furthermore,
thenumberoftimeswehireanewofficeassistantdiffersfordifferentinputs,andit
dependsontheranksofthevariouscandidates. Sincethisnumberdependsonlyon
theranksofthecandidates, wecanrepresent aparticular inputbylisting, inorder,
the ranks of the candidates, i.e., rank.1/;rank.2/;:::;rank.n/ . Given the rank
h i
listA 1;2;3;4;5;6;7;8;9;10 , anewofficeassistantisalwayshired10times,
1
Dh i
since each successive candidate is better than the previous one, and lines 5–6 are
executedineachiteration. GiventhelistofranksA 10;9;8;7;6;5;4;3;2;1 ,
2
Dh i
anewofficeassistant ishired only once, inthefirstiteration. Givenalistofranks
A 5; 2; 1; 8; 4; 7; 10; 9; 3; 6 , a new office assistant is hired three times,
3
D h i
upon interviewing the candidates with ranks 5, 8, and 10. Recalling that the cost
of our algorithm depends on how many times we hire a new office assistant, we
seethat there areexpensive inputs such asA ,inexpensive inputs such asA ,and
1 2
moderatelyexpensive inputssuchasA .
3
Consider, on the other hand, the randomized algorithm that first permutes the
candidates and then determines the best candidate. In this case, we randomize in
thealgorithm, notintheinputdistribution. Givenaparticularinput,sayA above,
3
we cannot say how many times the maximum is updated, because this quantity
differs with each run of the algorithm. The first timewe run the algorithm on A ,
3
it may produce the permutation A and perform 10 updates; but the second time
1
we run the algorithm, we may produce the permutation A and perform only one
2
update. Thethird time werun it, wemayperform some other number of updates.
Each time we run the algorithm, the execution depends on the random choices
madeand is likely to differ from the previous execution of thealgorithm. Forthis
algorithm and many other randomized algorithms, no particular input elicits its
worst-case behavior. Even your worst enemy cannot produce a bad input array,
since the random permutation makes the input order irrelevant. The randomized
algorithm performs badly only if the random-number generator produces an “un-
lucky”permutation.
Forthe hiring problem, the only change needed in the code isto randomly per-
mutethearray.
124 Chapter5 ProbabilisticAnalysisandRandomizedAlgorithms
RANDOMIZED-HIRE-ASSISTANT.n/
1 randomlypermutethelistofcandidates
2 best 0 //candidate 0isaleast-qualified dummycandidate
D
3 fori 1ton
D
4 interview candidate i
5 ifcandidate i isbetterthancandidate best
6 best i
D
7 hirecandidate i
With this simple change, we have created a randomized algorithm whose perfor-
mance matches that obtained by assuming that the candidates were presented in a
random order.
Lemma5.3
The expected hiring cost of the procedure RANDOMIZED-HIRE-ASSISTANT is
O.c lnn/.
h
Proof After permuting the input array, we have achieved a situation identical to
thatoftheprobabilistic analysis ofHIRE-ASSISTANT.
Comparing Lemmas5.2and5.3highlights the difference between probabilistic
analysisandrandomizedalgorithms. InLemma5.2,wemakeanassumptionabout
theinput. InLemma5.3,wemakenosuch assumption, although randomizing the
input takes some additional time. To remain consistent with our terminology, we
couched Lemma 5.2 in terms of the average-case hiring cost and Lemma 5.3 in
termsoftheexpectedhiringcost. Intheremainderofthissection,wediscusssome
issuesinvolved inrandomlypermutinginputs.
Randomlypermutingarrays
Many randomized algorithms randomize the input by permuting the given input
array. (There are other ways to use randomization.) Here, we shall discuss two
methodsfordoingso. WeassumethatwearegivenanarrayAwhich,withoutloss
of generality, contains theelements 1through n. Ourgoal istoproduce arandom
permutation ofthearray.
One common method is to assign each element AŒi of the array a random pri-
ority PŒi, and then sort the elements of A according to these priorities. For ex-
ample, if our initial array is A 1; 2; 3; 4 and we choose random priorities
D h i
P 36;3;62;19 , wewouldproduceanarrayB 2;4;1;3 ,sincethesecond
Dh i Dh i
priority is the smallest, followed by the fourth, then the first, and finally the third.
Wecallthisprocedure PERMUTE-BY-SORTING:
5.3 Randomizedalgorithms 125
PERMUTE-BY-SORTING.A/
1 n A:length
D
2 letPŒ1::nbeanewarray
3 fori 1ton
D
4 PŒi RANDOM.1;n3/
D
5 sortA,usingP assortkeys
Line 4 chooses a random number between 1 and n3. We use a range of 1 to n3
to make it likely that all the priorities in P are unique. (Exercise 5.3-5 asks you
to prove that the probability that all entries are unique is at least 1 1=n, and
(cid:0)
Exercise5.3-6askshowtoimplementthealgorithm eveniftwoormorepriorities
areidentical.) Letusassumethatallthepriorities areunique.
The time-consuming step in this procedure is the sorting in line 5. As we shall
see in Chapter 8, if we use a comparison sort, sorting takes .nlgn/ time. We
can achieve this lower bound, since we have seen that merge sort takes ‚.nlgn/
time. (We shall see other comparison sorts that take ‚.nlgn/ time in Part II.
Exercise8.3-4asksyoutosolvetheverysimilarproblemofsortingnumbersinthe
range 0ton3 1inO.n/time.) Aftersorting, ifPŒiisthejthsmallest priority,
(cid:0)
thenAŒiliesinpositionj oftheoutput. Inthismannerweobtainapermutation. It
remainstoprovethattheprocedureproducesauniformrandompermutation,that
is,thattheprocedureisequallylikelytoproduceeverypermutationofthenumbers
1throughn.
Lemma5.4
ProcedurePERMUTE-BY-SORTING producesauniformrandompermutationofthe
input,assumingthatallpriorities aredistinct.
Proof We start by considering the particular permutation in which each ele-
ment AŒi receives the ith smallest priority. We shall show that this permutation
occurs with probability exactly 1=nŠ. For i 1;2;:::;n, let E be the event
i
D
that element AŒi receives the ith smallest priority. Then we wish to compute the
probability thatforalli,eventE occurs, whichis
i
Pr E E E E E :
1 2 3 n 1 n
f \ \ \\ (cid:0) \ g
UsingExerciseC.2-5,thisprobability isequalto
Pr E Pr E E Pr E E E Pr E E E E
1 2 1 3 2 1 4 3 2 1
f g f j g f j \ g f j \ \ g
Pr E E E E Pr E E E :
i i 1 i 2 1 n n 1 1
 f j (cid:0) \ (cid:0) \\ g f j (cid:0) \\ g
We have that Pr E 1=n because it is the probability that one priority
1
f g D
chosen randomly out of a set of n is the smallest priority. Next, we observe
126 Chapter5 ProbabilisticAnalysisandRandomizedAlgorithms
that Pr E E 1=.n 1/ because given that element AŒ1 has the small-
2 1
f j g D (cid:0)
est priority, each of the remaining n 1 elements has an equal chance of hav-
(cid:0)
ing the second smallest priority. In general, for i 2;3;:::;n, we have that
D
Pr E E E E 1=.n i 1/,since,giventhatelementsAŒ1
i i 1 i 2 1
thrf oughj AŒi(cid:0) \ 1ha(cid:0) ve\ th e i\ 1smg D allestpr(cid:0) ioriC
ties(inorder),eachoftheremaining
(cid:0) (cid:0)
n .i 1/elementshasanequalchanceofhavingtheithsmallestpriority. Thus,
(cid:0) (cid:0)
wehave
1 1 1 1
Pr E E E E E
1 2 3 n 1 n
f \ \ \\ (cid:0) \ g D n n 1  2 1
  (cid:0)    
1
;
D nŠ
and we have shown that the probability of obtaining the identity permutation
is1=nŠ.
We can extend this proof to work for any permutation of priorities. Consider
any fixed permutation  .1/;.2/;:::;.n/ of the set 1;2;:::;n . Letus
D h i f g
denote by r the rank of the priority assigned to element AŒi, where the element
i
with the jth smallest priority has rank j. If we define E as the event in which
i
element AŒi receives the .i/th smallest priority, or r .i/, the same proof
i
D
still applies. Therefore, if we calculate the probability of obtaining any particular
permutation, thecalculation isidentical totheoneabove,sothattheprobability of
obtaining thispermutation isalso1=nŠ.
You might think that to prove that apermutation is auniform random permuta-
tion,itsufficestoshowthat,foreachelementAŒi,theprobabilitythattheelement
winds upinposition j is1=n. Exercise 5.3-4 showsthat thisweaker condition is,
infact,insufficient.
A better method for generating a random permutation is to permute the given
array in place. The procedure RANDOMIZE-IN-PLACE does so in O.n/ time. In
its ith iteration, it chooses the element AŒi randomly from among elements AŒi
through AŒn. Subsequent totheithiteration, AŒiisneveraltered.
RANDOMIZE-IN-PLACE.A/
1 n A:length
D
2 fori 1ton
D
3 swapAŒiwithAŒRANDOM.i;n/
We shall use a loop invariant to show that procedure RANDOMIZE-IN-PLACE
produces a uniform random permutation. A k-permutation on a set of n ele-
ments is a sequence containing k of the n elements, with no repetitions. (See
Appendix C.) TherearenŠ=.n k/Šsuchpossible k-permutations.
(cid:0)
5.3 Randomizedalgorithms 127
Lemma5.5
Procedure RANDOMIZE-IN-PLACE computesauniform randompermutation.
Proof Weusethefollowingloopinvariant:
Just prior to the ith iteration of the for loop of lines 2–3, for each possible
.i 1/-permutation of the n elements, the subarray AŒ1::i 1 contains
(cid:0) (cid:0)
this.i 1/-permutation withprobability .n i 1/Š=nŠ.
(cid:0) (cid:0) C
Weneedtoshowthatthisinvariantistruepriortothefirstloopiteration, thateach
iterationoftheloopmaintainstheinvariant,andthattheinvariantprovidesauseful
propertytoshowcorrectness whentheloopterminates.
Initialization: Consider the situation just before the first loop iteration, so that
i 1. The loop invariant says that for each possible 0-permutation, the sub-
D
array AŒ1::0 contains this 0-permutation withprobability .n i 1/Š=nŠ
(cid:0) C D
nŠ=nŠ 1. The subarray AŒ1::0 is an empty subarray, and a 0-permutation
D
hasnoelements. Thus,AŒ1::0containsany0-permutation withprobability1,
andtheloopinvariant holdspriortothefirstiteration.
Maintenance: We assume that just before the ith iteration, each possible
.i 1/-permutation appears in the subarray AŒ1::i 1 with probability
(cid:0) (cid:0)
.n i 1/Š=nŠ, and we shall show that after the ith iteration, each possible
(cid:0) C
i-permutation appears in the subarray AŒ1::i with probability .n i/Š=nŠ.
(cid:0)
Incrementing i forthenextiteration thenmaintainstheloopinvariant.
Let us examine the ith iteration. Consider a particular i-permutation, and de-
note the elements in it by x ; x ; :::; x . This permutation consists of an
1 2 i
h i
.i 1/-permutation x ;:::;x followed by thevalue x that thealgorithm
1 i 1 i
pla(cid:0) ces in AŒi. Let Eh denote th(cid:0) ei event in which the first i 1 iterations have
1
(cid:0)
createdtheparticular.i 1/-permutation x ;:::;x inAŒ1::i 1. Bythe
1 i 1
loopinvariant,Pr E (cid:0) .n i 1/Š=nŠ.h LetE be(cid:0) thi eeventthat(cid:0) ithiteration
1 2
f g D (cid:0) C
putsx inpositionAŒi. Thei-permutation x ;:::;x appearsinAŒ1::ipre-
i 1 i
h i
cisely when both E and E occur, and so we wish to compute Pr E E .
1 2 2 1
f \ g
Usingequation (C.14),wehave
Pr E E Pr E E Pr E :
2 1 2 1 1
f \ g D f j g f g
TheprobabilityPr E E equals1=.n i 1/becauseinline3thealgorithm
2 1
f j g (cid:0) C
choosesx randomlyfromthen i 1valuesinpositions AŒi::n. Thus,we
i
(cid:0) C
have
128 Chapter5 ProbabilisticAnalysisandRandomizedAlgorithms
Pr E E Pr E E Pr E
2 1 2 1 1
f \ g D f j g f g
1 .n i 1/Š
(cid:0) C
D n i 1  nŠ
(cid:0) C
.n i/Š
(cid:0) :
D nŠ
Termination: Attermination,i n 1,andwehavethatthesubarrayAŒ1::nis
D C
agivenn-permutationwithprobability.n .n 1/ 1/Š=nŠ 0Š=nŠ 1=nŠ.
(cid:0) C C D D
Thus, RANDOMIZE-IN-PLACE producesauniformrandom permutation.
A randomized algorithm is often the simplest and most efficient way to solve a
problem. Weshalluserandomized algorithms occasionally throughout thisbook.
Exercises
5.3-1
ProfessorMarceauobjectstotheloopinvariantusedintheproofofLemma5.5. He
questionswhetheritistruepriortothefirstiteration. Hereasonsthatwecouldjust
as easily declare that an empty subarray contains no 0-permutations. Therefore,
the probability that an empty subarray contains a 0-permutation should be 0, thus
invalidating the loop invariant prior to the first iteration. Rewrite the procedure
RANDOMIZE-IN-PLACE sothatitsassociatedloopinvariantappliestoanonempty
subarray prior to the first iteration, and modify the proof of Lemma 5.5 for your
procedure.
5.3-2
ProfessorKelpdecidestowriteaprocedure thatproduces atrandomanypermuta-
tionbesides theidentitypermutation. Heproposes thefollowingprocedure:
PERMUTE-WITHOUT-IDENTITY.A/
1 n A:length
D
2 fori 1ton 1
D (cid:0)
3 swapAŒiwithAŒRANDOM.i 1;n/
C
DoesthiscodedowhatProfessorKelpintends?
5.3-3
Suppose that instead of swapping element AŒi with a random element from the
subarray AŒi::n, we swapped it with a random element from anywhere in the
array:
5.3 Randomizedalgorithms 129
PERMUTE-WITH-ALL.A/
1 n A:length
D
2 fori 1ton
D
3 swapAŒiwithAŒRANDOM.1;n/
Doesthiscodeproduceauniform randompermutation? Whyorwhynot?
5.3-4
Professor Armstrong suggests the following procedure for generating a uniform
randompermutation:
PERMUTE-BY-CYCLIC.A/
1 n A:length
D
2 letBŒ1::nbeanewarray
3 offset RANDOM.1;n/
D
4 fori 1ton
D
5 dest i offset
D C
6 ifdest > n
7 dest dest n
D (cid:0)
8 BŒdest AŒi
D
9 returnB
ShowthateachelementAŒihasa1=nprobability ofwindingupinanyparticular
position in B. Then show that Professor Armstrong is mistaken by showing that
theresultingpermutation isnotuniformlyrandom.
5.3-5 ?
Prove that in the array P in procedure PERMUTE-BY-SORTING, the probability
thatallelementsareuniqueisatleast1 1=n.
(cid:0)
5.3-6
Explain how to implement the algorithm PERMUTE-BY-SORTING to handle the
case in which two or more priorities are identical. That is, your algorithm should
produceauniformrandompermutation,eveniftwoormoreprioritiesareidentical.
5.3-7
Suppose we want to create a random sample of the set 1;2;3;:::;n , that is,
f g
an m-element subset S, where 0 m n, such that each m-subset is equally
 
likely to be created. One way would be to set AŒi i for i 1;2;3;:::;n,
D D
call RANDOMIZE-IN-PLACE.A/, and then take just the first m array elements.
This method would make n calls to the RANDOM procedure. If n is much larger
than m, we can create a random sample with fewer calls to RANDOM. Show that
130 Chapter5 ProbabilisticAnalysisandRandomizedAlgorithms
thefollowingrecursiveprocedurereturnsarandomm-subsetS of 1;2;3;:::;n ,
f g
inwhicheachm-subsetisequally likely,whilemakingonlymcallstoRANDOM:
RANDOM-SAMPLE.m;n/
1 ifm == 0
2 return
;
3 elseS RANDOM-SAMPLE.m 1;n 1/
D (cid:0) (cid:0)
4 i RANDOM.1;n/
D
5 ifi S
2
6 S S n
D [f g
7 elseS S i
D [f g
8 returnS
? 5.4 Probabilisticanalysisand further uses ofindicatorrandom variables
This advanced section further illustrates probabilistic analysis by way of four ex-
amples. The first determines the probability that in a room of k people, two of
them share the samebirthday. Thesecond example examines whathappens when
we randomly toss balls into bins. The third investigates “streaks” of consecutive
headswhenweflipcoins. Thefinalexampleanalyzes avariantofthehiringprob-
lem in which you have to make decisions without actually interviewing all the
candidates.
5.4.1 Thebirthdayparadox
Our first example is the birthday paradox. How many people must there be in a
roombefore thereisa50%chance thattwoofthemwerebornonthesamedayof
theyear? Theanswerissurprisingly few. Theparadox isthatitisinfactfarfewer
thanthenumberofdaysinayear, orevenhalfthenumberofdaysinayear,aswe
shallsee.
To answer this question, we index the people in the room with the integers
1;2;:::;k, where k is the number of people in the room. We ignore the issue
of leap years and assume that all years have n 365 days. For i 1;2;:::;k,
D D
letb bethedayoftheyearonwhichperson i’sbirthday falls, where1 b n.
i i
 
We also assume that birthdays are uniformly distributed across the n days of the
year, sothatPr b r 1=nfori 1;2;:::;k andr 1;2;:::;n.
i
f D g D D D
The probability that two given people, say i and j, have matching birthdays
depends onwhether therandom selection ofbirthdays isindependent. Weassume
fromnowonthatbirthdaysareindependent,sothattheprobabilitythati’sbirthday
5.4 Probabilisticanalysisandfurtherusesofindicatorrandomvariables 131
andj’sbirthday bothfallondayr is
Pr b r andb r Pr b r Pr b r
i j i j
f D D g D f D g f D g
1=n2 :
D
Thus,theprobability thattheybothfallonthesamedayis
n
Pr b b Pr b r andb r
i j i j
f D g D f D D g
r 1
XD
n
.1=n2/
D
r 1
XD
1=n: (5.6)
D
Moreintuitively, onceb ischosen,theprobability thatb ischosentobethesame
i j
day is 1=n. Thus, the probability that i and j have the same birthday is the same
as the probability that the birthday of one of them falls on a given day. Notice,
however, that this coincidence depends on the assumption that the birthdays are
independent.
We can analyze the probability of at least 2 out of k people having matching
birthdays bylookingatthecomplementary event. Theprobability thatatleasttwo
ofthebirthdaysmatchis1minustheprobabilitythatallthebirthdaysaredifferent.
Theeventthatk peoplehavedistinctbirthdays is
k
B A ;
k i
D
i 1
\D
where A is the event that person i’s birthday is different from person j’s for
i
all j < i. Since we can write B A B , we obtain from equation (C.16)
k k k 1
D \ (cid:0)
therecurrence
Pr B Pr B Pr A B ; (5.7)
k k 1 k k 1
f g D f (cid:0) g f j (cid:0) g
where we take Pr B Pr A 1 as an initial condition. In other words,
1 1
f g D f g D
the probability that b ;b ;:::;b are distinct birthdays is the probability that
1 2 k
b ;b ;:::;b are distinct birthdays times the probability that b b for
1 2 k 1 k i
i 1;2;:::;(cid:0)k 1,giventhatb ;b ;:::;b aredistinct. ¤
1 2 k 1
D If b ;b ;:::;(cid:0) b are distinct, the condi(cid:0) tional probability that b b for
1 2 k 1 k i
i 1;2;:::;k 1(cid:0) is Pr A B .n k 1/=n, since out of th¤ e n days,
k k 1
nD .k 1/day(cid:0) sarenottf akenj . We(cid:0) itg erD atively(cid:0) appC lytherecurrence (5.7)toobtain
(cid:0) (cid:0)
132 Chapter5 ProbabilisticAnalysisandRandomizedAlgorithms
Pr B Pr B Pr A B
k k 1 k k 1
f g D f (cid:0) g f j (cid:0) g
Pr B Pr A B Pr A B
k 2 k 1 k 2 k k 1
D f (cid:0) g f (cid:0) j (cid:0) g f j (cid:0) g
:
:
:
Pr B Pr A B Pr A B Pr A B
1 2 1 3 2 k k 1
D f g f j g f j g f j (cid:0) g
n 1 n 2 n k 1
1 (cid:0) (cid:0) (cid:0) C
D  n n  n
    
1 2 k 1
1 1 1 1 (cid:0) :
D  (cid:0) n (cid:0) n  (cid:0) n
    
Inequality (3.12),1 x ex,givesus
C 
Pr B e 1=ne 2=n e .k 1/=n
k (cid:0) (cid:0) (cid:0) (cid:0)
f g  
e
(cid:0)
k iD(cid:0) 11i=n
D
e kP.k 1/=2n
(cid:0) (cid:0)
D
1=2

when k.k 1/=2n ln.1=2/. The probability that all k birthdays are distinct
(cid:0) (cid:0) 
is at most 1=2 when k.k 1/ 2nln2 or, solving the quadratic equation, when
(cid:0) 
k .1 1 .8ln2/n/=2. For n 365, we must have k 23. Thus, if at
 C C D 
least23peopleareinaroom,theprobabilityisatleast1=2thatatleasttwopeople
p
have the same birthday. On Mars, a year is 669 Martian days long; it therefore
takes31Martianstogetthesameeffect.
Ananalysisusingindicatorrandomvariables
Wecanuseindicator random variables toprovide asimplerbutapproximate anal-
ysis of the birthday paradox. Foreach pair .i;j/ of the k people in the room, we
definetheindicator randomvariable X ,for1 i < j k,by
ij
 
X I personi andpersonj havethesamebirthday
ij
D f g
1 ifpersoni andpersonj havethesamebirthday ;
D
(
0 otherwise:
Byequation (5.6),theprobability thattwopeoplehavematchingbirthdays is1=n,
andthusbyLemma5.1,wehave
EŒX  Pr personi andpersonj havethesamebirthday
ij
D f g
1=n:
D
Letting X be the random variable that counts the number of pairs of individuals
having thesamebirthday, wehave
5.4 Probabilisticanalysisandfurtherusesofindicatorrandomvariables 133
k k
X X :
ij
D
i 1j i 1
XD DXC
Takingexpectations ofbothsidesandapplying linearity ofexpectation, weobtain
k k
EŒX E X
ij
D
" #
i 1j i 1
XD DXC
k k
EŒX 
ij
D
i 1j i 1
XD DXC
k 1
D 2 n
!
k.k 1/
(cid:0) :
D 2n
When k.k 1/ 2n, therefore, the expected number of pairs of people with the
(cid:0) 
samebirthdayisatleast1. Thus,ifwehaveatleastp2n 1individualsinaroom,
C
wecanexpect atleasttwotohavethesamebirthday. Forn 365, ifk 28,the
D D
expected number of pairs with the same birthday is .28 27/=.2 365/ 1:0356.
  
Thus,withatleast28people, weexpecttofindatleastonematchingpairofbirth-
days. On Mars, where a year is 669 Martian days long, we need at least 38 Mar-
tians.
Thefirstanalysis, whichusedonlyprobabilities, determinedthenumberofpeo-
ple required for the probability to exceed 1=2 that a matching pair of birthdays
exists,andthesecondanalysis,whichusedindicatorrandomvariables,determined
the number such that the expected number of matching birthdays is 1. Although
theexactnumbersofpeopledifferforthetwosituations, theyarethesameasymp-
totically: ‚.pn/.
5.4.2 Ballsandbins
Consideraprocessinwhichwerandomlytossidenticalballsintobbins,numbered
1;2;:::;b. Thetosses are independent, and on each toss theball isequally likely
toendupinanybin. Theprobabilitythatatossedballlandsinanygivenbinis1=b.
Thus,theball-tossing process isasequence ofBernoullitrials(seeAppendixC.4)
with a probability 1=b of success, where success means that the ball falls in the
givenbin. Thismodelisparticularlyusefulforanalyzinghashing(seeChapter11),
andwecanansweravarietyofinterestingquestionsabouttheball-tossingprocess.
(ProblemC-1asksadditional questions aboutballsandbins.)
134 Chapter5 ProbabilisticAnalysisandRandomizedAlgorithms
Howmanyballsfallinagiven bin? Thenumberofballsthatfallinagivenbin
follows the binomial distribution b.k n;1=b/. If we toss n balls, equation (C.37)
I
tellsusthattheexpectednumberofballsthatfallinthegivenbinisn=b.
Howmanyballsmustwetoss,ontheaverage, untilagivenbincontainsaball?
The number of tosses until the given bin receives a ball follows the geometric
distribution withprobability 1=b and, byequation (C.32), theexpected number of
tossesuntilsuccessis1=.1=b/ b.
D
How many balls must we toss until every bin contains at least one ball? Letus
call a toss in which a ball falls into an empty bin a “hit.” We want to know the
expected numbernoftossesrequired togetb hits.
Usingthehits,wecanpartitionthentossesintostages. Theithstageconsistsof
thetossesafterthe.i 1/sthituntiltheithhit. Thefirststageconsists ofthefirst
(cid:0)
toss, since weare guaranteed to have a hit when all bins are empty. For each toss
during the ith stage, i 1 bins contain balls and b i 1 bins are empty. Thus,
(cid:0) (cid:0) C
foreachtossintheithstage, theprobability ofobtaining ahitis.b i 1/=b.
(cid:0) C
Letn denote the number of tosses in the ith stage. Thus, the number of tosses
i
required to get b hits is n b n . Each random variable n has a geometric
D i 1 i i
distributionwithprobabilityofsuDccess.b i 1/=b andthus,byequation(C.32),
P (cid:0) C
wehave
b
EŒn  :
i
D b i 1
(cid:0) C
Bylinearity ofexpectation, wehave
b
EŒn E n
i
D
" #
i 1
XD
b
EŒn 
i
D
i 1
XD
b
b
D b i 1
i 1 (cid:0) C
XD
b
1
b
D i
i 1
XD
b.lnb O.1// (byequation (A.7)) .
D C
It therefore takes approximately blnb tosses before we can expect that every bin
has a ball. This problem is also known as the coupon collector’s problem, which
says that a person trying to collect each of b different coupons expects to acquire
approximately blnb randomlyobtained couponsinordertosucceed.
5.4 Probabilisticanalysisandfurtherusesofindicatorrandomvariables 135
5.4.3 Streaks
Suppose you flip a fair coin n times. What is the longest streak of consecutive
heads that you expect to see? The answer is ‚.lgn/, as the following analysis
shows.
Wefirstprovethattheexpected lengthofthelongest streakofheadsisO.lgn/.
The probability that each coin flip is a head is 1=2. Let A be the event that a
ik
streak ofheads oflength atleast k begins withtheithcoinflipor, moreprecisely,
theeventthatthek consecutive coinflipsi;i 1;:::;i k 1yieldonlyheads,
C C (cid:0)
where1 k nand1 i n k 1. Sincecoinflipsaremutuallyindependent,
    (cid:0) C
foranygiveneventA ,theprobability thatallk flipsareheadsis
ik
Pr A 1=2k : (5.8)
ik
f g D
Fork 2 lgn ,
D d e
Pr A 1=22 lgn
f
i;2 dlgn
eg D
d e
1=22lgn

1=n2 ;
D
and thus the probability that a streak of heads of length at least 2 lgn begins in
d e
position i isquite small. There are at most n 2 lgn 1positions where such
(cid:0) d eC
astreakcanbegin. Theprobability thatastreakofheadsoflengthatleast2 lgn
d e
beginsanywhereistherefore
n 2 lgn 1 n 2 lgn 1
(cid:0) d eC (cid:0) d eC
Pr A 1=n2
(
i;2 dlgn
e) 
i 1 i 1
[D XD
n
< 1=n2
i 1
XD
1=n; (5.9)
D
since by Boole’s inequality (C.19), the probability of a union of events is at most
thesumoftheprobabilities oftheindividual events. (NotethatBoole’s inequality
holdsevenforeventssuchasthesethatarenotindependent.)
We now use inequality (5.9) to bound the length of the longest streak. For
j 0;1;2;:::;n,letL betheeventthatthelongeststreakofheadshaslengthex-
j
D
actlyj,andletLbethelengthofthelongeststreak. Bythedefinitionofexpected
value,wehave
n
EŒL j Pr L : (5.10)
j
D f g
j 0
XD
136 Chapter5 ProbabilisticAnalysisandRandomizedAlgorithms
We could try to evaluate this sum using upper bounds on each Pr L similar to
j
f g
those computed in inequality (5.9). Unfortunately, this method would yield weak
bounds. Wecan use some intuition gained by the above analysis to obtain agood
bound, however. Informally, we observe that for no individual term in the sum-
mation in equation (5.10) are both the factors j and Pr L large. Why? When
j
f g
j 2 lgn , then Pr L is very small, and when j < 2 lgn , then j is fairly
j
 d e f g d e
small. More formally, we note that the events L for j 0;1;:::;n are disjoint,
j
D
and so the probability that astreak of heads of length at least 2 lgn begins any-
whereis n Pr L . Byinequality(5.9),wehave n d Pre L < 1=n.
j 2 lgn f j g j 2 lgn f j g
Also, not PingD thd at e jn 0Pr fL
j
g D
1, we have that j2 dl Pg 0n e(cid:0)D1d Pr fe L
j
g 
1. Thus,
weobtain D D
P P
n
EŒL j Pr L
j
D f g
j 0
XD
2 lgn 1 n
d e(cid:0)
j Pr L j Pr L
j j
D f gC f g
j XD0 j DX2 dlgn
e
2 lgn 1 n
d e(cid:0)
< .2 lgn /Pr L nPr L
j j
d e f gC f g
j XD0 j DX2 dlgn
e
2 lgn 1 n
d e(cid:0)
2 lgn Pr L n Pr L
j j
D d e f gC f g
j XD0 j DX2 dlgn
e
< 2 lgn 1 n .1=n/
d e C 
O.lgn/:
D
The probability that a streak of heads exceeds r lgn flips diminishes quickly
d e
with r. For r 1, the probability that a streak of at least r lgn heads starts in
 d e
position i is
Pr A 1=2r lgn
f
i;r dlgn
eg D
d e
1=nr :

Thus, the probability is at most n=nr 1=nr 1 that the longest streak is at
(cid:0)
D
leastr lgn ,orequivalently, theprobability isatleast1 1=nr 1 thatthelongest
(cid:0)
d e (cid:0)
streakhaslengthlessthanr lgn .
d e
Asanexample, for n 1000 coin flips, the probability of having astreak ofat
D
least2 lgn 20headsisatmost1=n 1=1000. Thechance ofhaving astreak
d eD D
longerthan3 lgn 30headsisatmost1=n2 1=1,000,000.
d e D D
Wenowproveacomplementarylowerbound: theexpectedlengthofthelongest
streak ofheadsinncoinflipsis.lgn/. Toprove thisbound, welook forstreaks
5.4 Probabilisticanalysisandfurtherusesofindicatorrandomvariables 137
of length s by partitioning the n flips into approximately n=s groups of s flips
each. If we choose s .lgn/=2 , we can show that it is likely that at least one
D b c
of these groups comes up all heads, and hence it is likely that the longest streak
haslengthatleasts .lgn/. Wethenshowthatthelongest streakhasexpected
D
length.lgn/.
We partition the n coin flips into at least n= .lgn/=2 groups of .lgn/=2
b b cc b c
consecutive flips, and webound the probability that no group comes up all heads.
Byequation (5.8),theprobability thatthegroupstartinginposition i comesupall
headsis
Pr A 1=2 .lgn/=2
f
i; b.lgn/=2
cg D
b c
1=pn:

The probability that a streak of heads of length at least .lgn/=2 does not begin
b c
in position i is therefore at most 1 1=pn. Since the n= .lgn/=2 groups are
(cid:0) b b cc
formedfrom mutually exclusive, independent coinflips, theprobability thatevery
oneofthesegroupsfailstobeastreakoflength .lgn/=2 isatmost
b c
1 1=pn
bn= b.lgn/=2
cc 1 1=pn
n= b.lgn/=2 c(cid:0)1
(cid:0)  (cid:0)
(cid:0)  (cid:0)1
1=pn2n=lgn (cid:0)1
 (cid:0)
(cid:0)e (cid:0).2n=lgn (cid:0)1/ =pn

O.e lgn/
(cid:0)
D
O.1=n/:
D
Forthisargument, weusedinequality (3.12), 1 x ex,andthefact, whichyou
C 
mightwanttoverify,that.2n=lgn 1/=pn lgnforsufficientlylargen.
(cid:0) 
Thus,theprobability thatthelongest streakequalsorexceeds .lgn/=2 is
b c
n
Pr L 1 O.1=n/: (5.11)
j
f g  (cid:0)
j DbX.lgn/=2
c
Wecan now calculate a lower bound on the expected length of the longest streak,
beginning withequation (5.10)andproceeding inamannersimilartoouranalysis
oftheupperbound:
138 Chapter5 ProbabilisticAnalysisandRandomizedAlgorithms
n
EŒL j Pr L
j
D f g
j 0
XD
.lgn/=2 1 n
b c(cid:0)
j Pr L j Pr L
j j
D f gC f g
j XD0 j DbX.lgn/=2
c
.lgn/=2 1 n
b c(cid:0)
0 Pr L .lgn/=2 Pr L
j j
  f gC b c f g
j XD0 j DbX.lgn/=2
c
.lgn/=2 1 n
b c(cid:0)
0 Pr L .lgn/=2 Pr L
j j
D  f gCb c f g
j XD0 j DbX.lgn/=2
c
0 .lgn/=2 .1 O.1=n// (byinequality (5.11))
 Cb c (cid:0)
.lgn/:
D
Aswiththebirthdayparadox, wecanobtainasimplerbutapproximate analysis
using indicator random variables. We let X I A be the indicator random
ik ik
D f g
variable associated with a streak of heads of length at least k beginning with the
ithcoinflip. Tocountthetotalnumberofsuchstreaks, wedefine
n k 1
(cid:0) C
X X :
ik
D
i 1
XD
Takingexpectations andusinglinearity ofexpectation, wehave
n k 1
(cid:0) C
EŒX E X
ik
D
" #
i 1
XD
n k 1
(cid:0) C
EŒX 
ik
D
i 1
XD
n k 1
(cid:0) C
Pr A
ik
D f g
i 1
XD
n k 1
(cid:0) C
1=2k
D
i 1
XD
n k 1
(cid:0) C :
D 2k
By plugging in various values for k, we can calculate the expected number of
streaks of length k. If this number is large (much greater than 1), then we expect
many streaks of length k to occur and the probability that one occurs is high. If
5.4 Probabilisticanalysisandfurtherusesofindicatorrandomvariables 139
thisnumber issmall(muchless than 1), thenweexpect fewstreaks oflength k to
occur and the probability that one occurs is low. If k clgn, for some positive
D
constantc,weobtain
n clgn 1
EŒX (cid:0) C
D 2clgn
n clgn 1
(cid:0) C
D nc
1 .clgn 1/=n
(cid:0)
D nc 1 (cid:0) nc 1
(cid:0) (cid:0)
‚.1=nc 1/:
(cid:0)
D
Ifc islarge, the expected number of streaks of length clgn issmall, and wecon-
cludethattheyareunlikelytooccur. Ontheotherhand,ifc 1=2,thenweobtain
D
EŒX ‚.1=n1=2 1/ ‚.n1=2/, and we expect that there are a large number
(cid:0)
D D
of streaks of length .1=2/lgn. Therefore, one streak of such a length is likely to
occur. Fromtheseroughestimatesalone,wecanconcludethattheexpectedlength
ofthelongeststreakis‚.lgn/.
5.4.4 Theon-linehiringproblem
Asafinalexample,weconsideravariantofthehiringproblem. Supposenowthat
we do not wish to interview all the candidates in order to find the best one. We
alsodonotwishtohireandfireaswefindbetterandbetterapplicants. Instead,we
arewillingtosettleforacandidate whoisclosetothebest, inexchange forhiring
exactly once. We must obey one company requirement: after each interview we
musteitherimmediatelyofferthepositiontotheapplicantorimmediatelyrejectthe
applicant. What is the trade-off between minimizing the amount of interviewing
andmaximizingthequalityofthecandidate hired?
We can model this problem in the following way. After meeting an applicant,
weareabletogiveeachoneascore;letscore.i/denotethescorewegivetotheith
applicant,andassumethatnotwoapplicantsreceivethesamescore. Afterwehave
seen j applicants, we know which of the j has the highest score, but we do not
knowwhetheranyoftheremainingn j applicantswillreceiveahigherscore. We
(cid:0)
decidetoadoptthestrategyofselecting apositiveintegerk < n,interviewing and
thenrejectingthefirstkapplicants,andhiringthefirstapplicantthereafterwhohas
a higher score than all preceding applicants. If it turns out that the best-qualified
applicant was among the first k interviewed, then we hire the nth applicant. We
formalizethisstrategyintheprocedureON-LINE-MAXIMUM.k;n/,whichreturns
theindexofthecandidate wewishtohire.
140 Chapter5 ProbabilisticAnalysisandRandomizedAlgorithms
ON-LINE-MAXIMUM.k;n/
1 bestscore
D (cid:0)1
2 fori 1tok
D
3 ifscore.i/ > bestscore
4 bestscore score.i/
D
5 fori k 1ton
D C
6 ifscore.i/ > bestscore
7 returni
8 returnn
We wish to determine, for each possible value of k, the probability that we
hire the most qualified applicant. We then choose the best possible k, and
implement the strategy with that value. For the moment, assume that k is
fixed. Let M.j/ max score.i/ denote the maximum score among ap-
1 i j
plicants 1throughD j. LetS bef the eveng t that we succeed in choosing the best-
qualifiedapplicant, andletS betheeventthatwesucceedwhenthebest-qualified
i
applicant is the ith one interviewed. Since the various S are disjoint, we have
i
thatPr S n Pr S . Notingthatweneversucceedwhenthebest-qualified
f g D i 1 f i g
applicant isone oDfthefirstk,wehave thatPr S 0fori 1;2;:::;k. Thus,
i
P f g D D
weobtain
n
Pr S Pr S : (5.12)
i
f g D f g
i k 1
DXC
Wenow compute Pr S . Inorder tosucceed whenthebest-qualified applicant
i
f g
is the ith one, two things must happen. First, the best-qualified applicant must be
in position i, an event which we denote by B . Second, the algorithm must not
i
selectanyoftheapplicantsinpositionsk 1throughi 1,whichhappensonlyif,
C (cid:0)
foreachj suchthatk 1 j i 1,wefindthatscore.j/ < bestscoreinline6.
C   (cid:0)
(Becausescoresareunique,wecanignorethepossibilityofscore.j/ bestscore.)
D
In other words, all of the values score.k 1/ through score.i 1/ must be less
C (cid:0)
than M.k/; if any are greater than M.k/, we instead return the index of the first
one that is greater. We use O to denote the event that none of the applicants in
i
position k 1 through i 1 are chosen. Fortunately, the two events B and O
i i
C (cid:0)
areindependent. TheeventO depends onlyontherelative ordering ofthevalues
i
in positions 1 through i 1, whereas B depends only on whether the value in
i
(cid:0)
position i is greater than the values in all other positions. The ordering of the
values in positions 1through i 1 does not affect whether the value in position i
(cid:0)
is greater than all ofthem, and the value in position i does not affect the ordering
of the values in positions 1 through i 1. Thus we can apply equation (C.15) to
(cid:0)
obtain
5.4 Probabilisticanalysisandfurtherusesofindicatorrandomvariables 141
Pr S Pr B O Pr B Pr O :
i i i i i
f g D f \ g D f g f g
The probability Pr B is clearly 1=n, since the maximum is equally likely to
i
f g
be in any one of the n positions. For event O to occur, the maximum value in
i
positions1throughi 1,whichisequallylikelytobeinanyofthesei 1positions,
(cid:0) (cid:0)
must be in one of the first k positions. Consequently, Pr O k=.i 1/ and
i
f g D (cid:0)
Pr S k=.n.i 1//. Usingequation (5.12),wehave
i
f g D (cid:0)
n
Pr S Pr S
i
f g D f g
i k 1
DXC
n
k
D n.i 1/
i k 1 (cid:0)
DXC
n
k 1
D n i 1
i k 1 (cid:0)
DXC
n 1
k (cid:0) 1
:
D n i
i k
XD
Weapproximate by integrals to bound this summation from above and below. By
theinequalities (A.12),wehave
n 1 n (cid:0)1 1 n (cid:0)1 1
dx dx :
x  i  x
Zk
Xi Dk
Zk (cid:0)1
Evaluatingthesedefiniteintegrals givesusthebounds
k k
.lnn lnk/ Pr S .ln.n 1/ ln.k 1//;
n (cid:0)  f g  n (cid:0) (cid:0) (cid:0)
which provide a rather tight bound for Pr S . Because we wish to maximize our
f g
probability of success, let us focus on choosing the value of k that maximizes the
lowerboundonPr S . (Besides,thelower-boundexpressioniseasiertomaximize
f g
thantheupper-boundexpression.) Differentiatingtheexpression.k=n/.lnn lnk/
(cid:0)
withrespecttok,weobtain
1
.lnn lnk 1/:
n (cid:0) (cid:0)
Settingthisderivativeequalto0,weseethatwemaximizethelowerboundonthe
probabilitywhenlnk lnn 1 ln.n=e/or,equivalently, whenk n=e. Thus,
D (cid:0) D D
ifweimplementourstrategywithk n=e,wesucceedinhiringourbest-qualified
D
applicant withprobability atleast1=e.
142 Chapter5 ProbabilisticAnalysisandRandomizedAlgorithms
Exercises
5.4-1
How many people must there be in a room before the probability that someone
has the same birthday as you do is at least 1=2? How many people must there be
before the probability that at least two people have a birthday on July 4 is greater
than1=2?
5.4-2
Supposethatwetossballsintob binsuntilsomebincontains twoballs. Eachtoss
is independent, and each ball is equally likely to end up in any bin. What is the
expected numberofballtosses?
5.4-3 ?
Fortheanalysisofthebirthdayparadox,isitimportantthatthebirthdaysbemutu-
allyindependent, orispairwiseindependence sufficient? Justifyyouranswer.
5.4-4 ?
Howmanypeople should be invited toaparty in order tomake itlikely that there
arethreepeoplewiththesamebirthday?
5.4-5 ?
What isthe probability that ak-string overaset of size nforms ak-permutation?
Howdoesthisquestion relatetothebirthdayparadox?
5.4-6 ?
Supposethatnballsaretossedintonbins,whereeachtossisindependent andthe
ball is equally likely to end up in any bin. What is the expected number of empty
bins? Whatistheexpected numberofbinswithexactlyoneball?
5.4-7 ?
Sharpen thelowerbound onstreak length byshowingthatinnflipsofafaircoin,
theprobabilityislessthan1=nthatnostreaklongerthanlgn 2lglgnconsecutive
(cid:0)
headsoccurs.
ProblemsforChapter5 143
Problems
5-1 Probabilistic counting
Withab-bit counter, wecanordinarily only count up to2b 1. WithR.Morris’s
(cid:0)
probabilistic counting, we can count up to a much larger value at the expense of
somelossofprecision.
Weletacountervalueofi representacountofn fori 0;1;:::;2b 1,where
i
D (cid:0)
then formanincreasing sequence ofnonnegative values. Weassumethattheini-
i
tial value of the counter is 0, representing a count of n
0
0. The INCREMENT
D
operation works on a counter containing the value i in a probabilistic manner. If
i 2b 1, then the operation reports an overflow error. Otherwise, the INCRE-
D (cid:0)
MENT operation increases the counter by 1 with probability 1=.n i 1 n i/, and it
leavesthecounter unchanged withprobability 1 1=.n n /. C (cid:0)
i 1 i
If we select n i for all i 0, then the c(cid:0) ounter iC s a(cid:0) n ordinary one. More
i
D 
interesting situations arise if we select, say, n 2i 1 for i > 0 or n F (the
i (cid:0) i i
D D
ithFibonacci number—seeSection3.2).
For this problem, assume that n is large enough that the probability of an
2b 1
(cid:0)
overflowerrorisnegligible.
a. Show that the expected value represented by the counter after n INCREMENT
operations havebeenperformed isexactlyn.
b. The analysis of the variance of the count represented by the counter depends
on the sequence of the n . Let us consider a simple case: n 100i for
i i
D
alli 0. Estimatethevariance inthevalue represented bytheregister after n

INCREMENT operations havebeenperformed.
5-2 Searchinganunsortedarray
Thisproblemexaminesthreealgorithmsforsearching foravaluex inanunsorted
arrayAconsisting ofnelements.
Consider the following randomized strategy: pick a random index i into A. If
AŒi x, then weterminate; otherwise, wecontinue the search by picking anew
D
random indexintoA. Wecontinue picking random indices intoAuntilwefindan
index j such that AŒj x or until we have checked every element of A. Note
D
that we pick from the whole set of indices each time, so that we may examine a
givenelementmorethanonce.
a. Write pseudocode for a procedure RANDOM-SEARCH to implement the strat-
egyabove. BesurethatyouralgorithmterminateswhenallindicesintoAhave
beenpicked.
144 Chapter5 ProbabilisticAnalysisandRandomizedAlgorithms
b. Suppose that there is exactly one index i such that AŒi x. What is the
D
expected number of indices into A that we must pick before we find x and
RANDOM-SEARCH terminates?
c. Generalizing your solution to part (b), suppose that there are k 1 indices i

such that AŒi x. What is the expected number of indices into A that we
D
must pick before we find x and RANDOM-SEARCH terminates? Your answer
shouldbeafunctionofnandk.
d. Suppose that there are no indices i such that AŒi x. What is the expected
D
numberofindicesintoAthatwemustpickbeforewehavecheckedallelements
ofAand RANDOM-SEARCH terminates?
Now consider a deterministic linear search algorithm, which we refer to as
DETERMINISTIC-SEARCH. Specifically, the algorithm searches A for x in order,
considering AŒ1;AŒ2;AŒ3;:::;AŒn until either it finds AŒi x or it reaches
D
the end of the array. Assume that all possible permutations of the input array are
equally likely.
e. Suppose that there is exactly one index i such that AŒi x. What is the
D
average-case running time of DETERMINISTIC-SEARCH? What is the worst-
caserunning timeofDETERMINISTIC-SEARCH?
f. Generalizing your solution to part (e), suppose that there are k 1 indices i

suchthatAŒi x. Whatistheaverage-caserunningtimeofDETERMINISTIC-
D
SEARCH? Whatistheworst-case running timeof DETERMINISTIC-SEARCH?
Youranswershouldbeafunction ofnandk.
g. Supposethattherearenoindicesi suchthatAŒi x. Whatistheaverage-case
D
running time of DETERMINISTIC-SEARCH? What is the worst-case running
timeof DETERMINISTIC-SEARCH?
Finally, consider a randomized algorithm SCRAMBLE-SEARCH that works by
first randomly permuting the input array and then running the deterministic lin-
earsearchgivenaboveontheresultingpermuted array.
h. Lettingkbethenumberofindicesi suchthatAŒi x,givetheworst-caseand
D
expected running times of SCRAMBLE-SEARCH for the cases in which k 0
D
andk 1. Generalizeyoursolution tohandlethecaseinwhichk 1.
D 
i. Whichofthethreesearching algorithmswouldyouuse? Explainyouranswer.
NotesforChapter5 145
Chapter notes
Bolloba´s [53], Hofri[174], andSpencer [321]contain awealth ofadvanced prob-
abilistic techniques. The advantages of randomized algorithms are discussed and
surveyedbyKarp[200]andRabin[288]. ThetextbookbyMotwaniandRaghavan
[262]givesanextensivetreatmentofrandomizedalgorithms.
Severalvariantsofthehiringproblemhavebeenwidelystudied. Theseproblems
are more commonly referred to as “secretary problems.” An example of work in
thisareaisthepaperbyAjtai,Meggido,andWaarts[11].
II Sorting and Order Statistics
Introduction
Thispartpresents severalalgorithms thatsolvethefollowingsortingproblem:
Input: Asequence ofnnumbers a ;a ;:::;a .
1 2 n
h i
Output: A permutation (reordering) a ;a ;:::;a of the input sequence such
h
10 20 n0
i
thata a a .
10

20
  
n0
The input sequence is usually an n-element array, although it may be represented
insomeotherfashion, suchasalinked list.
Thestructureofthedata
Inpractice,thenumberstobesortedarerarelyisolatedvalues. Eachisusuallypart
of a collection of data called a record. Each record contains a key, which is the
valuetobesorted. Theremainderoftherecordconsistsofsatellitedata,whichare
usuallycarriedaroundwiththekey. Inpractice,whenasortingalgorithmpermutes
the keys, itmust permute the satellite data aswell. Ifeach record includes alarge
amountofsatellitedata,weoftenpermuteanarrayofpointerstotherecordsrather
thantherecordsthemselvesinordertominimizedatamovement.
In asense, it is these implementation details that distinguish an algorithm from
a full-blown program. A sorting algorithm describes the method by which we
determinethesortedorder,regardlessofwhetherwearesortingindividualnumbers
orlargerecordscontainingmanybytesofsatellitedata. Thus,whenfocusingonthe
problem of sorting, we typically assume that the input consists only of numbers.
Translating an algorithm for sorting numbers into a program for sorting records
148 PartII SortingandOrderStatistics
is conceptually straightforward, although in a given engineering situation other
subtleties maymaketheactualprogramming taskachallenge.
Whysorting?
Manycomputer scientists consider sorting tobethe mostfundamental problem in
thestudyofalgorithms. Thereareseveralreasons:
 Sometimes an application inherently needs to sort information. For example,
in order to prepare customer statements, banks need to sort checks by check
number.
 Algorithms often use sorting asakeysubroutine. Forexample, aprogram that
renders graphical objects which are layered on top of each other might have
to sort the objects according to an “above” relation so that it can draw these
objects from bottom totop. Weshall see numerous algorithms inthis text that
usesortingasasubroutine.
 We can draw from among a wide variety of sorting algorithms, and they em-
ployarichsetoftechniques. Infact,manyimportanttechniques usedthrough-
out algorithm design appear in the body of sorting algorithms that have been
developed over the years. In this way, sorting is also a problem of historical
interest.
 Wecanproveanontriviallowerboundforsorting(asweshalldoinChapter8).
Ourbestupperboundsmatchthelowerboundasymptotically, andsoweknow
that our sorting algorithms are asymptotically optimal. Moreover, we can use
thelowerboundforsortingtoprovelowerboundsforcertainotherproblems.
 Many engineering issues come to the fore when implementing sorting algo-
rithms. The fastest sorting program for a particular situation may depend on
many factors, such as prior knowledge about the keys and satellite data, the
memory hierarchy (caches and virtual memory) of the host computer, and the
software environment. Manyofthese issues arebestdealt withatthealgorith-
miclevel,ratherthanby“tweaking” thecode.
Sortingalgorithms
Weintroduced twoalgorithmsthatsortnrealnumbersinChapter2. Insertion sort
takes ‚.n2/ time in the worst case. Because its inner loops are tight, however,
it is a fast in-place sorting algorithm for small input sizes. (Recall that a sorting
algorithm sorts in place if only a constant number of elements of the input ar-
ray are ever stored outside the array.) Merge sort has a better asymptotic running
time,‚.nlgn/,butthe MERGE procedure itusesdoesnotoperateinplace.
PartII SortingandOrderStatistics 149
Inthispart,weshallintroducetwomorealgorithmsthatsortarbitraryrealnum-
bers. Heapsort,presentedinChapter6,sortsnnumbersinplaceinO.nlgn/time.
It uses an important data structure, called a heap, with which we can also imple-
mentapriority queue.
Quicksort,inChapter7,alsosortsnnumbersinplace,butitsworst-caserunning
time is ‚.n2/. Its expected running time is ‚.nlgn/, however, and it generally
outperforms heapsort inpractice. Likeinsertion sort,quicksort hastightcode,and
sothehidden constant factor initsrunning timeissmall. Itisapopular algorithm
forsorting largeinputarrays.
Insertionsort,mergesort,heapsort, andquicksort areallcomparisonsorts: they
determinethesortedorderofaninputarraybycomparingelements. Chapter8be-
ginsbyintroducingthedecision-treemodelinordertostudytheperformancelimi-
tationsofcomparisonsorts. Usingthismodel,weprovealowerboundof.nlgn/
on the worst-case running time of any comparison sort on n inputs, thus showing
thatheapsort andmergesortareasymptotically optimalcomparison sorts.
Chapter 8 then goes on to show that wecan beat this lower bound of .nlgn/
if we can gather information about the sorted order of the input by means other
thancomparing elements. Thecounting sortalgorithm, forexample, assumes that
the input numbers are in the set 0;1;:::;k . By using array indexing as a tool
f g
fordetermining relativeorder, countingsortcansortnnumbersin‚.k n/time.
C
Thus, when k O.n/, counting sort runs in time that is linear in the size of the
D
input array. A related algorithm, radix sort, can be used to extend the range of
counting sort. If there are n integers to sort, each integer has d digits, and each
digit can take on up to k possible values, then radix sort can sort the numbers
in ‚.d.n k// time. When d is a constant and k is O.n/, radix sort runs in
C
lineartime. Athirdalgorithm, bucket sort,requires knowledgeoftheprobabilistic
distribution of numbers in the input array. It can sort n real numbers uniformly
distributed inthehalf-open intervalŒ0;1/inaverage-case O.n/time.
Thefollowingtablesummarizestherunningtimesofthesortingalgorithmsfrom
Chapters2and6–8. Asusual,ndenotes thenumberofitemstosort. Forcounting
sort,theitemstosortareintegers intheset 0;1;:::;k . Forradixsort,eachitem
f g
is a d-digit number, where each digit takes on k possible values. For bucket sort,
we assume that the keys are real numbers uniformly distributed in the half-open
interval Œ0;1/. The rightmost column gives the average-case or expected running
time, indicating which it gives when it differs from the worst-case running time.
Weomittheaverage-case runningtimeofheapsortbecausewedonotanalyzeitin
thisbook.
150 PartII SortingandOrderStatistics
Worst-case Average-case/expected
Algorithm runningtime running time
Insertion sort ‚.n2/ ‚.n2/
Mergesort ‚.nlgn/ ‚.nlgn/
Heapsort O.nlgn/ —
Quicksort ‚.n2/ ‚.nlgn/ (expected)
Countingsort ‚.k n/ ‚.k n/
C C
Radixsort ‚.d.n k// ‚.d.n k//
C C
Bucketsort ‚.n2/ ‚.n/ (average-case)
Orderstatistics
The ith order statistic of a set of n numbers is the ith smallest number in the set.
We can, of course, select the ith order statistic by sorting the input and indexing
the ith element of the output. With no assumptions about the input distribution,
thismethodrunsin.nlgn/time,asthelowerboundprovedinChapter8shows.
In Chapter 9, we show that we can find the ith smallest element in O.n/ time,
evenwhentheelementsarearbitraryrealnumbers. Wepresentarandomizedalgo-
rithm with tight pseudocode that runs in ‚.n2/ time in the worst case, but whose
expected running time is O.n/. We also give a more complicated algorithm that
runsinO.n/worst-case time.
Background
Although most of this part does not rely on difficult mathematics, some sections
dorequiremathematicalsophistication. Inparticular, analysesofquicksort,bucket
sort, and the order-statistic algorithm use probability, which is reviewed in Ap-
pendix C,andthematerialonprobabilistic analysis andrandomized algorithms in
Chapter 5. The analysis of the worst-case linear-time algorithm for order statis-
tics involves somewhat more sophisticated mathematics than the other worst-case
analyses inthispart.
6 Heapsort
Inthischapter, weintroduce another sortingalgorithm: heapsort. Likemergesort,
but unlike insertion sort, heapsort’s running time isO.nlgn/. Likeinsertion sort,
but unlike merge sort, heapsort sorts in place: only a constant number of array
elements are stored outside the input array at any time. Thus, heapsort combines
thebetterattributes ofthetwosortingalgorithms wehavealreadydiscussed.
Heapsortalsointroducesanotheralgorithmdesigntechnique: usingadatastruc-
ture,inthiscaseonewecalla“heap,”tomanageinformation. Notonlyistheheap
datastructureusefulforheapsort,butitalsomakesanefficientpriorityqueue. The
heapdatastructure willreappear inalgorithms inlaterchapters.
Theterm“heap”wasoriginallycoinedinthecontextofheapsort,butithassince
come to refer to “garbage-collected storage,” such as the programming languages
Java and Lisp provide. Our heap data structure is not garbage-collected storage,
andwheneverwerefertoheaps inthisbook, weshallmeanadatastructure rather
thananaspectofgarbagecollection.
6.1 Heaps
The (binary) heap data structure is an array object that we can view as a
nearly complete binary tree (see Section B.5.3), as shown in Figure 6.1. Each
node of the tree corresponds to an element of the array. The tree is com-
pletely filled on all levels except possibly the lowest, which is filled from the
left up to a point. An array A that represents a heap is an object with two at-
tributes: A:length,which(asusual)givesthenumberofelementsinthearray,and
A:heap-size, which represents how many elements in the heap are stored within
arrayA. That is, although AŒ1::A:length may contain numbers, only the ele-
ments in AŒ1::A:heap-size, where 0 A:heap-size A:length, are valid ele-
 
mentsoftheheap. TherootofthetreeisAŒ1,andgiventheindexi ofanode, we
caneasilycomputetheindicesofitsparent, leftchild,andrightchild:
152 Chapter6 Heapsort
1
16
2 3
14 10
1 2 3 4 5 6 7 8 9 10
4 5 6 7
8 7 9 3 16 14 10 8 7 9 3 2 4 1
8 9 10
2 4 1
(a) (b)
Figure6.1 Amax-heapviewedas(a)abinarytreeand(b)anarray. Thenumberwithinthecircle
ateachnodeinthetreeisthevaluestoredatthatnode.Thenumberaboveanodeisthecorresponding
indexinthearray. Aboveandbelowthearrayarelinesshowingparent-childrelationships;parents
arealwaystotheleftoftheirchildren. Thetreehasheightthree;thenodeatindex4(withvalue8)
hasheightone.
PARENT.i/
1 return i=2
b c
LEFT.i/
1 return2i
RIGHT.i/
1 return2i 1
C
On most computers, the LEFT procedure can compute 2i in one instruction by
simplyshiftingthebinaryrepresentationofi leftbyonebitposition. Similarly,the
RIGHTprocedurecanquicklycompute2i 1byshiftingthebinaryrepresentation
C
of i left by one bit position and then adding in a 1 as the low-order bit. The
PARENT procedure can compute i=2 by shifting i right one bit position. Good
b c
implementationsofheapsortoftenimplementtheseproceduresas“macros”or“in-
line”procedures.
There are two kinds of binary heaps: max-heaps and min-heaps. In both kinds,
the values in the nodes satisfy a heap property, the specifics of which depend on
the kind of heap. In a max-heap, the max-heap property is that for every node i
otherthantheroot,
AŒPARENT.i/ AŒi;

that is, the value of a node is at most the value of its parent. Thus, the largest
elementinamax-heapisstoredattheroot,andthesubtreerootedatanodecontains
6.1 Heaps 153
values nolarger than thatcontained atthenodeitself. Amin-heapisorganized in
the opposite way; the min-heap property is that for every node i other than the
root,
AŒPARENT.i/ AŒi:

Thesmallestelementinamin-heapisattheroot.
For the heapsort algorithm, we use max-heaps. Min-heaps commonly imple-
ment priority queues, which we discuss in Section 6.5. We shall be precise in
specifying whether weneed a max-heap or a min-heap for any particular applica-
tion, andwhenproperties apply toeither max-heaps ormin-heaps, wejustusethe
term“heap.”
Viewing a heap as a tree, we define the height of a node in a heap to be the
numberofedgesonthelongestsimpledownwardpathfromthenodetoaleaf,and
wedefinetheheight oftheheap tobetheheight ofitsroot. Sinceaheap ofnele-
mentsisbasedonacompletebinarytree,itsheightis‚.lgn/(seeExercise6.1-2).
We shall see that the basic operations on heaps run in time at most proportional
totheheight ofthetree andthus take O.lgn/time. Theremainder ofthis chapter
presentssomebasicproceduresandshowshowtheyareusedinasortingalgorithm
andapriority-queue datastructure.
 TheMAX-HEAPIFY procedure, whichrunsinO.lgn/time,isthekeytomain-
taining themax-heap property.
 TheBUILD-MAX-HEAP procedure,whichrunsinlineartime,producesamax-
heapfromanunordered inputarray.
 The HEAPSORT procedure, which runs in O.nlgn/ time, sorts an array in
place.
 The MAX-HEAP-INSERT, HEAP-EXTRACT-MAX, HEAP-INCREASE-KEY,
and HEAP-MAXIMUM procedures, which run in O.lgn/ time, allow the heap
datastructure toimplementapriority queue.
Exercises
6.1-1
Whataretheminimumandmaximumnumbersofelementsinaheapofheighth?
6.1-2
Showthatann-elementheaphasheight lgn .
b c
6.1-3
Showthatinanysubtreeofamax-heap,therootofthesubtreecontainsthelargest
valueoccurring anywhereinthatsubtree.
154 Chapter6 Heapsort
6.1-4
Whereinamax-heapmightthesmallestelementreside,assumingthatallelements
aredistinct?
6.1-5
Isanarraythatisinsortedorderamin-heap?
6.1-6
Isthearraywithvalues 23;17;14;6;13;10;1;5;7;12 amax-heap?
h i
6.1-7
Show that, with the array representation for storing an n-element heap, the leaves
arethenodesindexedby n=2 1; n=2 2;:::;n.
b cC b cC
6.2 Maintainingthe heap property
Inordertomaintainthemax-heapproperty,wecalltheprocedure MAX-HEAPIFY.
Its inputs are an array A and an index i into the array. When it is called, MAX-
HEAPIFY assumes thatthebinary treesrooted at LEFT.i/and RIGHT.i/aremax-
heaps,butthatAŒimightbesmallerthanitschildren,thusviolatingthemax-heap
property. MAX-HEAPIFY lets the value at AŒi “float down” in the max-heap so
thatthesubtreerootedatindexi obeysthemax-heapproperty.
MAX-HEAPIFY.A;i/
1 l LEFT.i/
D
2 r RIGHT.i/
D
3 ifl A:heap-sizeandAŒl > AŒi

4 largest l
D
5 elselargest i
D
6 ifr A:heap-size andAŒr> AŒlargest

7 largest r
D
8 iflargest i
¤
9 exchange AŒiwithAŒlargest
10 MAX-HEAPIFY.A;largest/
Figure 6.2 illustrates the action of MAX-HEAPIFY. At each step, the largest of
the elements AŒi, AŒLEFT.i/, and AŒRIGHT.i/ is determined, and its index is
stored in largest. If AŒi is largest, then the subtree rooted at node i is already a
max-heapandtheprocedureterminates. Otherwise,oneofthetwochildrenhasthe
largest element, and AŒi is swapped with AŒlargest, which causes node i and its
6.2 Maintainingtheheapproperty 155
1 1
16 16
2 3 2 3
i 4 10 14 10
4 5 6 7 4 5 6 7
14 7 9 3 i 4 7 9 3
8 9 10 8 9 10
2 8 1 2 8 1
(a) (b)
1
16
2 3
14 10
4 5 6 7
8 7 9 3
8 9 i 10
2 4 1
(c)
Figure6.2 The action of MAX-HEAPIFY.A;2/, where A:heap-size 10. (a) The initial con-
D
figuration, with AŒ2 at node i 2 violating the max-heap property since it is not larger than
D
bothchildren. Themax-heappropertyisrestoredfornode2in(b)byexchanging AŒ2withAŒ4,
which destroys the max-heap property for node 4. The recursive call MAX-HEAPIFY.A;4/ now
hasi 4. AfterswappingAŒ4withAŒ9,asshownin(c),node4isfixedup,andtherecursivecall
D
MAX-HEAPIFY.A;9/yieldsnofurtherchangetothedatastructure.
children to satisfy the max-heap property. The node indexed by largest, however,
nowhastheoriginalvalueAŒi,andthusthesubtreerootedatlargestmightviolate
themax-heap property. Consequently, wecall MAX-HEAPIFY recursively onthat
subtree.
The running time of MAX-HEAPIFY on a subtree of size n rooted at a given
node i is the ‚.1/ time to fix up the relationships among the elements AŒi,
AŒLEFT.i/, and AŒRIGHT.i/, plus the time to run MAX-HEAPIFY on a subtree
rooted at one of the children of node i (assuming that the recursive call occurs).
Thechildren’s subtrees each havesize atmost2n=3—the worstcaseoccurs when
thebottom levelofthetreeisexactly halffull—and therefore wecandescribe the
runningtimeofMAX-HEAPIFY bytherecurrence
T.n/ T.2n=3/ ‚.1/:
 C
156 Chapter6 Heapsort
The solution to this recurrence, by case 2 of the master theorem (Theorem 4.1),
is T.n/ O.lgn/. Alternatively, we can characterize the running time of MAX-
D
HEAPIFY onanodeofheighthasO.h/.
Exercises
6.2-1
Using Figure 6.2 as a model, illustrate the operation of MAX-HEAPIFY.A;3/ on
thearrayA 27;17;3;16;13;10;1;5;7;12;4;8;9;0 .
Dh i
6.2-2
Starting with the procedure MAX-HEAPIFY, write pseudocode for the procedure
MIN-HEAPIFY.A;i/, which performs the corresponding manipulation on a min-
heap. How does the running time of MIN-HEAPIFY compare to that of MAX-
HEAPIFY?
6.2-3
Whatistheeffectofcalling MAX-HEAPIFY.A;i/whentheelementAŒiislarger
thanitschildren?
6.2-4
Whatistheeffectofcalling MAX-HEAPIFY.A;i/fori > A:heap-size=2?
6.2-5
Thecodefor MAX-HEAPIFY isquite efficient intermsofconstant factors, except
possibly for the recursive call in line 10, which might cause some compilers to
produce inefficient code. Write an efficient MAX-HEAPIFY that uses an iterative
control construct (aloop)insteadofrecursion.
6.2-6
Show that the worst-case running time of MAX-HEAPIFY on a heap of size n
is .lgn/. (Hint: For a heap with n nodes, give node values that cause MAX-
HEAPIFY to be called recursively at every node on a simple path from the root
downtoaleaf.)
6.3 Building a heap
We can use the procedure MAX-HEAPIFY in a bottom-up manner to convert an
array AŒ1::n, where n A:length, into a max-heap. By Exercise 6.1-7, the
D
elementsinthesubarrayAŒ. n=2 1/::nareallleavesofthetree,andsoeachis
b cC
6.3 Buildingaheap 157
a1-element heap tobegin with. Theprocedure BUILD-MAX-HEAP goes through
theremainingnodesofthetreeandruns MAX-HEAPIFY oneachone.
BUILD-MAX-HEAP.A/
1 A:heap-size A:length
D
2 fori A:length=2 downto1
D b c
3 MAX-HEAPIFY.A;i/
Figure6.3showsanexampleoftheactionofBUILD-MAX-HEAP.
To show why BUILD-MAX-HEAP works correctly, we use the following loop
invariant:
At the start of each iteration of the for loop of lines 2–3, each node i 1;
C
i 2;:::;nistherootofamax-heap.
C
Weneedtoshowthatthisinvariantistruepriortothefirstloopiteration, thateach
iterationoftheloopmaintainstheinvariant,andthattheinvariantprovidesauseful
propertytoshowcorrectness whentheloopterminates.
Initialization: Prior to the first iteration of the loop, i n=2 . Each node
D b c
n=2 1; n=2 2;:::;nisaleafandisthustherootofatrivialmax-heap.
b cC b cC
Maintenance: Toseethateachiterationmaintainstheloopinvariant,observethat
thechildrenofnodei arenumberedhigherthani. Bytheloopinvariant, there-
fore, theyarebothrootsofmax-heaps. Thisisprecisely thecondition required
for the call MAX-HEAPIFY.A;i/ to make node i a max-heap root. Moreover,
the MAX-HEAPIFY call preserves the property that nodes i 1;i 2;:::;n
C C
areallrootsofmax-heaps. Decrementingi intheforloopupdatereestablishes
theloopinvariant forthenextiteration.
Termination: Attermination, i 0. Bytheloopinvariant, eachnode1;2;:::;n
D
istherootofamax-heap. Inparticular, node1is.
We can compute a simple upper bound on the running time of BUILD-MAX-
HEAP as follows. Each call to MAX-HEAPIFY costs O.lgn/ time, and BUILD-
MAX-HEAP makes O.n/ such calls. Thus, the running time is O.nlgn/. This
upperbound, thoughcorrect, isnotasymptotically tight.
Wecanderiveatighter boundbyobserving thatthetimefor MAX-HEAPIFY to
runatanodevarieswiththeheightofthenodeinthetree,andtheheightsofmost
nodesaresmall. Ourtighteranalysisreliesonthepropertiesthatann-elementheap
hasheight lgn (seeExercise6.1-2)andatmost n=2h 1 nodesofanyheighth
C
b c
(seeExercise6.3-3).
˙ 
ThetimerequiredbyMAX-HEAPIFY whencalledonanodeofheighthisO.h/,
andsowecanexpressthetotalcostofBUILD-MAX-HEAP asbeingboundedfrom
aboveby
158 Chapter6 Heapsort
A 4 1 3 2 16 9 10 14 8 7
1 1
4 4
2 3 2 3
1 3 1 3
4 5 6 7 4 5 6 7
2 i 16 9 10 i 2 16 9 10
8 9 10 8 9 10
14 8 7 14 8 7
(a) (b)
1 1
4 4
2 3 2 3
1 3 i i 1 10
4 5 6 7 4 5 6 7
14 16 9 10 14 16 9 3
8 9 10 8 9 10
2 8 7 2 8 7
(c) (d)
1 1
i 4 16
2 3 2 3
16 10 14 10
4 5 6 7 4 5 6 7
14 7 9 3 8 7 9 3
8 9 10 8 9 10
2 8 1 2 4 1
(e) (f)
Figure 6.3 The operation of BUILD-MAX-HEAP, showing the data structure before the call to
MAX-HEAPIFY in line 3 of BUILD-MAX-HEAP. (a) A 10-element input array A and the bi-
nary tree it represents. The figure shows that the loop index i refers to node 5 before the call
MAX-HEAPIFY.A;i/. (b) The data structure that results. The loop index i for the next iteration
referstonode4. (c)–(e)SubsequentiterationsoftheforloopinBUILD-MAX-HEAP. Observethat
whenever MAX-HEAPIFYis called on a node, the two subtrees of that node are both max-heaps.
(f)Themax-heapafterBUILD-MAX-HEAPfinishes.
6.4 Theheapsortalgorithm 159
blgn
c n
blgn
c h
O.h/ O n :
2h 1 D 2h
Xh D0 l C m Xh D0 !
We evaluate the last summation by substituting x 1=2 in the formula (A.8),
D
yielding
1 h 1=2
2h D .1 1=2/2
h 0 (cid:0)
XD
2:
D
Thus,wecanboundtherunning timeof BUILD-MAX-HEAP as
blgn
c h 1 h
O n O n
2h D 2h
! !
h 0 h 0
XD XD
O.n/:
D
Hence,wecanbuildamax-heapfromanunordered arrayinlineartime.
We can build a min-heap by the procedure BUILD-MIN-HEAP, which is the
sameasBUILD-MAX-HEAP butwiththecalltoMAX-HEAPIFY inline3replaced
by a call to MIN-HEAPIFY (see Exercise 6.2-2). BUILD-MIN-HEAP produces a
min-heapfromanunordered lineararrayinlineartime.
Exercises
6.3-1
UsingFigure6.3asamodel,illustratetheoperationofBUILD-MAX-HEAP onthe
arrayA 5;3;17;10;84;19;6;22;9 .
Dh i
6.3-2
Whydowewanttheloopindexi inline2ofBUILD-MAX-HEAP todecreasefrom
A:length=2 to1ratherthanincrease from1to A:length=2 ?
b c b c
6.3-3
Showthatthereareatmost n=2h 1 nodesofheighthinanyn-elementheap.
C
˙ 
6.4 The heapsort algorithm
The heapsort algorithm starts by using BUILD-MAX-HEAP to build a max-heap
on the input array AŒ1::n, where n A:length. Since the maximum element
D
of the array is stored at the root AŒ1, we can put it into its correct final position
160 Chapter6 Heapsort
by exchanging it with AŒn. If we now discard node n from the heap—and we
can do so by simply decrementing A:heap-size—we observe that the children of
the root remain max-heaps, but the new root element might violate the max-heap
property. All we need to do to restore the max-heap property, however, is call
MAX-HEAPIFY.A;1/, which leaves a max-heap in AŒ1::n 1. The heapsort
(cid:0)
algorithm then repeats this process forthe max-heap ofsize n 1downtoaheap
(cid:0)
ofsize2. (SeeExercise6.4-2forapreciseloopinvariant.)
HEAPSORT.A/
1 BUILD-MAX-HEAP.A/
2 fori A:lengthdownto2
D
3 exchangeAŒ1withAŒi
4 A:heap-size A:heap-size 1
D (cid:0)
5 MAX-HEAPIFY.A;1/
Figure 6.4 shows an example of the operation of HEAPSORT after line 1 has built
the initial max-heap. The figure shows the max-heap before the first iteration of
theforloopoflines2–5andaftereachiteration.
The HEAPSORT procedure takestimeO.nlgn/,sincethecallto BUILD-MAX-
HEAP takes time O.n/ and each of the n 1 calls to MAX-HEAPIFY takes
(cid:0)
timeO.lgn/.
Exercises
6.4-1
Using Figure 6.4 as a model, illustrate the operation of HEAPSORT on the array
A 5;13;2;25;7;17;20;8;4 .
D h i
6.4-2
Arguethecorrectness ofHEAPSORT usingthefollowingloopinvariant:
At the start of each iteration of the for loop of lines 2–5, the subarray
AŒ1::i is a max-heap containing the i smallest elements of AŒ1::n, and
the subarray AŒi 1::n contains the n i largest elements of AŒ1::n,
C (cid:0)
sorted.
6.4-3
What is the running time of HEAPSORT on an array A of length n that is already
sortedinincreasing order? Whataboutdecreasing order?
6.4-4
Showthattheworst-caserunning timeof HEAPSORT is.nlgn/.
6.4 Theheapsortalgorithm 161
16 14 10
14 10 8 10 8 9
8 7 9 3 4 7 9 3 4 7 1 3
i
2 4 1 2 1 16 i 2 14 16
(a) (b) (c)
9 8 7
8 3 7 3 4 3
4 7 1 2 4 2 1 i 9 1 2 8 i 9
i 10 14 16 10 14 16 10 14 16
(d) (e) (f)
4 3 2
2 3 2 1 1 3 i
1 i 7 8 9 i 4 7 8 9 4 7 8 9
10 14 16 10 14 16 10 14 16
(g) (h) (i)
1
i 2 3
A 1 2 3 4 7 8 9 10 14 16
4 7 8 9
10 14 16
(j) (k)
Figure6.4 TheoperationofHEAPSORT.(a)Themax-heapdatastructurejustafterBUILD-MAX-
HEAPhasbuiltitinline1. (b)–(j)Themax-heapjustaftereachcallof MAX-HEAPIFYinline5,
showingthevalueofi atthattime. Onlylightlyshadednodesremainintheheap. (k)Theresulting
sortedarrayA.
162 Chapter6 Heapsort
6.4-5 ?
Showthatwhenallelementsaredistinct,thebest-caserunningtimeofHEAPSORT
is.nlgn/.
6.5 Priorityqueues
Heapsort is an excellent algorithm, but a good implementation of quicksort, pre-
sented inChapter 7,usually beatsitinpractice. Nevertheless, theheap datastruc-
ture itself has many uses. In this section, we present one of the most popular ap-
plications of aheap: asan efficient priority queue. Aswithheaps, priority queues
come in two forms: max-priority queues and min-priority queues. We will focus
here on how to implement max-priority queues, which are in turn based on max-
heaps;Exercise6.5-3asksyoutowritetheprocedures formin-priority queues.
A priority queue is a data structure for maintaining a set S of elements, each
withanassociatedvaluecalledakey. Amax-priorityqueuesupportsthefollowing
operations:
INSERT.S;x/inserts theelementx intothesetS,whichisequivalent totheoper-
ationS S x .
D [f g
MAXIMUM.S/returnstheelementofS withthelargestkey.
EXTRACT-MAX.S/removesandreturnstheelementofS withthelargest key.
INCREASE-KEY.S;x;k/increasesthevalueofelementx’skeytothenewvaluek,
whichisassumedtobeatleastaslargeasx’scurrent keyvalue.
Among their other applications, we can use max-priority queues to schedule
jobs on a shared computer. The max-priority queue keeps track of the jobs to
be performed and their relative priorities. When a job is finished or interrupted,
thescheduler selectsthehighest-priority jobfromamongthosependingbycalling
EXTRACT-MAX. The scheduler can add a new job to the queue at any time by
calling INSERT.
Alternatively,amin-priorityqueuesupportstheoperations INSERT, MINIMUM,
EXTRACT-MIN, and DECREASE-KEY. A min-priority queue can be used in an
event-driven simulator. The items in the queue are events to be simulated, each
with an associated time of occurrence that serves as its key. The events must be
simulated in order of their time of occurrence, because the simulation of an event
cancause othereventstobesimulated inthefuture. Thesimulation program calls
EXTRACT-MINateachsteptochoosethenexteventtosimulate. Asneweventsare
produced,thesimulatorinsertsthemintothemin-priorityqueuebycallingINSERT.
6.5 Priorityqueues 163
Weshallseeotherusesformin-priorityqueues, highlighting theDECREASE-KEY
operation, inChapters23and24.
Notsurprisingly,wecanuseaheaptoimplementapriorityqueue. Inagivenap-
plication, suchasjobscheduling orevent-drivensimulation, elementsofapriority
queue correspond toobjects intheapplication. Weoften need todetermine which
application object corresponds to a given priority-queue element, and vice versa.
When we use a heap to implement a priority queue, therefore, we often need to
store a handle to the corresponding application object in each heap element. The
exact makeup of the handle (such as a pointer or an integer) depends on the ap-
plication. Similarly, we need to store a handle to the corresponding heap element
in each application object. Here, the handle would typically be an array index.
Because heap elements change locations within the array during heap operations,
anactual implementation, upon relocating aheap element, would also havetoup-
date the array index in the corresponding application object. Because the details
of accessing application objects depend heavily on the application and its imple-
mentation, weshall not pursue them here, other than noting that in practice, these
handlesdoneedtobecorrectly maintained.
Nowwediscuss howtoimplement theoperations ofamax-priority queue. The
procedure HEAP-MAXIMUM implementsthe MAXIMUM operation in‚.1/time.
HEAP-MAXIMUM.A/
1 returnAŒ1
The procedure HEAP-EXTRACT-MAX implements the EXTRACT-MAX opera-
tion. Itissimilartotheforloopbody(lines3–5)ofthe HEAPSORT procedure.
HEAP-EXTRACT-MAX.A/
1 ifA:heap-size < 1
2 error“heapunderflow”
3 max AŒ1
D
4 AŒ1 AŒA:heap-size
D
5 A:heap-size A:heap-size 1
D (cid:0)
6 MAX-HEAPIFY.A;1/
7 returnmax
The running time of HEAP-EXTRACT-MAX is O.lgn/, since it performs only a
constantamountofworkontopoftheO.lgn/timefor MAX-HEAPIFY.
Theprocedure HEAP-INCREASE-KEY implements the INCREASE-KEY opera-
tion. Anindex i into thearrayidentifies thepriority-queue element whosekeywe
wish to increase. The procedure first updates the key of element AŒi to its new
value. Because increasing the key of AŒi might violate the max-heap property,
164 Chapter6 Heapsort
the procedure then, in a manner reminiscent of the insertion loop (lines 5–7) of
INSERTION-SORT fromSection2.1,traversesasimplepathfromthisnodetoward
the root tofind aproper place for the newly increased key. As HEAP-INCREASE-
KEY traverses this path, itrepeatedly compares anelement toits parent, exchang-
ingtheirkeysandcontinuingiftheelement’skeyislarger,andterminatingiftheel-
ement’skeyissmaller,sincethemax-heappropertynowholds. (SeeExercise6.5-5
forapreciseloopinvariant.)
HEAP-INCREASE-KEY.A;i;key/
1 ifkey <AŒi
2 error“newkeyissmallerthancurrentkey”
3 AŒi key
D
4 whilei > 1andAŒPARENT.i/ <AŒi
5 exchangeAŒiwithAŒPARENT.i/
6 i PARENT.i/
D
Figure 6.5showsanexampleofa HEAP-INCREASE-KEY operation. Therunning
time of HEAP-INCREASE-KEY on an n-element heap is O.lgn/, since the path
tracedfromthenodeupdatedinline3totheroothaslengthO.lgn/.
Theprocedure MAX-HEAP-INSERT implements the INSERT operation. Ittakes
asaninputthekeyofthenewelementtobeinsertedintomax-heapA. Theproce-
durefirstexpandsthemax-heapbyaddingtothetreeanewleafwhosekeyis .
(cid:0)1
Then it calls HEAP-INCREASE-KEY to set the key of this new node to its correct
valueandmaintainthemax-heap property.
MAX-HEAP-INSERT.A;key/
1 A:heap-size A:heap-size 1
D C
2 AŒA:heap-size
D (cid:0)1
3 HEAP-INCREASE-KEY.A;A:heap-size;key/
Therunning timeof MAX-HEAP-INSERT onann-elementheapisO.lgn/.
In summary, a heap can support any priority-queue operation on a set of size n
inO.lgn/time.
Exercises
6.5-1
Illustrate the operation of HEAP-EXTRACT-MAX on the heap A 15;13;9;5;
D h
12;8;7;4;0;6;2;1 .
i
6.5 Priorityqueues 165
16 16
14 10 14 10
8 7 9 3 8 7 9 3
i i
2 4 1 2 15 1
(a) (b)
16 16
i
14 10 15 10
i
15 7 9 3 14 7 9 3
2 8 1 2 8 1
(c) (d)
Figure6.5 Theoperation of HEAP-INCREASE-KEY. (a) The max-heap of Figure6.4(a) witha
node whose index is i heavily shaded. (b) This node has its key increased to 15. (c) After one
iterationofthewhileloopoflines4–6,thenodeanditsparenthaveexchangedkeys,andtheindexi
movesuptotheparent. (d)Themax-heapafteronemoreiterationofthewhileloop. Atthispoint,
AŒPARENT.i/ AŒi.Themax-heappropertynowholdsandtheprocedureterminates.

6.5-2
IllustratetheoperationofMAX-HEAP-INSERT.A;10/ontheheapA 15;13;9;
D h
5;12;8;7;4;0;6;2;1 .
i
6.5-3
Write pseudocode for the procedures HEAP-MINIMUM, HEAP-EXTRACT-MIN,
HEAP-DECREASE-KEY, and MIN-HEAP-INSERT that implement a min-priority
queuewithamin-heap.
6.5-4
Why do we bother setting the key of the inserted node to in line 2 of MAX-
(cid:0)1
HEAP-INSERT whenthenextthingwedoisincrease itskeytothedesired value?
166 Chapter6 Heapsort
6.5-5
Argue the correctness of HEAP-INCREASE-KEY using the following loop invari-
ant:
Atthestartofeachiterationofthewhileloopoflines4–6,AŒPARENT.i/

AŒLEFT.i/ and AŒPARENT.i/ AŒRIGHT.i/, if these nodes exist, and

thesubarrayAŒ1::A:heap-sizesatisfiesthemax-heapproperty,exceptthat
theremaybeoneviolation: AŒimaybelargerthanAŒPARENT.i/.
YoumayassumethatthesubarrayAŒ1::A:heap-sizesatisfiesthemax-heapprop-
ertyatthetimeHEAP-INCREASE-KEY iscalled.
6.5-6
Each exchange operation on line 5 of HEAP-INCREASE-KEY typically requires
three assignments. Show how to use the idea of the inner loop of INSERTION-
SORT toreducethethreeassignments downtojustoneassignment.
6.5-7
Show how to implement a first-in, first-out queue with a priority queue. Show
howtoimplementastackwithapriority queue. (Queuesandstacksaredefinedin
Section10.1.)
6.5-8
Theoperation HEAP-DELETE.A;i/deletes the item innode i from heap A. Give
an implementation of HEAP-DELETE that runs in O.lgn/ time for an n-element
max-heap.
6.5-9
Give an O.nlgk/-time algorithm to merge k sorted lists into one sorted list,
where n is the total number of elements in all the input lists. (Hint: Use a min-
heapfork-waymerging.)
Problems
6-1 Buildingaheapusinginsertion
We can build a heap by repeatedly calling MAX-HEAP-INSERT to insert the ele-
ments into the heap. Consider the following variation on the BUILD-MAX-HEAP
procedure:
ProblemsforChapter6 167
BUILD-MAX-HEAP0.A/
1 A:heap-size 1
D
2 fori 2toA:length
D
3 MAX-HEAP-INSERT.A;AŒi/
a. DotheproceduresBUILD-MAX-HEAP andBUILD-MAX-HEAP0alwayscreate
thesameheapwhenrunonthesameinputarray? Provethattheydo,orprovide
acounterexample.
b. Show that in the worst case, BUILD-MAX-HEAP0 requires ‚.nlgn/ time to
buildann-elementheap.
6-2 Analysisofd-aryheaps
A d-ary heap is like a binary heap, but (with one possible exception) non-leaf
nodeshaved children insteadof2children.
a. Howwouldyourepresent ad-aryheapinanarray?
b. Whatistheheightofad-aryheapofnelementsintermsofnandd?
c. GiveanefficientimplementationofEXTRACT-MAX inad-arymax-heap. An-
alyzeitsrunning timeintermsofd andn.
d. Give an efficient implementation of INSERT in a d-ary max-heap. Analyze its
running timeintermsofd andn.
e. Give an efficient implementation of INCREASE-KEY.A;i;k/, which flags an
errorifk <AŒi,butotherwisesetsAŒi k andthenupdatesthed-arymax-
D
heapstructureappropriately. Analyzeitsrunning timeintermsofd andn.
6-3 Youngtableaus
Anm n Youngtableau isanm n matrix such that the entries ofeach row are
 
insorted orderfromlefttorightandtheentries ofeachcolumnareinsorted order
from top to bottom. Someof the entries ofaYoung tableau maybe , which we
1
treat asnonexistent elements. Thus, aYoung tableau can be used to hold r mn

finitenumbers.
a. Drawa4 4Youngtableaucontainingtheelements 9;16;3;2;4;8;5;14;12 .
 f g
b. Argue that an m n Young tableau Y is empty if YŒ1;1 . Argue that Y
 D 1
isfull(contains mnelements)ifYŒm;n < .
1
168 Chapter6 Heapsort
c. Givean algorithm to implement EXTRACT-MIN on a nonempty m n Young

tableau that runs in O.m n/ time. Your algorithm should use a recur-
C
sive subroutine that solves an m n problem by recursively solving either

an .m 1/ n or an m .n 1/ subproblem. (Hint: Think about MAX-
(cid:0)   (cid:0)
HEAPIFY.) DefineT.p/,wherep m n,tobethemaximumrunning time
D C
of EXTRACT-MIN on any m n Young tableau. Give and solve a recurrence

forT.p/thatyieldstheO.m n/timebound.
C
d. Show how to insert a new element into a nonfull m n Young tableau in

O.m n/time.
C
e. Usingnoothersortingmethodasasubroutine,showhowtouseann nYoung

tableautosortn2 numbersinO.n3/time.
f. Give an O.m n/-time algorithm to determine whether a given number is
C
storedinagivenm nYoungtableau.

Chapter notes
The heapsort algorithm was invented by Williams [357], who also described how
to implement a priority queue with a heap. The BUILD-MAX-HEAP procedure
wassuggested byFloyd[106].
Weusemin-heapstoimplementmin-priorityqueuesinChapters16,23,and24.
Wealsogiveanimplementation withimprovedtimeboundsforcertainoperations
in Chapter 19 and, assuming that the keys are drawn from a bounded set of non-
negativeintegers,Chapter20.
If the data are b-bit integers, and the computer memory consists of addressable
b-bit words, Fredman and Willard [115] showed how to implement MINIMUM in
O.1/ time and INSERT and EXTRACT-MIN in O. lgn/ time. Thorup [337] has
improved the O. lgn/ bound to O.lglgn/ time. This bound uses an amount of
p
spaceunbounded inn,butitcanbeimplementedinlinearspacebyusingrandom-
p
izedhashing.
An important special case of priority queues occurs when the sequence of
EXTRACT-MIN operations is monotone, that is, the values returned by succes-
sive EXTRACT-MIN operations aremonotonically increasing overtime. Thiscase
arises in several important applications, such as Dijkstra’s single-source shortest-
paths algorithm, which we discuss in Chapter 24, and in discrete-event simula-
tion. ForDijkstra’salgorithmitisparticularly importantthattheDECREASE-KEY
operation be implemented efficiently. For the monotone case, if the data are in-
tegers in the range 1;2;:::;C, Ahuja, Mehlhorn, Orlin, and Tarjan [8] describe
NotesforChapter6 169
how to implement EXTRACT-MIN and INSERT in O.lgC/ amortized time (see
Chapter 17 for more on amortized analysis) and DECREASE-KEY in O.1/ time,
using a data structure called a radix heap. The O.lgC/ bound can be improved
to O. lgC/ using Fibonacci heaps (see Chapter 19) in conjunction with radix
heaps. Cherkassky, Goldberg, and Silverstein [65] further improved the bound to
p
O.lg1=3 CC/ expected time by combining the multilevel bucketing structure of
Denardo and Fox [85] with the heap of Thorup mentioned earlier. Raman [291]
further improved these results to obtain a bound of O.min.lg1=4 CC;lg1=3 Cn//,
foranyfixed > 0.
7 Quicksort
Thequicksort algorithm hasaworst-case running timeof‚.n2/onaninputarray
ofnnumbers. Despitethisslowworst-caserunningtime,quicksortisoftenthebest
practical choice for sorting because it is remarkably efficient on the average: its
expectedrunningtimeis‚.nlgn/,andtheconstantfactorshiddeninthe‚.nlgn/
notationarequitesmall. Italsohastheadvantageofsortinginplace(seepage17),
anditworkswelleveninvirtual-memory environments.
Section 7.1describes thealgorithm andanimportant subroutine used byquick-
sort for partitioning. Because the behavior of quicksort is complex, we start with
an intuitive discussion of its performance in Section 7.2 and postpone its precise
analysis to the end of the chapter. Section 7.3 presents a version of quicksort that
uses random sampling. This algorithm has a good expected running time, and no
particular input elicits its worst-case behavior. Section 7.4 analyzes the random-
izedalgorithm, showingthatitrunsin‚.n2/timeintheworstcaseand,assuming
distinct elements,inexpectedO.nlgn/time.
7.1 Descriptionofquicksort
Quicksort, like merge sort, applies the divide-and-conquer paradigm introduced
in Section 2.3.1. Here is the three-step divide-and-conquer process for sorting a
typicalsubarray AŒp::r:
Divide: Partition (rearrange) the array AŒp::r into two (possibly empty) subar-
raysAŒp::q 1andAŒq 1::rsuchthateachelementofAŒp :: q 1is
(cid:0) C (cid:0)
less than orequal to AŒq, which is, in turn, less than or equal to each element
ofAŒq 1::r. Computetheindexq aspartofthispartitioning procedure.
C
Conquer: SortthetwosubarraysAŒp::q 1andAŒq 1::rbyrecursivecalls
(cid:0) C
toquicksort.
7.1 Descriptionofquicksort 171
Combine: Becausethesubarraysarealreadysorted,noworkisneededtocombine
them: theentirearrayAŒp::risnowsorted.
Thefollowingprocedure implementsquicksort:
QUICKSORT.A;p;r/
1 ifp < r
2 q PARTITION.A;p;r/
D
3 QUICKSORT.A;p;q 1/
(cid:0)
4 QUICKSORT.A;q 1;r/
C
TosortanentirearrayA,theinitialcallisQUICKSORT.A;1;A:length/.
Partitioningthearray
Thekeytothealgorithm isthePARTITION procedure, whichrearranges thesubar-
rayAŒp::rinplace.
PARTITION.A;p;r/
1 x AŒr
D
2 i p 1
D (cid:0)
3 forj p tor 1
D (cid:0)
4 ifAŒj x

5 i i 1
D C
6 exchange AŒiwithAŒj
7 exchange AŒi 1withAŒr
C
8 returni 1
C
Figure 7.1 shows how PARTITION works on an 8-element array. PARTITION
alwaysselectsanelementx AŒrasapivotelementaroundwhichtopartitionthe
D
subarrayAŒp::r. Astheprocedureruns,itpartitionsthearrayintofour(possibly
empty)regions. Atthestartofeachiterationoftheforloopinlines3–6,theregions
satisfy certain properties, showninFigure 7.2. Westate these properties asaloop
invariant:
At the beginning of each iteration of the loop of lines 3–6, for any array
indexk,
1. Ifp k i,thenAŒk x.
  
2. Ifi 1 k j 1,thenAŒk >x.
C   (cid:0)
3. Ifk r,thenAŒk x.
D D
172 Chapter7 Quicksort
i p,j r
(a) 2 8 7 1 3 5 6 4
p,i j r
(b) 2 8 7 1 3 5 6 4
p,i j r
(c) 2 8 7 1 3 5 6 4
p,i j r
(d) 2 8 7 1 3 5 6 4
p i j r
(e) 2 1 7 8 3 5 6 4
p i j r
(f) 2 1 3 8 7 5 6 4
p i j r
(g) 2 1 3 8 7 5 6 4
p i r
(h) 2 1 3 8 7 5 6 4
p i r
(i) 2 1 3 4 7 5 6 8
Figure7.1 Theoperationof PARTITIONonasamplearray. ArrayentryAŒrbecomesthepivot
elementx. Lightlyshadedarrayelementsareallinthefirstpartitionwithvaluesnogreaterthanx.
Heavilyshaded elementsareinthesecond partitionwithvaluesgreater thanx. Theunshaded el-
ements have not yet been put in one of the first two partitions, and the final white element is the
pivotx. (a)Theinitialarrayandvariablesettings. Noneoftheelementshavebeenplacedineither
ofthefirsttwopartitions.(b)Thevalue2is“swappedwithitself”andputinthepartitionofsmaller
values. (c)–(d)Thevalues8and7areaddedtothepartitionoflargervalues. (e)Thevalues1and8
areswapped, andthesmallerpartitiongrows. (f)Thevalues3and7areswapped, andthesmaller
partitiongrows.(g)–(h)Thelargerpartitiongrowstoinclude5and6,andtheloopterminates.(i)In
lines7–8,thepivotelementisswappedsothatitliesbetweenthetwopartitions.
Theindicesbetweenj andr 1arenotcoveredbyanyofthethreecases,andthe
(cid:0)
valuesintheseentrieshavenoparticular relationship tothepivotx.
We need to show that this loop invariant is true prior to the first iteration, that
each iteration oftheloop maintains the invariant, and thatthe invariant provides a
usefulproperty toshowcorrectness whentheloopterminates.
7.1 Descriptionofquicksort 173
p i j r
x
£ x > x unrestricted
Figure7.2 ThefourregionsmaintainedbytheprocedurePARTITIONonasubarrayAŒp::r.The
valuesinAŒp::iarealllessthanorequaltox,thevaluesinAŒi 1::j 1areallgreaterthanx,
C (cid:0)
andAŒr x.ThesubarrayAŒj::r 1cantakeonanyvalues.
D (cid:0)
Initialization: Prior to the first iteration of the loop, i p 1 and j p. Be-
D (cid:0) D
causenovaluesliebetweenp andi andnovaluesliebetweeni 1andj 1,
C (cid:0)
the first twoconditions of the loop invariant are trivially satisfied. The assign-
mentinline1satisfiesthethirdcondition.
Maintenance: As Figure 7.3 shows, we consider two cases, depending on the
outcomeofthetestinline4. Figure7.3(a)showswhathappenswhenAŒj> x;
theonlyactionintheloopistoincrementj. Afterj isincremented,condition2
holdsforAŒj 1andallotherentriesremainunchanged. Figure7.3(b)shows
(cid:0)
what happens when AŒj x; the loop increments i, swaps AŒi and AŒj,

and then increments j. Because of the swap, wenow have that AŒi x, and

condition 1 is satisfied. Similarly, we also have that AŒj 1 > x, since the
(cid:0)
itemthatwasswappedintoAŒj 1is,bytheloopinvariant, greaterthanx.
(cid:0)
Termination: Attermination, j r. Therefore, everyentryinthearrayisinone
D
of the three sets described by the invariant, and wehave partitioned the values
in the array into three sets: those less than or equal to x, those greater than x,
andasingleton setcontaining x.
Thefinaltwolines of PARTITION finish upbyswapping thepivot element with
theleftmostelementgreaterthanx,therebymovingthepivotintoitscorrectplace
in the partitioned array, and then returning the pivot’s new index. The output of
PARTITION now satisfies the specifications given for the divide step. In fact, it
satisfies a slightly stronger condition: after line 2 of QUICKSORT, AŒq is strictly
lessthaneveryelementofAŒq 1::r.
C
The running time of PARTITION on the subarray AŒp::r is ‚.n/, where
n r p 1(seeExercise7.1-3).
D (cid:0) C
Exercises
7.1-1
Using Figure 7.1 as a model, illustrate the operation of PARTITION on the array
A 13;19;9;5;12;8;7;4;21;2;6;11 .
Dh i
174 Chapter7 Quicksort
p i j r
(a) >x x
£ x > x
p i j r
x
£ x > x
p i j r
(b) £ x x
£ x > x
p i j r
x
£ x > x
Figure7.3 Thetwocasesfor one iterationof procedure PARTITION. (a) If AŒj > x, theonly
actionistoincrementj,whichmaintainstheloopinvariant.(b)IfAŒj x,indexiisincremented,

AŒiandAŒjareswapped,andthenj isincremented.Again,theloopinvariantismaintained.
7.1-2
What value of q does PARTITION return when all elements in the array AŒp::r
have the same value? Modify PARTITION so that q .p r/=2 when all
D b C c
elementsinthearrayAŒp::rhavethesamevalue.
7.1-3
Giveabriefargument thattherunning timeof PARTITION onasubarray ofsizen
is‚.n/.
7.1-4
Howwouldyoumodify QUICKSORT tosortintononincreasing order?
7.2 Performanceofquicksort
The running time of quicksort depends on whether the partitioning is balanced or
unbalanced, which in turn depends on which elements are used for partitioning.
If the partitioning is balanced, the algorithm runs asymptotically as fast as merge
7.2 Performanceofquicksort 175
sort. Ifthepartitioning isunbalanced, however,itcanrunasymptoticallyasslowly
as insertion sort. In this section, we shall informally investigate how quicksort
performsundertheassumptions ofbalanced versusunbalanced partitioning.
Worst-case partitioning
The worst-case behavior for quicksort occurs when the partitioning routine pro-
duces one subproblem with n 1 elements and one with 0 elements. (We prove
(cid:0)
thisclaim inSection 7.4.1.) Letusassume thatthisunbalanced partitioning arises
in each recursive call. The partitioning costs ‚.n/ time. Since the recursive call
onanarrayofsize0justreturns,T.0/ ‚.1/,andtherecurrence fortherunning
D
timeis
T.n/ T.n 1/ T.0/ ‚.n/
D (cid:0) C C
T.n 1/ ‚.n/:
D (cid:0) C
Intuitively, if we sum the costs incurred at each level of the recursion, we get
an arithmetic series (equation (A.2)), which evaluates to ‚.n2/. Indeed, it is
straightforward tousethesubstitution methodtoprovethattherecurrenceT.n/
D
T.n 1/ ‚.n/hasthesolution T.n/ ‚.n2/. (SeeExercise7.2-1.)
(cid:0) C D
Thus,ifthepartitioning ismaximally unbalanced ateveryrecursive levelofthe
algorithm, the running time is ‚.n2/. Therefore the worst-case running time of
quicksortisnobetterthanthatofinsertionsort. Moreover,the‚.n2/runningtime
occurs when the input array is already completely sorted—a common situation in
whichinsertion sortrunsinO.n/time.
Best-casepartitioning
In the most even possible split, PARTITION produces two subproblems, each of
sizenomorethann=2,sinceoneisofsize n=2 andoneofsize n=2 1. Inthis
b c d e(cid:0)
case,quicksort runsmuchfaster. Therecurrence fortherunningtimeisthen
T.n/ 2T.n=2/ ‚.n/;
D C
wherewetoleratethesloppiness fromignoringthefloorandceilingandfromsub-
tracting 1. Bycase2ofthemastertheorem (Theorem 4.1),thisrecurrence hasthe
solution T.n/ ‚.nlgn/. Byequally balancing the two sides of the partition at
D
everyleveloftherecursion, wegetanasymptotically fasteralgorithm.
Balancedpartitioning
Theaverage-case running timeofquicksort ismuchclosertothebestcase thanto
the worst case, as the analyses in Section 7.4 will show. The key to understand-
176 Chapter7 Quicksort
PSfrag n cn
1 n 9 n cn
10 10
log n
10
1 n 9 n 9 n 81 n cn
100 100 100 100
log n
10=9
81 n 729 n cn
1 1000 1000
cn

1 cn

O.nlgn/
Figure7.4 ArecursiontreeforQUICKSORTinwhichPARTITIONalwaysproducesa9-to-1split,
yieldingarunningtimeofO.nlgn/.Nodesshowsubproblemsizes,withper-levelcostsontheright.
Theper-levelcostsincludetheconstantcimplicitinthe‚.n/term.
ing why is to understand how the balance of the partitioning is reflected in the
recurrence thatdescribes therunning time.
Suppose, for example, that the partitioning algorithm always produces a 9-to-1
proportional split,whichatfirstblushseemsquiteunbalanced. Wethenobtainthe
recurrence
T.n/ T.9n=10/ T.n=10/ cn;
D C C
ontherunningtimeofquicksort, wherewehaveexplicitlyincluded theconstant c
hidden in the ‚.n/ term. Figure 7.4 shows the recursion tree for this recurrence.
Noticethateverylevelofthetreehascostcn,untiltherecursion reachesabound-
arycondition atdepthlog n ‚.lgn/,andthenthelevelshavecostatmostcn.
10 D
The recursion terminates at depth log n ‚.lgn/. The total cost of quick-
10=9 D
sort is therefore O.nlgn/. Thus, with a 9-to-1 proportional split at every level of
recursion, which intuitively seems quite unbalanced, quicksort runs in O.nlgn/
time—asymptotically the sameasifthesplit wereright downthemiddle. Indeed,
evena99-to-1 split yieldsanO.nlgn/running time. Infact, anysplitofconstant
proportionalityyieldsarecursiontreeofdepth‚.lgn/,wherethecostateachlevel
is O.n/. The running time is therefore O.nlgn/ whenever the split has constant
proportionality.
7.2 Performanceofquicksort 177
n
Q (n) n Q (n)
0 n–1
(n–1)/2 (n–1)/2
(n–1)/2 – 1 (n–1)/2
(a) (b)
Figure7.5 (a) Two levels of a recursion tree for quicksort. The partitioning at the root costs n
andproducesa“bad”split: twosubarraysofsizes0andn 1. Thepartitioningofthesubarrayof
(cid:0)
sizen 1costsn 1andproducesa“good”split: subarraysofsize.n 1/=2 1and.n 1/=2.
(cid:0) (cid:0) (cid:0) (cid:0) (cid:0)
(b)Asinglelevelofarecursiontreethatisverywellbalanced.Inbothparts,thepartitioningcostfor
thesubproblemsshownwithellipticalshadingis‚.n/.Yetthesubproblemsremainingtobesolved
in(a),shownwithsquareshading,arenolargerthanthecorrespondingsubproblemsremainingtobe
solvedin(b).
Intuitionfortheaverage case
Todevelop aclear notion oftherandomized behavior ofquicksort, wemustmake
an assumption about how frequently we expect to encounter the various inputs.
The behavior of quicksort depends on the relative ordering of the values in the
array elements givenastheinput, and notbytheparticular values inthe array. As
in our probabilistic analysis of the hiring problem in Section 5.2, we will assume
fornowthatallpermutations oftheinputnumbersareequallylikely.
When we run quicksort on a random input array, the partitioning is highly un-
likely to happen in the same way at every level, as our informal analysis has as-
sumed. We expect that some of the splits will be reasonably well balanced and
thatsomewillbefairlyunbalanced. Forexample,Exercise7.2-6asksyoutoshow
thatabout80percentofthetimePARTITION producesasplitthatismorebalanced
than9to1,andabout20percentofthetimeitproducesasplitthatislessbalanced
than9to1.
Intheaveragecase, PARTITION producesamixof“good”and“bad”splits. Ina
recursiontreeforanaverage-caseexecutionofPARTITION,thegoodandbadsplits
are distributed randomly throughout the tree. Suppose, for the sake of intuition,
that the good and bad splits alternate levels in the tree, and that the good splits
are best-case splits and the bad splits are worst-case splits. Figure 7.5(a) shows
the splits at two consecutive levels in the recursion tree. At the root of the tree,
the cost is n for partitioning, and the subarrays produced have sizes n 1 and 0:
(cid:0)
the worst case. At the next level, the subarray of size n 1 undergoes best-case
(cid:0)
partitioning intosubarrays ofsize.n 1/=2 1and.n 1/=2. Let’sassumethat
(cid:0) (cid:0) (cid:0)
theboundary-condition costis1forthesubarrayofsize0.
178 Chapter7 Quicksort
Thecombination ofthebadsplitfollowedbythegoodsplitproduces threesub-
arrays of sizes 0, .n 1/=2 1, and .n 1/=2 at a combined partitioning cost
(cid:0) (cid:0) (cid:0)
of ‚.n/ ‚.n 1/ ‚.n/. Certainly, this situation is no worse than that in
C (cid:0) D
Figure 7.5(b), namely asingle levelofpartitioning thatproduces twosubarrays of
size .n 1/=2, at a cost of ‚.n/. Yet this latter situation is balanced! Intuitively,
(cid:0)
the ‚.n 1/ cost of the bad split can be absorbed into the ‚.n/ cost of the good
(cid:0)
split,andtheresultingsplitisgood. Thus,therunningtimeofquicksort,whenlev-
els alternate between good and bad splits, is like the running time for good splits
alone: stillO.nlgn/,butwithaslightly largerconstant hiddenbytheO-notation.
We shall give a rigorous analysis of the expected running time of a randomized
versionofquicksort inSection7.4.2.
Exercises
7.2-1
Usethesubstitution methodtoprovethattherecurrenceT.n/ T.n 1/ ‚.n/
D (cid:0) C
hasthesolution T.n/ ‚.n2/,asclaimedatthebeginning ofSection7.2.
D
7.2-2
What is the running time of QUICKSORT when all elements of array A have the
samevalue?
7.2-3
Show that the running time of QUICKSORT is ‚.n2/ when the array A contains
distinct elementsandissortedindecreasing order.
7.2-4
Banksoftenrecord transactions onanaccount inorder ofthetimesofthetransac-
tions, but many people like to receive their bank statements with checks listed in
orderbychecknumber. Peopleusuallywritechecksinorderbychecknumber,and
merchantsusuallycashthemwithreasonable dispatch. Theproblemofconverting
time-of-transaction ordering tocheck-number ordering istherefore theproblem of
sorting almost-sorted input. Argue that the procedure INSERTION-SORT would
tendtobeattheprocedure QUICKSORT onthisproblem.
7.2-5
Suppose thatthesplits ateverylevelofquicksort areintheproportion 1 ˛ to˛,
(cid:0)
where0 < ˛ 1=2isaconstant. Showthattheminimumdepthofaleafinthere-

cursiontreeisapproximately lgn=lg˛andthemaximumdepthisapproximately
(cid:0)
lgn=lg.1 ˛/. (Don’tworryaboutintegerround-off.)
(cid:0) (cid:0)
7.3 Arandomizedversionofquicksort 179
7.2-6 ?
Arguethatforanyconstant 0 < ˛ 1=2, theprobability isapproximately 1 2˛
 (cid:0)
thatonarandominputarray,PARTITIONproducesasplitmorebalancedthan1 ˛
(cid:0)
to˛.
7.3 A randomized versionofquicksort
Inexploring theaverage-case behavior ofquicksort, wehave madeanassumption
that all permutations of the input numbers are equally likely. In an engineering
situation, however, we cannot always expect this assumption to hold. (See Exer-
cise7.2-4.) AswesawinSection 5.3,wecan sometimes addrandomization toan
algorithminordertoobtaingoodexpectedperformanceoverallinputs. Manypeo-
ple regard the resulting randomized version of quicksort as the sorting algorithm
ofchoiceforlargeenoughinputs.
In Section 5.3, we randomized our algorithm by explicitly permuting the in-
put. We could do so for quicksort also, but a different randomization technique,
called random sampling, yields a simpler analysis. Instead of always using AŒr
asthepivot,wewillselectarandomlychosenelementfromthesubarrayAŒp::r.
We do so by first exchanging element AŒr with an element chosen at random
fromAŒp::r. Byrandomlysamplingtherangep;:::;r,weensurethatthepivot
element x AŒr is equally likely to be any of the r p 1 elements in the
D (cid:0) C
subarray. Because we randomly choose the pivot element, we expect the split of
theinputarraytobereasonably wellbalanced onaverage.
The changes to PARTITION and QUICKSORT are small. In the new partition
procedure, wesimplyimplementtheswapbeforeactuallypartitioning:
RANDOMIZED-PARTITION.A;p;r/
1 i RANDOM.p;r/
D
2 exchange AŒrwithAŒi
3 return PARTITION.A;p;r/
Thenewquicksort calls RANDOMIZED-PARTITION inplaceof PARTITION:
RANDOMIZED-QUICKSORT.A;p;r/
1 ifp < r
2 q RANDOMIZED-PARTITION.A;p;r/
D
3 RANDOMIZED-QUICKSORT.A;p;q 1/
(cid:0)
4 RANDOMIZED-QUICKSORT.A;q 1;r/
C
Weanalyze thisalgorithm inthenextsection.
180 Chapter7 Quicksort
Exercises
7.3-1
Whydoweanalyze theexpected running timeofarandomized algorithm and not
itsworst-caserunning time?
7.3-2
When RANDOMIZED-QUICKSORT runs,howmanycallsaremadetotherandom-
number generator RANDOM in the worst case? Howabout in the best case? Give
youranswerintermsof‚-notation.
7.4 Analysisofquicksort
Section 7.2 gave some intuition for the worst-case behavior of quicksort and for
whyweexpectittorunquickly. Inthissection, weanalyzethebehavior ofquick-
sortmorerigorously. Webegin withaworst-case analysis, whichapplies toeither
QUICKSORT orRANDOMIZED-QUICKSORT, andconcludewithananalysisofthe
expected runningtimeofRANDOMIZED-QUICKSORT.
7.4.1 Worst-case analysis
WesawinSection7.2thataworst-casesplitateverylevelofrecursioninquicksort
produces a ‚.n2/ running time, which, intuitively, is the worst-case running time
ofthealgorithm. Wenowprovethisassertion.
Using the substitution method (see Section 4.3), we can show that the running
time of quicksort is O.n2/. Let T.n/ be the worst-case time for the procedure
QUICKSORT onaninputofsizen. Wehavetherecurrence
T.n/ max .T.q/ T.n q 1// ‚.n/; (7.1)
D 0 q n 1 C (cid:0) (cid:0) C
  (cid:0)
where the parameter q ranges from 0 to n 1 because the procedure PARTITION
(cid:0)
produces two subproblems with total size n 1. We guess that T.n/ cn2 for
(cid:0) 
someconstant c. Substituting thisguessintorecurrence (7.1),weobtain
T.n/ max .cq2 c.n q 1/2/ ‚.n/
 0 q n 1 C (cid:0) (cid:0) C
  (cid:0)
c max .q2 .n q 1/2/ ‚.n/:
D 0 q n 1 C (cid:0) (cid:0) C
  (cid:0)
The expression q2 .n q 1/2 achieves a maximum over the parameter’s
C (cid:0) (cid:0)
range 0 q n 1ateither endpoint. Toverify thisclaim, note thatthe second
  (cid:0)
derivative oftheexpression withrespect toq ispositive (seeExercise7.4-3). This
7.4 Analysisofquicksort 181
observation gives us the bound max .q2 .n q 1/2/ .n 1/2
0 q n 1
n2 2n 1. Continuing withourbou nd in(cid:0) g ofTC .n/,w(cid:0) eob(cid:0) tain  (cid:0) D
(cid:0) C
T.n/ cn2 c.2n 1/ ‚.n/
 (cid:0) (cid:0) C
cn2 ;

since we can pick the constant c large enough so that the c.2n 1/ term dom-
(cid:0)
inates the ‚.n/ term. Thus, T.n/ O.n2/. We saw in Section 7.2 a specific
D
case in which quicksort takes .n2/ time: when partitioning is unbalanced. Al-
ternatively, Exercise7.4-1asksyoutoshowthatrecurrence (7.1)hasasolution of
T.n/ .n2/. Thus,the(worst-case) runningtimeofquicksort is‚.n2/.
D
7.4.2 Expectedrunningtime
We have already seen the intuition behind why the expected running time of
RANDOMIZED-QUICKSORT is O.nlgn/: if, in each level of recursion, the split
induced by RANDOMIZED-PARTITION puts any constant fraction of the elements
on one side of the partition, then the recursion tree has depth ‚.lgn/, and O.n/
workisperformedateachlevel. Evenifweaddafewnewlevelswiththemostun-
balanced split possible between these levels, thetotal timeremains O.nlgn/. We
can analyze the expected running time of RANDOMIZED-QUICKSORT precisely
byfirstunderstanding howthe partitioning procedure operates and then using this
understanding to derive an O.nlgn/ bound on the expected running time. This
upperboundontheexpected running time,combinedwiththe‚.nlgn/best-case
boundwesawinSection7.2,yieldsa‚.nlgn/expectedrunningtime. Weassume
throughout thatthevaluesoftheelementsbeingsortedaredistinct.
Runningtimeandcomparisons
The QUICKSORT and RANDOMIZED-QUICKSORT procedures differ only in how
theyselectpivotelements;theyarethesameinallotherrespects. Wecantherefore
couch ouranalysis of RANDOMIZED-QUICKSORT bydiscussing the QUICKSORT
and PARTITION procedures, but with the assumption that pivot elements are se-
lectedrandomlyfromthesubarray passedtoRANDOMIZED-PARTITION.
Therunning timeof QUICKSORT is dominated by the timespent in the PARTI-
TION procedure. Each time the PARTITION procedure is called, it selects a pivot
element,andthiselementisneverincludedinanyfuturerecursivecallstoQUICK-
SORT and PARTITION. Thus, there can be at most n calls to PARTITION over the
entire execution of the quicksort algorithm. One call to PARTITION takes O.1/
timeplus anamount oftime that is proportional to thenumber of iterations ofthe
for loop in lines 3–6. Each iteration of this for loop performs a comparison in
line 4, comparing the pivot element to another element of the array A. Therefore,
182 Chapter7 Quicksort
ifwecancountthetotalnumberoftimesthatline4isexecuted, wecanboundthe
totaltimespentintheforloopduringtheentireexecution ofQUICKSORT.
Lemma7.1
Let X be the number of comparisons performed in line 4 of PARTITION over the
entire execution of QUICKSORT on an n-element array. Then the running time of
QUICKSORT isO.n X/.
C
Proof By the discussion above, the algorithm makes at most n calls to PARTI-
TION, each of which does a constant amount of work and then executes the for
loopsomenumberoftimes. Eachiterationoftheforloopexecutesline4.
Ourgoal,therefore,istocomputeX,thetotalnumberofcomparisonsperformed
in all calls to PARTITION. Wewillnot attempt to analyze how many comparisons
aremadeineachcalltoPARTITION. Rather,wewillderiveanoverallboundonthe
total number of comparisons. To do so, we must understand when the algorithm
comparestwoelementsofthearrayandwhenitdoesnot. Foreaseofanalysis, we
renametheelementsofthearrayAas´ ;´ ;:::;´ ,with´ beingtheithsmallest
1 2 n i
element. Wealso define the set Z ´ ;´ ;:::;´ tobe the setof elements
ij i i 1 j
between´ and´ ,inclusive. D f C g
i j
When does the algorithm compare ´ and ´ ? Toanswer this question, we first
i j
observe that each pair of elements is compared at most once. Why? Elements
are compared only to the pivot element and, after a particular call of PARTITION
finishes, the pivot element used in that call is never again compared to any other
elements.
Ouranalysis usesindicator random variables(seeSection5.2). Wedefine
X I ´ iscomparedto´ ;
ij i j
D f g
where we are considering whether the comparison takes place at any time during
the execution of the algorithm, not just during one iteration or one call of PARTI-
TION. Since each pair is compared at most once, we can easily characterize the
totalnumberofcomparisons performedbythealgorithm:
n 1 n
(cid:0)
X X :
ij
D
i 1j i 1
XD DXC
Taking expectations of both sides, and then using linearity of expectation and
Lemma5.1,weobtain
n 1 n
(cid:0)
EŒX E X
ij
D
" #
i 1j i 1
XD DXC
7.4 Analysisofquicksort 183
n 1 n
(cid:0)
EŒX 
ij
D
i 1j i 1
XD DXC
n 1 n
(cid:0)
Pr ´ iscompared to´ : (7.2)
i j
D f g
i 1j i 1
XD DXC
It remains to compute Pr ´ iscomparedto´ . Our analysis assumes that the
i j
f g
RANDOMIZED-PARTITION procedure chooses each pivot randomly and indepen-
dently.
Let us think about when two items are not compared. Consider an input to
quicksort of the numbers 1 through 10 (in any order), and suppose that the first
pivotelementis7. ThenthefirstcalltoPARTITION separatesthenumbersintotwo
sets: 1;2;3;4;5;6 and 8;9;10 . In doing so, the pivot element 7 is compared
f g f g
to all other elements, but no number from the first set (e.g., 2) is or ever will be
comparedtoanynumberfromthesecondset(e.g.,9).
In general, because we assume that element values are distinct, once a pivot x
is chosen with ´ < x < ´ , we know that ´ and ´ cannot be compared at any
i j i j
subsequenttime. If,ontheotherhand,´ ischosenasapivotbeforeanyotheritem
i
in Z , then ´ will be compared to each item in Z , except for itself. Similarly,
ij i ij
if´ ischosenasapivotbeforeanyotheriteminZ ,then´ willbecomparedto
j ij j
eachiteminZ ,exceptforitself. Inourexample,thevalues7and9arecompared
ij
because7isthefirstitemfromZ tobechosenasapivot. Incontrast,2and9will
7;9
neverbecompared because thefirstpivotelementchosenfromZ is7. Thus,´
2;9 i
and´ arecomparedifandonlyifthefirstelementtobechosenasapivotfromZ
j ij
iseither´ or´ .
i j
We now compute the probability that this event occurs. Prior to the point at
whichanelementfromZ hasbeenchosenasapivot,thewholesetZ istogether
ij ij
inthesamepartition. Therefore, anyelementofZ isequallylikelytobethefirst
ij
onechosenasapivot. BecausethesetZ hasj i 1elements,andbecausepivots
ij
(cid:0) C
are chosen randomly and independently, the probability that any given element is
thefirstonechosenasapivotis1=.j i 1/. Thus,wehave
(cid:0) C
Pr ´ iscomparedto´ Pr ´ or´ isfirstpivotchosen fromZ
i j i j ij
f g D f g
Pr ´ isfirstpivotchosen fromZ
i ij
D f g
Pr ´ isfirstpivotchosenfromZ
j ij
C f g
1 1
D j i 1 C j i 1
(cid:0) C (cid:0) C
2
: (7.3)
D j i 1
(cid:0) C
184 Chapter7 Quicksort
Thesecondlinefollowsbecausethetwoeventsaremutuallyexclusive. Combining
equations (7.2)and(7.3),wegetthat
n 1 n
(cid:0) 2
EŒX :
D j i 1
i 1j i 1 (cid:0) C
XD DXC
We can evaluate this sum using a change of variables (k j i) and the bound
D (cid:0)
ontheharmonicseriesinequation (A.7):
n 1 n
(cid:0) 2
EŒX
D j i 1
i 1j i 1 (cid:0) C
XD DXC
n 1 n i
(cid:0) (cid:0) 2
D k 1
i 1k 1 C
XD XD
n 1 n
(cid:0) 2
<
k
i 1k 1
XD XD
n 1
(cid:0)
O.lgn/
D
i 1
XD
O.nlgn/: (7.4)
D
Thus we conclude that, using RANDOMIZED-PARTITION, the expected running
timeofquicksort isO.nlgn/whenelementvaluesaredistinct.
Exercises
7.4-1
Showthatintherecurrence
T.n/ max .T.q/ T.n q 1// ‚.n/;
D 0 q n 1 C (cid:0) (cid:0) C
  (cid:0)
T.n/ .n2/.
D
7.4-2
Showthatquicksort’s best-case runningtimeis.nlgn/.
7.4-3
Show that the expression q2 .n q 1/2 achieves a maximum over q
C (cid:0) (cid:0) D
0;1;:::;n 1whenq 0orq n 1.
(cid:0) D D (cid:0)
7.4-4
Showthat RANDOMIZED-QUICKSORT’s expectedrunning timeis.nlgn/.
ProblemsforChapter7 185
7.4-5
Wecanimprovetherunningtimeofquicksortinpracticebytakingadvantageofthe
fast running time of insertion sort when its input is “nearly” sorted. Upon calling
quicksort on a subarray with fewer than k elements, let it simply return without
sortingthesubarray. Afterthetop-levelcalltoquicksort returns, runinsertion sort
on the entire array to finish the sorting process. Argue that this sorting algorithm
runs inO.nk nlg.n=k//expected time. Howshould wepick k, both intheory
C
andinpractice?
7.4-6 ?
ConsidermodifyingthePARTITIONprocedurebyrandomlypickingthreeelements
from array A and partitioning about their median (the middle value of the three
elements). Approximatetheprobabilityofgettingatworstan˛-to-.1 ˛/split,as
(cid:0)
afunction of˛ intherange0 < ˛ < 1.
Problems
7-1 Hoarepartition correctness
The version of PARTITION given in this chapter is not the original partitioning
algorithm. Hereistheoriginalpartitionalgorithm, whichisduetoC.A.R.Hoare:
HOARE-PARTITION.A;p;r/
1 x AŒp
D
2 i p 1
D (cid:0)
3 j r 1
D C
4 whileTRUE
5 repeat
6 j j 1
D (cid:0)
7 untilAŒj x

8 repeat
9 i i 1
D C
10 untilAŒi x

11 ifi < j
12 exchangeAŒiwithAŒj
13 elsereturnj
a. Demonstratetheoperation ofHOARE-PARTITION onthearrayA 13;19;9;
D h
5;12;8;7;4;11;2;6;21 ,showing thevaluesofthearrayandauxiliary values
i
aftereachiteration ofthewhileloopinlines4–13.
186 Chapter7 Quicksort
The next three questions ask you to give a careful argument that the procedure
HOARE-PARTITION is correct. Assuming that the subarray AŒp::r contains at
leasttwoelements,provethefollowing:
b. Theindices i and j are such that wenever access an element of Aoutside the
subarray AŒp::r.
c. WhenHOARE-PARTITIONterminates,itreturnsavaluej suchthatp j <r.

d. EveryelementofAŒp::jislessthanorequaltoeveryelementofAŒj 1::r
C
whenHOARE-PARTITION terminates.
The PARTITION procedure in Section 7.1 separates the pivot value (originally
in AŒr) from the two partitions it forms. The HOARE-PARTITION procedure, on
the other hand, always places the pivot value (originally in AŒp) into one of the
two partitions AŒp::j and AŒj 1::r. Since p j < r, this split is always
C 
nontrivial.
e. Rewritethe QUICKSORT procedure touse HOARE-PARTITION.
7-2 Quicksortwithequalelementvalues
TheanalysisoftheexpectedrunningtimeofrandomizedquicksortinSection7.4.2
assumes that all element values are distinct. In this problem, we examine what
happens whentheyarenot.
a. Suppose that all element values are equal. What would be randomized quick-
sort’srunningtimeinthiscase?
b. The PARTITION procedure returns an index q such that each element of
AŒp::q 1 is less than or equal to AŒq and each element of AŒq 1::r
(cid:0) C
isgreater thanAŒq. Modify the PARTITION procedure toproduce aprocedure
PARTITION0.A;p;r/,whichpermutestheelementsofAŒp::randreturnstwo
indicesq andt,wherep q t r,suchthat
  
 allelementsofAŒq::tareequal,
 eachelementofAŒp::q 1islessthanAŒq,and
(cid:0)
 eachelementofAŒt 1::risgreaterthanAŒq.
C
LikePARTITION, your PARTITION0 procedure should take‚.r p/time.
(cid:0)
c. Modify the RANDOMIZED-PARTITION procedure to call PARTITION0, and
name the new procedure RANDOMIZED-PARTITION0. Then modify the
QUICKSORTproceduretoproduceaprocedureQUICKSORT0.A;p;r/thatcalls
ProblemsforChapter7 187
RANDOMIZED-PARTITION0 and recurses only on partitions of elements not
knowntobeequaltoeachother.
d. Using QUICKSORT0, how would you adjust the analysis in Section 7.4.2 to
avoidtheassumption thatallelementsaredistinct?
7-3 Alternativequicksort analysis
An alternative analysis of the running time of randomized quicksort focuses on
the expected running time of each individual recursive call to RANDOMIZED-
QUICKSORT, ratherthanonthenumberofcomparisons performed.
a. Arguethat, givenanarrayofsizen,theprobability thatanyparticular element
is chosen as the pivot is 1=n. Use this to define indicator random variables
X I ithsmallestelementischosenasthepivot . WhatisEŒX ?
i i
D f g
b. Let T.n/ be a random variable denoting the running time of quicksort on an
arrayofsizen. Arguethat
n
EŒT.n/ E X .T.q 1/ T.n q/ ‚.n// : (7.5)
q
D (cid:0) C (cid:0) C
" #
q 1
XD
c. Showthatwecanrewriteequation (7.5)as
n 1
2 (cid:0)
EŒT.n/ EŒT.q/ ‚.n/: (7.6)
D n C
q 2
XD
d. Showthat
n 1
(cid:0) 1 1
klgk n2lgn n2 : (7.7)
 2 (cid:0) 8
k 2
XD
(Hint:Splitthesummationintotwoparts,onefork 2;3;:::; n=2 1and
D d e(cid:0)
onefork n=2 ;:::;n 1.)
D d e (cid:0)
e. Usingtheboundfromequation(7.7),showthattherecurrenceinequation(7.6)
has the solution EŒT.n/ ‚.nlgn/. (Hint: Show, by substitution, that
D
EŒT.n/ anlgnforsufficiently largenandforsomepositiveconstant a.)

188 Chapter7 Quicksort
7-4 Stackdepthforquicksort
The QUICKSORT algorithm of Section 7.1 contains two recursive calls to itself.
After QUICKSORT calls PARTITION, itrecursively sorts theleftsubarray and then
it recursively sorts the right subarray. The second recursive call in QUICKSORT
is not really necessary; we can avoid it by using an iterative control structure.
Thistechnique,calledtailrecursion,isprovidedautomaticallybygoodcompilers.
Considerthefollowingversionofquicksort, whichsimulatestailrecursion:
TAIL-RECURSIVE-QUICKSORT.A;p;r/
1 whilep < r
2 //Partitionandsortleftsubarray.
3 q PARTITION.A;p;r/
D
4 TAIL-RECURSIVE-QUICKSORT.A;p;q 1/
(cid:0)
5 p q 1
D C
a. Argue that TAIL-RECURSIVE-QUICKSORT.A;1;A:length/ correctly sorts the
arrayA.
Compilersusuallyexecuterecursiveprocedures byusingastackthatcontainsper-
tinent information, including the parameter values, for each recursive call. The
information for the most recent call is at the top of the stack, and the information
for the initial call is at the bottom. Upon calling a procedure, its information is
pushed onto the stack; when it terminates, its information is popped. Since we
assumethatarrayparameters arerepresented bypointers, theinformation foreach
procedure callonthestackrequiresO.1/stackspace. Thestackdepthisthemax-
imumamountofstackspaceusedatanytimeduringacomputation.
b. Describe a scenario in which TAIL-RECURSIVE-QUICKSORT’s stack depth is
‚.n/onann-elementinputarray.
c. Modify the code for TAIL-RECURSIVE-QUICKSORT so that the worst-case
stack depth is ‚.lgn/. Maintain the O.nlgn/ expected running time of the
algorithm.
7-5 Median-of-3 partition
One way to improve the RANDOMIZED-QUICKSORT procedure is to partition
around a pivot that is chosen more carefully than by picking a random element
from the subarray. One common approach is the median-of-3 method: choose
thepivot asthemedian (middle element) ofasetof3elements randomly selected
from the subarray. (See Exercise 7.4-6.) For this problem, let us assume that the
elements in the input array AŒ1::n are distinct and that n 3. We denote the

ProblemsforChapter7 189
sorted output array by AŒ1::n. Using the median-of-3 method to choose the
0
pivotelementx,definep Pr x AŒi .
i 0
D f D g
a. Give an exact formula for p as a function of n and i for i 2;3;:::;n 1.
i
D (cid:0)
(Notethatp p 0.)
1 n
D D
b. By what amount have we increased the likelihood of choosing the pivot as
x AŒ .n 1/=2 , the median of AŒ1::n, compared with the ordinary
0
D b C c
implementation? Assume that n , and give the limiting ratio of these
! 1
probabilities.
c. If we define a “good” split to mean choosing the pivot as x AŒi, where
0
D
n=3 i 2n=3, by what amount have weincreased the likelihood of getting
 
agood split compared withthe ordinary implementation? (Hint: Approximate
thesumbyanintegral.)
d. Argue thatinthe.nlgn/running timeofquicksort, themedian-of-3 method
affectsonlytheconstant factor.
7-6 Fuzzysortingofintervals
Consider a sorting problem in which we do not know the numbers exactly. In-
stead, for each number, we know an interval on the real line to which it belongs.
That is, we are given n closed intervals of the form Œa ;b , where a b . We
i i i i

wish to fuzzy-sort these intervals, i.e., to produce a permutation i ;i ;:::;i of
1 2 n
h i
the intervals such that for j 1;2;:::;n, there exist c Œa ;b  satisfying
D
j
2
ij ij
c c c .
1 2 n
   
a. Design a randomized algorithm for fuzzy-sorting n intervals. Your algorithm
should have the general structure of an algorithm that quicksorts the left end-
points (the a values), but it should take advantage of overlapping intervals to
i
improve the running time. (As the intervals overlap more and more, the prob-
lemoffuzzy-sortingtheintervalsbecomesprogressivelyeasier. Youralgorithm
shouldtakeadvantage ofsuchoverlapping, totheextentthatitexists.)
b. Argue that your algorithm runs inexpected time‚.nlgn/in general, but runs
inexpectedtime‚.n/whenalloftheintervalsoverlap(i.e.,whenthereexistsa
valuex suchthatx Œa ;b foralli). Youralgorithm should notbechecking
i i
2
for this case explicitly; rather, its performance should naturally improve asthe
amountofoverlapincreases.
190 Chapter7 Quicksort
Chapter notes
Thequicksort procedure wasinvented byHoare[170];Hoare’sversion appears in
Problem7-1. ThePARTITION procedure giveninSection7.1isduetoN.Lomuto.
The analysis in Section 7.4 is due to Avrim Blum. Sedgewick [305] and Bent-
ley [43] provide a good reference on the details of implementation and how they
matter.
McIlroy [248] showed how to engineer a “killer adversary” that produces an
array onwhichvirtually anyimplementation ofquicksort takes ‚.n2/time. Ifthe
implementation is randomized, the adversary produces the array after seeing the
random choicesofthequicksort algorithm.
8 Sorting in Linear Time
We have now introduced several algorithms that can sort n numbers in O.nlgn/
time. Mergesortandheapsortachievethisupperboundintheworstcase;quicksort
achieves it on average. Moreover, for each of these algorithms, wecan produce a
sequence ofninputnumbersthatcausesthealgorithm torunin.nlgn/time.
Thesealgorithms share aninteresting property: thesorted order they determine
is based only on comparisons between the input elements. We call such sorting
algorithms comparison sorts. All the sorting algorithms introduced thus far are
comparison sorts.
In Section 8.1, we shall prove that any comparison sort must make .nlgn/
comparisons in the worst case to sort n elements. Thus, merge sort and heapsort
are asymptotically optimal, and no comparison sort exists that is faster by more
thanaconstant factor.
Sections8.2,8.3,and8.4examinethreesortingalgorithms—countingsort,radix
sort, and bucket sort—that run in linear time. Of course, these algorithms use
operations other than comparisons to determine the sorted order. Consequently,
the.nlgn/lowerbounddoesnotapplytothem.
8.1 Lowerbounds forsorting
In a comparison sort, we use only comparisons between elements to gain order
information aboutaninput sequence a ;a ;:::;a . Thatis,giventwoelements
1 2 n
h i
a and a , we perform one of the tests a < a , a a , a a , a a , or
i j i j i j i j i j
 D 
a > a to determine their relative order. We may not inspect the values of the
i j
elementsorgainorderinformation abouttheminanyotherway.
In this section, weassume without loss of generality that all the input elements
are distinct. Given this assumption, comparisons ofthe form a a are useless,
i j
D
so we can assume that no comparisons of this form are made. We also note that
the comparisons a a , a a , a > a , and a < a are allequivalent in that
i j i j i j i j
 
192 Chapter8 SortinginLinearTime
1:2
£ >
2:3 1:3
£ > £ >
Æ1,2,3æ 1:3 Æ2,1,3æ 2:3
£ > £ >
Æ1,3,2æ Æ3,1,2æ Æ2,3,1æ Æ3,2,1æ
Figure8.1 Thedecisiontreeforinsertionsortoperatingonthreeelements. Aninternalnodean-
notated by i:j indicates a comparison between a i and a j. A leaf annotated by the permutation
.1/;.2/;:::;.n/ indicates the ordering a .1/ a .2/ a .n/. The shaded path
h i    
indicates the decisions made when sorting the input sequence a1 6;a2 8;a3 5 ; the
h D D D i
permutation 3;1;2 attheleafindicatesthatthesortedorderingisa3 5 a1 6 a2 8.
h i D  D  D
Thereare3Š 6possiblepermutationsoftheinputelements,andsothedecisiontreemusthaveat
D
least6leaves.
theyyieldidenticalinformation abouttherelativeorderofa anda . Wetherefore
i j
assumethatallcomparisons havetheforma a .
i j

Thedecision-treemodel
We can view comparison sorts abstractly in terms of decision trees. A decision
tree is a full binary tree that represents the comparisons between elements that
are performed by a particular sorting algorithm operating on an input of a given
size. Control, data movement, and all other aspects of the algorithm are ignored.
Figure 8.1 shows the decision tree corresponding to the insertion sort algorithm
fromSection2.1operating onaninputsequence ofthreeelements.
Inadecisiontree,weannotateeachinternalnodebyi:j forsomei andj inthe
range 1 i;j n,wheren isthe number ofelements intheinput sequence. We
 
alsoannotateeachleafbyapermutation .1/;.2/;:::;.n/ . (SeeSectionC.1
h i
for background on permutations.) The execution of the sorting algorithm corre-
sponds to tracing a simple path from the root of the decision tree down to a leaf.
Each internal node indicates a comparison a a . The left subtree then dictates
i j

subsequent comparisons onceweknowthata a ,andtherightsubtreedictates
i j

subsequent comparisons knowing thata > a . Whenwecometoaleaf, thesort-
i j
ing algorithm has established the ordering a a a . Because
.1/ .2/ .n/
   
anycorrectsortingalgorithmmustbeabletoproduceeachpermutationofitsinput,
each of thenŠpermutations onn elements mustappear asone ofthe leaves ofthe
decisiontreeforacomparisonsorttobecorrect. Furthermore,eachoftheseleaves
must be reachable from the root by a downward path corresponding to an actual
8.1 Lowerboundsforsorting 193
execution of the comparison sort. (We shall refer to such leaves as “reachable.”)
Thus, we shall consider only decision trees in which each permutation appears as
areachable leaf.
Alowerboundfortheworstcase
The length of the longest simple path from the root of a decision tree to any of
itsreachable leavesrepresents theworst-case numberofcomparisons thatthecor-
responding sorting algorithm performs. Consequently, the worst-case number of
comparisonsforagivencomparisonsortalgorithmequalstheheightofitsdecision
tree. Alowerboundontheheights ofalldecision treesinwhicheachpermutation
appears as a reachable leaf is therefore a lower bound on the running time of any
comparisonsortalgorithm. Thefollowingtheoremestablishessuchalowerbound.
Theorem8.1
Anycomparison sortalgorithm requires.nlgn/comparisons intheworstcase.
Proof From the preceding discussion, it suffices to determine the height of a
decision tree in which each permutation appears as a reachable leaf. Consider a
decision tree of height h with l reachable leaves corresponding to a comparison
sort on n elements. Because each of the nŠ permutations of the input appears as
some leaf, we have nŠ l. Since a binary tree of height h has no more than 2h

leaves,wehave
nŠ l 2h ;
 
which,bytakinglogarithms, implies
h lg.nŠ/ (sincethelgfunctionismonotonically increasing)

.nlgn/ (byequation (3.19)) .
D
Corollary8.2
Heapsortandmergesortareasymptotically optimalcomparison sorts.
Proof TheO.nlgn/upper bounds on the running times for heapsort and merge
sortmatchthe.nlgn/worst-case lowerboundfromTheorem8.1.
Exercises
8.1-1
What is the smallest possible depth of a leaf in a decision tree for a comparison
sort?
194 Chapter8 SortinginLinearTime
8.1-2
Obtain asymptotically tight bounds on lg.nŠ/ without using Stirling’s approxi-
mation. Instead, evaluate the summation n lgk using techniques from Sec-
k 1
tionA.2. D
P
8.1-3
Showthatthereisnocomparisonsortwhoserunningtimeislinearforatleasthalf
ofthenŠinputsoflengthn. Whataboutafractionof1=noftheinputsoflengthn?
Whataboutafraction1=2n?
8.1-4
Suppose that you are given a sequence of n elements to sort. The input sequence
consistsofn=ksubsequences,eachcontainingkelements. Theelementsinagiven
subsequence are all smaller than the elements in the succeeding subsequence and
larger than the elements in the preceding subsequence. Thus, all that is needed to
sort the whole sequence of length n is to sort the k elements in each of the n=k
subsequences. Show an .nlgk/ lower bound on the number of comparisons
needed to solve this variant of the sorting problem. (Hint: It is not rigorous to
simplycombinethelowerboundsfortheindividual subsequences.)
8.2 Counting sort
Countingsort assumesthat eachoftheninput elements isaninteger intherange
0tok,forsomeintegerk. Whenk O.n/,thesortrunsin‚.n/time.
D
Countingsortdetermines,foreachinputelementx,thenumberofelementsless
than x. It uses this information to place element x directly into its position in the
output array. Forexample, if17elementsarelessthanx,thenx belongs inoutput
position 18. Wemustmodify thisschemeslightly tohandle thesituation inwhich
several elements have the same value, since we do not want to put them all in the
sameposition.
Inthecodeforcounting sort, weassumethattheinputisanarrayAŒ1::n,and
thus A:length n. We require two other arrays: the array BŒ1::n holds the
D
sortedoutput, andthearrayCŒ0::kprovidestemporary workingstorage.
8.2 Countingsort 195
1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8
A 2 5 3 0 2 3 0 3 B 3
0 1 2 3 4 5
C 2 2 4 7 7 8
0 1 2 3 4 5 0 1 2 3 4 5
C 2 0 2 3 0 1 C 2 2 4 6 7 8
(a) (b) (c)
1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8
B 0 3 B 0 3 3
1 2 3 4 5 6 7 8
B 0 0 2 2 3 3 3 5
0 1 2 3 4 5 0 1 2 3 4 5
C 1 2 4 6 7 8 C 1 2 4 5 7 8
(d) (e) (f)
Figure 8.2 The operation of COUNTING-SORTon an input array AŒ1::8, where each element
ofAisanonnegativeintegernolargerthank 5. (a)ThearrayAandtheauxiliaryarrayC after
D
line5. (b)ThearrayC afterline8. (c)–(e)TheoutputarrayB andtheauxiliaryarrayC afterone,
two,andthreeiterationsoftheloopinlines10–12,respectively.Onlythelightlyshadedelementsof
arrayBhavebeenfilledin.(f)ThefinalsortedoutputarrayB.
COUNTING-SORT.A;B;k/
1 letCŒ0::kbeanewarray
2 fori 0tok
D
3 CŒi 0
D
4 forj 1toA:length
D
5 CŒAŒj CŒAŒj 1
D C
6 //CŒinowcontainsthenumberofelementsequaltoi.
7 fori 1tok
D
8 CŒi CŒi CŒi 1
D C (cid:0)
9 //CŒinowcontainsthenumberofelementslessthanorequaltoi.
10 forj A:lengthdownto1
D
11 BŒCŒAŒj AŒj
D
12 CŒAŒj CŒAŒj 1
D (cid:0)
Figure 8.2 illustrates counting sort. After the for loop of lines 2–3 initializes the
array C to all zeros, the for loop of lines 4–5 inspects each input element. If the
value of an input element is i, we increment CŒi. Thus, after line 5, CŒi holds
thenumberofinputelementsequaltoi foreachintegeri 0;1;:::;k. Lines7–8
D
determineforeachi 0;1;:::;k howmanyinputelementsarelessthanorequal
D
toi bykeepingarunning sumofthearrayC.
196 Chapter8 SortinginLinearTime
Finally, the for loop of lines 10–12 places each element AŒj into its correct
sorted position in the output array B. If all n elements are distinct, then when we
first enter line 10, for each AŒj, the value CŒAŒj is the correct final position
of AŒj in the output array, since there are CŒAŒj elements less than or equal
to AŒj. Because the elements might not be distinct, we decrement CŒAŒj each
time we place a value AŒj into the B array. Decrementing CŒAŒj causes the
next input element with a value equal to AŒj, if one exists, to go to the position
immediately beforeAŒjintheoutput array.
How much time does counting sort require? The for loop of lines 2–3 takes
time‚.k/,theforloopoflines4–5takestime‚.n/,theforloopoflines7–8takes
time‚.k/,andtheforloopoflines10–12takestime‚.n/. Thus,theoveralltime
is‚.k n/. Inpractice,weusuallyusecountingsortwhenwehavek O.n/,in
C D
whichcasetherunning timeis‚.n/.
Counting sortbeatsthelowerbound of.nlgn/provedinSection8.1because
it is not a comparison sort. In fact, no comparisons between input elements occur
anywhereinthecode. Instead,counting sortusestheactualvaluesoftheelements
toindexintoanarray. The.nlgn/lowerboundforsorting doesnotapplywhen
wedepartfromthecomparison sortmodel.
Animportantpropertyofcountingsortisthatitisstable: numberswiththesame
valueappearintheoutputarrayinthesameorderastheydointheinputarray. That
is, itbreaks ties between two numbers by the rule that whichever number appears
first in the input array appears first in the output array. Normally, the property of
stability is important only when satellite data are carried around with the element
being sorted. Counting sort’s stability is important for another reason: counting
sortisoftenusedasasubroutine inradixsort. Asweshallseeinthenextsection,
inorderforradixsorttoworkcorrectly, counting sortmustbestable.
Exercises
8.2-1
Using Figure 8.2 as a model, illustrate the operation of COUNTING-SORT on the
arrayA 6;0;2;0;1;3;4;6;1;3;2 .
Dh i
8.2-2
Provethat COUNTING-SORT isstable.
8.2-3
Suppose that weweretorewritetheforloop header inline 10ofthe COUNTING-
SORT as
10 forj 1toA:length
D
Showthatthealgorithm stillworksproperly. Isthemodifiedalgorithm stable?
8.3 Radixsort 197
8.2-4
Describe an algorithm that, given n integers in the range 0 to k, preprocesses its
input and then answers any query about how many of the n integers fall into a
range Œa::b in O.1/ time. Your algorithm should use ‚.n k/ preprocessing
C
time.
8.3 Radixsort
Radixsortisthealgorithmusedbythecard-sortingmachinesyounowfindonlyin
computermuseums. Thecardshave80columns,andineachcolumnamachinecan
punch a hole in one of 12 places. The sorter can be mechanically “programmed”
to examine agiven column of each card in a deck and distribute the card into one
of 12 bins depending on which place has been punched. An operator can then
gatherthecardsbinbybin,sothatcardswiththefirstplacepunched areontopof
cardswiththesecond placepunched, andsoon.
For decimal digits, each column uses only 10 places. (The other two places
are reserved for encoding nonnumeric characters.) A d-digit number would then
occupy a field of d columns. Since the card sorter can look at only one column
at a time, the problem of sorting n cards on a d-digit number requires a sorting
algorithm.
Intuitively, you might sort numbers on their most significant digit, sort each of
theresulting binsrecursively, andthencombine thedecks inorder. Unfortunately,
since the cards in 9 of the 10 bins must be put aside to sort each of the bins, this
procedure generates manyintermediate pilesofcardsthatyouwouldhavetokeep
trackof. (SeeExercise8.3-5.)
Radixsortsolvestheproblemofcardsorting—counterintuitively—bysortingon
theleastsignificant digitfirst. Thealgorithmthencombinesthecardsintoasingle
deck, with the cards in the 0 bin preceding the cards in the 1 bin preceding the
cardsinthe2bin,andsoon. Thenitsortstheentiredeckagainonthesecond-least
significant digit and recombines the deck ina like manner. Theprocess continues
untilthecardshavebeensortedonalld digits. Remarkably, atthatpointthecards
are fully sorted on the d-digit number. Thus, only d passes through the deck are
required to sort. Figure 8.3 shows how radix sort operates on a “deck” of seven
3-digitnumbers.
Inorder forradix sort towork correctly, the digit sorts must bestable. Thesort
performed by a card sorter is stable, but the operator has to be wary about not
changingtheorderofthecardsastheycomeoutofabin,eventhoughallthecards
inabinhavethesamedigitinthechosen column.
198 Chapter8 SortinginLinearTime
329 720 720 329
457 355 329 355
657 436 436 436
839 457 839 457
436 657 355 657
720 329 457 720
355 839 657 839
Figure8.3 Theoperationofradixsortonalistofseven3-digitnumbers. Theleftmostcolumnis
theinput.Theremainingcolumnsshowthelistaftersuccessivesortsonincreasinglysignificantdigit
positions.Shadingindicatesthedigitpositionsortedontoproduceeachlistfromthepreviousone.
In a typical computer, which is a sequential random-access machine, we some-
timesuseradixsorttosortrecordsofinformationthatarekeyedbymultiplefields.
Forexample, wemightwishtosortdatesbythreekeys: year, month,andday. We
could run a sorting algorithm with a comparison function that, given two dates,
compares years, and if there is a tie, compares months, and if another tie occurs,
compares days. Alternatively, we could sort the information three times with a
stablesort: firstonday,nextonmonth,andfinallyonyear.
Thecodeforradixsortisstraightforward. Thefollowingprocedureassumesthat
eachelementinthen-elementarrayAhasd digits,wheredigit1isthelowest-order
digitanddigitd isthehighest-order digit.
RADIX-SORT.A;d/
1 fori 1tod
D
2 useastablesorttosortarrayAondigiti
Lemma8.3
Given n d-digit numbers in which each digit can take on up to k possible values,
RADIX-SORT correctly sortsthesenumbersin‚.d.n k//timeifthestablesort
C
itusestakes‚.n k/time.
C
Proof The correctness of radix sort follows by induction on the column being
sorted(seeExercise8.3-3). Theanalysisoftherunningtimedependsonthestable
sort used as the intermediate sorting algorithm. When each digit is in the range 0
tok 1(sothatitcantakeonkpossiblevalues),andkisnottoolarge,countingsort
(cid:0)
istheobviouschoice. Eachpassovernd-digitnumbersthentakestime‚.n k/.
C
Thereared passes, andsothetotaltimeforradixsortis‚.d.n k//.
C
When d is constant and k O.n/, we can make radix sort run in linear time.
D
Moregenerally, wehavesomeflexibilityinhowtobreakeachkeyintodigits.
8.3 Radixsort 199
Lemma8.4
Givennb-bitnumbersandanypositiveintegerr b,RADIX-SORTcorrectlysorts

these numbers in ‚..b=r/.n 2r// time if the stable sort it uses takes ‚.n k/
C C
timeforinputsintherange0tok.
Proof Foravaluer b,wevieweachkeyashaving d b=r digitsofr bits
 Dd e
each. Eachdigitisanintegerintherange 0to2r 1,sothatwecanusecounting
(cid:0)
sortwithk 2r 1. (Forexample,wecanviewa32-bitwordashavingfour8-bit
D (cid:0)
digits,sothatb 32,r 8,k 2r 1 255,andd b=r 4.) Eachpassof
D D D (cid:0) D D D
countingsorttakestime‚.n k/ ‚.n 2r/andthereared passes,foratotal
C D C
runningtimeof‚.d.n 2r// ‚..b=r/.n 2r//.
C D C
For given values of n and b, we wish to choose the value of r, with r b,

that minimizes the expression .b=r/.n 2r/. If b < lgn , then for any value
C b c
ofr b, wehave that .n 2r/ ‚.n/. Thus, choosing r b yields a running
 C D D
time of .b=b/.n 2b/ ‚.n/, which is asymptotically optimal. If b lgn ,
C D  b c
then choosing r lgn gives the best time to within a constant factor, which
D b c
wecanseeasfollows. Choosing r lgn yields arunning timeof‚.bn=lgn/.
D b c
As we increase r above lgn , the 2r term in the numerator increases faster than
b c
the r term in the denominator, and so increasing r above lgn yields a running
b c
time of .bn=lgn/. If instead we were to decrease r below lgn , then the b=r
b c
termincreases andthen 2r termremainsat‚.n/.
C
Isradix sort preferable toacomparison-based sorting algorithm, suchasquick-
sort? Ifb O.lgn/,asisoftenthecase,andwechooser lgn,thenradixsort’s
D 
runningtimeis‚.n/,whichappearstobebetterthanquicksort’sexpectedrunning
time of ‚.nlgn/. The constant factors hidden in the ‚-notation differ, however.
Although radix sort may make fewer passes than quicksort over the n keys, each
passofradixsortmaytakesignificantlylonger. Whichsortingalgorithmweprefer
depends on the characteristics of the implementations, of the underlying machine
(e.g., quicksort often uses hardware caches more effectively than radix sort), and
oftheinputdata. Moreover, theversionofradixsortthatusescounting sortasthe
intermediate stable sort does not sort in place, which many of the ‚.nlgn/-time
comparison sorts do. Thus, when primary memory storage is at a premium, we
mightpreferanin-placealgorithm suchasquicksort.
Exercises
8.3-1
Using Figure 8.3 as a model, illustrate the operation of RADIX-SORT on the fol-
lowing list of English words: COW,DOG, SEA,RUG, ROW,MOB, BOX, TAB,
BAR,EAR,TAR,DIG,BIG,TEA,NOW,FOX.
200 Chapter8 SortinginLinearTime
8.3-2
Which of the following sorting algorithms are stable: insertion sort, merge sort,
heapsort, and quicksort? Give a simple scheme that makes any sorting algorithm
stable. Howmuchadditional timeandspacedoesyourschemeentail?
8.3-3
Use induction to prove that radix sort works. Where does your proof need the
assumption thattheintermediate sortisstable?
8.3-4
Showhowtosortnintegersintherange0ton3 1inO.n/time.
(cid:0)
8.3-5 ?
Inthefirstcard-sorting algorithm inthissection, exactly howmanysorting passes
are needed tosort d-digit decimal numbers in theworst case? Howmanypiles of
cardswouldanoperator needtokeeptrackofintheworstcase?
8.4 Bucket sort
Bucketsortassumesthattheinputisdrawnfromauniformdistribution andhasan
average-case running timeofO.n/. Likecounting sort,bucket sortisfastbecause
itassumessomethingabouttheinput. Whereascountingsortassumesthattheinput
consistsofintegersinasmallrange,bucketsortassumesthattheinputisgenerated
by a random process that distributes elements uniformly and independently over
theintervalŒ0;1/. (SeeSectionC.2foradefinition ofuniform distribution.)
BucketsortdividestheintervalŒ0;1/intonequal-sizedsubintervals, orbuckets,
andthendistributes theninputnumbersintothebuckets. Sincetheinputsareuni-
formly and independently distributed over Œ0;1/, wedo not expect manynumbers
tofallintoeachbucket. Toproducetheoutput,wesimplysortthenumbersineach
bucket andthengothroughthebuckets inorder, listingtheelementsineach.
Our code for bucket sort assumes that the input is an n-element array A and
that each element AŒi in the array satisfies 0 AŒi < 1. The code requires an

auxiliary array BŒ0::n 1 of linked lists (buckets) and assumes that there is a
(cid:0)
mechanism for maintaining such lists. (Section 10.2 describes how to implement
basicoperations onlinkedlists.)
8.4 Bucketsort 201
A B
1 .78 0
2 .17 1 .12 .17
3 .39 2 .21 .23 .26
4 .26 3 .39
5 .72 4
6 .94 5
7 .21 6 .68
8 .12 7 .72 .78
9 .23 8
10 .68 9 .94
(a) (b)
Figure8.4 Theoperationof BUCKET-SORTforn 10. (a)TheinputarrayAŒ1::10. (b)The
D
array BŒ0::9 of sorted lists(buckets) after line8 of thealgorithm. Bucket i holds values inthe
half-openintervalŒi=10;.i 1/=10/. Thesortedoutputconsistsofaconcatenationinorderofthe
C
listsBŒ0;BŒ1;:::;BŒ9.
BUCKET-SORT.A/
1 n A:length
D
2 letBŒ0::n 1beanewarray
(cid:0)
3 fori 0ton 1
D (cid:0)
4 makeBŒianemptylist
5 fori 1ton
D
6 insertAŒiintolistBŒ nAŒi 
b c
7 fori 0ton 1
D (cid:0)
8 sortlistBŒiwithinsertion sort
9 concatenate thelistsBŒ0;BŒ1;:::;BŒn 1together inorder
(cid:0)
Figure8.4showstheoperation ofbucket sortonaninputarrayof10numbers.
Toseethatthisalgorithm works,consider twoelementsAŒiandAŒj. Assume
without loss of generality that AŒi AŒj. Since nAŒi nAŒj , either
 b c  b c
elementAŒigoesintothesamebucketasAŒjoritgoesintoabucketwithalower
index. IfAŒiandAŒjgointothesamebucket, thentheforloopoflines7–8puts
them into the proper order. If AŒi and AŒj go into different buckets, then line 9
putsthemintotheproperorder. Therefore, bucketsortworkscorrectly.
Toanalyze therunning time, observe that alllines except line 8take O.n/ time
intheworstcase. Weneedtoanalyzethetotaltimetakenbythencallstoinsertion
sortinline8.
202 Chapter8 SortinginLinearTime
To analyze the cost of the calls to insertion sort, let n be the random variable
i
denoting the number of elements placed in bucket BŒi. Since insertion sort runs
inquadratic time(seeSection2.2),therunningtimeofbucket sortis
n 1
(cid:0)
T.n/ ‚.n/ O.n2/:
D C i
i 0
XD
Wenowanalyzetheaverage-case runningtimeofbucketsort,bycomputingthe
expected value of the running time, where we take the expectation over the input
distribution. Taking expectations of both sides and using linearity of expectation,
wehave
n 1
(cid:0)
EŒT.n/ E ‚.n/ O.n2/
D C i
" #
i 0
XD
n 1
(cid:0)
‚.n/ E O.n2/ (bylinearityofexpectation)
D C i
i 0
XD  
n 1
(cid:0)
‚.n/ O E n2 (byequation(C.22)) . (8.1)
D C i
i 0
XD (cid:0)  
Weclaimthat
E n2 2 1=n (8.2)
i D (cid:0)
for i  0;1;:::;n 1. It is no surprise that each bucket i has the same value of
D (cid:0)
EŒn2, since each value inthe input array Ais equally likely tofall inany bucket.
i
Toproveequation (8.2),wedefineindicator randomvariables
X I AŒjfallsinbucketi
ij
D f g
fori 0;1;:::;n 1andj 1;2;:::;n. Thus,
D (cid:0) D
n
n X :
i ij
D
j 1
XD
TocomputeEŒn2,weexpandthesquareandregroupterms:
i
8.4 Bucketsort 203
n 2
E n2 E X
i D ij
" ! #
j 1
  XD
n n
E X X
ij ik
D
" #
j 1k 1
XD XD
n
E X2 X X
D ij C ij ik
2 3
j 1 1 j n 1 k n
XD X kXj
n4 ¤ 5
E X2 EŒX X  ; (8.3)
D ij C ij ik
j 1 1 j n 1 k n
XD   X X k j
¤
where the last line follows by linearity of expectation. We evaluate the two sum-
mations separately. Indicator random variable X is1withprobability 1=n and 0
ij
otherwise, andtherefore
1 1
E X2 12 02 1
ij D  n C  (cid:0) n
 
  1
:
D n
Whenk j,thevariables X andX areindependent, andhence
ij ik
¤
EŒX X  EŒX EŒX 
ij ik ij ik
D
1 1
D n  n
1
:
D n2
Substituting thesetwoexpectedvaluesinequation (8.3),weobtain
n
1 1
E n2
i D n C n2
j 1 1 j n 1 k n
  XD X kXj
¤
1 1
n n.n 1/
D  n C (cid:0)  n2
n 1
1 (cid:0)
D C n
1
2 ;
D (cid:0) n
whichprovesequation (8.2).
204 Chapter8 SortinginLinearTime
Using this expected value in equation (8.1), we conclude that the average-case
running timeforbucketsortis‚.n/ n O.2 1=n/ ‚.n/.
C  (cid:0) D
Eveniftheinput isnotdrawnfrom auniform distribution, bucket sortmaystill
runinlineartime. Aslongastheinputhasthepropertythatthesumofthesquares
ofthebucket sizesislinearinthetotal numberofelements, equation (8.1)tellsus
thatbucket sortwillruninlineartime.
Exercises
8.4-1
UsingFigure8.4asamodel,illustratetheoperationofBUCKET-SORTonthearray
A :79; :13; :16; :64; :39; :20; :89; :53; :71; :42 .
D h i
8.4-2
Explain why the worst-case running time for bucket sort is ‚.n2/. What simple
change to the algorithm preserves its linear average-case running time and makes
itsworst-caserunning timeO.nlgn/?
8.4-3
Let X be a random variable that is equal to the number of heads in two flips of a
faircoin. WhatisEŒX2? WhatisE2ŒX?
8.4-4 ?
Wearegivennpoints intheunit circle, p .x ;y /,such that0 < x2 y2 1
i D i i i C i 
for i 1;2;:::;n. Suppose that the points are uniformly distributed; that is, the
D
probability offinding apoint inanyregion of thecircle isproportional tothe area
of that region. Design analgorithm with anaverage-case running time of‚.n/to
sortthenpointsbytheirdistances d x2 y2 fromtheorigin. (Hint:Design
i D i C i
the bucket sizes in BUCKET-SORT toreflect the uniform distribution ofthe points
p
intheunitcircle.)
8.4-5 ?
A probability distribution function P.x/ for a random variable X is defined
by P.x/ Pr X x . Suppose that we draw a list of n random variables
D f  g
X ;X ;:::;X fromacontinuous probability distribution function P thatiscom-
1 2 n
putableinO.1/time. Giveanalgorithmthatsortsthesenumbersinlinearaverage-
casetime.
ProblemsforChapter8 205
Problems
8-1 Probabilistic lowerboundsoncomparison sorting
Inthisproblem,weproveaprobabilistic.nlgn/lowerboundontherunningtime
of any deterministic or randomized comparison sort on n distinct input elements.
We begin by examining a deterministic comparison sort A with decision tree T .
A
Weassumethateverypermutation ofA’sinputsisequallylikely.
a. Suppose that each leaf of T is labeled with the probability that it is reached
A
givenarandominput. ProvethatexactlynŠleavesarelabeled1=nŠandthatthe
restarelabeled0.
b. Let D.T/ denote the external path length of a decision tree T; that is, D.T/
is the sum of the depths of all the leaves of T. Let T be a decision tree with
k > 1leaves, andlet LT andRT bethe leftandright subtrees ofT. Showthat
D.T/ D.LT/ D.RT/ k.
D C C
c. Letd.k/ be the minimum value of D.T/ over all decision trees T with k > 1
leaves. Showthatd.k/ min d.i/ d.k i/ k . (Hint:Consider
1 i k 1
adecisiontreeT withkD leavesth a ta(cid:0) chif evestC hemini(cid:0) mumC . Leg ti bethenumber
0
ofleavesinLT andk i thenumberofleavesinRT.)
0
(cid:0)
d. Prove that for a given value of k > 1 and i in the range 1 i k 1, the
  (cid:0)
function ilgi .k i/lg.k i/ is minimized at i k=2. Conclude that
C (cid:0) (cid:0) D
d.k/ .klgk/.
D
e. Prove that D.T / .nŠlg.nŠ//, and conclude that the average-case time to
A
D
sortnelementsis.nlgn/.
Now, consider a randomized comparison sort B. We can extend the decision-
treemodeltohandlerandomization byincorporating twokindsofnodes: ordinary
comparison nodes and “randomization” nodes. A randomization node models a
random choice of the form RANDOM.1;r/ made by algorithm B; the node has r
children, each of which is equally likely to be chosen during an execution of the
algorithm.
f. Show that for any randomized comparison sort B, there exists a deterministic
comparison sort A whose expected number of comparisons is no more than
thosemadebyB.
206 Chapter8 SortinginLinearTime
8-2 Sortinginplaceinlineartime
Suppose that we have an array of n data records to sort and that the key of each
record has the value 0 or 1. An algorithm for sorting such a set of records might
possess somesubsetofthefollowingthreedesirable characteristics:
1. Thealgorithm runsinO.n/time.
2. Thealgorithm isstable.
3. Thealgorithm sorts in place, using no more than a constant amount of storage
spaceinaddition totheoriginalarray.
a. Giveanalgorithm thatsatisfiescriteria1and2above.
b. Giveanalgorithm thatsatisfiescriteria1and3above.
c. Giveanalgorithm thatsatisfiescriteria2and3above.
d. Can you use any of your sorting algorithms from parts (a)–(c) as the sorting
method used in line 2 of RADIX-SORT, so that RADIX-SORT sorts n records
withb-bitkeysinO.bn/time? Explainhoworwhynot.
e. Suppose that the n records have keys in the range from 1 to k. Show how to
modifycountingsortsothatitsortstherecordsinplaceinO.n k/time. You
C
mayuseO.k/ storage outside theinput array. Isyouralgorithm stable? (Hint:
Howwouldyoudoitfork 3?)
D
8-3 Sortingvariable-length items
a. Youare given anarray of integers, where different integers mayhave different
numbersofdigits,butthetotalnumberofdigitsoveralltheintegersinthearray
isn. ShowhowtosortthearrayinO.n/time.
b. You are given an array of strings, where different strings may have different
numbers of characters, but the total number of characters over all the strings
isn. ShowhowtosortthestringsinO.n/time.
(Notethatthedesiredorderhereisthestandardalphabeticalorder;forexample,
a <ab < b.)
8-4 Waterjugs
Supposethatyouaregivennredandnbluewaterjugs,allofdifferent shapesand
sizes. Allredjugsholddifferentamountsofwater,asdotheblueones. Moreover,
foreveryredjug,thereisabluejugthatholdsthesameamountofwater,andvice
versa.
ProblemsforChapter8 207
Yourtaskistofindagroupingofthejugsintopairsofredandbluejugsthathold
thesameamountofwater. Todoso,youmayperformthefollowingoperation: pick
a pair of jugs in which one is red and one is blue, fill the red jug with water, and
then pour the water into the blue jug. This operation will tell you whether the red
or the blue jug can hold more water, or that they have the same volume. Assume
that such a comparison takes one time unit. Your goal is to find an algorithm that
makes aminimum number of comparisons todetermine the grouping. Remember
thatyoumaynotdirectly comparetworedjugsortwobluejugs.
a. Describe a deterministic algorithm that uses ‚.n2/ comparisons to group the
jugsintopairs.
b. Prove alower bound of .nlgn/ for the number of comparisons that an algo-
rithmsolvingthisproblemmustmake.
c. Give a randomized algorithm whose expected number of comparisons is
O.nlgn/, and prove that this bound is correct. What is the worst-case num-
berofcomparisons foryouralgorithm?
8-5 Averagesorting
Supposethat,insteadofsortinganarray,wejustrequirethattheelementsincrease
on average. More precisely, we call an n-element array A k-sorted if, for all
i 1;2;:::;n k,thefollowingholds:
D (cid:0)
i k 1AŒj i k AŒj
jC Di(cid:0) jC Di C1 :
k  k
P P
a. Whatdoesitmeanforanarraytobe1-sorted?
b. Giveapermutation ofthenumbers1;2;:::;10thatis2-sorted, butnotsorted.
c. Provethatann-element arrayisk-sorted ifandonlyifAŒi AŒi kforall
 C
i 1;2;:::;n k.
D (cid:0)
d. Giveanalgorithm thatk-sortsann-elementarrayinO.nlg.n=k//time.
Wecan also show a lower bound on the time to produce a k-sorted array, when k
isaconstant.
e. Show that we can sort a k-sorted array of length n in O.nlgk/ time. (Hint:
Usethesolution toExercise6.5-9. )
f. Showthatwhenkisaconstant,k-sortingann-elementarrayrequires.nlgn/
time. (Hint: Use the solution to the previous part along with the lower bound
oncomparison sorts.)
208 Chapter8 SortinginLinearTime
8-6 Lowerboundonmergingsortedlists
The problem of merging two sorted lists arises frequently. We have seen a pro-
cedure for it as the subroutine MERGE in Section 2.3.1. In this problem, we will
provealowerbound of2n 1ontheworst-case numberofcomparisons required
(cid:0)
tomergetwosortedlists,eachcontaining nitems.
Firstwewillshowalowerboundof2n o.n/comparisons byusingadecision
(cid:0)
tree.
a. Given 2n numbers, compute the number of possible ways to divide them into
twosortedlists,eachwithnnumbers.
b. Usingadecision treeandyouranswertopart(a), showthatanyalgorithm that
correctlymergestwosortedlistsmustperformatleast2n o.n/comparisons.
(cid:0)
Nowwewillshowaslightlytighter2n 1bound.
(cid:0)
c. Showthatiftwoelementsareconsecutiveinthesortedorderandfromdifferent
lists,thentheymustbecompared.
d. Useyouranswertothepreviousparttoshowalowerboundof2n 1compar-
(cid:0)
isonsformergingtwosortedlists.
8-7 The0-1sortinglemmaandcolumnsort
Acompare-exchangeoperationontwoarrayelementsAŒiandAŒj,wherei < j,
hastheform
COMPARE-EXCHANGE.A;i;j/
1 ifAŒi> AŒj
2 exchangeAŒiwithAŒj
Afterthecompare-exchange operation, weknowthatAŒi AŒj.

An oblivious compare-exchange algorithm operates solely by a sequence of
prespecified compare-exchange operations. Theindicesofthepositions compared
in the sequence must be determined in advance, and although they can depend
on the number of elements being sorted, they cannot depend on the values being
sorted,norcantheydependontheresultofanypriorcompare-exchangeoperation.
For example, here is insertion sort expressed as an oblivious compare-exchange
algorithm:
INSERTION-SORT.A/
1 forj 2toA:length
D
2 fori j 1downto1
D (cid:0)
3 COMPARE-EXCHANGE.A;i;i 1/
C
ProblemsforChapter8 209
The 0-1 sorting lemma provides a powerful way to prove that an oblivious
compare-exchange algorithm produces a sorted result. It states that if an oblivi-
ous compare-exchange algorithm correctly sorts all input sequences consisting of
only0sand1s,thenitcorrectly sortsallinputscontaining arbitrary values.
Youwillprovethe0-1sortinglemmabyprovingitscontrapositive: ifanoblivi-
ouscompare-exchange algorithm fails tosort aninput containing arbitrary values,
thenitfailstosortsome0-1input. Assumethatanobliviouscompare-exchangeal-
gorithm Xfailstocorrectly sortthearrayAŒ1::n. LetAŒpbethesmallestvalue
in A that algorithm X puts into the wrong location, and let AŒq be the value that
algorithm X moves to the location into which AŒp should have gone. Define an
arrayBŒ1::nof0sand1sasfollows:
0 ifAŒi AŒp;
BŒi 
D
(
1 ifAŒi> AŒp:
a. ArguethatAŒq> AŒp,sothatBŒp 0andBŒq 1.
D D
b. Tocompletetheproofofthe0-1sortinglemma,provethatalgorithm Xfailsto
sortarrayB correctly.
Nowyou willuse the 0-1 sorting lemma to prove that aparticular sorting algo-
rithm works correctly. The algorithm, columnsort, works on a rectangular array
of n elements. The array has r rows and s columns (so that n rs), subject to
D
threerestrictions:
 r mustbeeven,
 s mustbeadivisorofr,and
 r 2s2.

When columnsort completes, the array is sorted in column-major order: reading
downthecolumns, fromlefttoright, theelementsmonotonically increase.
Columnsort operates in eight steps, regardless of the value of n. The odd steps
areallthesame: sorteachcolumnindividually. Eachevenstepisafixedpermuta-
tion. Herearethesteps:
1. Sorteachcolumn.
2. Transpose the array, but reshape it back to r rows and s columns. In other
words, turn the leftmost column into the top r=s rows, in order; turn the next
columnintothenextr=s rows,inorder;andsoon.
3. Sorteachcolumn.
4. Performtheinverseofthepermutation performed instep2.
210 Chapter8 SortinginLinearTime
10 14 5 4 1 2 4 8 10 1 3 6 1 4 11
8 7 17 8 3 5 12 16 18 2 5 7 3 8 14
12 1 6 10 7 6 1 3 7 4 8 10 6 10 17
16 9 11 12 9 11 9 14 15 9 13 15 2 9 12
4 15 2 16 14 13 2 5 6 11 14 17 5 13 16
18 3 13 18 15 17 11 13 17 12 16 18 7 15 18
(a) (b) (c) (d) (e)
1 4 11 5 10 16 4 10 16 1 7 13
2 8 12 6 13 17 5 11 17 2 8 14
3 9 14 7 15 18 6 12 18 3 9 15
5 10 16 1 4 11 1 7 13 4 10 16
6 13 17 2 8 12 2 8 14 5 11 17
7 15 18 3 9 14 3 9 15 6 12 18
(f) (g) (h) (i)
Figure8.5 Thesteps of columnsort. (a) Theinput array with6rows and 3columns. (b)After
sortingeachcolumninstep1. (c)Aftertransposingandreshapinginstep2. (d)Aftersortingeach
columninstep3. (e)Afterperformingstep4,whichinvertsthepermutationfromstep2. (f)After
sortingeachcolumninstep5. (g)Aftershiftingbyhalfacolumninstep6. (h)Aftersortingeach
columninstep7. (i)Afterperformingstep8,whichinvertsthepermutationfromstep6. Thearray
isnowsortedincolumn-majororder.
5. Sorteachcolumn.
6. Shiftthetophalfofeachcolumnintothebottom halfofthesamecolumn, and
shiftthebottom halfofeachcolumnintothetophalfofthenextcolumntothe
right. Leave the top half of the leftmost column empty. Shift the bottom half
of the last column into the top half of a new rightmost column, and leave the
bottomhalfofthisnewcolumnempty.
7. Sorteachcolumn.
8. Performtheinverse ofthepermutation performedinstep6.
Figure 8.5 shows an example of the steps of columnsort with r 6 and s 3.
D D
(Even though this example violates the requirement that r 2s2, it happens to

work.)
c. Argue that we can treat columnsort as an oblivious compare-exchange algo-
rithm,evenifwedonotknowwhatsorting methodtheoddstepsuse.
Although it might seem hard to believe that columnsort actually sorts, you will
use the 0-1 sorting lemma to prove that it does. The 0-1 sorting lemma applies
because we can treat columnsort as an oblivious compare-exchange algorithm. A
NotesforChapter8 211
coupleofdefinitionswillhelpyouapplythe0-1sortinglemma. Wesaythatanarea
of an array is clean if we know that it contains either all 0s or all 1s. Otherwise,
thearea mightcontain mixed0sand 1s, anditisdirty. Fromhere on, assume that
the input array contains only 0s and 1s, and that wecan treat it as an array with r
rowsands columns.
d. Provethataftersteps1–3,thearrayconsistsofsomecleanrowsof0satthetop,
somecleanrowsof1satthebottom,andatmosts dirtyrowsbetweenthem.
e. Provethatafterstep4,thearray,readincolumn-majororder,startswithaclean
area of 0s, ends with a clean area of 1s, and has a dirty area of at most s2
elementsinthemiddle.
f. Prove that steps 5–8 produce afully sorted 0-1 output. Conclude that column-
sortcorrectly sortsallinputscontaining arbitraryvalues.
g. Now suppose that s does not divide r. Prove that after steps 1–3, the array
consists of some clean rows of 0s at the top, some clean rows of 1s at the
bottom, and at most 2s 1 dirty rows between them. How large must r be,
(cid:0)
comparedwiths,forcolumnsort tocorrectly sortwhens doesnotdivider?
h. Suggest a simple change to step 1 that allows us to maintain the requirement
that r 2s2 even when s does not divide r, and prove that with your change,

columnsort correctly sorts.
Chapter notes
The decision-tree model for studying comparison sorts was introduced by Ford
and Johnson [110]. Knuth’s comprehensive treatise on sorting [211] covers many
variationsonthesortingproblem,includingtheinformation-theoretic lowerbound
on the complexity of sorting given here. Ben-Or [39] studied lower bounds for
sortingusinggeneralizations ofthedecision-tree model.
KnuthcreditsH.H.Sewardwithinventingcountingsortin1954,aswellaswith
theideaofcombiningcountingsortwithradixsort. Radixsortingstartingwiththe
least significant digit appears to be a folk algorithm widely used by operators of
mechanical card-sorting machines. According to Knuth, the first published refer-
ence to the method is a 1929 document by L. J. Comrie describing punched-card
equipment. Bucket sorting has been in use since 1956, when the basic idea was
proposed byE.J.IsaacandR.C.Singleton[188].
Munro and Raman[263]giveastable sorting algorithm that performs O.n1 /
C
comparisons in the worst case, where 0 <  1 is any fixed constant. Although

212 Chapter8 SortinginLinearTime
any of the O.nlgn/-time algorithms make fewer comparisons, the algorithm by
MunroandRamanmovesdataonlyO.n/timesandoperates inplace.
The case of sorting n b-bit integers in o.nlgn/ time has been considered by
manyresearchers. Severalpositiveresults have beenobtained, each underslightly
different assumptions about the model of computation and the restrictions placed
onthealgorithm. Alltheresults assumethatthecomputer memoryisdivided into
addressableb-bitwords. FredmanandWillard[115]introducedthefusiontreedata
structure and used it to sort n integers in O.nlgn=lglgn/ time. This bound was
later improved to O.n lgn/ time by Andersson [16]. These algorithms require
theuseofmultiplication andseveralprecomputed constants. Andersson,Hagerup,
p
Nilsson, and Raman [17] have shown how to sort n integers in O.nlglgn/ time
without using multiplication, but their method requires storage that can be un-
bounded in terms of n. Using multiplicative hashing, we can reduce the storage
needed to O.n/, but then the O.nlglgn/ worst-case bound on the running time
becomes an expected-time bound. Generalizing the exponential search trees of
Andersson [16], Thorup [335] gave an O.n.lglgn/2/-time sorting algorithm that
does notusemultiplication orrandomization, andituseslinear space. Combining
these techniques with some new ideas, Han [158] improved the bound for sorting
to O.nlglgnlglglgn/ time. Although these algorithms are important theoretical
breakthroughs, theyareallfairlycomplicatedandatthepresenttimeseemunlikely
tocompetewithexistingsortingalgorithms inpractice.
Thecolumnsort algorithm inProblem8-7isbyLeighton[227].
9 Medians and Order Statistics
The ith order statistic of a set of n elements is the ith smallest element. For
example, the minimum of a set of elements is the first order statistic (i 1),
D
and the maximum is the nth order statistic (i n). A median, informally, is
D
the “halfway point” of the set. When n is odd, the median is unique, occurring at
i .n 1/=2. Whenniseven, there aretwomedians, occurring ati n=2and
D C D
i n=2 1. Thus,regardlessoftheparityofn,mediansoccurati .n 1/=2
D C D b C c
(the lower median) and i .n 1/=2 (the upper median). For simplicity in
D d C e
thistext,however,weconsistentlyusethephrase“themedian”torefertothelower
median.
This chapter addresses the problem of selecting the ith order statistic from a
set of n distinct numbers. We assume for convenience that the set contains dis-
tinct numbers, although virtually everything that wedo extends to the situation in
which a set contains repeated values. We formally specify the selection problem
asfollows:
Input: AsetAofn(distinct) numbersandanintegeri,with1 i n.
 
Output: Theelementx Athatislargerthanexactlyi 1otherelementsofA.
2 (cid:0)
Wecan solve the selection problem in O.nlgn/ time, since wecan sort the num-
bers using heapsort or merge sort and then simply index the ith element in the
outputarray. Thischapterpresents fasteralgorithms.
In Section 9.1, we examine the problem of selecting the minimum and maxi-
mumofasetofelements. Moreinterestingisthegeneralselectionproblem,which
we investigate in the subsequent two sections. Section 9.2 analyzes a practical
randomizedalgorithmthatachievesanO.n/expectedrunningtime,assumingdis-
tinct elements. Section 9.3 contains an algorithm of more theoretical interest that
achievestheO.n/runningtimeintheworstcase.
214 Chapter9 MediansandOrderStatistics
9.1 Minimum andmaximum
How many comparisons are necessary to determine the minimum of a set of n
elements? We can easily obtain an upper bound of n 1 comparisons: examine
(cid:0)
each element of the set in turn and keep track of the smallest element seen so
far. In the following procedure, we assume that the set resides in array A, where
A:length n.
D
MINIMUM.A/
1 min AŒ1
D
2 fori 2toA:length
D
3 ifmin >AŒi
4 min AŒi
D
5 returnmin
Wecan,ofcourse, findthemaximumwithn 1comparisons aswell.
(cid:0)
Is this the best we can do? Yes, since we can obtain a lower bound of n 1
(cid:0)
comparisonsfortheproblemofdeterminingtheminimum. Thinkofanyalgorithm
that determines the minimumasatournament among theelements. Eachcompar-
ison is a match in the tournament in which the smaller of the two elements wins.
Observing that every element except the winner must lose at least one match, we
conclude thatn 1comparisons arenecessarytodeterminetheminimum. Hence,
(cid:0)
the algorithm MINIMUM is optimal with respect to the number of comparisons
performed.
Simultaneousminimumandmaximum
In some applications, we must find both the minimum and the maximum of a set
of n elements. Forexample, a graphics program may need to scale aset of .x;y/
data to fit onto a rectangular display screen or other graphical output device. To
doso,theprogram mustfirstdeterminetheminimumandmaximumvalueofeach
coordinate.
Atthispoint, itshould beobvious how todetermine both theminimum and the
maximumofnelementsusing‚.n/comparisons,whichisasymptoticallyoptimal:
simply find the minimum and maximum independently, using n 1 comparisons
(cid:0)
foreach,foratotalof2n 2comparisons.
(cid:0)
Infact, wecanfindboththeminimumandthemaximumusing atmost3 n=2
b c
comparisons. Wedosobymaintainingboththeminimumandmaximumelements
seen thus far. Rather than processing each element of the input by comparing it
againstthecurrentminimumandmaximum,atacostof2comparisonsperelement,
9.2 Selectioninexpectedlineartime 215
we process elements in pairs. We compare pairs of elements from the input first
with each other, and then we compare the smaller with the current minimum and
thelargertothecurrentmaximum,atacostof3comparisonsforevery2elements.
How we set up initial values for the current minimum and maximum depends
onwhether n isodd oreven. Ifn isodd, wesetboth the minimum and maximum
to the value of the first element, and then we process the rest of the elements in
pairs. If n is even, we perform 1 comparison on the first 2 elements to determine
the initial values of the minimum and maximum, and then process the rest of the
elementsinpairsasinthecaseforoddn.
Let us analyze the total number of comparisons. If n is odd, then we perform
3 n=2 comparisons. If n is even, we perform 1 initial comparison followed by
b c
3.n 2/=2 comparisons, for a total of 3n=2 2. Thus, in either case, the total
(cid:0) (cid:0)
numberofcomparisons isatmost3 n=2 .
b c
Exercises
9.1-1
Show that the second smallest of n elements can be found with n lgn 2
C d e (cid:0)
comparisons intheworstcase. (Hint:Alsofindthesmallestelement.)
9.1-2 ?
Prove the lower bound of 3n=2 2 comparisons in the worst case to find both
d e (cid:0)
the maximum and minimum of n numbers. (Hint: Consider how many numbers
arepotentiallyeitherthemaximumorminimum,andinvestigatehowacomparison
affectsthesecounts.)
9.2 Selection inexpected lineartime
The general selection problem appears more difficult than the simple problem of
finding a minimum. Yet, surprisingly, the asymptotic running time for both prob-
lemsisthesame: ‚.n/. Inthissection,wepresentadivide-and-conquer algorithm
fortheselectionproblem. Thealgorithm RANDOMIZED-SELECT ismodeledafter
thequicksort algorithm ofChapter 7. Asinquicksort, wepartition the input array
recursively. But unlike quicksort, which recursively processes both sides of the
partition, RANDOMIZED-SELECT works on only one side of the partition. This
difference shows up in the analysis: whereas quicksort has an expected running
time of ‚.nlgn/, the expected running time of RANDOMIZED-SELECT is ‚.n/,
assumingthattheelementsaredistinct.
216 Chapter9 MediansandOrderStatistics
RANDOMIZED-SELECT uses the procedure RANDOMIZED-PARTITION intro-
ducedinSection7.3. Thus,likeRANDOMIZED-QUICKSORT, itisarandomizedal-
gorithm,sinceitsbehaviorisdeterminedinpartbytheoutputofarandom-number
generator. ThefollowingcodeforRANDOMIZED-SELECT returnstheithsmallest
elementofthearrayAŒp::r.
RANDOMIZED-SELECT.A;p;r;i/
1 ifp ==r
2 returnAŒp
3 q RANDOMIZED-PARTITION.A;p;r/
D
4 k q p 1
D (cid:0) C
5 ifi ==k //thepivotvalueistheanswer
6 returnAŒq
7 elseifi <k
8 return RANDOMIZED-SELECT.A;p;q 1;i/
(cid:0)
9 elsereturn RANDOMIZED-SELECT.A;q 1;r;i k/
C (cid:0)
The RANDOMIZED-SELECT procedure worksasfollows. Line1checks forthe
base case of the recursion, in which the subarray AŒp::r consists of just one
element. In this case, i must equal 1, and we simply return AŒp in line 2 as the
ith smallest element. Otherwise, the call to RANDOMIZED-PARTITION in line 3
partitions the array AŒp::r into two (possibly empty) subarrays AŒp::q 1
(cid:0)
and AŒq 1::r such that each element of AŒp::q 1 is less than or equal
C (cid:0)
to AŒq, which in turn is less than each element of AŒq 1::r. As in quicksort,
C
we will refer to AŒq as the pivot element. Line 4 computes the number k of
elements in the subarray AŒp::q, that is, the number of elements in the low side
ofthepartition, plusoneforthepivotelement. Line5thencheckswhetherAŒqis
theithsmallestelement. Ifitis,thenline6returnsAŒq. Otherwise,thealgorithm
determines in which of the two subarrays AŒp::q 1 and AŒq 1::r the ith
(cid:0) C
smallest element lies. If i < k, then the desired element lies on the low side of
thepartition, andline8recursively selectsitfromthesubarray. Ifi > k,however,
then the desired element lies on the high side of the partition. Since we already
knowk valuesthataresmallerthantheithsmallestelementofAŒp::r—namely,
the elements of AŒp::q—the desired element is the .i k/th smallest element
(cid:0)
ofAŒq 1::r,whichline9findsrecursively. Thecodeappearstoallowrecursive
C
calls to subarrays with 0 elements, but Exercise 9.2-1 asks you to show that this
situation cannothappen.
Theworst-case running timefor RANDOMIZED-SELECT is‚.n2/,eventofind
theminimum,becausewecouldbeextremelyunluckyandalwayspartitionaround
the largest remaining element, and partitioning takes ‚.n/ time. We will see that
9.2 Selectioninexpectedlineartime 217
thealgorithmhasalinearexpectedrunningtime,though,andbecauseitisrandom-
ized,noparticular inputelicitstheworst-casebehavior.
ToanalyzetheexpectedrunningtimeofRANDOMIZED-SELECT, welettherun-
ning time on an input array AŒp::r of n elements be a random variable that we
denote by T.n/, and we obtain an upper bound on EŒT.n/ as follows. The pro-
cedure RANDOMIZED-PARTITION is equally likely to return any element as the
pivot. Therefore, foreachk suchthat1 k n,thesubarray AŒp::qhask ele-
 
ments(alllessthanorequaltothepivot)withprobability1=n. Fork 1;2;:::;n,
D
wedefineindicator random variablesX where
k
X I thesubarray AŒp::qhasexactlyk elements ;
k
D f g
andso,assumingthattheelementsaredistinct, wehave
EŒX  1=n: (9.1)
k
D
WhenwecallRANDOMIZED-SELECT andchooseAŒqasthepivotelement,we
do not know, a priori, if we will terminate immediately with the correct answer,
recurse on the subarray AŒp::q 1, or recurse on the subarray AŒq 1::r.
(cid:0) C
This decision depends on where the ith smallest element falls relative to AŒq.
Assuming that T.n/ is monotonically increasing, we can upper-bound the time
neededfortherecursivecallbythetimeneededfortherecursivecallonthelargest
possible input. In other words, to obtain an upper bound, we assume that the ith
elementisalwaysonthesideofthepartition withthegreaternumberofelements.
Foragiven call of RANDOMIZED-SELECT, the indicator random variable X
k
has
thevalue1forexactlyonevalueofk,anditis0forallotherk. WhenX 1,the
k
D
twosubarrays onwhich wemight recurse have sizes k 1and n k. Hence, we
(cid:0) (cid:0)
havetherecurrence
n
T.n/ X .T.max.k 1;n k// O.n//
k
  (cid:0) (cid:0) C
k 1
XD
n
X T.max.k 1;n k// O.n/:
k
D  (cid:0) (cid:0) C
k 1
XD
218 Chapter9 MediansandOrderStatistics
Takingexpected values,wehave
EŒT.n/
n
E X T.max.k 1;n k// O.n/
k
  (cid:0) (cid:0) C
" #
k 1
XD
n
EŒX T.max.k 1;n k// O.n/ (bylinearity ofexpectation)
k
D  (cid:0) (cid:0) C
k 1
XD
n
EŒX  EŒT.max.k 1;n k// O.n/ (byequation (C.24))
k
D  (cid:0) (cid:0) C
k 1
XD
n
1
EŒT.max.k 1;n k// O.n/ (byequation (9.1)) .
D n  (cid:0) (cid:0) C
k 1
XD
Inordertoapplyequation (C.24),werelyonX andT.max.k 1;n k//being
k
(cid:0) (cid:0)
independent random variables. Exercise9.2-2asksyoutojustifythisassertion.
Letusconsidertheexpression max.k 1;n k/. Wehave
(cid:0) (cid:0)
k 1 ifk > n=2 ;
max.k 1;n k/ (cid:0) d e
(cid:0) (cid:0) D
(
n k ifk n=2 :
(cid:0)  d e
If n is even, each term from T. n=2 / up to T.n 1/ appears exactly twice in
d e (cid:0)
thesummation,andifnisodd,allthesetermsappeartwiceandT. n=2 /appears
b c
once. Thus,wehave
n 1
2 (cid:0)
EŒT.n/ EŒT.k/ O.n/:
 n C
k n=2
DXb c
We show that EŒT.n/ O.n/ by substitution. Assume that EŒT.n/ cn for
D 
some constant c that satisfies the initial conditions of the recurrence. We assume
that T.n/ O.1/ for n less than some constant; weshall pick this constant later.
D
WealsopickaconstantasuchthatthefunctiondescribedbytheO.n/termabove
(which describes the non-recursive component of the running time of the algo-
rithm)isboundedfromabovebyanforalln > 0. Usingthisinductivehypothesis,
wehave
n 1
2 (cid:0)
EŒT.n/ ck an
 n C
k n=2
DXb c
n 1 n=2 1
2c (cid:0) b c(cid:0)
k k an
D n (cid:0) C
!
k 1 k 1
XD XD
9.2 Selectioninexpectedlineartime 219
2c .n 1/n . n=2 1/ n=2
(cid:0) b c(cid:0) b c an
D n 2 (cid:0) 2 C
 
2c .n 1/n .n=2 2/.n=2 1/
(cid:0) (cid:0) (cid:0) an
 n 2 (cid:0) 2 C
 
2c n2 n n2=4 3n=2 2
(cid:0) (cid:0) C an
D n 2 (cid:0) 2 C
 
c 3n2 n
2 an
D n 4 C 2 (cid:0) C
 
3n 1 2
c an
D 4 C 2 (cid:0) n C
 
3cn c
an
 4 C 2 C
cn c
cn an :
D (cid:0) 4 (cid:0) 2 (cid:0)
 
In order to complete the proof, we need to show that for sufficiently large n, this
last expression is at most cn or, equivalently, that cn=4 c=2 an 0. If we
(cid:0) (cid:0) 
add c=2 to both sides and factor out n, we get n.c=4 a/ c=2. As long as we
(cid:0) 
choose the constant c so that c=4 a > 0, i.e., c > 4a, wecan divide both sides
(cid:0)
byc=4 a,giving
(cid:0)
c=2 2c
n :
 c=4 a D c 4a
(cid:0) (cid:0)
Thus,ifweassumethatT.n/ O.1/forn < 2c=.c 4a/,thenEŒT.n/ O.n/.
D (cid:0) D
We conclude that we can find any order statistic, and in particular the median, in
expectedlineartime,assumingthattheelementsaredistinct.
Exercises
9.2-1
ShowthatRANDOMIZED-SELECT nevermakesarecursivecalltoa0-lengtharray.
9.2-2
Argue that the indicator random variable X and the value T.max.k 1;n k//
k
(cid:0) (cid:0)
areindependent.
9.2-3
WriteaniterativeversionofRANDOMIZED-SELECT.
220 Chapter9 MediansandOrderStatistics
9.2-4
Suppose we use RANDOMIZED-SELECT to select the minimum element of the
arrayA 3;2;9;0;7;5;4;8;6;1 . Describeasequence ofpartitions thatresults
D h i
inaworst-caseperformance of RANDOMIZED-SELECT.
9.3 Selectioninworst-caselineartime
We now examine a selection algorithm whose running time is O.n/ in the worst
case. Like RANDOMIZED-SELECT, the algorithm SELECT finds the desired ele-
ment by recursively partitioning the input array. Here, however, we guarantee a
good split upon partitioning the array. SELECT uses the deterministic partitioning
algorithm PARTITION from quicksort (see Section 7.1), but modified to take the
elementtopartition aroundasaninputparameter.
The SELECT algorithm determines the ith smallest of an input array of n > 1
distinctelementsbyexecutingthefollowingsteps. (Ifn 1,thenSELECT merely
D
returnsitsonlyinputvalueastheithsmallest.)
1. Divide the n elements of the input array into n=5 groups of 5elements each
b c
andatmostonegroupmadeupoftheremainingn mod5elements.
2. Find the median of each of the n=5 groups by first insertion-sorting the ele-
d e
mentsofeachgroup(ofwhichthereareatmost5)andthenpickingthemedian
fromthesortedlistofgroupelements.
3. Use SELECT recursively to find the median x of the n=5 medians found in
d e
step 2. (If there are an even number of medians, then by our convention, x is
thelowermedian.)
4. Partition the input array around the median-of-medians x using the modified
version of PARTITION. Letk beone more than thenumber ofelements onthe
lowsideofthepartition,sothatxisthekthsmallestelementandtherearen k
(cid:0)
elementsonthehighsideofthepartition.
5. If i k, then return x. Otherwise, use SELECT recursively to find the ith
D
smallestelement onthelowsideifi < k,orthe.i k/thsmallest elementon
(cid:0)
thehighsideifi > k.
ToanalyzetherunningtimeofSELECT,wefirstdeterminealowerboundonthe
number of elements that are greater than the partitioning element x. Figure 9.1
helps us to visualize this bookkeeping. At least half of the medians found in
9.3 Selectioninworst-caselineartime 221
x
Figure9.1 Analysisofthealgorithm SELECT. Thenelementsarerepresentedbysmallcircles,
andeachgroupof5elementsoccupiesacolumn. Themediansofthegroupsarewhitened,andthe
median-of-mediansx islabeled. (Whenfindingthemedianofanevennumberofelements,weuse
thelower median.) Arrowsgofromlarger elementstosmaller, fromwhichwecanseethat3out
ofeveryfullgroupof5elementstotherightofx aregreaterthanx,and3outofeverygroupof5
elementstotheleftofxarelessthanx.Theelementsknowntobegreaterthanxappearonashaded
background.
step 2 are greater than or equal to the median-of-medians x.1 Thus, at least half
of the n=5 groups contribute at least 3 elements that are greater than x, except
d e
fortheonegroupthathasfewerthan5elementsif5doesnotdividenexactly,and
theonegroupcontainingx itself. Discountingthesetwogroups,itfollowsthatthe
numberofelementsgreaterthanx isatleast
1 n 3n
3 2 6:
2 5 (cid:0)  10 (cid:0)
  
l m
Similarly, at least 3n=10 6 elements are less than x. Thus, in the worst case,
(cid:0)
step5calls SELECT recursively onatmost7n=10 6elements.
C
We can now develop a recurrence for the worst-case running time T.n/ of the
algorithm SELECT. Steps 1, 2, and 4 take O.n/ time. (Step 2 consists of O.n/
callsofinsertionsortonsetsofsizeO.1/.) Step3takestimeT. n=5 /,andstep5
d e
takes time at most T.7n=10 6/, assuming that T is monotonically increasing.
C
Wemaketheassumption,whichseemsunmotivatedatfirst,thatanyinputoffewer
than140elementsrequiresO.1/time;theoriginofthemagicconstant140willbe
clearshortly. Wecantherefore obtaintherecurrence
1Because of our assumption that thenumbers are distinct, all medians except x areeither greater
thanorlessthanx.
222 Chapter9 MediansandOrderStatistics
O.1/ ifn < 140;
T.n/

(
T. n=5 / T.7n=10 6/ O.n/ ifn 140:
d e C C C 
Weshowthattherunning timeislinear bysubstitution. Morespecifically, wewill
showthatT.n/ cnforsomesuitablylargeconstantcandalln > 0. Webeginby

assuming that T.n/ cnfor somesuitably large constant c and alln < 140; this

assumptionholdsifc islargeenough. Wealsopickaconstantasuchthatthefunc-
tiondescribed bytheO.n/termabove(whichdescribes thenon-recursive compo-
nent of the running time of the algorithm) is bounded above by an for all n > 0.
Substituting this inductive hypothesis into the right-hand side of the recurrence
yields
T.n/ c n=5 c.7n=10 6/ an
 d eC C C
cn=5 c 7cn=10 6c an
 C C C C
9cn=10 7c an
D C C
cn . cn=10 7c an/;
D C (cid:0) C C
whichisatmostcnif
cn=10 7c an 0: (9.2)
(cid:0) C C 
Inequality (9.2)isequivalenttotheinequality c 10a.n=.n 70//whenn >70.
 (cid:0)
Because we assume that n 140, we have n=.n 70/ 2, and so choos-
 (cid:0) 
ing c 20a willsatisfy inequality (9.2). (Notethat there isnothing special about

the constant 140; we could replace it by any integer strictly greater than 70 and
then choose c accordingly.) The worst-case running time of SELECT is therefore
linear.
Asinacomparisonsort(seeSection8.1), SELECT and RANDOMIZED-SELECT
determineinformation abouttherelativeorderofelementsonlybycomparingele-
ments. Recall from Chapter 8that sorting requires .nlgn/time in the compari-
sonmodel, evenonaverage (seeProblem 8-1). Thelinear-time sorting algorithms
in Chapter 8 make assumptions about the input. In contrast, the linear-time se-
lection algorithms in this chapter do not require any assumptions about the input.
They are not subject to the .nlgn/ lower bound because they manage to solve
theselectionproblemwithoutsorting. Thus,solvingtheselectionproblembysort-
ingandindexing, aspresented intheintroduction tothischapter, isasymptotically
inefficient.
9.3 Selectioninworst-caselineartime 223
Exercises
9.3-1
In the algorithm SELECT, the input elements are divided into groups of 5. Will
the algorithm work in linear time if they are divided into groups of 7? Argue that
SELECT doesnotruninlineartimeifgroupsof3areused.
9.3-2
Analyze SELECT toshowthatifn 140, thenatleast n=4 elementsaregreater
 d e
thanthemedian-of-medians x andatleast n=4 elementsarelessthanx.
d e
9.3-3
Show how quicksort can be made to run in O.nlgn/ time in the worst case, as-
sumingthatallelementsaredistinct.
9.3-4 ?
Suppose that an algorithm uses only comparisons to find the ith smallest element
in a set of n elements. Show that it can also find the i 1 smaller elements and
(cid:0)
then i larger elementswithoutperforming anyadditional comparisons.
(cid:0)
9.3-5
Suppose that you have a “black-box” worst-case linear-time median subroutine.
Givea simple, linear-time algorithm that solves the selection problem foran arbi-
traryorderstatistic.
9.3-6
Thekthquantiles ofann-element set arethe k 1order statistics that divide the
(cid:0)
sorted set into k equal-sized sets (to within 1). Give an O.nlgk/-time algorithm
tolistthekthquantiles ofaset.
9.3-7
Describe an O.n/-time algorithm that, given a set S of n distinct numbers and
a positive integer k n, determines the k numbers in S that are closest to the

medianofS.
9.3-8
Let XŒ1::n and YŒ1::n be two arrays, each containing n numbers already in
sortedorder. GiveanO.lgn/-timealgorithmtofindthemedianofall2nelements
inarraysX andY.
9.3-9
ProfessorOlayisconsultingforanoilcompany,whichisplanningalargepipeline
runningeasttowestthroughanoilfieldofnwells. Thecompanywantstoconnect
224 Chapter9 MediansandOrderStatistics
Figure9.2 ProfessorOlayneedstodeterminethepositionoftheeast-westoilpipelinethatmini-
mizesthetotallengthofthenorth-southspurs.
a spur pipeline from each well directly to the main pipeline along ashortest route
(either north or south), asshowninFigure 9.2. Giventhe x-andy-coordinates of
thewells,howshouldtheprofessor picktheoptimallocation ofthemainpipeline,
whichwouldbetheonethatminimizesthetotallengthofthespurs? Showhowto
determine theoptimallocation inlineartime.
Problems
9-1 Largesti numbersinsortedorder
Given a set of n numbers, we wish to find the i largest in sorted order using a
comparison-based algorithm. Find the algorithm that implements each of the fol-
lowingmethodswiththebestasymptoticworst-caserunningtime,andanalyzethe
running timesofthealgorithmsintermsofnandi.
a. Sortthenumbers,andlistthei largest.
b. Buildamax-priorityqueuefromthenumbers,andcallEXTRACT-MAXi times.
c. Useanorder-statistic algorithm tofindtheithlargest number, partition around
thatnumber,andsortthei largest numbers.
ProblemsforChapter9 225
9-2 Weightedmedian
For n distinct elements x ;x ;:::;x with positive weights w ;w ;:::;w such
1 2 n 1 2 n
that n w 1,theweighted(lower)medianistheelementx satisfying
i 1 i D k
D
1
Pw <
i
2
x Xi<xk
and
1
w :
i
 2
x Xi>xk
For example, if the elements are 0:1;0:35;0:05;0:1;0:15;0:05;0:2 and each ele-
mentequalsitsweight(thatis,w x fori 1;2;:::;7),thenthemedianis0:1,
i i
D D
buttheweightedmedianis0:2.
a. Argue that the median of x ;x ;:::;x is the weighted median of the x with
1 2 n i
weightsw 1=nfori 1;2;:::;n.
i
D D
b. Show how to compute the weighted median of n elements in O.nlgn/ worst-
casetimeusingsorting.
c. Show how to compute the weighted median in ‚.n/ worst-case time using a
linear-time medianalgorithm suchasSELECT fromSection9.3.
The post-office location problem is defined as follows. We are given n points
p ;p ;:::;p withassociated weights w ;w ;:::;w . Wewishtofindapoint p
1 2 n 1 2 n
(notnecessarilyoneoftheinputpoints)thatminimizesthesum n w d.p;p /,
i 1 i i
whered.a;b/isthedistancebetweenpointsaandb. D
P
d. Argue that the weighted median is a best solution for the 1-dimensional post-
office location problem, in which points are simply real numbers and the dis-
tancebetweenpointsa andb isd.a;b/ a b .
D j (cid:0) j
e. Find the best solution for the 2-dimensional post-office location problem, in
which the points are .x;y/ coordinate pairs and the distance between points
a .x ;y / and b .x ;y / is the Manhattan distance given by d.a;b/
1 1 2 2
D D D
x x y y .
1 2 1 2
j (cid:0) jCj (cid:0) j
9-3 Smallorderstatistics
We showed that the worst-case number T.n/ of comparisons used by SELECT
to select the ith order statistic from n numbers satisfies T.n/ ‚.n/, but the
D
constanthiddenbythe‚-notationisratherlarge. Wheni issmallrelativeton,we
can implement a different procedure that uses SELECT as a subroutine but makes
fewercomparisons intheworstcase.
226 Chapter9 MediansandOrderStatistics
a. DescribeanalgorithmthatusesU .n/comparisonstofindtheithsmallestofn
i
elements,where
T.n/ ifi n=2;
U .n/ 
i
D
(
n=2 U i. n=2 / T.2i/ otherwise:
b cC d e C
(Hint: Begin with n=2 disjoint pairwise comparisons, and recurse on the set
b c
containing thesmallerelementfromeachpair.)
b. Showthat,ifi < n=2,thenU .n/ n O.T.2i/lg.n=i//.
i
D C
c. Showthatifi isaconstant lessthann=2,thenU .n/ n O.lgn/.
i
D C
d. Showthatifi n=k fork 2,thenU .n/ n O.T.2n=k/lgk/.
i
D  D C
9-4 Alternativeanalysisofrandomizedselection
Inthisproblem, weuseindicator random variables toanalyze the RANDOMIZED-
SELECTprocedureinamannerakintoouranalysisofRANDOMIZED-QUICKSORT
inSection7.4.2.
As in the quicksort analysis, we assume that all elements are distinct, and we
rename the elements of the input array A as ´ ;´ ;:::;´ , where ´ is the ith
1 2 n i
smallestelement. Thus,thecall RANDOMIZED-SELECT.A;1;n;k/returns ´ k.
For1 i <j n,let
 
X I ´ iscomparedwith´ sometimeduring theexecution ofthealgorithm
ijk i j
D f
tofind´ :
k
g
a. Giveanexact expression forEŒX . (Hint: Yourexpression mayhave differ-
ijk
entvalues,depending onthevaluesofi,j,andk.)
b. Let X denote the total number of comparisons between elements of array A
k
whenfinding´ . Showthat
k
k n n k 2
1 j k 1 (cid:0) k i 1
EŒX  2 (cid:0) (cid:0) (cid:0) (cid:0) :
k
 j i 1 C j k 1 C k i 1
!
i 1j k (cid:0) C j k 1 (cid:0) C i 1 (cid:0) C
XD XD DXC XD
c. ShowthatEŒX  4n.
k

d. Conclude that, assuming all elements of array A are distinct, RANDOMIZED-
SELECT runsinexpectedtimeO.n/.
NotesforChapter9 227
Chapter notes
Theworst-caselinear-timemedian-findingalgorithmwasdevisedbyBlum,Floyd,
Pratt, Rivest, and Tarjan [50]. Thefast randomized version is due to Hoare [169].
FloydandRivest[108]havedevelopedanimprovedrandomizedversionthatparti-
tionsaroundanelementrecursively selectedfromasmallsampleoftheelements.
It is still unknown exactly how many comparisons are needed to determine the
median. Bent and John [41] gave a lower bound of 2n comparisons for median
finding,andScho¨nhage,Paterson,andPippenger[302]gaveanupperboundof3n.
Dor and Zwick have improved on both of these bounds. Their upper bound [93]
is slightly less than 2:95n, and their lower bound [94] is .2 /n, for a small
C
positive constant , thereby improving slightly on related work by Dor et al. [92].
Paterson[272]describes someoftheseresults alongwithotherrelatedwork.
III Data Structures
Introduction
Setsareasfundamental tocomputer science asthey aretomathematics. Whereas
mathematical sets are unchanging, the sets manipulated by algorithms can grow,
shrink, or otherwise change over time. We call such sets dynamic. The next five
chapters present some basic techniques for representing finite dynamic sets and
manipulating themonacomputer.
Algorithmsmayrequireseveraldifferenttypesofoperationstobeperformedon
sets. For example, many algorithms need only the ability to insert elements into,
delete elements from, and test membership in a set. We call a dynamic set that
supportstheseoperationsadictionary. Otheralgorithmsrequiremorecomplicated
operations. For example, min-priority queues, which Chapter 6 introduced in the
context of the heap data structure, support the operations of inserting an element
into and extracting the smallest element from a set. The best way to implement a
dynamicsetdepends upontheoperations thatmustbesupported.
Elementsofadynamicset
In a typical implementation of a dynamic set, each element is represented by an
object whose attributes can be examined and manipulated if we have a pointer to
the object. (Section 10.3 discusses the implementation of objects and pointers in
programming environments that do not contain them as basic data types.) Some
kinds of dynamic sets assume that one of the object’s attributes is an identifying
key. If the keys are all different, we can think of the dynamic set as being a set
of key values. The object may contain satellite data, which are carried around in
other object attributes butareotherwise unused bythesetimplementation. Itmay
230 PartIII DataStructures
alsohaveattributesthataremanipulatedbythesetoperations; theseattributesmay
contain dataorpointers tootherobjectsintheset.
Some dynamic sets presuppose that the keys are drawn from a totally ordered
set, such as the real numbers, or the set of all words under the usual alphabetic
ordering. Atotalordering allowsustodefinethe minimumelement oftheset, for
example,ortospeakofthenextelementlargerthanagivenelementinaset.
Operationsondynamicsets
Operations on a dynamic set can be grouped into two categories: queries, which
simply return information about the set, and modifying operations, which change
the set. Here is a list of typical operations. Any specific application will usually
requireonlyafewofthesetobeimplemented.
SEARCH.S;k/
Aquerythat,givenasetS andakeyvaluek,returnsapointerx toanelement
inS suchthatx:key k,or NIL ifnosuchelementbelongs toS.
D
INSERT.S;x/
A modifying operation that augments the set S with the element pointed to
by x. We usually assume that any attributes in element x needed by the set
implementation havealready beeninitialized.
DELETE.S;x/
A modifying operation that, given a pointer x to an element in the set S, re-
movesx from S. (Notethat this operation takes apointer toanelement x,not
akeyvalue.)
MINIMUM.S/
A query on a totally ordered set S that returns a pointer to the element of S
withthesmallestkey.
MAXIMUM.S/
A query on a totally ordered set S that returns a pointer to the element of S
withthelargestkey.
SUCCESSOR.S;x/
A query that, given an element x whose key is from a totally ordered set S,
returns a pointer to the next larger element in S, or NIL if x is the maximum
element.
PREDECESSOR.S;x/
A query that, given an element x whose key is from a totally ordered set S,
returns a pointer to the next smaller element in S, or NIL if x is the minimum
element.
PartIII DataStructures 231
In some situations, we can extend the queries SUCCESSOR and PREDECESSOR
so that they apply to sets with nondistinct keys. For a set on n keys, the normal
presumption is that a call to MINIMUM followed by n 1 calls to SUCCESSOR
(cid:0)
enumeratestheelementsinthesetinsortedorder.
Weusuallymeasurethetimetakentoexecuteasetoperationintermsofthesize
oftheset. Forexample,Chapter13describes adatastructure thatcansupport any
oftheoperations listedaboveonasetofsizenintimeO.lgn/.
OverviewofPartIII
Chapters 10–14 describe several data structures that we can use to implement
dynamic sets; we shall use many of these later to construct efficient algorithms
for a variety of problems. We already saw another important data structure—the
heap—inChapter6.
Chapter 10 presents the essentials of working with simple data structures such
as stacks, queues, linked lists, and rooted trees. It also shows how to implement
objects and pointers in programming environments that do not support them as
primitives. If you have taken an introductory programming course, then much of
thismaterialshould befamiliartoyou.
Chapter 11 introduces hash tables, which support the dictionary operations IN-
SERT,DELETE,andSEARCH. Intheworstcase,hashingrequires‚.n/timetoper-
formaSEARCHoperation,buttheexpectedtimeforhash-tableoperationsisO.1/.
The analysis of hashing relies on probability, but most of the chapter requires no
background inthesubject.
Binary search trees, which are covered in Chapter 12, support all the dynamic-
setoperations listedabove. Intheworstcase,eachoperation takes‚.n/timeona
treewithnelements, butonarandomlybuiltbinarysearchtree,theexpected time
foreachoperationisO.lgn/. Binarysearchtreesserveasthebasisformanyother
datastructures.
Chapter13introducesred-blacktrees,whichareavariantofbinarysearchtrees.
Unlikeordinarybinarysearchtrees,red-blacktreesareguaranteedtoperformwell:
operationstakeO.lgn/timeintheworstcase. Ared-blacktreeisabalancedsearch
tree; Chapter 18 in Part V presents another kind of balanced search tree, called a
B-tree. Althoughthemechanicsofred-black treesaresomewhatintricate, youcan
glean most oftheir properties from the chapter without studying the mechanics in
detail. Nevertheless, you probably will find walking through the code to be quite
instructive.
In Chapter 14, we show how to augment red-black trees to support operations
other than the basic ones listed above. First, we augment them so that we can
dynamically maintain order statistics foraset ofkeys. Then, weaugment them in
adifferentwaytomaintainintervalsofrealnumbers.
10 Elementary Data Structures
Inthischapter,weexaminetherepresentationofdynamicsetsbysimpledatastruc-
tures that use pointers. Although we can construct many complex data structures
using pointers, wepresent only the rudimentary ones: stacks, queues, linked lists,
and rooted trees. We also show ways to synthesize objects and pointers from ar-
rays.
10.1 Stacks andqueues
Stacks and queues are dynamic sets in which the element removed from the set
by the DELETE operation is prespecified. In a stack, the element deleted from
the set is the one most recently inserted: the stack implements a last-in, first-out,
or LIFO,policy. Similarly, in aqueue, the element deleted isalways the one that
has been in the set for the longest time: the queue implements afirst-in, first-out,
orFIFO,policy. Thereareseveral efficient waystoimplement stacks and queues
on a computer. In this section we show how to use a simple array to implement
each.
Stacks
The INSERT operation on a stack is often called PUSH, and the DELETE opera-
tion, which does not take an element argument, is often called POP. These names
are allusions to physical stacks, such as the spring-loaded stacks of plates used
in cafeterias. The order in which plates are popped from the stack is the reverse
of the order in which they were pushed onto the stack, since only the top plate is
accessible.
As Figure 10.1 shows, we can implement a stack of at most n elements with
an array SŒ1::n. The array has an attribute S:top that indexes the most recently
10.1 Stacksandqueues 233
1 2 3 4 5 6 7 1 2 3 4 5 6 7 1 2 3 4 5 6 7
S 15 6 2 9 S 15 6 2 9 17 3 S 15 6 2 9 17 3
S:top 4 S:top 6 S:top 5
D D D
(a) (b) (c)
Figure10.1 AnarrayimplementationofastackS.Stackelementsappearonlyinthelightlyshaded
positions.(a)StackS has4elements.Thetopelementis9.(b)StackS afterthecallsPUSH.S;17/
andPUSH.S;3/.(c)StackS afterthecallPOP.S/hasreturnedtheelement3,whichistheonemost
recentlypushed. Althoughelement3stillappearsinthearray,itisnolongerinthestack;thetopis
element17.
inserted element. The stack consists of elements SŒ1::S:top, where SŒ1 is the
elementatthebottom ofthestackandSŒS:topistheelementatthetop.
When S:top 0, the stack contains no elements and is empty. We can test to
D
see whether the stack is empty by query operation STACK-EMPTY. Ifwe attempt
to pop an empty stack, we say the stack underflows, which is normally an error.
If S:top exceeds n, the stack overflows. (In our pseudocode implementation, we
don’tworryaboutstackoverflow.)
Wecanimplementeachofthestackoperations withjustafewlinesofcode:
STACK-EMPTY.S/
1 ifS:top ==0
2 return TRUE
3 elsereturn FALSE
PUSH.S;x/
1 S:top S:top 1
D C
2 SŒS:top x
D
POP.S/
1 ifSTACK-EMPTY.S/
2 error“underflow”
3 elseS:top S:top 1
D (cid:0)
4 returnSŒS:top 1
C
Figure10.1showstheeffectsofthemodifyingoperations PUSH andPOP. Eachof
thethreestackoperations takesO.1/time.
234 Chapter10 ElementaryDataStructures
1 2 3 4 5 6 7 8 9 10 11 12
(a) Q 15 6 9 8 4
Q:head 7 Q:tail 12
D D
1 2 3 4 5 6 7 8 9 10 11 12
(b) Q 3 5 15 6 9 8 4 17
Q:tail 3 Q:head 7
D D
1 2 3 4 5 6 7 8 9 10 11 12
(c) Q 3 5 15 6 9 8 4 17
Q:tail 3 Q:head 8
D D
Figure10.2 AqueueimplementedusinganarrayQŒ1::12. Queueelementsappearonlyinthe
lightlyshadedpositions.(a)Thequeuehas5elements,inlocationsQŒ7::11.(b)Theconfiguration
of the queue after the calls ENQUEUE.Q;17/, ENQUEUE.Q;3/, and ENQUEUE.Q;5/. (c) The
configuration of the queue after the call DEQUEUE.Q/ returns the key value 15 formerly at the
headofthequeue.Thenewheadhaskey6.
Queues
We call the INSERT operation on a queue ENQUEUE, and we call the DELETE
operation DEQUEUE;likethestackoperation POP,DEQUEUEtakesnoelementar-
gument. TheFIFOpropertyofaqueuecausesittooperatelikealineofcustomers
waiting topayacashier. Thequeuehasahead andatail. Whenanelementisen-
queued, ittakes itsplace atthetailofthequeue, just asanewlyarriving customer
takes a place at the end of the line. The element dequeued is always the one at
theheadofthequeue, likethecustomer attheheadofthelinewhohaswaitedthe
longest.
Figure 10.2 shows one way to implement a queue of at most n 1 elements
(cid:0)
usinganarrayQŒ1::n. ThequeuehasanattributeQ:head thatindexes,orpoints
to, its head. The attribute Q:tail indexes the next location at which anewlyarriv-
ing element will be inserted into the queue. The elements in the queue reside in
locations Q:head;Q:head 1;:::;Q:tail 1, where we “wrap around” in the
C (cid:0)
sense that location 1 immediately follows location n in a circular order. When
Q:head Q:tail, the queue is empty. Initially, we have Q:head Q:tail 1.
D D D
Ifweattempt to dequeue anelement from an emptyqueue, the queue underflows.
10.1 Stacksandqueues 235
When Q:head Q:tail 1 or both Q:head 1 and Q:tail Q:length, the
D C D D
queueisfull,andifweattempttoenqueueanelement,thenthequeueoverflows.
InourproceduresENQUEUEandDEQUEUE,wehaveomittedtheerrorchecking
for underflow and overflow. (Exercise 10.1-4 asks you to supply code that checks
forthesetwoerrorconditions.) Thepseudocode assumesthatn Q:length.
D
ENQUEUE.Q;x/
1 QŒQ:tail x
D
2 ifQ:tail ==Q:length
3 Q:tail 1
D
4 elseQ:tail Q:tail 1
D C
DEQUEUE.Q/
1 x QŒQ:head
D
2 ifQ:head == Q:length
3 Q:head 1
D
4 elseQ:head Q:head 1
D C
5 returnx
Figure 10.2 shows the effects of the ENQUEUE and DEQUEUE operations. Each
operation takesO.1/time.
Exercises
10.1-1
UsingFigure10.1asamodel,illustratetheresultofeachoperationinthesequence
PUSH.S;4/, PUSH.S;1/, PUSH.S;3/, POP.S/, PUSH.S;8/, and POP.S/ on an
initiallyemptystackS storedinarraySŒ1::6.
10.1-2
Explain how to implement two stacks in one array AŒ1::n in such a way that
neitherstackoverflowsunlessthetotalnumberofelementsinbothstackstogether
isn. ThePUSH and POPoperations shouldruninO.1/time.
10.1-3
Using Figure 10.2 as a model, illustrate the result of each operation in the
sequence ENQUEUE.Q;4/, ENQUEUE.Q;1/, ENQUEUE.Q;3/, DEQUEUE.Q/,
ENQUEUE.Q;8/, and DEQUEUE.Q/ on an initially empty queue Q stored in
arrayQŒ1::6.
10.1-4
RewriteENQUEUE and DEQUEUE todetectunderflowandoverflowofaqueue.
236 Chapter10 ElementaryDataStructures
10.1-5
Whereas a stack allows insertion and deletion of elements at only one end, and a
queue allows insertion at one end and deletion at the other end, a deque (double-
ended queue) allows insertion and deletion at both ends. Write four O.1/-time
procedures to insert elements into and delete elements from both ends of a deque
implemented byanarray.
10.1-6
Showhowtoimplementaqueueusingtwostacks. Analyzetherunningtimeofthe
queueoperations.
10.1-7
Showhowtoimplementastackusingtwoqueues. Analyzetherunningtimeofthe
stackoperations.
10.2 Linked lists
A linked list isadata structure inwhich theobjects arearranged inalinear order.
Unlike an array, however, in which the linear order is determined by the array
indices, theorderinalinked listisdetermined byapointer ineachobject. Linked
listsprovideasimple,flexiblerepresentation fordynamicsets,supporting (though
notnecessarily efficiently)alltheoperations listedonpage230.
AsshowninFigure10.3,eachelementofadoublylinkedlistLisanobjectwith
an attribute key and two other pointer attributes: next and pre. The object may
alsocontainothersatellite data. Givenanelementx inthelist,x:nextpointstoits
successor inthelinked list,andx:pre pointstoitspredecessor. Ifx:pre NIL,
D
the element x has no predecessor and is therefore the first element, or head, of
the list. If x:next NIL, the element x has no successor and is therefore the last
D
element, or tail, of the list. An attribute L:head points to the first element of the
list. IfL:head NIL,thelistisempty.
D
A list may have one of several forms. It may be either singly linked or doubly
linked, it may be sorted or not, and it may be circular or not. If a list is singly
linked,weomitthepre pointerineachelement. Ifalistissorted,thelinearorder
ofthelistcorresponds tothelinear orderofkeysstored inelements ofthelist; the
minimum element is then the head of the list, and the maximum element is the
tail. Ifthelistisunsorted, theelements canappear inanyorder. Inacircular list,
the pre pointer of the head of the list points to the tail, and the next pointer of
the tail of the list points to the head. We can think of a circular list as a ring of
10.2 Linkedlists 237
prev key next
(a) L:head 9 16 4 1
(b) L:head 25 9 16 4 1
(c) L:head 25 9 16 1
Figure10.3 (a)AdoublylinkedlistLrepresentingthedynamicset 1;4;9;16 . Eachelementin
f g
thelistisanobjectwithattributesforthekeyandpointers(shownbyarrows)tothenextandprevious
objects.ThenextattributeofthetailandthepreattributeoftheheadareNIL,indicatedbyadiagonal
slash. TheattributeL:headpointstothehead. (b)FollowingtheexecutionofLIST-INSERT.L;x/,
wherex:key 25, thelinkedlisthasanewobjectwithkey25asthenewhead. Thisnewobject
D
pointstotheoldheadwithkey9.(c)TheresultofthesubsequentcallLIST-DELETE.L;x/,wherex
pointstotheobjectwithkey4.
elements. Intheremainder ofthissection, weassumethatthelists withwhich we
areworkingareunsorted anddoublylinked.
Searchingalinkedlist
The procedure LIST-SEARCH.L;k/ finds the first element with key k in list L
by a simple linear search, returning a pointer to this element. If no object with
key k appears in the list, then the procedure returns NIL. For the linked list in
Figure10.3(a), thecall LIST-SEARCH.L;4/returns apointer tothethirdelement,
andthecall LIST-SEARCH.L;7/returns NIL.
LIST-SEARCH.L;k/
1 x L:head
D
2 whilex NIL andx:key k
¤ ¤
3 x x:next
D
4 returnx
To search a list of n objects, the LIST-SEARCH procedure takes ‚.n/ time in the
worstcase,sinceitmayhavetosearchtheentirelist.
Insertingintoalinkedlist
Given an element x whose key attribute has already been set, the LIST-INSERT
procedure “splices” x ontothefrontofthelinkedlist,asshowninFigure10.3(b).
238 Chapter10 ElementaryDataStructures
LIST-INSERT.L;x/
1 x:next L:head
D
2 ifL:head NIL
¤
3 L:head:pre x
D
4 L:head x
D
5 x:pre NIL
D
(Recall that our attribute notation can cascade, so that L:head:pre denotes the
pre attribute of the object that L:head points to.) The running time for LIST-
INSERT onalistofnelementsisO.1/.
Deletingfromalinkedlist
The procedure LIST-DELETE removes an element x from a linked list L. It must
begivenapointer tox,anditthen“splices” x outofthelistbyupdating pointers.
Ifwewishtodelete anelement withagivenkey,wemustfirstcall LIST-SEARCH
toretrieveapointertotheelement.
LIST-DELETE.L;x/
1 ifx:pre NIL
¤
2 x:pre:next x:next
D
3 elseL:head x:next
D
4 ifx:next NIL
¤
5 x:next:pre x:pre
D
Figure 10.3(c) shows how an element is deleted from alinked list. LIST-DELETE
runsinO.1/time,butifwewishtodeleteanelementwithagivenkey,‚.n/time
is required in the worst case because we must first call LIST-SEARCH to find the
element.
Sentinels
The code for LIST-DELETE would be simpler if we could ignore the boundary
conditions attheheadandtailofthelist:
LIST-DELETE0.L;x/
1 x:pre:next x:next
D
2 x:next:pre x:pre
D
A sentinel isadummy object that allows us tosimplify boundary conditions. For
example, suppose that we provide with list L an object L:nil that represents NIL
10.2 Linkedlists 239
(a) L:nil
(b) L:nil 9 16 4 1
(c) L:nil 25 9 16 4 1
(d) L:nil 25 9 16 4
Figure10.4 Acircular,doublylinkedlistwithasentinel. ThesentinelL:nilappearsbetweenthe
head and tail. TheattributeL:head isno longer needed, sincewe can access thehead of thelist
byL:nil:next.(a)Anemptylist.(b)ThelinkedlistfromFigure10.3(a),withkey9attheheadand
key1atthetail.(c)ThelistafterexecutingLIST-INSERT0.L;x/,wherex:key D25.Thenewobject
becomesthehead ofthelist. (d)Thelistafterdeletingtheobject withkey1. Thenew tailisthe
objectwithkey4.
but has all the attributes of the other objects in the list. Wherever we have a ref-
erence to NIL in list code, we replace it by a reference to the sentinel L:nil. As
shown in Figure 10.4, this change turns a regular doubly linked list into a circu-
lar,doublylinkedlistwithasentinel,inwhichthesentinel L:nilliesbetweenthe
headandtail. TheattributeL:nil:nextpointstotheheadofthelist,andL:nil:pre
points to the tail. Similarly, both the next attribute of the tail and the pre at-
tribute of the head point to L:nil. Since L:nil:next points to the head, we can
eliminate the attribute L:head altogether, replacing references to it by references
toL:nil:next. Figure 10.4(a) showsthat anemptylistconsists ofjustthesentinel,
andbothL:nil:next andL:nil:pre pointtoL:nil.
Thecodefor LIST-SEARCH remainsthesameasbefore,butwiththereferences
toNIL andL:head changed asspecifiedabove:
LIST-SEARCH0.L;k/
1 x L:nil:next
D
2 whilex L:nilandx:key k
¤ ¤
3 x x:next
D
4 returnx
We use the two-line procedure LIST-DELETE0 from before to delete an element
fromthelist. Thefollowingprocedure insertsanelementintothelist:
240 Chapter10 ElementaryDataStructures
LIST-INSERT0.L;x/
1 x:next L:nil:next
D
2 L:nil:next:pre x
D
3 L:nil:next x
D
4 x:pre L:nil
D
Figure10.4showstheeffectsofLIST-INSERT0andLIST-DELETE0onasamplelist.
Sentinelsrarelyreduce theasymptotic timebounds ofdatastructure operations,
but they can reduce constant factors. The gain from using sentinels within loops
is usually a matter of clarity of code rather than speed; the linked list code, for
example, becomes simpler when we use sentinels, but we save only O.1/ time in
theLIST-INSERT0andLIST-DELETE0procedures. Inothersituations,however,the
useofsentinelshelpstotightenthecodeinaloop,thusreducingthecoefficientof,
say,norn2 intherunning time.
Weshould use sentinels judiciously. When there are manysmall lists, the extra
storage used by their sentinels can represent significant wasted memory. In this
book, weusesentinels onlywhentheytrulysimplifythecode.
Exercises
10.2-1
Can you implement the dynamic-set operation INSERT on a singly linked list
inO.1/time? Howabout DELETE?
10.2-2
Implement a stack using a singly linked list L. The operations PUSH and POP
shouldstilltakeO.1/time.
10.2-3
Implement a queue by a singly linked list L. The operations ENQUEUE and DE-
QUEUE shouldstilltakeO.1/time.
10.2-4
Aswritten, eachloop iteration inthe LIST-SEARCH0 procedure requires twotests:
one for x L:nil and one for x:key k. Show how to eliminate the test for
¤ ¤
x L:nilineachiteration.
¤
10.2-5
Implement the dictionary operations INSERT, DELETE, and SEARCH using singly
linked, circularlists. Whataretherunning timesofyourprocedures?
10.3 Implementingpointersandobjects 241
10.2-6
Thedynamic-set operation UNION takes twodisjoint sets S
1
and S
2
as input, and
it returns a set S S S consisting of all the elements of S and S . The
1 2 1 2
D [
setsS 1andS 2areusuallydestroyedbytheoperation. ShowhowtosupportUNION
inO.1/timeusingasuitable listdatastructure.
10.2-7
Give a ‚.n/-time nonrecursive procedure that reverses a singly linked list of n
elements. The procedure should use no more than constant storage beyond that
neededforthelistitself.
10.2-8 ?
Explainhowtoimplementdoublylinkedlistsusingonlyonepointervaluex:npper
iteminsteadoftheusualtwo(nextandpre). Assumethatallpointervaluescanbe
interpreted as k-bit integers, and define x:np to be x:np x:next XORx:pre,
D
thek-bit“exclusive-or”ofx:nextandx:pre. (ThevalueNILisrepresentedby0.)
Besuretodescribewhatinformationyouneedtoaccesstheheadofthelist. Show
how to implement the SEARCH, INSERT, and DELETE operations on such a list.
AlsoshowhowtoreversesuchalistinO.1/time.
10.3 Implementing pointers andobjects
Howdoweimplementpointersandobjectsinlanguagesthatdonotprovidethem?
Inthissection,weshallseetwowaysofimplementinglinkeddatastructures with-
out an explicit pointer data type. We shall synthesize objects and pointers from
arraysandarrayindices.
Amultiple-arrayrepresentation ofobjects
We can represent a collection of objects that have the same attributes by using an
arrayforeachattribute. Asanexample,Figure10.5showshowwecanimplement
the linked list of Figure 10.3(a) with three arrays. The array key holds the values
of the keys currently in the dynamic set, and the pointers reside in the arrays next
and pre. Fora given array index x, the array entries keyŒx, nextŒx, and preŒx
representanobjectinthelinkedlist. Underthisinterpretation,apointerxissimply
acommonindexintothekey,next,andpre arrays.
In Figure 10.3(a), the object with key 4 follows the object with key 16 in the
linked list. In Figure 10.5, key 4 appears in keyŒ2, and key 16 appears in keyŒ5,
andsonextŒ5 2andpreŒ2 5. Althoughtheconstant NILappearsinthenext
D D
242 Chapter10 ElementaryDataStructures
1 2 3 4 5 6 7 8
L 7
next 3 2 5
key 4 1 16 9
prev 5 2 7
Figure10.5 ThelinkedlistofFigure10.3(a)representedbythearrayskey,next,andpre. Each
verticalsliceofthearraysrepresentsasingleobject. Storedpointerscorrespondtothearrayindices
shownatthetop;thearrowsshowhowtointerpretthem.Lightlyshadedobjectpositionscontainlist
elements.ThevariableLkeepstheindexofthehead.
attribute of the tail and the pre attribute of the head, we usually use an integer
(such as0or 1)thatcannot possibly represent anactual index into thearrays. A
(cid:0)
variable Lholdstheindexoftheheadofthelist.
Asingle-array representation ofobjects
The words in a computer memory are typically addressed by integers from 0
to M 1, where M is a suitably large integer. In many programming languages,
(cid:0)
anobjectoccupiesacontiguoussetoflocationsinthecomputermemory. Apointer
issimplytheaddressofthefirstmemorylocationoftheobject,andwecanaddress
othermemorylocations withintheobjectbyaddinganoffsettothepointer.
We can use the same strategy for implementing objects in programming envi-
ronments thatdonotprovideexplicit pointer datatypes. Forexample,Figure10.6
shows how to use a single array A to store the linked list from Figures 10.3(a)
and 10.5. An object occupies a contiguous subarray AŒj ::k. Each attribute of
the object corresponds to an offset in the range from 0 to k j, and a pointer to
(cid:0)
theobjectistheindexj. InFigure10.6,theoffsetscorresponding tokey,next,and
pre are0,1,and2,respectively. Toreadthevalueofi:pre,givenapointeri,we
addthevaluei ofthepointertotheoffset2,thusreadingAŒi 2.
C
The single-array representation is flexible in that it permits objects of different
lengths tobestored inthesamearray. Theproblem ofmanaging suchaheteroge-
neouscollectionofobjectsismoredifficultthantheproblemofmanagingahomo-
geneous collection, where all objects have the same attributes. Since most of the
data structures we shall consider are composed of homogeneous elements, it will
besufficientforourpurposes tousethemultiple-array representation ofobjects.
10.3 Implementingpointersandobjects 243
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
L 19
A 4 7 13 1 4 16 4 19 9 13
key prev
next
Figure10.6 ThelinkedlistofFigures10.3(a)and10.5representedinasinglearrayA. Eachlist
element is an object that occupies a contiguous subarray of length 3 within the array. The three
attributeskey,next,andpre correspondtotheoffsets0,1,and2,respectively,withineachobject.
Apointertoanobjectistheindexofthefirstelementoftheobject.Objectscontaininglistelements
arelightlyshaded,andarrowsshowthelistordering.
Allocatingandfreeingobjects
Toinsert akeyinto adynamic set represented byadoubly linked list, wemustal-
locateapointertoacurrentlyunusedobjectinthelinked-list representation. Thus,
it is useful to manage the storage of objects not currently used in the linked-list
representation so that one can be allocated. In some systems, a garbage collec-
tor is responsible for determining which objects are unused. Many applications,
however, are simple enough that they can bear responsibility for returning an un-
used object toa storage manager. Weshall now explore the problem of allocating
and freeing (or deallocating) homogeneous objects using the example of adoubly
linkedlistrepresented bymultiplearrays.
Suppose that the arrays in the multiple-array representation have length m and
that at some moment the dynamic set contains n m elements. Then n objects

representelementscurrentlyinthedynamicset,andtheremainingm nobjectsare
(cid:0)
free; the freeobjects areavailable torepresent elements inserted into thedynamic
setinthefuture.
Wekeep the free objects in asingly linked list, which wecall the free list. The
free list uses only the next array, which stores the next pointers within the list.
The head of the free list is held in the global variable free. When the dynamic
set represented by linked list L isnonempty, the free list may be intertwined with
listL,asshowninFigure10.7. Notethateachobjectintherepresentation iseither
inlistLorinthefreelist,butnotinboth.
Thefreelist actslike astack: thenext object allocated isthelastone freed. We
canusealistimplementation ofthestack operations PUSH and POP toimplement
theprocedures forallocating andfreeing objects, respectively. Weassumethatthe
global variable free used in the following procedures points tothe first element of
thefreelist.
244 Chapter10 ElementaryDataStructures
1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8
free 4 free 8
L 7 L 4
next 3 8 2 1 5 6 next 3 7 2 1 5 6
key 4 1 16 9 key 4 1 25 16 9
prev 5 2 7 prev 5 2 7 4
(a) (b)
1 2 3 4 5 6 7 8
free 5
L 4
next 3 7 8 1 2 6
key 4 1 25 9
prev 7 2 4
(c)
Figure 10.7 The effect of the ALLOCATE-OBJECTand FREE-OBJECTprocedures. (a) The list
ofFigure10.5(lightlyshaded)andafreelist(heavilyshaded). Arrowsshowthefree-liststructure.
(b) The result of calling ALLOCATE-OBJECT./ (which returns index 4), setting keyŒ4 to 25, and
calling LIST-INSERT.L;4/. Thenewfree-listheadisobject8,whichhadbeennextŒ4onthefree
list. (c)AfterexecutingLIST-DELETE.L;5/,wecallFREE-OBJECT.5/. Object5becomesthenew
free-listhead,withobject8followingitonthefreelist.
ALLOCATE-OBJECT./
1 iffree == NIL
2 error“outofspace”
3 elsex free
D
4 free x:next
D
5 returnx
FREE-OBJECT.x/
1 x:next free
D
2 free x
D
Thefreelistinitially containsallnunallocated objects. Oncethefreelisthasbeen
exhausted, running the ALLOCATE-OBJECT procedure signals an error. We can
even service several linked lists withjust asingle free list. Figure 10.8 shows two
linkedlistsandafreelistintertwinedthrough key,next,andpre arrays.
The two procedures run in O.1/ time, which makes them quite practical. We
canmodifythemtoworkforanyhomogeneouscollectionofobjectsbylettingany
oneoftheattributes intheobjectactlikeanext attribute inthefreelist.
10.3 Implementingpointersandobjects 245
free 10 1 2 3 4 5 6 7 8 9 10
next 5 6 8 2 1 7 4
L 9
2 key k k k k k k k
1 2 3 5 6 7 9
L 3 prev 7 6 1 3 9
1
Figure10.8 Twolinkedlists,L1 (lightlyshaded) andL2 (heavilyshaded), andafreelist(dark-
ened)intertwined.
Exercises
10.3-1
Draw a picture of the sequence 13;4;8;19;5;11 stored as a doubly linked list
h i
usingthemultiple-array representation. Dothesameforthesingle-array represen-
tation.
10.3-2
Writetheprocedures ALLOCATE-OBJECT andFREE-OBJECT forahomogeneous
collection ofobjects implementedbythesingle-array representation.
10.3-3
Whydon’tweneedtosetorresetthepre attributes ofobjectsintheimplementa-
tionofthe ALLOCATE-OBJECT and FREE-OBJECT procedures?
10.3-4
Itisoftendesirabletokeepallelementsofadoublylinkedlistcompactinstorage,
using,forexample,thefirstmindexlocationsinthemultiple-array representation.
(This is the case in a paged, virtual-memory computing environment.) Explain
howtoimplementtheprocedures ALLOCATE-OBJECT and FREE-OBJECT sothat
therepresentation iscompact. Assumethattherearenopointerstoelementsofthe
linkedlistoutsidethelistitself. (Hint:Usethearrayimplementation ofastack.)
10.3-5
Let L be a doubly linked list of length n stored in arrays key, pre, and next of
length m. Suppose that these arrays are managed by ALLOCATE-OBJECT and
FREE-OBJECT procedures that keep a doubly linked free list F. Suppose further
that of the m items, exactly n are on list L and m n are on the free list. Write
(cid:0)
a procedure COMPACTIFY-LIST.L;F/ that, given the list L and the free list F,
movestheitemsinLsothattheyoccupyarraypositions1;2;:::;nandadjuststhe
freelistF sothatitremainscorrect,occupyingarraypositionsn 1;n 2;:::;m.
C C
The running time of your procedure should be ‚.n/, and it should use only a
constantamountofextraspace. Arguethatyourprocedure iscorrect.
246 Chapter10 ElementaryDataStructures
10.4 Representing rooted trees
The methods for representing lists given inthe previous section extend to any ho-
mogeneous data structure. In this section, we look specifically at the problem of
representing rooted trees by linked data structures. We first look at binary trees,
andthenwepresentamethodforrootedtreesinwhichnodescanhaveanarbitrary
numberofchildren.
We represent each node of a tree by an object. As with linked lists, weassume
that each node contains a key attribute. The remaining attributes of interest are
pointers toothernodes,andtheyvaryaccording tothetypeoftree.
Binarytrees
Figure 10.9 shows how we use the attributes p, left, and right to store pointers to
theparent,leftchild,andrightchildofeachnodeinabinarytreeT. Ifx:p NIL,
D
then x isthe root. Ifnode x hasnoleftchild, thenx:left NIL,and similarly for
D
therightchild. TherootoftheentiretreeT ispointedtobytheattributeT:root. If
T:root NIL,thenthetreeisempty.
D
Rootedtreeswithunboundedbranching
We can extend the scheme for representing a binary tree to any class of trees in
whichthenumberofchildren ofeachnodeisatmostsomeconstantk: wereplace
the left and right attributes by child ;child ;:::;child . This scheme no longer
1 2 k
workswhenthenumberofchildrenofanodeisunbounded, sincewedonotknow
howmanyattributes (arrays inthemultiple-array representation) toallocate inad-
vance. Moreover, evenifthe number of children k isbounded byalarge constant
butmostnodeshaveasmallnumberofchildren, wemaywastealotofmemory.
Fortunately,thereisacleverschemetorepresenttreeswitharbitrarynumbersof
children. IthastheadvantageofusingonlyO.n/spaceforanyn-noderootedtree.
The left-child, right-sibling representation appears in Figure 10.10. As before,
each node contains a parent pointer p, and T:root points to the root of tree T.
Instead of having a pointer to each of its children, however, each node x has only
twopointers:
1. x:left-child pointstotheleftmostchildofnodex,and
2. x:right-sibling pointstothesibling ofx immediatelytoitsright.
If node x has no children, then x:left-child NIL, and if node x is the rightmost
D
childofitsparent, thenx:right-sibling NIL.
D
10.4 Representingrootedtrees 247
T:root
Figure10.9 TherepresentationofabinarytreeT.Eachnodexhastheattributesx:p(top),x:left
(lowerleft),andx:right(lowerright).Thekeyattributesarenotshown.
T:root
Figure10.10 Theleft-child,right-siblingrepresentationofatreeT.Eachnodexhasattributesx:p
(top),x:left-child(lowerleft),andx:right-sibling(lowerright).Thekeyattributesarenotshown.
248 Chapter10 ElementaryDataStructures
Othertreerepresentations
We sometimes represent rooted trees in other ways. In Chapter 6, for example,
werepresented aheap, whichisbasedonacomplete binarytree,byasinglearray
plustheindexofthelastnodeintheheap. Thetreesthatappear inChapter 21are
traversed only toward the root, and so only the parent pointers are present; there
are no pointers to children. Many other schemes are possible. Which scheme is
bestdepends ontheapplication.
Exercises
10.4-1
Draw the binary tree rooted at index 6 that is represented by the following at-
tributes:
index key left right
1 12 7 3
2 15 8 NIL
3 4 10 NIL
4 10 5 9
5 2 NIL NIL
6 18 1 4
7 7 NIL NIL
8 14 6 2
9 21 NIL NIL
10 5 NIL NIL
10.4-2
Write an O.n/-time recursive procedure that, given an n-node binary tree, prints
outthekeyofeachnodeinthetree.
10.4-3
Write an O.n/-time nonrecursive procedure that, given an n-node binary tree,
printsoutthekeyofeachnodeinthetree. Useastackasanauxiliarydatastructure.
10.4-4
Write an O.n/-time procedure that prints all the keys of an arbitrary rooted tree
withnnodes, wherethetreeisstoredusingtheleft-child, right-sibling representa-
tion.
10.4-5 ?
Write an O.n/-time nonrecursive procedure that, given an n-node binary tree,
prints out the key of each node. Use no more than constant extra space outside
ProblemsforChapter10 249
of the tree itself and do not modify the tree, even temporarily, during the proce-
dure.
10.4-6 ?
The left-child, right-sibling representation of an arbitrary rooted tree uses three
pointers in each node: left-child, right-sibling, and parent. From any node, its
parent can be reached and identified in constant time and all its children can be
reached and identified in time linear in the number of children. Show how to use
only twopointers and one boolean value ineach node so that theparent ofanode
or all of its children can be reached and identified in time linear in the number of
children.
Problems
10-1 Comparisonsamonglists
For each of the four types of lists in the following table, what is the asymptotic
worst-caserunning timeforeachdynamic-set operation listed?
unsorted, sorted, unsorted, sorted,
singly singly doubly doubly
linked linked linked linked
SEARCH.L;k/
INSERT.L;x/
DELETE.L;x/
SUCCESSOR.L;x/
PREDECESSOR.L;x/
MINIMUM.L/
MAXIMUM.L/
250 Chapter10 ElementaryDataStructures
10-2 Mergeableheapsusinglinkedlists
Amergeableheapsupportsthefollowingoperations: MAKE-HEAP(whichcreates
an empty mergeable heap), INSERT, MINIMUM, EXTRACT-MIN, and UNION.1
Showhowtoimplementmergeableheapsusinglinkedlistsineachofthefollowing
cases. Try to make each operation as efficient as possible. Analyze the running
timeofeachoperationintermsofthesizeofthedynamicset(s)beingoperatedon.
a. Listsaresorted.
b. Listsareunsorted.
c. Listsareunsorted, anddynamicsetstobemergedaredisjoint.
10-3 Searchingasortedcompactlist
Exercise 10.3-4 asked how we might maintain an n-element list compactly in the
firstnpositions ofanarray. Weshall assume thatallkeys aredistinct andthat the
compactlistisalsosorted,thatis,keyŒi < keyŒnextŒiforalli 1;2;:::;nsuch
D
that nextŒi NIL. We will also assume that we have a variable L that contains
¤
the index of the first element on the list. Under these assumptions, you will show
that we can use the following randomized algorithm to search the list in O.pn/
expected time.
COMPACT-LIST-SEARCH.L;n;k/
1 i L
D
2 whilei NIL andkeyŒi < k
¤
3 j RANDOM.1;n/
D
4 ifkeyŒi < keyŒjandkeyŒj k

5 i j
D
6 ifkeyŒi ==k
7 returni
8 i nextŒi
D
9 ifi == NIL orkeyŒi> k
10 return NIL
11 elsereturni
If we ignore lines 3–7 of the procedure, we have an ordinary algorithm for
searching asortedlinkedlist,inwhichindexi pointstoeachposition ofthelistin
1BecausewehavedefinedamergeableheaptosupportMINIMUMandEXTRACT-MIN,wecanalso
refertoitasamergeablemin-heap. Alternatively,ifitsupportedMAXIMUMandEXTRACT-MAX,
itwouldbeamergeablemax-heap.
ProblemsforChapter10 251
turn. Thesearch terminates once the index i “falls off” the end of the list or once
keyŒi k. In the latter case, if keyŒi k, clearly we have found a key with the
 D
value k. If, however, keyŒi > k, then we will never find a key with the value k,
andsoterminating thesearchwastherightthingtodo.
Lines 3–7 attempt to skip ahead to a randomly chosen position j. Such a skip
benefits us if keyŒj is larger than keyŒi and no larger than k; in such a case, j
marksapositioninthelistthati wouldhavetoreachduringanordinarylistsearch.
Becausethelistiscompact,weknowthatanychoiceofj between1andnindexes
someobjectinthelistratherthanaslotonthefreelist.
Instead of analyzing the performance of COMPACT-LIST-SEARCH directly, we
shall analyze a related algorithm, COMPACT-LIST-SEARCH0, which executes two
separate loops. This algorithm takes an additional parameter t which determines
anupperboundonthenumberofiterations ofthefirstloop.
COMPACT-LIST-SEARCH0.L;n;k;t/
1 i L
D
2 forq 1tot
D
3 j RANDOM.1;n/
D
4 ifkeyŒi < keyŒjandkeyŒj k

5 i j
D
6 ifkeyŒi ==k
7 returni
8 whilei NIL andkeyŒi <k
¤
9 i nextŒi
D
10 ifi == NIL orkeyŒi > k
11 return NIL
12 elsereturni
To compare the execution of the algorithms COMPACT-LIST-SEARCH.L;n;k/
andCOMPACT-LIST-SEARCH0.L;n;k;t/,assumethatthesequenceofintegersre-
turnedbythecallsofRANDOM.1;n/isthesameforbothalgorithms.
a. Suppose that COMPACT-LIST-SEARCH.L;n;k/ takes t iterations of thewhile
loop of lines 2–8. Argue that COMPACT-LIST-SEARCH0.L;n;k;t/ returns the
same answer and that the total number of iterations of both the for and while
loopswithin COMPACT-LIST-SEARCH0 isatleastt.
InthecallCOMPACT-LIST-SEARCH0.L;n;k;t/,letX
t
betherandomvariablethat
describes thedistance inthelinked list(thatis,through thechain ofnext pointers)
from position i to the desired key k after t iterations of the for loop of lines 2–7
haveoccurred.
252 Chapter10 ElementaryDataStructures
b. Argue that the expected running time of COMPACT-LIST-SEARCH0.L;n;k;t/
isO.t EŒX /.
t
C
c. ShowthatEŒX  n .1 r=n/t. (Hint:Useequation (C.25).)
t  r 1 (cid:0)
D
d. Showthat n r(cid:0)1 0rt P nt C1=.t C1/.
D
e. ProvethatP EŒX  n=.t 1/.
t
 C
f. Show that COMPACT-LIST-SEARCH0.L;n;k;t/ runs in O.t n=t/ expected
C
time.
g. Concludethat COMPACT-LIST-SEARCH runsinO.pn/expectedtime.
h. Whydoweassumethatallkeys aredistinct in COMPACT-LIST-SEARCH? Ar-
guethatrandomskipsdonotnecessarilyhelpasymptoticallywhenthelistcon-
tainsrepeated keyvalues.
Chapter notes
Aho, Hopcroft, and Ullman [6] and Knuth [209] are excellent references for ele-
mentarydatastructures. Manyothertextscoverbothbasicdatastructuresandtheir
implementation inaparticularprogramminglanguage. Examplesofthesetypesof
textbooks include Goodrich and Tamassia [147], Main [241], Shaffer [311], and
Weiss [352, 353, 354]. Gonnet [145] provides experimental data on the perfor-
manceofmanydata-structure operations.
The origin of stacks and queues as data structures in computer science is un-
clear,sincecorresponding notionsalreadyexistedinmathematicsandpaper-based
business practices before the introduction of digital computers. Knuth [209] cites
A.M.Turingforthedevelopment ofstacksforsubroutine linkagein1947.
Pointer-based data structures also seem to be a folk invention. According to
Knuth,pointerswereapparentlyusedinearlycomputerswithdrummemories. The
A-1language developed byG.M.Hopperin1951 represented algebraic formulas
asbinarytrees. KnuthcreditstheIPL-IIlanguage,developedin1956byA.Newell,
J. C. Shaw, and H. A. Simon, for recognizing the importance and promoting the
useofpointers. TheirIPL-IIIlanguage, developed in1957, includedexplicitstack
operations.
11 Hash Tables
Many applications require a dynamic set that supports only the dictionary opera-
tions INSERT, SEARCH, and DELETE. For example, a compiler that translates a
programming language maintains a symbol table, in which the keys of elements
arearbitrary character strings corresponding toidentifiers inthelanguage. Ahash
tableisaneffectivedatastructureforimplementingdictionaries. Althoughsearch-
ingforanelementinahashtablecantakeaslongassearching foranelementina
linkedlist—‚.n/timeintheworstcase—inpractice, hashingperformsextremely
well. Under reasonable assumptions, the average time to search for an element in
ahashtableisO.1/.
A hash table generalizes the simpler notion of an ordinary array. Directly ad-
dressing into an ordinary array makes effective use of our ability to examine an
arbitrary position in an array in O.1/ time. Section 11.1 discusses direct address-
inginmoredetail. Wecantakeadvantageofdirectaddressingwhenwecanafford
toallocate anarraythathasonepositionforeverypossible key.
Whenthenumberofkeysactually storedissmallrelativetothetotalnumberof
possiblekeys,hashtablesbecomeaneffectivealternativetodirectlyaddressing an
array, since ahash table typically usesan array ofsizeproportional tothe number
ofkeysactuallystored. Insteadofusingthekeyasanarrayindexdirectly,thearray
indexiscomputed fromthekey. Section11.2presents themainideas, focusing on
“chaining”asawaytohandle“collisions,”inwhichmorethanonekeymapstothe
samearray index. Section 11.3describes howwecancompute array indices from
keys using hash functions. Wepresent and analyze several variations on the basic
theme. Section11.4looksat“openaddressing,” whichisanotherwaytodealwith
collisions. The bottom line is that hashing is an extremely effective and practical
technique: the basic dictionary operations require only O.1/ time on the average.
Section 11.5 explains how “perfect hashing” can support searches in O.1/ worst-
case time, when theset ofkeys being stored isstatic (that is, when theset ofkeys
neverchangesoncestored).
254 Chapter11 HashTables
11.1 Direct-address tables
Direct addressing is a simple technique that works well when the universe U of
keysisreasonablysmall. Supposethatanapplicationneedsadynamicsetinwhich
each element has akeydrawnfrom the universe U 0;1;:::;m 1 , wherem
D f (cid:0) g
isnottoolarge. Weshallassumethatnotwoelementshavethesamekey.
To represent the dynamic set, we use an array, or direct-address table, denoted
by TŒ0::m 1, in which each position, or slot, corresponds to a key in the uni-
(cid:0)
verseU. Figure11.1illustrates theapproach; slotk pointstoanelementintheset
withkeyk. Ifthesetcontains noelementwithkeyk,thenTŒk NIL.
D
Thedictionary operations aretrivialtoimplement:
DIRECT-ADDRESS-SEARCH.T;k/
1 returnTŒk
DIRECT-ADDRESS-INSERT.T;x/
1 TŒx:key x
D
DIRECT-ADDRESS-DELETE.T;x/
1 TŒx:key NIL
D
Eachoftheseoperations takesonlyO.1/time.
T
0
key satellite data
U 1
(universe of keys) 2
2
0 6
9 3
7 3
4
4
1
K
2 5
5
(actual 3 6
5
keys)
8 7
8
8
9
Figure11.1 Howtoimplementadynamicsetbyadirect-addresstableT.Eachkeyintheuniverse
U 0;1;:::;9 corresponds to an index in the table. The set K 2;3;5;8 of actual keys
D f g D f g
determinestheslotsinthetablethatcontainpointerstoelements. Theotherslots,heavilyshaded,
containNIL.
11.1 Direct-addresstables 255
Forsomeapplications,thedirect-addresstableitselfcanholdtheelementsinthe
dynamic set. That is, rather than storing an element’s key and satellite data in an
objectexternal tothedirect-address table,withapointer fromaslotinthetableto
the object, we can store the object in the slot itself, thus saving space. We would
use a special key within an object to indicate an empty slot. Moreover, it is often
unnecessary to store the key of the object, since if we have the index of an object
in the table, we have its key. If keys are not stored, however, we must have some
waytotellwhethertheslotisempty.
Exercises
11.1-1
SupposethatadynamicsetS isrepresentedbyadirect-addresstableT oflengthm.
DescribeaprocedurethatfindsthemaximumelementofS. Whatistheworst-case
performance ofyourprocedure?
11.1-2
A bit vector is simply an array of bits (0s and 1s). A bit vector of length m takes
much less space than an array of m pointers. Describe how to use a bit vector
to represent a dynamic set of distinct elements with no satellite data. Dictionary
operations shouldruninO.1/time.
11.1-3
Suggest how to implement a direct-address table in which the keys of stored el-
ements do not need to be distinct and the elements can have satellite data. All
three dictionary operations (INSERT, DELETE, and SEARCH) should run in O.1/
time. (Don’t forget that DELETE takes as anargument apointer toanobject tobe
deleted, notakey.)
11.1-4 ?
Wewishtoimplement adictionary byusing direct addressing onahuge array. At
the start, the array entries may contain garbage, and initializing the entire array
is impractical because of its size. Describe a scheme for implementing a direct-
address dictionary on a huge array. Each stored object should use O.1/ space;
the operations SEARCH, INSERT, and DELETE should take O.1/ time each; and
initializingthedatastructureshouldtakeO.1/time. (Hint:Useanadditionalarray,
treated somewhat like a stack whose size is the number of keys actually stored in
thedictionary, tohelpdeterminewhetheragivenentryinthehugearrayisvalidor
not.)
256 Chapter11 HashTables
11.2 Hashtables
The downside of direct addressing is obvious: if the universe U is large, storing
a table T of size U may be impractical, or even impossible, given the memory
j j
available on a typical computer. Furthermore, the set K of keys actually stored
may be so small relative to U that most of the space allocated for T would be
wasted.
When the set K of keys stored in a dictionary is much smaller than the uni-
verse U ofallpossible keys, ahash tablerequires muchlessstorage than adirect-
addresstable. Specifically,wecanreducethestoragerequirementto‚. K /while
j j
wemaintainthebenefitthatsearchingforanelementinthehashtablestillrequires
onlyO.1/time. Thecatchisthatthisboundisfortheaverage-case time,whereas
fordirectaddressing itholdsfortheworst-case time.
Withdirect addressing, anelementwithkeyk isstored inslotk. Withhashing,
thiselementisstoredinsloth.k/;thatis,weuseahashfunctionhtocomputethe
slot from the key k. Here, h maps the universe U of keys into the slots of a hash
tableTŒ0::m 1:
(cid:0)
h U 0;1;:::;m 1 ;
W ! f (cid:0) g
where thesize mofthe hash table istypically much lessthan U . Wesay thatan
j j
elementwithkeyk hashestosloth.k/;wealsosaythath.k/isthehashvalueof
key k. Figure 11.2 illustrates the basic idea. The hash function reduces the range
of array indices and hence the size of the array. Instead of asize of U , the array
j j
canhavesizem.
T
0
U
(universe of keys)
h(k )
1
h(k )
k 4
1
K
k
(actual 4 k 5 h(k 2) = h(k 5)
keys)
k
2 k 3 h(k 3)
m–1
Figure11.2 Usingahashfunctionhtomapkeystohash-tableslots.Becausekeysk2andk5map
tothesameslot,theycollide.
11.2 Hashtables 257
T
U k k
1 4
(universe of keys)
k
1
K
k k
(actual 4 k 5 k 5 k 2 k 7
keys) 7
k k
k 2 k 8 3 k 3
6
k k
8 6
Figure11.3 Collisionresolutionbychaining. Eachhash-tableslotTŒjcontainsalinkedlistof
all thekeys whose hash value is j. For example, h.k1/ h.k4/ and h.k5/ h.k7/ h.k2/.
D D D
Thelinkedlistcanbeeithersinglyordoublylinked;weshowitasdoublylinkedbecausedeletionis
fasterthatway.
There is one hitch: two keys may hash to the same slot. We call this situation
a collision. Fortunately, we have effective techniques for resolving the conflict
createdbycollisions.
Ofcourse, the ideal solution would be to avoid collisions altogether. Wemight
try to achieve this goal by choosing a suitable hash function h. One idea is to
make h appear to be “random,” thus avoiding collisions or at least minimizing
their number. The very term “to hash,” evoking images of random mixing and
chopping,capturesthespiritofthisapproach. (Ofcourse,ahashfunctionhmustbe
deterministic inthatagiveninputk shouldalwaysproducethesameoutputh.k/.)
Because U >m,however,theremustbeatleasttwokeysthathavethesamehash
j j
value; avoiding collisions altogether is therefore impossible. Thus, while a well-
designed, “random”-looking hashfunctioncanminimizethenumberofcollisions,
westillneedamethodforresolving thecollisions thatdooccur.
The remainder of this section presents the simplest collision resolution tech-
nique,calledchaining. Section11.4introducesanalternativemethodforresolving
collisions, calledopenaddressing.
Collisionresolution bychaining
In chaining, we place all the elements that hash to the same slot into the same
linkedlist,asFigure11.3shows. Slotj containsapointertotheheadofthelistof
allstoredelementsthathashtoj;iftherearenosuchelements,slotj containsNIL.
258 Chapter11 HashTables
The dictionary operations on a hash table T are easy to implement when colli-
sionsareresolvedbychaining:
CHAINED-HASH-INSERT.T;x/
1 insertx attheheadoflistTŒh.x:key/
CHAINED-HASH-SEARCH.T;k/
1 searchforanelementwithkeyk inlistTŒh.k/
CHAINED-HASH-DELETE.T;x/
1 deletex fromthelistTŒh.x:key/
Theworst-case running timefor insertion is O.1/. Theinsertion procedure isfast
inpartbecauseitassumesthattheelementxbeinginsertedisnotalreadypresentin
thetable;ifnecessary,wecancheckthisassumption(atadditionalcost)bysearch-
ing for an element whose key is x:key before we insert. Forsearching, the worst-
case running time is proportional to the length of the list; we shall analyze this
operation more closely below. We can delete an element in O.1/ time if the lists
are doubly linked, as Figure 11.3 depicts. (Note that CHAINED-HASH-DELETE
takesasinputanelementx andnotitskeyk,sothatwedon’thavetosearchforx
first. Ifthehashtablesupportsdeletion,thenitslinkedlistsshouldbedoublylinked
so that we can delete an item quickly. If the lists were only singly linked, then to
delete element x, we would first have to find x in the list TŒh.x:key/ so that we
could update the next attribute of x’s predecessor. With singly linked lists, both
deletion andsearching wouldhavethesameasymptoticrunning times.)
Analysisofhashingwithchaining
Howwelldoeshashingwithchainingperform? Inparticular, howlongdoesittake
tosearchforanelementwithagivenkey?
Given a hash table T with m slots that stores n elements, we define the load
factor ˛ for T as n=m, that is, the average number of elements stored in a chain.
Our analysis will be in terms of ˛, which can be less than, equal to, or greater
than1.
The worst-case behavior of hashing with chaining is terrible: all n keys hash
to the same slot, creating a list of length n. The worst-case time for searching is
thus ‚.n/ plus the time to compute the hash function—no better than if we used
one linked list for all the elements. Clearly, we do not use hash tables for their
worst-case performance. (Perfecthashing, described inSection11.5,doesprovide
goodworst-caseperformance whenthesetofkeysisstatic,however.)
The average-case performance of hashing depends on how well the hash func-
tion h distributes the set of keys to be stored among the m slots, on the average.
11.2 Hashtables 259
Section 11.3 discusses these issues, but for now we shall assume that any given
element is equally likely to hash into any of the m slots, independently of where
any other element has hashed to. We call this the assumption of simple uniform
hashing.
Forj 0;1;:::;m 1,letusdenotethelengthofthelistTŒjbyn ,sothat
j
D (cid:0)
n n n n ; (11.1)
0 1 m 1
D C CC (cid:0)
andtheexpectedvalueofn isEŒn  ˛ n=m.
j j
D D
We assume that O.1/ time suffices to compute the hash value h.k/, so that
the time required to search for an element with key k depends linearly on the
length n of the list TŒh.k/. Setting aside the O.1/ time required to compute
h.k/
the hash function and to access slot h.k/, let us consider the expected number of
elements examined bythesearch algorithm, that is, thenumber ofelements inthe
listTŒh.k/thatthealgorithmcheckstoseewhetheranyhaveakeyequaltok. We
shall consider twocases. Inthe first,thesearch isunsuccessful: no element inthe
tablehaskeyk. Inthesecond,thesearchsuccessfully findsanelementwithkeyk.
Theorem11.1
Inahashtableinwhichcollisionsareresolvedbychaining,anunsuccessfulsearch
takesaverage-casetime‚.1 ˛/,undertheassumptionofsimpleuniformhashing.
C
Proof Under the assumption of simple uniform hashing, any key k not already
storedinthetableisequallylikelytohashtoanyofthemslots. Theexpectedtime
to search unsuccessfully for a key k is the expected time to search to the end of
listTŒh.k/,whichhasexpected length EŒn  ˛. Thus,theexpected number
h.k/
D
of elements examined in an unsuccessful search is ˛, and the total time required
(including thetimeforcomputing h.k/)is‚.1 ˛/.
C
The situation for a successful search is slightly different, since each list is not
equally likely tobesearched. Instead, theprobability thatalistissearched ispro-
portional to the number of elements it contains. Nonetheless, the expected search
timestillturnsouttobe‚.1 ˛/.
C
Theorem11.2
In a hash table in which collisions are resolved by chaining, a successful search
takesaverage-casetime‚.1 ˛/,undertheassumptionofsimpleuniformhashing.
C
Proof Weassume that theelement being searched for isequally likely tobe any
of the n elements stored in the table. The number of elements examined during a
successful search for an element x is one more than the number of elements that
260 Chapter11 HashTables
appear before x in x’s list. Because new elements are placed at the front of the
list, elements before x in the list were all inserted after x was inserted. To find
the expected number of elements examined, we take the average, over the n ele-
ments x in the table, of 1 plus the expected number of elements added to x’s list
after x was added to the list. Let x denote the ith element inserted into the ta-
i
ble, for i 1;2;:::;n, and let k x :key. For keys k and k , we define the
i i i j
D D
indicator random variableX I h.k / h.k / . Undertheassumption ofsim-
ij i j
D f D g
ple uniform hashing, we have Pr h.k / h.k / 1=m, and so by Lemma 5.1,
i j
f D g D
EŒX  1=m. Thus, the expected number of elements examined in a successful
ij
D
searchis
n n
1
E 1 X
ij
n C
" !#
i 1 j i 1
XD DXC
n n
1
1 EŒX  (bylinearityofexpectation)
ij
D n C
!
i 1 j i 1
XD DXC
n n
1 1
1
D n C m
!
i 1 j i 1
XD DXC
n
1
1 .n i/
D C nm (cid:0)
i 1
XD
n n
1
1 n i
D C nm (cid:0)
!
i 1 i 1
XD XD
1 n.n 1/
1 n2 C (byequation(A.1))
D C nm (cid:0) 2
 
n 1
1 (cid:0)
D C 2m
˛ ˛
1 :
D C 2 (cid:0) 2n
Thus, the total time required for a successful search (including the time for com-
putingthehashfunction) is‚.2 ˛=2 ˛=2n/ ‚.1 ˛/.
C (cid:0) D C
What does this analysis mean? Ifthe number ofhash-table slots is at least pro-
portional to the number of elements in the table, we have n O.m/ and, con-
D
sequently, ˛ n=m O.m/=m O.1/. Thus, searching takes constant time
D D D
on average. Since insertion takes O.1/ worst-case time and deletion takes O.1/
worst-case time when the lists are doubly linked, we can support all dictionary
operations inO.1/timeonaverage.
11.2 Hashtables 261
Exercises
11.2-1
Suppose we use a hash function h to hash n distinct keys into an array T of
length m. Assuming simple uniform hashing, what is the expected number of
collisions? More precisely, whatisthe expected cardinality of k;l k l and
ff g W ¤
h.k/ h.l/ ?
D g
11.2-2
Demonstratewhathappenswhenweinsertthekeys5;28;19;15;20;33;12;17;10
into a hash table with collisions resolved by chaining. Let the table have 9 slots,
andletthehashfunctionbeh.k/ k mod 9.
D
11.2-3
ProfessorMarleyhypothesizesthathecanobtainsubstantialperformancegainsby
modifyingthechainingschemetokeepeachlistinsortedorder. Howdoesthepro-
fessor’s modification affect therunning timeforsuccessful searches, unsuccessful
searches, insertions, anddeletions?
11.2-4
Suggest how to allocate and deallocate storage for elements within the hash table
itself by linking all unused slots into a free list. Assume that one slot can store
a flag and either one element plus a pointer or two pointers. All dictionary and
free-listoperations shouldruninO.1/expected time. Doesthefreelistneedtobe
doublylinked, ordoesasinglylinkedfreelistsuffice?
11.2-5
Supposethatwearestoringasetofnkeysintoahashtableofsizem. Showthatif
thekeysaredrawnfromauniverseU with U > nm,thenU hasasubsetofsizen
j j
consisting of keys that all hash to the same slot, so that the worst-case searching
timeforhashingwithchaining is‚.n/.
11.2-6
Supposewehavestorednkeysinahashtableofsizem,withcollisionsresolvedby
chaining, andthatweknowthelengthofeachchain, including thelength Lofthe
longest chain. Describe a procedure that selects a key uniformly at random from
amongthekeysinthehashtableandreturnsitinexpectedtimeO.L .1 1=˛//.
 C
262 Chapter11 HashTables
11.3 Hashfunctions
Inthissection,wediscusssomeissuesregardingthedesignofgoodhashfunctions
andthenpresent threeschemesfortheircreation. Twooftheschemes, hashing by
division and hashing by multiplication, are heuristic in nature, whereas the third
scheme, universal hashing, uses randomization to provide provably good perfor-
mance.
Whatmakesagoodhashfunction?
A good hash function satisfies (approximately) the assumption of simple uniform
hashing: each keyisequally likely tohash toanyofthe mslots, independently of
where any other key has hashed to. Unfortunately, we typically have no way to
checkthiscondition, sincewerarelyknowtheprobability distribution fromwhich
thekeysaredrawn. Moreover,thekeysmightnotbedrawnindependently.
Occasionally we do know the distribution. For example, if we know that the
keys are random real numbers k independently and uniformly distributed in the
range0 k <1,thenthehashfunction

h.k/ km
D b c
satisfiesthecondition ofsimpleuniformhashing.
In practice, we can often employ heuristic techniques to create a hash function
that performs well. Qualitative information about the distribution of keys may be
useful in this design process. Forexample, consider acompiler’s symbol table, in
whichthe keysarecharacter strings representing identifiers inaprogram. Closely
related symbols, such as pt and pts, often occur in the same program. A good
hashfunctionwouldminimizethechance thatsuchvariantshashtothesameslot.
A good approach derives the hash value in awaythat weexpect tobe indepen-
dentofanypatternsthatmightexistinthedata. Forexample,the“divisionmethod”
(discussed in Section 11.3.1) computes the hash value as the remainder when the
key is divided by a specified prime number. This method frequently gives good
results, assuming that we choose a prime number that is unrelated to any patterns
inthedistribution ofkeys.
Finally,wenotethatsomeapplications ofhashfunctions mightrequirestronger
properties than are provided by simple uniform hashing. For example, we might
want keys that are “close” in some sense to yield hash values that are far apart.
(Thisproperty isespecially desirable whenweareusinglinearprobing, definedin
Section 11.4.) Universal hashing, described in Section 11.3.3, often provides the
desired properties.
11.3 Hashfunctions 263
Interpretingkeysasnaturalnumbers
Most hash functions assume that the universe of keys is the set N 0;1;2;:::
D f g
of natural numbers. Thus, if the keys are not natural numbers, we find a way to
interpretthemasnaturalnumbers. Forexample,wecaninterpretacharacterstring
as an integer expressed in suitable radix notation. Thus, we might interpret the
identifierptasthepairofdecimalintegers.112;116/,sincep 112andt 116
D D
in the ASCII character set; then, expressed as a radix-128 integer, pt becomes
.112 128/ 116 14452. Inthe context of agiven application, wecan usually
 C D
devise some such method for interpreting each key as a (possibly large) natural
number. Inwhatfollows,weassumethatthekeysarenaturalnumbers.
11.3.1 Thedivisionmethod
Inthe division method for creating hash functions, wemapakey k into one of m
slotsbytakingtheremainderofk dividedbym. Thatis,thehashfunction is
h.k/ k modm:
D
For example, if the hash table has size m 12 and the key is k 100, then
D D
h.k/ 4. Sinceitrequires onlyasingledivisionoperation, hashingbydivisionis
D
quitefast.
When using the division method, we usually avoid certain values of m. For
example, m should not be a power of 2, since if m 2p, then h.k/ is just the p
D
lowest-orderbitsofk. Unlessweknowthatalllow-orderp-bitpatternsareequally
likely,wearebetteroffdesigning thehashfunction todependonallthebitsofthe
key. As Exercise 11.3-3 asks you to show, choosing m 2p 1 when k is a
D (cid:0)
character string interpreted in radix 2p may be a poor choice, because permuting
thecharacters ofk doesnotchangeitshashvalue.
A prime not too close to an exact power of 2 is often a good choice for m. For
example, suppose we wish to allocate a hash table, with collisions resolved by
chaining, toholdroughlyn 2000characterstrings,whereacharacterhas8bits.
D
Wedon’tmindexamining anaverageof3elementsinanunsuccessful search,and
so we allocate a hash table of size m 701. We could choose m 701 because
D D
it is a prime near 2000=3 but not near any power of 2. Treating each key k as an
integer, ourhashfunction wouldbe
h.k/ k mod701:
D
11.3.2 Themultiplicationmethod
Themultiplicationmethodforcreatinghashfunctionsoperatesintwosteps. First,
we multiply the key k by a constant A in the range 0 < A < 1 and extract the
264 Chapter11 HashTables
wbits
k
· s A 2w
D 
r r
1 0
extractpbits
h.k/
Figure11.4 Themultiplicationmethodofhashing. Thew-bitrepresentationofthekeykismulti-
pliedbythew-bitvalues A 2w. Thephighest-orderbitsofthelowerw-bithalfoftheproduct
D 
formthedesiredhashvalueh.k/.
fractional part of kA. Then, wemultiply this value by m and take the floor of the
result. Inshort,thehashfunction is
h.k/ m.kA mod 1/ ;
D b c
where“kA mod 1”meansthefractional partofkA,thatis,kA kA .
(cid:0)b c
Anadvantage of the multiplication method is that the value of mis not critical.
We typically choose it to be a power of 2 (m 2p for some integer p), since we
D
can then easily implement the function on most computers as follows. Suppose
that the word size of the machine is w bits and that k fits into a single word. We
restrict A to be a fraction of the form s=2w, where s is an integer in the range
0 < s < 2w. Referring to Figure 11.4, we first multiply k by the w-bit integer
s A 2w. Theresultisa2w-bitvaluer 2w r ,wherer isthehigh-orderword
1 0 1
D  C
oftheproduct andr isthelow-order wordoftheproduct. Thedesired p-bithash
0
valueconsistsofthep mostsignificant bitsofr .
0
Although this method works with any value of the constant A, it works better
with some values than with others. The optimal choice depends on the character-
isticsofthedatabeinghashed. Knuth[211]suggests that
A .p5 1/=2 0:6180339887::: (11.2)
 (cid:0) D
islikelytoworkreasonably well.
As an example, suppose we have k 123456, p 14, m 214 16384,
D D D D
and w 32. Adapting Knuth’s suggestion, wechoose A to be the fraction of the
D
form s=232 that is closest to .p5 1/=2, so that A 2654435769=232. Then
(cid:0) D
k s 327706022297664 .76300 232/ 17612864, and so r 76300
1
 D D  C D
andr 17612864. The14mostsignificantbitsofr yieldthevalueh.k/ 67.
0 0
D D
11.3 Hashfunctions 265
?
11.3.3 Universalhashing
Ifamaliciousadversarychoosesthekeystobehashedbysomefixedhashfunction,
thentheadversary canchoosenkeysthatallhashtothesameslot,yielding anav-
erageretrievaltimeof‚.n/. Anyfixedhashfunctionisvulnerable tosuchterrible
worst-case behavior; the only effective way to improve the situation is to choose
thehashfunctionrandomlyinawaythatisindependentofthekeysthatareactually
going to be stored. This approach, called universal hashing, can yield provably
goodperformance onaverage, nomatterwhichkeystheadversary chooses.
In universal hashing, at the beginning of execution we select the hash function
at random from a carefully designed class of functions. As in the case of quick-
sort, randomization guarantees that no single input will always evoke worst-case
behavior. Because we randomly select the hash function, the algorithm can be-
have differently on each execution, even for the same input, guaranteeing good
average-case performance foranyinput. Returning totheexample ofacompiler’s
symboltable,wefindthattheprogrammer’schoiceofidentifierscannotnowcause
consistently poor hashing performance. Poor performance occurs only when the
compiler chooses a random hash function that causes the set of identifiers to hash
poorly, but the probability of this situation occurring is small and is the same for
anysetofidentifiersofthesamesize.
Let H be a finite collection of hash functions that map a given universe U of
keys into the range 0;1;:::;m 1 . Such a collection is said to be universal
if for each pair of dif stinct keys k(cid:0) ;l g U, the number of hash functions h H
for which h.k/ h.l/ is at most
H2
=m. In other words, with a hash
fun2
ction
randomly chosenD from H , the chanj cej of acollision between distinct keys k and l
isnomorethanthechance1=mofacollision ifh.k/andh.l/wererandomlyand
independently chosenfromtheset 0;1;:::;m 1 .
f (cid:0) g
Thefollowingtheoremshowsthatauniversalclassofhashfunctionsgivesgood
average-case behavior. Recallthatn denotes thelengthoflistTŒi.
i
Theorem11.3
Suppose that a hash function h is chosen randomly from a universal collection of
hash functions and has been used to hash n keys into a table T of size m, us-
ing chaining to resolve collisions. If key k is not in the table, then the expected
lengthEŒn ofthelistthatkeyk hashes toisatmosttheloadfactor˛ n=m.
h.k/
D
Ifkeykisinthetable,thentheexpectedlengthEŒn ofthelistcontainingkeyk
h.k/
isatmost1 ˛.
C
Proof We note that the expectations here are over the choice of the hash func-
tion and do not depend on any assumptions about the distribution of the keys.
For each pair k and l of distinct keys, define the indicator random variable
266 Chapter11 HashTables
X I h.k/ h.l/ . Since by the definition of a universal collection of hash
kl
D f D g
functions, a single pair of keys collides with probability at most 1=m, we have
Pr h.k/ h.l/ 1=m. ByLemma5.1,therefore, wehaveEŒX  1=m.
kl
f D g  
Next we define, for each key k, the random variable Y that equals the number
k
ofkeysotherthank thathashtothesameslotask,sothat
Y X :
k kl
D
l T
X l2k
¤
Thuswehave
EŒY  E X
k kl
D
2l T 3
X l2k
¤
4 5
EŒX  (bylinearity ofexpectation)
kl
D
l T
X l2k
¤
1
:
 m
l T
X l2k
¤
Theremainder oftheproofdependsonwhetherkeyk isintableT.
 If k T, then n Y and l l T andl k n. Thus EŒn 
h.k/ k h.k/
62 D jf W 2 ¤ gj D D
EŒY  n=m ˛.
k
 D
 Ifk T,thenbecause keyk appears inlistTŒh.k/andthecountY doesnot
k
2
include key k, we have n Y 1 and l l T andl k n 1.
h.k/ k
D C jf W 2 ¤ gj D (cid:0)
ThusEŒn  EŒY  1 .n 1/=m 1 1 ˛ 1=m < 1 ˛.
h.k/ k
D C  (cid:0) C D C (cid:0) C
The following corollary says universal hashing provides the desired payoff: it
hasnowbecomeimpossible foranadversary topickasequence ofoperations that
forces the worst-case running time. By cleverly randomizing the choice of hash
functionatruntime,weguaranteethatwecanprocesseverysequenceofoperations
withagoodaverage-case running time.
Corollary 11.4
Using universal hashing and collision resolution by chaining in an initially empty
tablewithmslots,ittakesexpectedtime‚.n/tohandleanysequenceofnINSERT,
SEARCH,and DELETE operations containing O.m/ INSERT operations.
Proof Since the number of insertions is O.m/, we have n O.m/ and so
D
˛ O.1/. The INSERT and DELETE operations take constant time and, by The-
D
orem 11.3, the expected timefor each SEARCH operation isO.1/. Bylinearity of
11.3 Hashfunctions 267
expectation, therefore, the expected time for the entire sequence of n operations
isO.n/. Sinceeachoperation takes.1/time,the‚.n/boundfollows.
Designingauniversalclassofhashfunctions
It is quite easy to design a universal class of hash functions, as a little number
theory will help us prove. You may wish to consult Chapter 31 first if you are
unfamiliarwithnumbertheory.
We begin by choosing a prime number p large enough so that every possible
keyk isintherange0top 1,inclusive. LetZ denotetheset 0;1;:::;p 1 ,
p
and let Z denote the set 1(cid:0) ;2;:::;p 1 . Since p is prime, wf e can solve e(cid:0) quag -
p
f (cid:0) g
tionsmodulopwiththemethodsgiveninChapter31. Becauseweassumethatthe
sizeoftheuniverseofkeysisgreaterthanthenumberofslotsinthehashtable,we
havep >m.
We now define the hash function h for any a Z and any b Z using a
ab
2
p
2
p
lineartransformation followedbyreductions modulop andthenmodulom:
h .k/ ..ak b/ modp/ mod m: (11.3)
ab
D C
For example, with p 17 and m 6, we have h .8/ 5. The family of all
3;4
D D D
suchhashfunctions is
H h a Z andb Z : (11.4)
pm
D
ab
W 2
p
2
p
Each has˚h function h maps Z t(cid:9)o Z . This class of hash functions has the nice
ab p m
propertythatthesizemoftheoutputrangeisarbitrary—not necessarily prime—a
feature which we shall use in Section 11.5. Since we have p 1 choices for a
andp choicesforb,thecollection H contains p.p 1/hash(cid:0) functions.
pm
(cid:0)
Theorem11.5
H
Theclass ofhashfunctionsdefinedbyequations(11.3)and(11.4)isuniversal.
pm
Proof Consider two distinct keys k and l from Z , so that k l. For a given
p
¤
hashfunction h welet
ab
r .ak b/mod p ;
D C
s .al b/ mod p :
D C
Wefirstnotethatr s. Why? Observethat
¤
r s a.k l/ .mod p/:
(cid:0)  (cid:0)
It follows that r s because p is prime and both a and .k l/ are nonzero
¤ (cid:0)
modulop,andsotheir product mustalsobenonzero modulop byTheorem 31.6.
Therefore,whencomputinganyh H ,distinctinputsk andl maptodistinct
ab pm
2
268 Chapter11 HashTables
valuesr andsmodulop;therearenocollisionsyetatthe“modplevel.” Moreover,
eachofthepossiblep.p 1/choicesforthepair.a;b/witha 0yieldsadifferent
(cid:0) ¤
resulting pair.r;s/withr s,sincewecansolvefora andb givenr ands:
¤
a .r s/..k l/ 1 mod p/ mod p ;
(cid:0)
D (cid:0) (cid:0)
b .r ak/ mod p ;
D (cid:0) (cid:0) 
where ..k l/ 1 mod p/ denotes the unique multiplicative inverse, modulo p,
(cid:0)
(cid:0)
of k l. Since there are only p.p 1/ possible pairs .r;s/ with r s, there
(cid:0) (cid:0) ¤
is a one-to-one correspondence between pairs .a;b/ with a 0 and pairs .r;s/
¤
withr s. Thus,foranygivenpairofinputs k andl,ifwepick.a;b/uniformly
atrando¤ m fromZ Z ,theresultingpair.r;s/isequallylikelytobeanypairof
p

p
distinct valuesmodulop.
Therefore, theprobability thatdistinct keysk andl collide isequal totheprob-
ability that r s .mod m/ when r and s are randomly chosen as distinct values

modulop. Foragivenvalueofr,ofthep 1possibleremainingvaluesfors,the
(cid:0)
numberofvaluess suchthats r ands r .mod m/isatmost
¤ 
p=m 1 ..p m 1/=m/ 1 (byinequality (3.6))
d e(cid:0)  C (cid:0) (cid:0)
.p 1/=m:
D (cid:0)
The probability that s collides with r when reduced modulo m is at most
..p 1/=m/=.p 1/ 1=m.
Th(cid:0) erefore, fora(cid:0) nypaD irofdistinctvaluesk;l Z ,
p
2
Pr h .k/ h .l/ 1=m;
ab ab
f D g 
H
sothat isindeed universal.
pm
Exercises
11.3-1
Suppose we wish to search a linked list of length n, where each element contains
a key k along with a hash value h.k/. Each key is a long character string. How
mightwetakeadvantage ofthehashvalueswhensearching thelistforanelement
withagivenkey?
11.3-2
Suppose that we hash a string of r characters into m slots by treating it as a
radix-128 number and then using the division method. We can easily represent
the number m asa32-bit computer word, but the string of r characters, treated as
aradix-128 number, takesmanywords. Howcanweapplythedivision methodto
compute thehash value ofthecharacter string without using morethan aconstant
numberofwordsofstorageoutsidethestringitself?
11.4 Openaddressing 269
11.3-3
Consider a version of the division method in which h.k/ k mod m, where
D
m 2p 1 and k is a character string interpreted in radix 2p. Show that if we
D (cid:0)
canderivestringx fromstringy bypermutingitscharacters, thenx andy hashto
thesamevalue. Giveanexampleofanapplicationinwhichthispropertywouldbe
undesirable inahashfunction.
11.3-4
Considerahashtableofsizem 1000andacorresponding hashfunctionh.k/
D D
m.kA mod 1/ for A .p5 1/=2. Compute the locations to which the keys
b c D (cid:0)
61,62,63,64,and65aremapped.
11.3-5 ?
Define a family H of hash functions from a finite set U to a finite set B to be
-universalifforallpairsofdistinctelementsk andl inU,
Pr h.k/ h.l/  ;
f D g 
where the probability is over the choice of the hash function h drawn at random
fromthefamilyH . Showthatan-universalfamilyofhashfunctions musthave
1 1
 :
 B (cid:0) U
j j j j
11.3-6 ?
Let U be the set of n-tuples of values drawn from Z , and let B Z , where p
p p
is prime. Define the hash function h U B for b Z on aD n input n-tuple
b p
W ! 2
a ;a ;:::;a fromU as
0 1 n 1
h (cid:0) i
n 1
(cid:0)
h . a ;a ;:::;a / a bj mod p ;
b 0 1 n 1 j
h (cid:0) i D !
j 0
XD
and let H h b Z . Argue that H is ..n 1/=p/-universal according to
b p
D f W 2 g (cid:0)
thedefinitionof-universalinExercise11.3-5. (Hint:SeeExercise31.4-4.)
11.4 Openaddressing
In open addressing, all elements occupy the hash table itself. That is, each table
entry contains either an element of the dynamic set or NIL. When searching for
an element, we systematically examine table slots until either we find the desired
element or we have ascertained that the element is not in the table. No lists and
270 Chapter11 HashTables
no elements are stored outside the table, unlike in chaining. Thus, in open ad-
dressing, thehashtablecan“fillup”sothatnofurtherinsertionscanbemade;one
consequence isthattheloadfactor˛ canneverexceed1.
Of course, we could store the linked lists for chaining inside the hash table, in
the otherwise unused hash-table slots (see Exercise 11.2-4), but the advantage of
openaddressing isthatitavoids pointers altogether. Instead offollowingpointers,
wecompute thesequence ofslots tobeexamined. Theextramemoryfreedbynot
storing pointers provides the hash table withalarger number ofslots forthesame
amountofmemory,potentially yielding fewercollisions andfasterretrieval.
Toperforminsertionusingopenaddressing, wesuccessivelyexamine,orprobe,
thehashtableuntilwefindanemptyslotinwhichtoputthekey. Insteadofbeing
fixedintheorder0;1;:::;m 1(whichrequires‚.n/searchtime),thesequence
(cid:0)
ofpositionsprobeddependsuponthekeybeinginserted. Todeterminewhichslots
toprobe,weextendthehashfunctiontoincludetheprobenumber(startingfrom0)
asasecondinput. Thus,thehashfunction becomes
h U 0;1;:::;m 1 0;1;:::;m 1 :
W f (cid:0) g ! f (cid:0) g
Withopenaddressing, werequirethatforeverykeyk,theprobesequence
h.k;0/;h.k;1/;:::;h.k;m 1/
h (cid:0) i
beapermutationof 0;1;:::;m 1 ,sothateveryhash-tablepositioniseventually
h (cid:0) i
consideredasaslotforanewkeyasthetablefillsup. Inthefollowingpseudocode,
we assume that the elements in the hash table T are keys with no satellite infor-
mation; the key k is identical to the element containing key k. Each slot contains
either a key or NIL (if the slot is empty). The HASH-INSERT procedure takes as
input a hash table T and a key k. It either returns the slot number where it stores
keyk orflagsanerrorbecause thehashtableisalreadyfull.
HASH-INSERT.T;k/
1 i 0
D
2 repeat
3 j h.k;i/
D
4 ifTŒj == NIL
5 TŒj k
D
6 returnj
7 elsei i 1
D C
8 untili ==m
9 error“hashtableoverflow”
Thealgorithmforsearchingforkeyk probesthesamesequenceofslotsthatthe
insertion algorithm examined when key k was inserted. Therefore, the search can
11.4 Openaddressing 271
terminate (unsuccessfully) when it finds an empty slot, since k would have been
insertedthereandnotlaterinitsprobesequence. (Thisargumentassumesthatkeys
arenotdeletedfromthehashtable.) Theprocedure HASH-SEARCH takesasinput
ahashtableT andakeyk,returning j ifitfindsthatslotj containskeyk,orNIL
ifkeyk isnotpresentintableT.
HASH-SEARCH.T;k/
1 i 0
D
2 repeat
3 j h.k;i/
D
4 ifTŒj== k
5 returnj
6 i i 1
D C
7 untilTŒj== NIL ori == m
8 return NIL
Deletion from an open-address hash table is difficult. When we delete a key
from slot i, we cannot simply mark that slot as empty by storing NIL in it. If
we did, we might be unable to retrieve any key k during whose insertion we had
probed slot i and found it occupied. We can solve this problem by marking the
slot,storinginitthespecialvalueDELETED insteadofNIL. Wewouldthenmodify
theprocedure HASH-INSERT totreatsuchaslotasifitwereemptysothatwecan
insertanewkeythere. WedonotneedtomodifyHASH-SEARCH,sinceitwillpass
over DELETED values while searching. When weuse the special value DELETED,
however, search times no longer depend on the load factor ˛, and for this reason
chainingismorecommonlyselectedasacollisionresolutiontechnique whenkeys
mustbedeleted.
In our analysis, we assume uniform hashing: the probe sequence of each key
is equally likely to be any of the mŠ permutations of 0; 1; :::; m 1 . Uni-
h (cid:0) i
formhashing generalizes thenotionofsimpleuniformhashing definedearliertoa
hashfunction that produces notjust asingle number, butawhole probe sequence.
True uniform hashing is difficult to implement, however, and in practice suitable
approximations (suchasdoublehashing, definedbelow)areused.
We will examine three commonly used techniques to compute the probe se-
quences required foropenaddressing: linear probing, quadratic probing, anddou-
blehashing. Thesetechniquesallguaranteethat h.k;0/;h.k;1/;:::;h.k;m 1/
h (cid:0) i
isapermutation of 0;1;:::;m 1 foreachkeyk. Noneofthesetechniques ful-
h (cid:0) i
fillstheassumption ofuniformhashing, however,sincenoneofthemiscapableof
generatingmorethanm2 differentprobesequences(insteadofthemŠthatuniform
hashingrequires). Doublehashinghasthegreatestnumberofprobesequencesand,
asonemightexpect,seemstogivethebestresults.
272 Chapter11 HashTables
Linearprobing
Givenanordinary hashfunction h U 0;1;:::;m 1 ,whichwerefertoas
0
W ! f (cid:0) g
anauxiliaryhashfunction,themethodoflinearprobingusesthehashfunction
h.k;i/ .h.k/ i/ mod m
0
D C
for i 0;1;:::;m 1. Given key k, we first probe TŒh.k/, i.e., the slot given
0
D (cid:0)
by the auxiliary hash function. We next probe slot TŒh.k/ 1, and so on up to
0
C
slot TŒm 1. Thenwewraparound toslotsTŒ0;TŒ1;:::until wefinallyprobe
(cid:0)
slot TŒh.k/ 1. Because the initial probe determines theentire probe sequence,
0
(cid:0)
thereareonlymdistinct probesequences.
Linear probing is easy to implement, but it suffers from a problem known as
primary clustering. Long runs of occupied slots build up, increasing the average
searchtime. Clustersarisebecauseanemptyslotprecededbyi fullslotsgetsfilled
next with probability .i 1/=m. Long runs of occupied slots tend to get longer,
C
andtheaveragesearchtimeincreases.
Quadraticprobing
Quadraticprobingusesahashfunction oftheform
h.k;i/ .h.k/ c i c i2/ modm; (11.5)
0 1 2
D C C
where h is an auxiliary hash function, c and c are positive auxiliary constants,
0 1 2
and i 0;1;:::;m 1. The initial position probed is TŒh.k/; later positions
0
D (cid:0)
probedareoffsetbyamountsthatdependinaquadraticmannerontheprobenum-
ber i. Thismethod works muchbetter than linear probing, but tomakefull useof
the hash table, the values of c , c , and m are constrained. Problem 11-3 shows
1 2
one way to select these parameters. Also, if two keys have the same initial probe
position, then their probe sequences are the same, since h.k ;0/ h.k ;0/ im-
1 2
D
pliesh.k ;i/ h.k ;i/. Thispropertyleadstoamilderformofclustering, called
1 2
D
secondary clustering. Asin linear probing, the initial probe determines the entire
sequence, andsoonlymdistinct probesequences areused.
Doublehashing
Double hashing offers one of the best methods available for open addressing be-
cause the permutations produced have many of the characteristics of randomly
chosenpermutations. Doublehashingusesahashfunction oftheform
h.k;i/ .h .k/ ih .k// mod m;
1 2
D C
where both h and h are auxiliary hash functions. Theinitial probe goes toposi-
1 2
tionTŒh .k/;successiveprobepositionsareoffsetfrompreviouspositionsbythe
1
11.4 Openaddressing 273
0
1 79
2
3
4 69
5 98
6
7 72
8
9 14
10
11 50
12
Figure 11.5 Insertion by double hashing. Here we have a hash table of size 13 with h1.k/
D
kmod13andh2.k/ 1 .kmod11/. Since14 1 .mod 13/and14 3 .mod11/,weinsert
D C  
thekey14intoemptyslot9,afterexaminingslots1and5andfindingthemtobeoccupied.
amounth .k/,modulom. Thus,unlikethecaseoflinearorquadratic probing, the
2
probe sequence here depends in two ways upon the key k, since the initial probe
position, the offset, or both, may vary. Figure 11.5 gives an example of insertion
bydouble hashing.
Thevalue h .k/ mustbe relatively prime tothe hash-table size mfor the entire
2
hash table to besearched. (SeeExercise 11.4-4.) Aconvenient waytoensure this
conditionistoletmbeapowerof2andtodesignh sothatitalwaysproducesan
2
odd number. Another way is to let m be prime and to design h so that it always
2
returns apositive integer lessthan m. Forexample, wecouldchoose mprimeand
let
h .k/ k mod m;
1
D
h .k/ 1 .k mod m/;
2 0
D C
where m is chosen to be slightly less than m (say, m 1). For example, if
0
(cid:0)
k 123456,m 701, and m 700, wehave h .k/ 80and h .k/ 257, so
0 1 2
D D D D D
that wefirstprobe position 80, and then weexamine every257th slot (modulo m)
untilwefindthekeyorhaveexaminedeveryslot.
When m is prime or a power of 2, double hashing improves over linear or qua-
dratic probing in that ‚.m2/ probe sequences are used, rather than ‚.m/, since
eachpossible .h .k/;h .k//pair yields adistinct probe sequence. Asaresult, for
1 2
274 Chapter11 HashTables
such values of m, the performance of double hashing appears to be very close to
theperformance ofthe“ideal” schemeofuniform hashing.
Although values of m other than primes or powers of 2 could in principle be
used with double hashing, in practice it becomes more difficult to efficiently gen-
erateh .k/inawaythatensuresthatitisrelativelyprimetom,inpartbecausethe
2
relativedensity .m/=mofsuchnumbersmaybesmall(seeequation (31.24)).
Analysisofopen-addresshashing
Asinouranalysisofchaining,weexpressouranalysisofopenaddressinginterms
ofthe loadfactor ˛ n=m ofthehash table. Ofcourse, withopen addressing, at
D
mostoneelementoccupies eachslot,andthusn m,whichimplies˛ 1.
 
We assume that we are using uniform hashing. In this idealized scheme, the
probe sequence h.k; 0/; h.k; 1/; :::; h.k; m 1/ used to insert or search for
h (cid:0) i
eachkeyk isequallylikelytobeanypermutation of 0;1;:::;m 1 . Ofcourse,
h (cid:0) i
a given key has a unique fixed probe sequence associated with it; what we mean
here is that, considering the probability distribution on the space of keys and the
operationofthehashfunctiononthekeys,eachpossibleprobesequenceisequally
likely.
Wenowanalyze theexpected number ofprobes forhashing withopen address-
ing under the assumption of uniform hashing, beginning with an analysis of the
numberofprobesmadeinanunsuccessful search.
Theorem11.6
Given an open-address hash table with load factor ˛ n=m < 1, the expected
D
numberofprobesinanunsuccessfulsearchisatmost1=.1 ˛/,assuminguniform
(cid:0)
hashing.
Proof In an unsuccessful search, every probe but the last accesses an occupied
slot thatdoes notcontain thedesired key, andthelast slotprobed isempty. Letus
definetherandom variable X tobethenumberofprobes madeinanunsuccessful
search, and let us also define the event A , for i 1;2;:::, to be the event that
i
D
an ith probe occurs and it is to an occupied slot. Then the event X i is the
f  g
intersectionofeventsA A A . WewillboundPr X i bybounding
1 2 i 1
Pr A A A \ . By\ E x e\ rcise(cid:0) C.2-5, f  g
1 2 i 1
f \ \\ (cid:0) g
Pr A A A Pr A Pr A A Pr A A A
1 2 i 1 1 2 1 3 1 2
f \ \\ (cid:0) gD f g f j g f j \ g
Pr A A A A :
i 1 1 2 i 2
f (cid:0) j \ \\ (cid:0) g
Sincetherearenelementsandmslots,Pr A n=m. Forj > 1,theprobability
1
f g D
that there is a jth probe and it is to an occupied slot, given that the first j 1
(cid:0)
probesweretooccupiedslots,is.n j 1/=.m j 1/. Thisprobabilityfollows
(cid:0) C (cid:0) C
11.4 Openaddressing 275
because wewould be finding one of the remaining .n .j 1// elements in one
(cid:0) (cid:0)
ofthe.m .j 1//unexaminedslots,andbytheassumptionofuniformhashing,
(cid:0) (cid:0)
the probability is the ratio of these quantities. Observing that n < m implies that
.n j/=.m j/ n=mforallj suchthat0 j < m,wehaveforalli suchthat
(cid:0) (cid:0)  
1 i m,
 
n n 1 n 2 n i 2
Pr X i (cid:0) (cid:0) (cid:0) C
f  g D m  m 1  m 2 m i 2
(cid:0) (cid:0) (cid:0) C
n i 1
(cid:0)
 m
 ˛i 1 :
(cid:0)
D
Now,weuseequation (C.25)toboundtheexpectednumberofprobes:
1
EŒX Pr X i
D f  g
i 1
XD
1
˛i 1
(cid:0)

i 1
XD
1
˛i
D
i 0
XD
1
:
D 1 ˛
(cid:0)
Thisboundof1=.1 ˛/ 1 ˛ ˛2 ˛3 hasanintuitiveinterpretation.
(cid:0) D C C C C
Wealways makethe firstprobe. With probability approximately ˛,the firstprobe
finds an occupied slot, so that we need to probe a second time. With probability
approximately ˛2, the first two slots are occupied so that we make a third probe,
andsoon.
If˛isaconstant,Theorem11.6predictsthatanunsuccessfulsearchrunsinO.1/
time. Forexample,ifthehashtableishalffull,theaveragenumberofprobesinan
unsuccessful search is atmost 1=.1 :5/ 2. If itis 90percent full, the average
(cid:0) D
numberofprobesisatmost1=.1 :9/ 10.
(cid:0) D
Theorem11.6givesustheperformance ofthe HASH-INSERT procedure almost
immediately.
Corollary11.7
Inserting anelementintoanopen-address hashtablewithloadfactor˛ requires at
most1=.1 ˛/probesonaverage, assuminguniformhashing.
(cid:0)
276 Chapter11 HashTables
Proof An element is inserted only if there is room in the table, and thus ˛ < 1.
Insertingakeyrequiresanunsuccessfulsearchfollowedbyplacingthekeyintothe
firstemptyslotfound. Thus,theexpectednumberofprobesisatmost1=.1 ˛/.
(cid:0)
Wehavetodoalittle moreworktocompute theexpected numberofprobes for
asuccessful search.
Theorem11.8
Givenan open-address hash table withload factor ˛ < 1, the expected number of
probesinasuccessful searchisatmost
1 1
ln ;
˛ 1 ˛
(cid:0)
assuminguniformhashingandassumingthateachkeyinthetableisequallylikely
tobesearched for.
Proof A search for a key k reproduces the same probe sequence as when the
element with key k was inserted. By Corollary 11.7, if k was the .i 1/st key
C
inserted intothehashtable, theexpected number ofprobes madeinasearch fork
isatmost1=.1 i=m/ m=.m i/. Averaging overallnkeysinthehashtable
(cid:0) D (cid:0)
givesustheexpectednumberofprobesinasuccessful search:
n 1 n 1
1 (cid:0) m m (cid:0) 1
n m i D n m i
i 0 (cid:0) i 0 (cid:0)
XD XD
m
1 1
D ˛ k
k m n 1
DX(cid:0) C
1 m
.1=x/dx (byinequality (A.12))
 ˛
Zm n
1 (cid:0) m
ln
D ˛ m n
(cid:0)
1 1
ln :
D ˛ 1 ˛
(cid:0)
Ifthehashtableishalffull,theexpectednumberofprobesinasuccessfulsearch
islessthan1:387. Ifthehashtableis90percentfull,theexpectednumberofprobes
islessthan2:559.
11.5 Perfecthashing 277
Exercises
11.4-1
Consider inserting the keys 10;22;31;4;15;28;17;88;59 into a hash table of
length m 11using openaddressing withtheauxiliary hashfunction h.k/ k.
0
D D
Illustrate the result of inserting these keys using linear probing, using quadratic
probing with c 1 and c 3, and using double hashing with h .k/ k and
1 2 1
D D D
h .k/ 1 .k mod.m 1//.
2
D C (cid:0)
11.4-2
Write pseudocode for HASH-DELETE as outlined in the text, and modify HASH-
INSERT tohandlethespecialvalue DELETED.
11.4-3
Consider an open-address hash table with uniform hashing. Give upper bounds
on the expected number of probes in an unsuccessful search and on the expected
number of probes in a successful search when the load factor is 3=4 and when it
is7=8.
11.4-4 ?
Supposethatweusedoublehashingtoresolvecollisions—that is,weusethehash
function h.k;i/ .h .k/ ih .k// mod m. Show that if m and h .k/ have
1 2 2
D C
greatest common divisor d 1 for some key k, then an unsuccessful search for

key k examines .1=d/th of the hash table before returning to slot h .k/. Thus,
1
whend 1,sothatmandh .k/arerelativelyprime,thesearchmayexaminethe
2
D
entirehashtable. (Hint:SeeChapter31.)
11.4-5 ?
Consideranopen-addresshashtablewithaloadfactor˛. Findthenonzerovalue˛
for which the expected number of probes in an unsuccessful search equals twice
theexpected numberofprobesinasuccessful search. Usetheupperboundsgiven
byTheorems11.6and11.8fortheseexpectednumbersofprobes.
? 11.5 Perfect hashing
Although hashing is often a good choice for its excellent average-case perfor-
mance,hashingcanalsoprovideexcellentworst-caseperformancewhenthesetof
keys is static: once the keys are stored in the table, the set of keys never changes.
Some applications naturally have static sets of keys: consider the set of reserved
words in a programming language, or the set of file names on a CD-ROM. We
278 Chapter11 HashTables
S
T m a b 0
0 0 0
0 1 0 0 10
1 m a b 0 S 2
2 2 2
2 9 10 18 60 72 75
3 0 1 2 3 4 5 6 7 8
S
4 m a b 5
5 5 5
5 1 0 0 70
6 m a b 0 S 7
7 7 7
7 16 23 88 40 52 22 37
8 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Figure11.6 Using perfect hashing tostore theset K 10;22;37;40;52;60;70;72;75 . The
D f g
outer hashfunction ish.k/ ..ak b/modp/modm, wherea 3, b 42, p 101, and
D C D D D
m 9. For example, h.75/ 2, and so key 75 hashes to slot 2 of table T. A secondary hash
tablD eS
j
storesallkeyshashingD toslotj. ThesizeofhashtableS
j
ism
j
Dn j2,andtheassociated
hashfunctionish j.k/ ..a jk b j/modp/modm j.Sinceh2.75/ 7,key75isstoredinslot7
D C D
ofsecondaryhashtableS2.Nocollisionsoccurinanyofthesecondaryhashtables,andsosearching
takesconstanttimeintheworstcase.
callahashing technique perfecthashingifO.1/memoryaccesses arerequired to
perform asearchintheworstcase.
Tocreateaperfecthashingscheme,weusetwolevelsofhashing,withuniversal
hashing ateachlevel. Figure11.6illustrates theapproach.
The first level is essentially the same as for hashing with chaining: we hash
thenkeysintomslots usingahash function hcarefully selected fromafamilyof
universal hashfunctions.
Instead ofmaking alinked list ofthe keys hashing to slot j, however, weuse a
small secondary hash table S with an associated hash function h . By choosing
j j
thehashfunctions h carefully, wecanguaranteethattherearenocollisions atthe
j
secondary level.
Inordertoguaranteethattherearenocollisionsatthesecondarylevel,however,
wewill need to let the size m of hash table S be the square of the number n of
j j j
keys hashing to slot j. Although you might think that the quadratic dependence
ofm onn mayseemlikely tocausetheoverallstorage requirement tobeexces-
j j
sive,weshallshowthatbychoosingthefirst-levelhashfunctionwell,wecanlimit
theexpected totalamountofspaceusedtoO.n/.
We use hash functions chosen from the universal classes of hash functions of
H
Section 11.3.3. The first-level hash function comes from the class , where as
pm
in Section 11.3.3, p is a prime number greater than any key value. Those keys
11.5 Perfecthashing 279
hashing to slot j are re-hashed into a secondary hash table S of size m using a
j j
hashfunction h chosenfromtheclassH .1
j p;mj
We shall proceed in two steps. First, we shall determine how to ensure that
the secondary tables have no collisions. Second, we shall show that the expected
amountofmemoryusedoverall—for theprimaryhashtableandallthesecondary
hashtables—isO.n/.
Theorem11.9
Supposethatwestorenkeysinahashtableofsizem n2usingahashfunctionh
D
randomlychosenfromauniversal classofhashfunctions. Then,theprobability is
lessthan1=2thatthereareanycollisions.
n
Proof Thereare pairs ofkeys that maycollide; each pair collides with prob-
2
ability1=mifhischosenatrandom fromauniversal familyH ofhashfunctions.
(cid:0) 
Let X be a random variable that counts the number of collisions. When m n2,
D
theexpectednumberofcollisions is
n 1
EŒX
D 2  n2
!
n2 n 1
(cid:0)
D 2  n2
< 1=2:
(This analysis is similar to the analysis of the birthday paradox in Section 5.4.1.)
Applying Markov’s inequality (C.30), Pr X t EŒX=t, with t 1, com-
f  g  D
pletestheproof.
InthesituationdescribedinTheorem11.9,wherem n2,itfollowsthatahash
function hchosen atrandom fromH ismorelikelythaD nnottohavenocollisions.
Giventhe setK ofnkeystobehashed (remember thatK isstatic), itisthus easy
tofindacollision-free hashfunction hwithafewrandom trials.
When nis large, however, ahash table ofsize m n2 is excessive. Therefore,
D
weadoptthetwo-levelhashingapproach,andweusetheapproachofTheorem11.9
only to hash the entries within each slot. We use an outer, or first-level, hash
function h to hash the keys into m n slots. Then, if n keys hash to slot j, we
j
D
use a secondary hash table S of size m n2 to provide collision-free constant-
j j D j
timelookup.
1When n j m j 1, we don’t really need a hash function for slot j; when we choose a hash
D D
functionh ab.k/ ..ak b/modp/modm
j
forsuchaslot,wejustusea b 0.
D C D D
280 Chapter11 HashTables
We now turn to the issue of ensuring that the overall memory used is O.n/.
Since the size m of the jth secondary hash table grows quadratically with the
j
number n ofkeysstored, weruntheriskthattheoverallamount ofstorage could
j
beexcessive.
If the first-level table size is m n, then the amount of memory used is O.n/
D
for the primary hash table, for the storage of the sizes m of the secondary hash
j
tables, andforthestorageoftheparametersa andb definingthesecondary hash
j j
functions h drawn from the class H of Section 11.3.3 (except when n 1
j p;mj j
D
andweusea b 0). Thefollowingtheoremandacorollaryprovideaboundon
D D
the expected combined sizes of all the secondary hash tables. A second corollary
bounds the probability that the combined size of all the secondary hash tables is
superlinear (actually, thatitequalsorexceeds4n).
Theorem11.10
Supposethatwestorenkeysinahashtableofsizem nusingahashfunctionh
D
randomlychosen fromauniversal classofhashfunctions. Then,wehave
m 1
(cid:0)
E n2 < 2n;
j
" #
j 0
XD
wheren isthenumberofkeyshashingtoslotj.
j
Proof Westartwiththefollowingidentity,whichholdsforanynonnegativeinte-
gera:
a
a2 a 2 : (11.6)
D C 2
!
Wehave
m 1
(cid:0)
E n2
j
" #
j 0
XD
m 1
(cid:0) n j
E n 2 (byequation (11.6))
j
D C 2
" !!#
j 0
XD
m 1 m 1
E (cid:0) n 2E (cid:0) n j (bylinearity ofexpectation)
j
D C 2
" # " !#
j 0 j 0
XD XD
m 1
(cid:0) n j
EŒn 2E (byequation (11.1))
D C 2
" !#
j 0
XD
11.5 Perfecthashing 281
m 1
(cid:0) n j
n 2E (sincenisnotarandom variable) .
D C 2
" !#
j 0
XD
Toevaluatethesummation jm (cid:0)01 n 2j ,weobservethatitisjustthetotalnumber
ofpairsofkeysinthehashtablethDatcollide. Bythepropertiesofuniversalhashing,
P (cid:0) 
theexpectedvalueofthissummationisatmost
n 1 n.n 1/
(cid:0)
2 m D 2m
!
n 1
(cid:0) ;
D 2
sincem n. Thus,
D
m 1
(cid:0) n 1
E n2 n 2 (cid:0)
j  C 2
" #
j 0
XD
2n 1
D (cid:0)
< 2n:
Corollary11.11
Suppose that we store n keys in a hash table of size m n using a hash func-
D
tion h randomly chosen from a universal class of hash functions, and we set the
size of each secondary hash table to m n2 for j 0;1;:::;m 1. Then,
j D j D (cid:0)
the expected amount of storage required for all secondary hash tables in a perfect
hashingschemeislessthan2n.
Proof Sincem n2 forj 0;1;:::;m 1,Theorem11.10gives
j D j D (cid:0)
m 1 m 1
(cid:0) (cid:0)
E m E n2
j D j
" # " #
j 0 j 0
XD XD
< 2n; (11.7)
whichcompletestheproof.
Corollary11.12
Supposethatwestorenkeysinahashtableofsizem nusingahashfunctionh
D
randomly chosen from a universal class of hash functions, and we set the size
of each secondary hash table to m n2 for j 0;1;:::;m 1. Then, the
j D j D (cid:0)
probability is less than 1=2 that the total storage used for secondary hash tables
equalsorexceeds4n.
282 Chapter11 HashTables
Proof Again we apply Markov’s inequality (C.30), Pr X t EŒX=t, this
timetoinequality (11.7), withX
D
jm (cid:0)01m
j
andt
D
4nf :  g 
D
Pr
m (cid:0)1
m
j
4n
E
jm D(cid:0)01mP
j
  4n
(
j 0
) P 
XD
2n
<
4n
1=2:
D
From Corollary 11.12, wesee that ifwetest afew randomly chosen hash func-
tions from the universal family, we will quickly find one that uses a reasonable
amountofstorage.
Exercises
11.5-1 ?
Suppose that we insert n keys into a hash table of size m using open addressing
anduniformhashing. Letp.n;m/betheprobabilitythatnocollisionsoccur. Show
that p.n;m/ e n.n 1/=2m. (Hint: See equation (3.12).) Argue that when n ex-
(cid:0) (cid:0)

ceedspm,theprobability ofavoiding collisions goesrapidly tozero.
Problems
11-1 Longest-probe boundforhashing
Suppose that we use an open-addressed hash table of size m to store n m=2

items.
a. Assuming uniform hashing, show that for i 1;2;:::;n, theprobability isat
D
most2 k thattheithinsertionrequires strictlymorethank probes.
(cid:0)
b. Showthatfori 1;2;:::;n, theprobability isO.1=n2/that theithinsertion
D
requiresmorethan2lgnprobes.
Lettherandom variableX denote thenumberofprobes required bytheithinser-
i
tion. Youhaveshowninpart(b)thatPr X > 2lgn O.1=n2/. Lettherandom
i
f g D
variable X max X denote the maximum number of probes required by
1 i n i
anyofthenD insertion s.
c. ShowthatPr X > 2lgn O.1=n/.
f g D
d. Showthattheexpected lengthEŒXofthelongestprobesequenceisO.lgn/.
ProblemsforChapter11 283
11-2 Slot-sizeboundforchaining
Suppose that wehave a hash table withn slots, with collisions resolved bychain-
ing, andsuppose that nkeys are inserted into the table. Eachkey isequally likely
tobehashedtoeachslot. LetM bethemaximumnumberofkeysinanyslotafter
allthekeyshavebeen inserted. Yourmission istoproveanO.lgn=lglgn/upper
boundonEŒM,theexpected valueofM.
a. Argue that the probability Q that exactly k keys hash to a particular slot is
k
givenby
1 k 1 n k n
(cid:0)
Q 1 :
k
D n (cid:0) n k
!
   
b. Let P be the probability that M k, that is, the probability that the slot
k
D
containing themostkeyscontains k keys. ShowthatP nQ .
k k

c. UseStirling’s approximation, equation (3.18),toshowthatQ < ek=kk.
k
d. Show that there exists a constant c > 1 such that Q < 1=n3 for k
k0 0
D
clgn=lglgn. ConcludethatP < 1=n2 fork k clgn=lglgn.
k 0
 D
e. Arguethat
clgn clgn clgn
EŒM Pr M > n Pr M :
 lglgn  C  lglgn  lglgn
   
ConcludethatEŒM O.lgn=lglgn/.
D
11-3 Quadraticprobing
Suppose that we are given a key k to search for in a hash table with positions
0;1;:::;m 1,andsupposethatwehaveahashfunctionhmappingthekeyspace
(cid:0)
intotheset 0;1;:::;m 1 . Thesearchschemeisasfollows:
f (cid:0) g
1. Computethevaluej h.k/,andseti 0.
D D
2. Probe in position j for the desired key k. If you find it, or if this position is
empty,terminatethesearch.
3. Set i i 1. If i now equals m, the table is full, so terminate the search.
D C
Otherwise,setj .i j/ mod m,andreturntostep2.
D C
Assumethatmisapowerof2.
a. Showthatthisschemeisaninstanceofthegeneral“quadraticprobing”scheme
byexhibiting theappropriate constants c andc forequation(11.5).
1 2
b. Provethatthisalgorithm examineseverytableposition intheworstcase.
284 Chapter11 HashTables
11-4 Hashingandauthentication
Let H be a class of hash functions in which each hash function h H maps the
universeU ofkeysto 0;1;:::;m 1 . WesaythatH isk-univers2 alif,forevery
f (cid:0) g
fixed sequence of k distinct keys x.1/; x.2/; :::; x.k/ and for any h chosen at
randomfromH ,thesequence h.xh .1//;h.x.2//;:::;h.xi .k// isequallylikelytobe
h i
anyofthemk sequences oflengthk withelementsdrawnfrom 0;1;:::;m 1 .
f (cid:0) g
a. ShowthatifthefamilyH ofhashfunctions is2-universal, thenitisuniversal.
b. Suppose that the universe U is the set of n-tuples of values drawn from
Z 0;1;:::;p 1 , where p is prime. Consider an element x
p
D f (cid:0) g D
x ;x ;:::;x U. For any n-tuple a a ; a ; :::; a U, de-
0 1 n 1 0 1 n 1
fih nethehashfu(cid:0) nci tio2 n h by D h (cid:0) i 2
a
n 1
(cid:0)
h .x/ a x mod p :
a j j
D
!
j 0
XD
LetH h . ShowthatH isuniversal,butnot2-universal. (Hint:Findakey
a
D f g H
forwhichallhashfunctions in producethesamevalue.)
c. Suppose that we modify H slightly from part (b): for any a U and for any
b Z ,define 2
p
2
n 1
(cid:0)
h .x/ a x b mod p
0ab
D
j j
C
!
j 0
XD
and H h . Argue that H is 2-universal. (Hint: Consider fixed n-tuples
0
D f
0abg 0
x U and y U, with x y for some i. What happens to h .x/
and2 h .y/asa 2 andb
rangeoi
ve¤ rZ
i
?)
0ab
0ab i p
d. Suppose that Alice and Bob secretly agree on a hash function h from a
2-universal family H ofhash functions. Each h H mapsfrom auniverse of
keysU toZ ,wherepisprime. Later,Alicesend2 samessagemtoBoboverthe
p
Internet, wherem U. Sheauthenticates thismessagetoBobbyalsosending
2
anauthentication tagt h.m/,andBobchecksthatthepair.m;t/hereceives
D
indeedsatisfiest h.m/. Supposethatanadversary intercepts .m;t/enroute
D
and tries to fool Bob by replacing the pair .m;t/ with a different pair .m;t /.
0 0
Argue that the probability that the adversary succeeds in fooling Bob into ac-
cepting .m;t / is at most 1=p, no matter how much computing power the ad-
0 0
H
versary has, and even if the adversary knows the family of hash functions
used.
NotesforChapter11 285
Chapter notes
Knuth [211] and Gonnet [145] are excellent references for the analysis of hash-
ing algorithms. Knuth credits H. P. Luhn (1953) for inventing hash tables, along
with the chaining method for resolving collisions. At about the same time, G. M.
Amdahloriginated theideaofopenaddressing.
CarterandWegmanintroduced thenotionofuniversalclassesofhashfunctions
in1979[58].
Fredman, Komlo´s, and Szemere´di [112] developed the perfect hashing scheme
forstatic sets presented inSection 11.5. Anextension oftheirmethod todynamic
sets, handling insertions and deletions in amortized expected time O.1/, has been
givenbyDietzfelbinger etal.[86].
12 Binary Search Trees
The search tree data structure supports many dynamic-set operations, including
SEARCH, MINIMUM, MAXIMUM, PREDECESSOR, SUCCESSOR, INSERT, and
DELETE. Thus, we can use a search tree both as a dictionary and as a priority
queue.
Basic operations on a binary search tree take time proportional to the height of
the tree. For a complete binary tree with n nodes, such operations run in ‚.lgn/
worst-case time. If the tree is a linear chain of n nodes, however, the same oper-
ations take ‚.n/ worst-case time. We shall see in Section 12.4 that the expected
height ofarandomly built binary search treeisO.lgn/,so thatbasic dynamic-set
operations onsuchatreetake‚.lgn/timeonaverage.
In practice, we can’t always guarantee that binary search trees are built ran-
domly, but we can design variations of binary search trees with good guaranteed
worst-case performance on basic operations. Chapter 13 presents one such vari-
ation, red-black trees, which have height O.lgn/. Chapter 18 introduces B-trees,
whichareparticularlygoodformaintainingdatabasesonsecondary(disk)storage.
After presenting the basic properties of binary search trees, the following sec-
tionsshowhowtowalkabinarysearchtreetoprintitsvaluesinsortedorder, how
tosearchforavalueinabinarysearchtree,howtofindtheminimumormaximum
element, howtofindthepredecessor orsuccessor ofanelement,andhowtoinsert
intoordeletefromabinarysearchtree. Thebasicmathematicalpropertiesoftrees
appearinAppendixB.
12.1 Whatis abinary searchtree?
Abinarysearch treeisorganized, asthenamesuggests, inabinary tree, asshown
in Figure 12.1. We can represent such a tree by a linked data structure in which
each node is an object. In addition to a key and satellite data, each node contains
attributes left, right, and p that point to the nodes corresponding to its left child,
12.1 Whatisabinarysearchtree? 287
6 2
5 7 5
2 5 8 7
6 8
5
(a) (b)
Figure12.1 Binarysearchtrees.Foranynodex,thekeysintheleftsubtreeofxareatmostx:key,
andthekeysintherightsubtreeofx areatleastx:key. Differentbinarysearchtreescanrepresent
thesamesetofvalues. Theworst-caserunningtimeformostsearch-treeoperationsisproportional
totheheightofthetree.(a)Abinarysearchtreeon6nodeswithheight2.(b)Alessefficientbinary
searchtreewithheight4thatcontainsthesamekeys.
its right child, and its parent, respectively. If a child or the parent is missing, the
appropriate attribute contains the value NIL. Theroot node isthe only node inthe
treewhoseparentisNIL.
Thekeysinabinarysearchtreearealwaysstoredinsuchawayastosatisfythe
binary-search-tree property:
Let x be a node in a binary search tree. If y is a node in the left subtree
of x, then y:key x:key. If y is a node in the right subtree of x, then

y:key x:key.

Thus, in Figure 12.1(a), the key of the root is 6, the keys 2, 5, and 5 in its left
subtreearenolargerthan6,andthekeys7and8initsrightsubtreearenosmaller
than6. Thesamepropertyholdsforeverynodeinthetree. Forexample,thekey5
intheroot’sleftchildisnosmallerthanthekey2inthatnode’sleftsubtreeandno
largerthanthekey5intherightsubtree.
The binary-search-tree property allows us to print out all the keys in a binary
search tree in sorted order by a simple recursive algorithm, called an inorder tree
walk. Thisalgorithm issonamedbecause itprints thekeyoftheroot ofasubtree
betweenprintingthevaluesinitsleftsubtreeandprintingthoseinitsrightsubtree.
(Similarly, apreorder tree walk prints the root before thevalues in either subtree,
and a postorder tree walk prints the root after the values in its subtrees.) To use
thefollowing procedure toprintalltheelements inabinary search treeT,wecall
INORDER-TREE-WALK.T:root/.
288 Chapter12 BinarySearchTrees
INORDER-TREE-WALK.x/
1 ifx NIL
¤
2 INORDER-TREE-WALK.x:left/
3 printx:key
4 INORDER-TREE-WALK.x:right/
As an example, the inorder tree walk prints the keys in each of the two binary
search trees from Figure 12.1 in the order 2;5;5;6;7;8. The correctness of the
algorithm followsbyinduction directlyfromthebinary-search-tree property.
It takes ‚.n/ time to walk an n-node binary search tree, since after the ini-
tial call, the procedure calls itself recursively exactly twice for each node in the
tree—once for its left child and once for its right child. The following theorem
givesaformalproofthatittakeslineartimetoperformaninordertreewalk.
Theorem12.1
If x is the root of an n-node subtree, then the call INORDER-TREE-WALK.x/
takes‚.n/time.
Proof Let T.n/ denote the time taken by INORDER-TREE-WALK when it is
called ontherootofann-node subtree. Since INORDER-TREE-WALK visits alln
nodesofthesubtree,wehaveT.n/ .n/. ItremainstoshowthatT.n/ O.n/.
D D
Since INORDER-TREE-WALK takes a small, constant amount of time on an
emptysubtree (forthetestx NIL),wehaveT.0/ c forsomeconstant c > 0.
¤ D
For n > 0, suppose that INORDER-TREE-WALK is called on a node x whose
leftsubtreehask nodesandwhoserightsubtreehasn k 1nodes. Thetimeto
(cid:0) (cid:0)
performINORDER-TREE-WALK.x/isboundedbyT.n/ T.k/ T.n k 1/ d
 C (cid:0) (cid:0) C
for some constant d > 0 that reflects an upper bound on the time to execute the
bodyofINORDER-TREE-WALK.x/,exclusiveofthetimespentinrecursive calls.
We use the substitution method to show that T.n/ O.n/ by proving that
D
T.n/ .c d/n c. Forn 0,wehave.c d/ 0 c c T.0/. Forn > 0,
 C C D C  C D D
wehave
T.n/ T.k/ T.n k 1/ d
 C (cid:0) (cid:0) C
..c d/k c/ ..c d/.n k 1/ c/ d
D C C C C (cid:0) (cid:0) C C
.c d/n c .c d/ c d
D C C (cid:0) C C C
.c d/n c ;
D C C
whichcompletes theproof.
12.2 Queryingabinarysearchtree 289
Exercises
12.1-1
Forthesetof 1;4;5;10;16;17;21 ofkeys,drawbinarysearchtreesofheights2,
f g
3,4,5,and6.
12.1-2
What is the difference between the binary-search-tree property and the min-heap
property (see page 153)? Canthe min-heap property be used to print out the keys
ofann-nodetreeinsortedorderinO.n/time? Showhow,orexplainwhynot.
12.1-3
Givea nonrecursive algorithm that performs an inorder tree walk. (Hint: Aneasy
solution uses a stack as an auxiliary data structure. A more complicated, but ele-
gant,solutionusesnostackbutassumesthatwecantesttwopointersforequality.)
12.1-4
Giverecursive algorithms that perform preorder and postorder tree walks in ‚.n/
timeonatreeofnnodes.
12.1-5
Argue that since sorting n elements takes .nlgn/ time in the worst case in
the comparison model, any comparison-based algorithm for constructing a binary
search tree from an arbitrary list of n elements takes .nlgn/ time in the worst
case.
12.2 Querying abinary searchtree
We often need to search for a key stored in a binary search tree. Besides the
SEARCH operation, binary search trees can support such queries as MINIMUM,
MAXIMUM, SUCCESSOR, and PREDECESSOR. In this section, we shall examine
these operations and show how to support each one in time O.h/ on any binary
searchtreeofheighth.
Searching
We use the following procedure to search for a node with a given key in a binary
search tree. Given a pointer to the root of the tree and a key k, TREE-SEARCH
returnsapointertoanodewithkeyk ifoneexists;otherwise, itreturns NIL.
290 Chapter12 BinarySearchTrees
15
6 18
3 7 17 20
2 4 13
9
Figure12.2 Queriesonabinarysearchtree.Tosearchforthekey13inthetree,wefollowthepath
15 6 7 13fromtheroot. Theminimumkeyinthetreeis2,whichisfoundbyfollowing
! ! !
leftpointersfromtheroot.Themaximumkey20isfoundbyfollowingrightpointersfromtheroot.
Thesuccessorofthenodewithkey15isthenodewithkey17,sinceitistheminimumkeyinthe
rightsubtreeof15. Thenodewithkey13hasnorightsubtree,andthusitssuccessorisitslowest
ancestorwhoseleftchildisalsoanancestor.Inthiscase,thenodewithkey15isitssuccessor.
TREE-SEARCH.x;k/
1 ifx == NIL ork == x:key
2 returnx
3 ifk < x:key
4 return TREE-SEARCH.x:left;k/
5 elsereturn TREE-SEARCH.x:right;k/
The procedure begins its search at the root and traces a simple path downward in
the tree, as shown in Figure 12.2. For each node x it encounters, it compares the
key k with x:key. If the two keys are equal, the search terminates. If k is smaller
than x:key, the search continues in the left subtree of x, since the binary-search-
treepropertyimpliesthatkcouldnotbestoredintherightsubtree. Symmetrically,
if k is larger than x:key, the search continues in the right subtree. The nodes
encountered during the recursion form a simple path downward from the root of
thetree,andthustherunningtimeofTREE-SEARCH isO.h/,wherehistheheight
ofthetree.
Wecanrewritethisprocedureinaniterativefashionby“unrolling”therecursion
intoawhileloop. Onmostcomputers, theiterativeversionismoreefficient.
12.2 Queryingabinarysearchtree 291
ITERATIVE-TREE-SEARCH.x;k/
1 whilex NIL andk x:key
¤ ¤
2 ifk <x:key
3 x x:left
D
4 elsex x:right
D
5 returnx
Minimumandmaximum
Wecanalwaysfindanelementinabinarysearchtreewhosekeyisaminimumby
following left child pointers from the root until we encounter a NIL, as shown in
Figure12.2. Thefollowingprocedure returnsapointertotheminimumelementin
thesubtreerootedatagivennodex,whichweassumetobenon-NIL:
TREE-MINIMUM.x/
1 whilex:left NIL
¤
2 x x:left
D
3 returnx
The binary-search-tree property guarantees that TREE-MINIMUM is correct. If a
nodexhasnoleftsubtree,thensinceeverykeyintherightsubtreeofxisatleastas
large asx:key,theminimumkeyinthesubtree rooted atx isx:key. Ifnodex has
aleftsubtree,thensincenokeyintherightsubtreeissmallerthanx:keyandevery
key in the left subtree is not larger than x:key, the minimum key in the subtree
rootedatx residesinthesubtreerootedatx:left.
Thepseudocode forTREE-MAXIMUM issymmetric:
TREE-MAXIMUM.x/
1 whilex:right NIL
¤
2 x x:right
D
3 returnx
Bothoftheseprocedures runinO.h/timeonatreeofheighthsince,asinTREE-
SEARCH,thesequence ofnodesencountered formsasimplepathdownwardfrom
theroot.
Successorandpredecessor
Given a node in a binary search tree, sometimes we need to find its successor in
the sorted order determined by an inorder tree walk. If all keys are distinct, the
292 Chapter12 BinarySearchTrees
successor of a node x is the node with the smallest key greater than x:key. The
structure of a binary search tree allows us to determine the successor of a node
without ever comparing keys. The following procedure returns the successor of a
node x in a binary search tree if it exists, and NIL if x has the largest key in the
tree:
TREE-SUCCESSOR.x/
1 ifx:right NIL
¤
2 return TREE-MINIMUM.x:right/
3 y x:p
D
4 whiley NIL andx == y:right
¤
5 x y
D
6 y y:p
D
7 returny
We break the code for TREE-SUCCESSOR into two cases. If the right subtree
of node x is nonempty, then the successor of x is just the leftmost node in x’s
right subtree, which we find in line 2 by calling TREE-MINIMUM.x:right/. For
example, the successor of the node with key 15 in Figure 12.2 is the node with
key17.
On the other hand, as Exercise 12.2-6 asks you to show, if the right subtree of
nodex isemptyandx hasasuccessor y,theny isthelowestancestor ofx whose
left child is also an ancestor of x. In Figure 12.2, the successor of the node with
key13isthenodewithkey15. Tofindy,wesimplygoupthetreefromx untilwe
encounteranodethatistheleftchildofitsparent;lines3–7ofTREE-SUCCESSOR
handlethiscase.
Therunning timeof TREE-SUCCESSOR onatreeofheight hisO.h/, sincewe
either follow asimple path up the tree or follow asimple path downthe tree. The
procedure TREE-PREDECESSOR, whichis symmetric to TREE-SUCCESSOR, also
runsintimeO.h/.
Even if keys are not distinct, we define the successor and predecessor of any
node x as the node returned by calls made to TREE-SUCCESSOR.x/ and TREE-
PREDECESSOR.x/, respectively.
Insummary,wehaveprovedthefollowingtheorem.
Theorem12.2
We can implement the dynamic-set operations SEARCH, MINIMUM, MAXIMUM,
SUCCESSOR, and PREDECESSOR so that each one runs in O.h/ time on a binary
searchtreeofheighth.
12.2 Queryingabinarysearchtree 293
Exercises
12.2-1
Supposethatwehavenumbersbetween1and1000inabinarysearchtree,andwe
wanttosearchforthenumber363. Whichofthefollowingsequencescouldnotbe
thesequenceofnodesexamined?
a. 2,252,401,398,330,344,397,363.
b. 924,220,911,244,898,258,362,363.
c. 925,202,911,240,912,245,363.
d. 2,399,387,219,266,382,381,278,363.
e. 935,278,347,621,299,392,358,363.
12.2-2
Writerecursive versionsof TREE-MINIMUM and TREE-MAXIMUM.
12.2-3
Writethe TREE-PREDECESSOR procedure.
12.2-4
ProfessorBunyanthinkshehasdiscoveredaremarkableproperty ofbinarysearch
trees. Suppose that the search for key k in a binary search tree ends up in a leaf.
Consider three sets: A, the keys to the left of the search path; B, the keys on the
search path; and C, the keys to the right of the search path. Professor Bunyan
claimsthatanythreekeysa A,b B,andc C mustsatisfya b c. Give
2 2 2  
asmallestpossiblecounterexample totheprofessor’s claim.
12.2-5
Showthatifanodeinabinary searchtreehastwochildren, thenitssuccessor has
noleftchildanditspredecessor hasnorightchild.
12.2-6
Consider a binary search tree T whose keys are distinct. Show that if the right
subtree of a node x in T is empty and x has a successor y, then y is the lowest
ancestor of x whose left child is also an ancestor of x. (Recall that every node is
itsownancestor.)
12.2-7
An alternative method of performing an inorder tree walk of an n-node binary
searchtreefindstheminimumelementinthetreebycalling TREE-MINIMUM and
then making n 1 calls to TREE-SUCCESSOR. Prove that this algorithm runs
(cid:0)
in‚.n/time.
294 Chapter12 BinarySearchTrees
12.2-8
Prove that no matter what node we start at in a height-h binary search tree, k
successive callsto TREE-SUCCESSOR takeO.k h/time.
C
12.2-9
LetT beabinarysearchtreewhosekeysaredistinct,letx bealeafnode,andlety
be its parent. Show that y:key is either the smallest key in T larger than x:key or
thelargest keyinT smallerthanx:key.
12.3 Insertion anddeletion
The operations of insertion and deletion cause the dynamic set represented by a
binary search tree to change. The data structure must be modified to reflect this
change, but in such a way that the binary-search-tree property continues to hold.
As we shall see, modifying the tree to insert a new element is relatively straight-
forward, buthandling deletionissomewhatmoreintricate.
Insertion
To insert a new value  into a binary search tree T, we use the procedure TREE-
INSERT. The procedure takes a node ´ for which ´:key , ´:left NIL,
D D
and´:right NIL. ItmodifiesT andsomeoftheattributesof´insuchawaythat
D
itinserts´intoanappropriate position inthetree.
TREE-INSERT.T;´/
1 y NIL
D
2 x T:root
D
3 whilex NIL
¤
4 y x
D
5 if´:key < x:key
6 x x:left
D
7 elsex x:right
D
8 ´:p y
D
9 ify == NIL
10 T:root ´ //treeT wasempty
D
11 elseif´:key < y:key
12 y:left ´
D
13 elsey:right ´
D
12.3 Insertionanddeletion 295
12
5 18
2 9 15 19
13 17
Figure12.3 Insertinganitemwithkey13intoabinarysearchtree.Lightlyshadednodesindicate
the simple path from the root down to the position where the item is inserted. The dashed line
indicatesthelinkinthetreethatisaddedtoinserttheitem.
Figure 12.3 shows how TREE-INSERT works. Just like the procedures TREE-
SEARCH and ITERATIVE-TREE-SEARCH, TREE-INSERT begins attherootofthe
tree and the pointer x traces asimple path downward looking for a NIL to replace
withtheinputitem´. Theproceduremaintainsthetrailingpointery astheparent
of x. After initialization, the while loop in lines 3–7 causes these two pointers
to move down the tree, going left or right depending on the comparison of ´:key
withx:key,untilx becomes NIL. ThisNIL occupiesthepositionwherewewishto
placetheinputitem´. Weneedthetrailing pointery,becausebythetimewefind
the NIL where ´belongs, the search has proceeded one step beyond the node that
needstobechanged. Lines8–13setthepointers thatcause´tobeinserted.
Liketheotherprimitiveoperationsonsearchtrees,theprocedure TREE-INSERT
runsinO.h/timeonatreeofheighth.
Deletion
The overall strategy for deleting a node ´ from a binary search tree T has three
basiccasesbut,asweshallsee,oneofthecasesisabittricky.
 If ´ has no children, then we simply remove it by modifying its parent to re-
place´withNIL asitschild.
 If´hasjustonechild,thenweelevatethatchildtotake´’spositioninthetree
bymodifying´’sparenttoreplace´by´’schild.
 If´hastwochildren, thenwefind´’ssuccessory—whichmustbein´’sright
subtree—and have y take ´’sposition inthe tree. Therest of´’soriginal right
subtree becomes y’s new right subtree, and ´’s left subtree becomes y’s new
left subtree. This case is the tricky one because, as we shall see, it matters
whethery is´’srightchild.
296 Chapter12 BinarySearchTrees
The procedure for deleting agiven node ´ from a binary search tree T takes as
argumentspointerstoT and´. Itorganizesitscasesabitdifferentlyfromthethree
casesoutlined previously byconsidering thefourcasesshowninFigure12.4.
 If´hasnoleftchild(part(a)ofthefigure),thenwereplace´byitsrightchild,
whichmayormaynotbeNIL. When´’srightchildisNIL,thiscasedealswith
the situation in which ´ has no children. When ´’s right child is non-NIL, this
casehandlesthesituationinwhich´hasjustonechild,whichisitsrightchild.
 If ´ has just one child, which is its left child (part (b) of the figure), then we
replace´byitsleftchild.
 Otherwise, ´ has both a left and a right child. We find ´’s successor y, which
liesin´’sright subtree andhasnoleftchild (seeExercise 12.2-5). Wewantto
splicey outofitscurrent locationandhaveitreplace´inthetree.
 If y is ´’s right child (part (c)), then we replace ´ by y, leaving y’s right
childalone.
 Otherwise, y lieswithin´’srightsubtree butisnot´’srightchild(part(d)).
In this case, wefirstreplace y byits ownright child, and then wereplace ´
byy.
In order to move subtrees around within the binary search tree, we define a
subroutine TRANSPLANT, which replaces onesubtree asachild ofitsparent with
another subtree. When TRANSPLANT replaces the subtree rooted at node u with
the subtree rooted at node , node u’s parent becomes node ’s parent, and u’s
parentendsuphaving  asitsappropriate child.
TRANSPLANT.T;u;/
1 ifu:p == NIL
2 T:root 
D
3 elseifu ==u:p:left
4 u:p:left 
D
5 elseu:p:right 
D
6 if NIL
¤
7 :p u:p
D
Lines1–2handle thecaseinwhichuistherootofT. Otherwise, uiseitheraleft
child or a right child of its parent. Lines 3–4 take care of updating u:p:left if u
is a left child, and line 5 updates u:p:right if u is a right child. We allow  to be
NIL, and lines 6–7 update :p if  is non-NIL. Note that TRANSPLANT does not
attempttoupdate:leftand:right;doingso,ornotdoingso,istheresponsibility
of TRANSPLANT’s caller.
12.3 Insertionanddeletion 297
q q
(a) z r
NIL r
q q
(b) z l
l NIL
q q
(c) z y
l y l x
NIL x
q q q
(d) z z y y
l r l NIL r l r
y x x
NIL x
Figure12.4 Deletinganode´fromabinarysearchtree. Node´maybetheroot,aleftchildof
nodeq,orarightchildofq. (a)Node´hasnoleftchild. Wereplace´byitsrightchildr,which
mayormaynotbeNIL.(b)Node´hasaleftchildlbutnorightchild.Wereplace´byl.(c)Node´
hastwochildren;itsleftchildisnodel,itsrightchildisitssuccessory,andy’srightchildisnodex.
Wereplace´byy,updatingy’sleftchildtobecomel,butleavingxasy’srightchild. (d)Node´
hastwochildren(leftchildlandrightchildr),anditssuccessory rlieswithinthesubtreerooted
¤
atr. Wereplacey byitsownrightchildx,andwesety tober’sparent. Then,wesety tobeq’s
childandtheparentofl.
298 Chapter12 BinarySearchTrees
With the TRANSPLANT procedure in hand, here is the procedure that deletes
node´frombinarysearchtreeT:
TREE-DELETE.T;´/
1 if´:left == NIL
2 TRANSPLANT.T;´;´:right/
3 elseif´:right == NIL
4 TRANSPLANT.T;´;´:left/
5 elsey TREE-MINIMUM.´:right/
D
6 ify:p ´
¤
7 TRANSPLANT.T;y;y:right/
8 y:right ´:right
D
9 y:right:p y
D
10 TRANSPLANT.T;´;y/
11 y:left ´:left
D
12 y:left:p y
D
The TREE-DELETE procedure executes the four cases as follows. Lines 1–2
handle thecaseinwhichnode ´hasnoleftchild, andlines3–4handle thecasein
which´hasaleftchildbutnorightchild. Lines5–12dealwiththeremainingtwo
cases, in which ´ has two children. Line 5 finds node y, which is the successor
of ´. Because ´ has a nonempty right subtree, its successor must be the node in
thatsubtreewiththesmallestkey;hencethecalltoTREE-MINIMUM.´:right/. As
wenotedbefore,y hasnoleftchild. Wewanttosplicey outofitscurrentlocation,
anditshouldreplace´inthetree. Ify is´’srightchild,thenlines10–12replace´
asachildofitsparentbyyandreplacey’sleftchildby´’sleftchild. Ifyisnot´’s
right child, lines 7–9 replace y as a child of its parent by y’s right child and turn
´’s right child into y’s right child, and then lines 10–12 replace ´ as a child of its
parentbyy andreplacey’sleftchildby´’sleftchild.
EachlineofTREE-DELETE, includingthecallstoTRANSPLANT,takesconstant
time, except for the call to TREE-MINIMUM in line 5. Thus, TREE-DELETE runs
inO.h/timeonatreeofheighth.
Insummary,wehaveprovedthefollowingtheorem.
Theorem12.3
We can implement the dynamic-set operations INSERT and DELETE so that each
onerunsinO.h/timeonabinarysearchtreeofheighth.
12.4 Randomlybuiltbinarysearchtrees 299
Exercises
12.3-1
Givearecursiveversionofthe TREE-INSERT procedure.
12.3-2
Supposethatweconstructabinarysearchtreebyrepeatedly insertingdistinctval-
ues into the tree. Argue that the number of nodes examined in searching for a
value in the tree is one plus the number of nodes examined when the value was
firstinsertedintothetree.
12.3-3
Wecansortagivensetofnnumbersbyfirstbuilding abinarysearchtreecontain-
ing these numbers (using TREE-INSERT repeatedly to insert the numbers one by
one) and then printing the numbers by an inorder tree walk. What are the worst-
caseandbest-case runningtimesforthissortingalgorithm?
12.3-4
Istheoperation ofdeletion “commutative” inthe sense that deleting x andthen y
fromabinarysearchtreeleavesthesametreeasdeletingy andthenx? Arguewhy
itisorgiveacounterexample.
12.3-5
Suppose that instead of each node x keeping the attribute x:p, pointing to x’s
parent, it keeps x:succ, pointing to x’s successor. Give pseudocode for SEARCH,
INSERT, and DELETE on a binary search tree T using this representation. These
proceduresshouldoperateintimeO.h/,wherehistheheightofthetreeT. (Hint:
Youmaywishtoimplementasubroutine thatreturnstheparentofanode.)
12.3-6
When node ´ in TREE-DELETE has two children, we could choose node y as
its predecessor rather than its successor. What other changes to TREE-DELETE
would be necessary if we did so? Some have argued that a fair strategy, giving
equal priority to predecessor and successor, yields better empirical performance.
Howmight TREE-DELETE bechangedtoimplementsuchafairstrategy?
? 12.4 Randomly builtbinary searchtrees
We have shown that each of the basic operations on a binary search tree runs
in O.h/ time, where h is the height of the tree. The height of a binary search
300 Chapter12 BinarySearchTrees
treevaries,however,asitemsareinserted anddeleted. If,forexample,thenitems
are inserted in strictly increasing order, the tree will bea chain with height n 1.
(cid:0)
On the other hand, Exercise B.5-4 shows that h lgn . As with quicksort, we
 b c
canshowthatthebehavior oftheaveragecaseismuchclosertothebestcasethan
totheworstcase.
Unfortunately, little is known about the average height of a binary search tree
when both insertion and deletion are used to create it. When the tree is created
by insertion alone, the analysis becomes more tractable. Let us therefore define a
randomly built binary search tree on n keys as one that arises from inserting the
keysinrandomorderintoaninitiallyemptytree,whereeachofthenŠpermutations
oftheinputkeysisequallylikely. (Exercise12.4-3asksyoutoshowthatthisnotion
isdifferentfromassumingthateverybinarysearchtreeonnkeysisequallylikely.)
Inthissection, weshallprovethefollowingtheorem.
Theorem12.4
The expected height of a randomly built binary search tree on n distinct keys is
O.lgn/.
Proof We start by defining three random variables that help measure the height
of a randomly built binary search tree. We denote the height of a randomly built
binary search on n keys by X , and we define the exponential height Y 2Xn.
n n
D
When we build a binary search tree on n keys, we choose one key as that of the
root, and we let R denote the random variable that holds this key’s rank within
n
the set of n keys; that is, R holds the position that this key would occupy if the
n
setofkeysweresorted. ThevalueofR isequally likely tobeanyelement ofthe
n
set 1;2;:::;n . If R i, then the left subtree of the root is a randomly built
n
f g D
binary search tree on i 1 keys, and the right subtree is a randomly built binary
(cid:0)
search tree on n i keys. Because the height of a binary tree is 1 more than the
(cid:0)
larger of the heights of the two subtrees of the root, the exponential height of a
binary treeistwicethelarger ofthe exponential heights ofthe twosubtrees ofthe
root. IfweknowthatR i,itfollowsthat
n
D
Y 2 max.Y ;Y /:
n i 1 n i
D  (cid:0) (cid:0)
Asbasecases,wehavethatY 1,becausetheexponentialheightofatreewith1
1
D
nodeis20 1and,forconvenience, wedefineY 0.
0
D D
Next,defineindicator randomvariables Z ;Z ;:::;Z ,where
n;1 n;2 n;n
Z I R i :
n;i n
D f D g
Because R is equally likely to be any element of 1;2;:::;n , it follows that
n
f g
Pr R i 1=nfori 1;2;:::;n,andhence,byLemma5.1,wehave
n
f D g D D
EŒZ  1=n; (12.1)
n;i
D
12.4 Randomlybuiltbinarysearchtrees 301
fori 1;2;:::;n. BecauseexactlyonevalueofZ is1andallothersare0,we
n;i
D
alsohave
n
Y Z .2 max.Y ;Y // :
n n;i i 1 n i
D  (cid:0) (cid:0)
i 1
XD
We shall show that EŒY  is polynomial in n, which will ultimately imply that
n
EŒX  O.lgn/.
n
D
We claim that the indicator random variable Z I R i is independent
n;i n
D f D g
of the values of Y and Y . Having chosen R i, the left subtree (whose
i 1 n i n
exponential height(cid:0) is Y ) is(cid:0) randomly built on theD i 1 keys whose ranks are
i 1
less than i. This subtree(cid:0) is just like any other random(cid:0) ly built binary search tree
on i 1 keys. Other than the number of keys it contains, this subtree’s structure
(cid:0)
is not affected at all by the choice of R i, and hence the random variables
n
D
Y and Z are independent. Likewise, the right subtree, whose exponential
i 1 n;i
he(cid:0) ight is Y , israndomly built on then i keys whose ranks are greater than i.
n i
Its structure(cid:0) is independent of the value o(cid:0) f R , and so the random variables Y
n n i
andZ areindependent. Hence,wehave (cid:0)
n;i
n
EŒY  E Z .2 max.Y ;Y //
n n;i i 1 n i
D "  (cid:0) (cid:0) #
i 1
XD
n
EŒZ .2 max.Y ;Y // (bylinearity ofexpectation)
n;i i 1 n i
D  (cid:0) (cid:0)
i 1
XD
n
EŒZ EŒ2 max.Y ;Y / (byindependence)
n;i i 1 n i
D  (cid:0) (cid:0)
i 1
XD
n
1
EŒ2 max.Y ;Y / (byequation (12.1))
i 1 n i
D n   (cid:0) (cid:0)
i 1
XD
n
2
EŒmax.Y ;Y / (byequation (C.22))
i 1 n i
D n (cid:0) (cid:0)
i 1
XD
n
2
.EŒY  EŒY / (byExerciseC.3-4) .
i 1 n i
 n (cid:0) C (cid:0)
i 1
XD
Since each term EŒY ;EŒY ;:::;EŒY  appears twice in the last summation,
0 1 n 1
onceasEŒY andonceasEŒY ,we(cid:0) havetherecurrence
i 1 n i
(cid:0) (cid:0)
n 1
4 (cid:0)
EŒY  EŒY  : (12.2)
n i
 n
i 0
XD
302 Chapter12 BinarySearchTrees
Usingthesubstitution method,weshallshowthatforallpositiveintegersn,the
recurrence (12.2)hasthesolution
1 n 3
EŒY  C :
n
 4 3
!
Indoingso,weshallusetheidentity
n 1
(cid:0) i 3 n 3
C C : (12.3)
3 D 4
! !
i 0
XD
(Exercise12.4-1asksyoutoprovethisidentity.)
Forthebasecases, wenotethatthebounds 0 Y EŒY  .1=4/ 3 1=4
D 0 D 0  3 D
and1
D
Y 1
D
EŒY 1

.1=4/ 1 C 33
D
1hold. Fortheinductivecase,we (cid:0)h avethat
4 n (cid:0)1 (cid:0) 
EŒY  EŒY 
n i
 n
i 0
XD
n 1
4 (cid:0) 1 i 3
C (bytheinductivehypothesis)
 n 4 3
!
i 0
XD
n 1
1 (cid:0) i 3
C
D n 3
!
i 0
XD
1 n 3
C (byequation (12.3))
D n 4
!
1 .n 3/Š
C
D n  4Š.n 1/Š
(cid:0)
1 .n 3/Š
C
D 4  3ŠnŠ
1 n 3
C :
D 4 3
!
We have bounded EŒY , but our ultimate goal is to bound EŒX . As Exer-
n n
cise 12.4-4 asks you to show, the function f.x/ 2x is convex (see page 1199).
D
Therefore, wecanemployJensen’s inequality (C.26),whichsaysthat
2EŒXn E 2Xn

EŒY  ;
D  n 
asfollows:
1 n 3
2EŒXn C
 4 3
!
ProblemsforChapter12 303
1 .n 3/.n 2/.n 1/
C C C
D 4  6
n3 6n2 11n 6
C C C :
D 24
TakinglogarithmsofbothsidesgivesEŒX  O.lgn/.
n
D
Exercises
12.4-1
Proveequation (12.3).
12.4-2
Describe a binary search tree on n nodes such that the average depth of a node in
the tree is ‚.lgn/ but the height of the tree is !.lgn/. Give an asymptotic upper
boundontheheight ofann-nodebinary search treeinwhichtheaveragedepth of
anodeis‚.lgn/.
12.4-3
Show that the notion of a randomly chosen binary search tree on n keys, where
each binary search tree of n keys is equally likely to be chosen, is different from
the notion of a randomly built binary search tree given in this section. (Hint: List
thepossibilities whenn 3.)
D
12.4-4
Showthatthefunction f.x/ 2x isconvex.
D
12.4-5 ?
Consider RANDOMIZED-QUICKSORT operating on a sequence of n distinct input
numbers. Prove that for any constant k > 0, all but O.1=nk/ of the nŠ input
permutations yieldanO.nlgn/running time.
Problems
12-1 Binarysearchtreeswithequalkeys
Equalkeysposeaproblem fortheimplementation ofbinarysearchtrees.
a. What is the asymptotic performance of TREE-INSERT when used to insert n
itemswithidentical keysintoaninitially emptybinarysearchtree?
WeproposetoimproveTREE-INSERTbytestingbeforeline5todeterminewhether
´:key x:keyand bytesting before line 11todetermine whether ´:key y:key.
D D
304 Chapter12 BinarySearchTrees
Ifequality holds, weimplement oneofthefollowing strategies. Foreachstrategy,
find the asymptotic performance of inserting n items with identical keys into an
initiallyemptybinarysearchtree. (Thestrategiesaredescribedforline5,inwhich
we compare the keys of ´ and x. Substitute y for x to arrive at the strategies for
line11.)
b. Keep a boolean flag x:b at node x, and set x to either x:left or x:right based
on the value of x:b, which alternates between FALSE and TRUE each time we
visitx whileinserting anodewiththesamekeyasx.
c. Keepalistofnodeswithequalkeysatx,andinsert´intothelist.
d. Randomly set x to either x:left or x:right. (Give the worst-case performance
andinformallyderivetheexpected runningtime.)
12-2 Radixtrees
Giventwostringsa a a :::a andb b b :::b ,whereeacha andeachb
0 1 p 0 1 q i j
D D
is in some ordered set of characters, wesay that string a is lexicographically less
thanstringb ifeither
1. there exists an integer j, where 0 j min.p;q/, such that a b for all
i i
  D
i 0;1;:::;j 1anda < b ,or
j j
D (cid:0)
2. p <q anda b foralli 0;1;:::;p.
i i
D D
For example, if a and b are bit strings, then 10100 < 10110 by rule 1 (letting
j 3) and 10100 < 101000 by rule 2. This ordering is similar to that used in
D
English-language dictionaries.
The radix tree data structure shown in Figure 12.5 stores the bit strings 1011,
10, 011, 100, and 0. When searching for a key a a a :::a , we go left at a
0 1 p
D
node ofdepth i ifa 0and right ifa 1. LetS be asetof distinct bit strings
i i
D D
whose lengths sum to n. Show how to use aradix tree to sort S lexicographically
in ‚.n/time. Fortheexample inFigure 12.5, theoutput of the sortshould bethe
sequence 0,011,10,100,1011.
12-3 Averagenodedepthinarandomlybuiltbinarysearchtree
In this problem, we prove that the average depth of a node in a randomly built
binary search tree with n nodes is O.lgn/. Although this result is weaker than
that of Theorem 12.4, the technique we shall use reveals a surprising similarity
between the building of a binary search tree and the execution of RANDOMIZED-
QUICKSORT fromSection7.3.
We define the total path length P.T/ of a binary tree T as the sum, over all
nodesx inT,ofthedepthofnodex,whichwedenotebyd.x;T/.
ProblemsforChapter12 305
0 1
0
1 0
10
1 0 1
011 100
1
1011
Figure12.5 Aradixtreestoringthebitstrings1011,10,011,100,and0. Wecandetermineeach
node’skeybytraversingthesimplepathfromtheroottothatnode. Thereisnoneed,therefore,to
storethekeysinthenodes; thekeysappear hereforillustrativepurposes only. Nodesareheavily
shadedifthekeyscorrespondingtothemarenotinthetree;suchnodesarepresentonlytoestablish
apathtoothernodes.
a. Arguethattheaverage depthofanodeinT is
1 1
d.x;T/ P.T/:
n D n
x T
X2
Thus,wewishtoshowthattheexpectedvalueofP.T/isO.nlgn/.
b. LetT and T denote theleftand right subtrees oftreeT,respectively. Argue
L R
thatifT hasnnodes, then
P.T/ P.T / P.T / n 1:
L R
D C C (cid:0)
c. LetP.n/denotetheaveragetotalpathlengthofarandomlybuiltbinarysearch
treewithnnodes. Showthat
n 1
1 (cid:0)
P.n/ .P.i/ P.n i 1/ n 1/:
D n C (cid:0) (cid:0) C (cid:0)
i 0
XD
d. ShowhowtorewriteP.n/as
n 1
2 (cid:0)
P.n/ P.k/ ‚.n/:
D n C
k 1
XD
e. Recalling thealternative analysis oftherandomized version ofquicksort given
inProblem7-3,conclude thatP.n/ O.nlgn/.
D
306 Chapter12 BinarySearchTrees
At each recursive invocation of quicksort, we choose a random pivot element to
partition theset ofelements being sorted. Eachnode ofabinary search tree parti-
tionsthesetofelementsthatfallintothesubtreerootedatthatnode.
f. Describeanimplementationofquicksortinwhichthecomparisonstosortaset
ofelementsareexactlythesameasthecomparisons toinserttheelements into
abinarysearchtree. (Theorderinwhichcomparisonsaremademaydiffer,but
thesamecomparisons mustoccur.)
12-4 Numberofdifferentbinarytrees
Let b denote the number of different binary trees with n nodes. In this problem,
n
youwillfindaformulaforb ,aswellasanasymptotic estimate.
n
a. Showthatb 1andthat,forn 1,
0
D 
n 1
(cid:0)
b b b :
n k n 1 k
D (cid:0) (cid:0)
k 0
XD
b. Referring to Problem 4-4 for the definition of a generating function, let B.x/
bethegenerating function
1
B.x/ b xn :
n
D
n 0
XD
Showthat B.x/ xB.x/2 1, and hence one wayto express B.x/in closed
D C
formis
1
B.x/ 1 p1 4x :
D 2x (cid:0) (cid:0)
(cid:0) 
TheTaylorexpansion off.x/aroundthepointx aisgivenby
D
1
f.k/.a/
f.x/ .x a/k ;
D kŠ (cid:0)
k 0
XD
wheref.k/.x/isthekthderivativeoff evaluatedatx.
c. Showthat
1 2n
b
n
D n 1 n
!
C
NotesforChapter12 307
(the nth Catalan number) by using the Taylor expansion of p1 4x around
(cid:0)
x 0. (If you wish, instead of using the Taylor expansion, you may use
D
the generalization ofthe binomial expansion (C.4)tononintegral exponents n,
where for any real number n and for any integer k, we interpret n to be
k
n.n 1/ .n k 1/=kŠifk 0,and0otherwise.)
(cid:0)  (cid:0) C  (cid:0) 
d. Showthat
4n
b .1 O.1=n// :
n
D pn3=2 C
Chapter notes
Knuth [211] contains a good discussion of simple binary search trees as well as
manyvariations. Binary search trees seem to have been independently discovered
byanumberofpeopleinthelate1950s. Radixtreesareoftencalled“tries,”which
comes from the middle letters in the word retrieval. Knuth [211] also discusses
them.
Manytexts, including the firsttwo editions of this book, have asomewhat sim-
plermethodofdeleting anode fromabinary searchtreewhenbothofitschildren
are present. Instead of replacing node ´ by its successor y, we delete node y but
copy its key and satellite data into node ´. The downside of this approach is that
the node actually deleted might not be the node passed tothe delete procedure. If
other components of a program maintain pointers to nodes in the tree, they could
mistakenly endupwith“stale”pointerstonodesthathavebeendeleted. Although
thedeletionmethodpresentedinthiseditionofthisbookisabitmorecomplicated,
itguarantees thatacalltodeletenode´deletesnode´andonlynode´.
Section 15.5 will show how to construct an optimal binary search tree when
we know the search frequencies before constructing the tree. That is, given the
frequencies of searching for each key and the frequencies of searching for values
that fall between keys in the tree, we construct a binary search tree for which a
set of searches that follows these frequencies examines the minimum number of
nodes.
The proof in Section 12.4 that bounds the expected height of a randomly built
binarysearchtreeisduetoAslam[24]. Mart´ınezandRoura[243]giverandomized
algorithms for insertion into and deletion from binary search trees in which the
result of either operation is a random binary search tree. Their definition of a
random binary search tree differs—only slightly—from that of a randomly built
binarysearchtreeinthischapter, however.
13 Red-Black Trees
Chapter12showedthatabinarysearchtreeofheighthcansupportanyofthebasic
dynamic-set operations—such as SEARCH, PREDECESSOR, SUCCESSOR, MINI-
MUM, MAXIMUM, INSERT, and DELETE—in O.h/time. Thus,thesetoperations
are fast if the height of the search tree is small. If its height is large, however, the
set operations may run no faster than with a linked list. Red-black trees are one
of many search-tree schemes that are “balanced” in order to guarantee that basic
dynamic-set operations takeO.lgn/timeintheworstcase.
13.1 Properties ofred-black trees
A red-black tree is a binary search tree with one extra bit of storage per node: its
color, whichcanbeeither RED or BLACK. Byconstraining thenodecolorsonany
simplepathfromtheroottoaleaf,red-blacktreesensurethatnosuchpathismore
thantwiceaslongasanyother, sothatthetreeisapproximately balanced.
Eachnodeofthetreenowcontainstheattributes color,key,left,right,andp. If
a child or the parent of a node does not exist, the corresponding pointer attribute
ofthenodecontainsthevalue NIL. Weshallregardthese NILsasbeingpointersto
leaves(externalnodes)ofthebinarysearchtreeandthenormal,key-bearingnodes
asbeinginternal nodesofthetree.
Ared-blacktreeisabinarytreethatsatisfiesthefollowingred-blackproperties:
1. Everynodeiseitherredorblack.
2. Therootisblack.
3. Everyleaf(NIL)isblack.
4. Ifanodeisred,thenbothitschildrenareblack.
5. Foreachnode, allsimplepathsfrom thenode todescendant leaves contain the
samenumberofblacknodes.
13.1 Propertiesofred-blacktrees 309
Figure13.1(a)showsanexampleofared-black tree.
As a matter of convenience in dealing with boundary conditions in red-black
treecode,weuseasinglesentineltorepresent NIL (seepage238). Forared-black
tree T, the sentinel T:nil isan object withthe same attributes asan ordinary node
in the tree. Its color attribute is BLACK, and its other attributes—p, left, right,
andkey—cantakeonarbitraryvalues. AsFigure13.1(b)shows,allpointerstoNIL
arereplaced bypointerstothesentinelT:nil.
We use the sentinel so that we can treat a NIL child of a node x as an ordinary
node whose parent is x. Although we instead could add a distinct sentinel node
for each NIL in the tree, so that the parent of each NIL is well defined, that ap-
proach would waste space. Instead, we use the one sentinel T:nil to represent all
theNILs—allleavesandtheroot’sparent. Thevaluesoftheattributesp,left,right,
andkeyofthesentinelareimmaterial,althoughwemaysetthemduringthecourse
ofaprocedure forourconvenience.
Wegenerally confineourinterest totheinternal nodes ofared-black tree, since
theyholdthekeyvalues. Intheremainderofthischapter,weomittheleaveswhen
wedrawred-black trees,asshowninFigure13.1(c).
Wecallthenumberofblacknodesonanysimplepathfrom,butnotincluding, a
nodex downtoaleaftheblack-heightofthenode,denotedbh.x/. Byproperty5,
the notion of black-height is well defined, since all descending simple paths from
the node have the same number of black nodes. We define the black-height of a
red-black treetobetheblack-height ofitsroot.
Thefollowinglemmashowswhyred-black treesmakegoodsearchtrees.
Lemma13.1
Ared-black treewithninternal nodeshasheightatmost2lg.n 1/.
C
Proof Westartbyshowingthatthesubtreerootedatanynodex containsatleast
2bh.x/ 1 internal nodes. We prove this claim by induction on the height of x. If
(cid:0)
theheightofxis0,thenxmustbealeaf(T:nil),andthesubtreerootedatxindeed
contains at least 2bh.x/ 1 20 1 0 internal nodes. For the inductive step,
(cid:0) D (cid:0) D
consideranodexthathaspositiveheightandisaninternalnodewithtwochildren.
Eachchild hasablack-height ofeither bh.x/orbh.x/ 1,depending onwhether
(cid:0)
its color is red or black, respectively. Since the height of a child of x is less than
theheight of x itself, wecan apply theinductive hypothesis toconclude that each
childhasatleast2bh.x/ 1 1internalnodes. Thus,thesubtreerootedatx contains
(cid:0)
(cid:0)
atleast.2bh.x/ 1 1/ .2bh.x/ 1 1/ 1 2bh.x/ 1internalnodes,whichproves
(cid:0) (cid:0)
(cid:0) C (cid:0) C D (cid:0)
theclaim.
To complete the proof of the lemma, let h be the height of the tree. According
toproperty4,atleasthalfthenodesonanysimplepathfromtheroottoaleaf,not
310 Chapter13 Red-BlackTrees
3 26
3 17 41 2
2 14 2 21 2 30 1 47
2 10 1 16 1 19 1 23 1 28 1 38 NIL NIL
1 7 1 12 1 15 NIL NIL 1 20 NIL NIL NIL NIL 1 35 1 39
1 3 NIL NIL NIL NIL NIL NIL NIL NIL NIL NIL NIL
NIL NIL (a)
26
17 41
14 21 30 47
10 16 19 23 28 38
7 12 15 20 35 39
3
T:nil
(b)
26
17 41
14 21 30 47
10 16 19 23 28 38
7 12 15 20 35 39
3
(c)
Figure13.1 Ared-blacktreewithblacknodesdarkened andrednodesshaded. Everynodeina
red-blacktreeiseitherredorblack,thechildrenofarednodearebothblack,andeverysimplepath
fromanodetoadescendantleafcontainsthesamenumberofblacknodes. (a)Everyleaf,shown
asa NIL,isblack. Eachnon-NIL nodeismarked withitsblack-height; NILshaveblack-height 0.
(b)Thesamered-blacktreebutwitheachNILreplacedbythesinglesentinelT:nil,whichisalways
black,andwithblack-heightsomitted.Theroot’sparentisalsothesentinel. (c)Thesamered-black
tree but with leaves and the root’s parent omitted entirely. We shall use this drawing style in the
remainderofthischapter.
13.1 Propertiesofred-blacktrees 311
including theroot, mustbeblack. Consequently, theblack-height oftherootmust
beatleasth=2;thus,
n 2h=2 1:
 (cid:0)
Moving the 1 to the left-hand side and taking logarithms on both sides yields
lg.n 1/ h=2,orh 2lg.n 1/.
C   C
Asanimmediateconsequenceofthislemma,wecanimplementthedynamic-set
operations SEARCH, MINIMUM, MAXIMUM, SUCCESSOR, and PREDECESSOR
in O.lgn/ time on red-black trees, since each can run in O.h/ time on a binary
searchtreeofheighth(asshowninChapter12)andanyred-blacktreeonnnodes
is a binary search tree with height O.lgn/. (Of course, references to NIL in the
algorithms of Chapter 12 would have to be replaced by T:nil.) Although the al-
gorithms TREE-INSERT and TREE-DELETE from Chapter 12 run in O.lgn/ time
whengiven ared-black tree asinput, they donot directly support the dynamic-set
operations INSERT and DELETE, since theydonotguarantee thatthemodifiedbi-
nary search tree will be a red-black tree. We shall see in Sections 13.3 and 13.4,
however,howtosupport thesetwooperations inO.lgn/time.
Exercises
13.1-1
Inthe style ofFigure 13.1(a), draw the complete binary search tree of height 3on
the keys 1;2;:::;15 . Add the NIL leaves and color the nodes in three different
f g
wayssuchthattheblack-heights oftheresulting red-black treesare2,3,and4.
13.1-2
Draw the red-black tree that results after TREE-INSERT is called on the tree in
Figure 13.1 with key 36. If the inserted node is colored red, is the resulting tree a
red-black tree? Whatifitiscoloredblack?
13.1-3
Let us define a relaxed red-black tree as a binary search tree that satisfies red-
blackproperties 1,3,4,and5. Inotherwords,therootmaybeeitherredorblack.
Consider a relaxed red-black tree T whose root is red. If we color the root of T
blackbutmakenootherchanges toT,istheresulting treeared-black tree?
13.1-4
Suppose that we “absorb” every red node in a red-black tree into its black parent,
so that the children of the red node become children of the black parent. (Ignore
whathappens tothe keys.) What arethepossible degrees of ablack node after all
312 Chapter13 Red-BlackTrees
its red children are absorbed? What can you say about the depths of the leaves of
theresulting tree?
13.1-5
Showthatthelongestsimplepathfromanodexinared-blacktreetoadescendant
leaf has length at most twice that of the shortest simple path from node x to a
descendant leaf.
13.1-6
Whatisthelargestpossiblenumberofinternalnodesinared-blacktreewithblack-
heightk? Whatisthesmallestpossible number?
13.1-7
Describeared-blacktreeonnkeysthatrealizesthelargestpossibleratioofredin-
ternalnodestoblackinternal nodes. Whatisthisratio? Whattreehasthesmallest
possible ratio,andwhatistheratio?
13.2 Rotations
Thesearch-tree operations TREE-INSERT and TREE-DELETE, whenrunonared-
black treewithnkeys, takeO.lgn/time. Because theymodifythetree, theresult
may violate the red-black properties enumerated in Section 13.1. To restore these
properties, we must change the colors of some of the nodes in the tree and also
changethepointer structure.
We change the pointer structure through rotation, which is a local operation in
asearch treethatpreserves thebinary-search-tree property. Figure13.2showsthe
twokindsofrotations: leftrotationsandrightrotations. Whenwedoaleftrotation
on a node x, we assume that its right child y is not T:nil; x may be any node in
the tree whose right child is not T:nil. The left rotation “pivots” around the link
fromx toy. Itmakesy thenewrootofthesubtree,withx asy’sleftchildandy’s
leftchildasx’srightchild.
The pseudocode for LEFT-ROTATE assumes that x:right T:nil and that the
¤
root’sparentisT:nil.
13.2 Rotations 313
LEFT-ROTATE(T, x)
y x
x g a y
RIGHT-ROTATE(T, y)
a b b g
Figure13.2 Therotationoperationsonabinarysearchtree. Theoperation LEFT-ROTATE.T;x/
transformstheconfigurationofthetwonodesontherightintotheconfigurationontheleftbychang-
ingaconstantnumberofpointers. TheinverseoperationRIGHT-ROTATE.T;y/transformsthecon-
figurationontheleftintotheconfigurationontheright. Theletters˛,ˇ,and representarbitrary
subtrees.Arotationoperationpreservesthebinary-search-treeproperty:thekeysin˛precedex:key,
whichprecedesthekeysinˇ,whichprecedey:key,whichprecedesthekeysin .
LEFT-ROTATE.T;x/
1 y x:right //sety
D
2 x:right y:left //turny’sleftsubtreeintox’srightsubtree
D
3 ify:left T:nil
¤
4 y:left:p x
D
5 y:p x:p //linkx’sparenttoy
D
6 ifx:p==T:nil
7 T:root y
D
8 elseifx ==x:p:left
9 x:p:left y
D
10 elsex:p:right y
D
11 y:left x //putx ony’sleft
D
12 x:p y
D
Figure 13.3 shows an example of how LEFT-ROTATE modifies a binary search
tree. ThecodeforRIGHT-ROTATE issymmetric. BothLEFT-ROTATE andRIGHT-
ROTATE run in O.1/ time. Only pointers are changed by a rotation; all other
attributes inanoderemainthesame.
Exercises
13.2-1
Writepseudocode for RIGHT-ROTATE.
13.2-2
Argue that in every n-node binary search tree, there are exactly n 1 possible
(cid:0)
rotations.
314 Chapter13 Red-BlackTrees
7
4 11 x
3 6 9 18 y
2 14 19
12 17 22
LEFT-ROTATE(T, x)
20
7
4 18 y
3 6 x 11 19
2 9 14 22
12 17 20
Figure13.3 AnexampleofhowtheprocedureLEFT-ROTATE.T;x/modifiesabinarysearchtree.
Inordertreewalksoftheinputtreeandthemodifiedtreeproducethesamelistingofkeyvalues.
13.2-3
Leta,b,andc bearbitrary nodesinsubtrees ˛,ˇ,and ,respectively, intheright
tree of Figure 13.2. How do the depths of a, b, and c change when a left rotation
isperformedonnodex inthefigure?
13.2-4
Showthatanyarbitraryn-nodebinarysearchtreecanbetransformedintoanyother
arbitrary n-node binary search tree using O.n/ rotations. (Hint: Firstshow thatat
mostn 1rightrotations sufficetotransform thetreeintoaright-going chain.)
(cid:0)
13.2-5 ?
WesaythatabinarysearchtreeT canberight-converted tobinarysearchtreeT
1 2
if it ispossible to obtain T
2
from T
1
via aseries of calls to RIGHT-ROTATE. Give
an example of two trees T and T such that T cannot be right-converted to T .
1 2 1 2
Then, show thatifatree T canberight-converted toT ,itcanberight-converted
1 2
usingO.n2/callstoRIGHT-ROTATE.
13.3 Insertion 315
13.3 Insertion
We can insert a node into an n-node red-black tree in O.lgn/ time. To do so, we
use a slightly modified version of the TREE-INSERT procedure (Section 12.3) to
insertnode´intothetreeT asifitwereanordinarybinarysearchtree,andthenwe
color ´ red. (Exercise 13.3-1 asks you to explain why we choose to make node ´
redratherthanblack.) Toguaranteethatthered-blackpropertiesarepreserved, we
then call anauxiliary procedure RB-INSERT-FIXUP torecolor nodes and perform
rotations. ThecallRB-INSERT.T;´/insertsnode´,whosekeyisassumedtohave
alreadybeenfilledin,intothered-black treeT.
RB-INSERT.T;´/
1 y T:nil
D
2 x T:root
D
3 whilex T:nil
¤
4 y x
D
5 if´:key <x:key
6 x x:left
D
7 elsex x:right
D
8 ´:p y
D
9 ify == T:nil
10 T:root ´
D
11 elseif´:key < y:key
12 y:left ´
D
13 elsey:right ´
D
14 ´:left T:nil
D
15 ´:right T:nil
D
16 ´:color RED
D
17 RB-INSERT-FIXUP.T;´/
The procedures TREE-INSERT and RB-INSERT differ in four ways. First, all
instances of NIL in TREE-INSERT are replaced by T:nil. Second, we set ´:left
and ´:right to T:nil in lines 14–15 of RB-INSERT, in order to maintain the
proper tree structure. Third, we color ´ red in line 16. Fourth, because col-
oring ´ red may cause a violation of one of the red-black properties, we call
RB-INSERT-FIXUP.T;´/ in line 17 of RB-INSERT to restore the red-black prop-
erties.
316 Chapter13 Red-BlackTrees
RB-INSERT-FIXUP.T;´/
1 while´:p:color == RED
2 if´:p ==´:p:p:left
3 y ´:p:p:right
D
4 ify:color == RED
5 ´:p:color BLACK //case1
D
6 y:color BLACK //case1
D
7 ´:p:p:color RED //case1
D
8 ´ ´:p:p //case1
D
9 elseif´== ´:p:right
10 ´ ´:p //case2
D
11 LEFT-ROTATE.T;´/ //case2
12 ´:p:color BLACK //case3
D
13 ´:p:p:color RED //case3
D
14 RIGHT-ROTATE.T;´:p:p/ //case3
15 else(sameasthenclause
with“right”and“left”exchanged)
16 T:root:color BLACK
D
To understand how RB-INSERT-FIXUP works, we shall break our examination
of the code into three major steps. First, we shall determine what violations of
the red-black properties are introduced in RB-INSERT when node ´ is inserted
and colored red. Second, we shall examine the overall goal of the while loop in
lines 1–15. Finally, we shall explore each of the three cases1 within the while
loop’s body and see how they accomplish the goal. Figure 13.4 shows how RB-
INSERT-FIXUP operates onasamplered-black tree.
Which of the red-black properties might be violated upon the call to RB-
INSERT-FIXUP? Property 1 certainly continues to hold, as does property 3, since
both children of the newly inserted red node are the sentinel T:nil. Property 5,
which says that the number of black nodes is the same on every simple path from
agivennode,issatisfiedaswell,because node´replaces the(black)sentinel, and
node ´ is red with sentinel children. Thus, the only properties that might be vi-
olated are property 2, which requires the root to be black, and property 4, which
says that arednode cannot havearedchild. Bothpossible violations are dueto´
being colored red. Property 2isviolated if´istheroot, andproperty 4isviolated
if´’sparentisred. Figure13.4(a) showsaviolation ofproperty 4afterthenode´
hasbeeninserted.
1Case2fallsthroughintocase3,andsothesetwocasesarenotmutuallyexclusive.
13.3 Insertion 317
11
2 14
(a) 1 7 15
5 8 y
z 4 Case 1
11
2 14 y
(b) 1 7 z 15
5 8
44 Case 2
11
7 14 y
(c) z 2 8 15
1 5
Case 3
4
7
z 2 11
(d) 1 5 8 14
4 15
Figure13.4 Theoperationof RB-INSERT-FIXUP. (a)Anode´afterinsertion. Becauseboth´
anditsparent´:parered, aviolationofproperty4occurs. Since´’suncley isred,case1inthe
codeapplies.Werecolornodesandmovethepointer´upthetree,resultinginthetreeshownin(b).
Onceagain,´anditsparentarebothred,but´’suncleyisblack. Since´istherightchildof´:p,
case2applies.Weperformaleftrotation,andthetreethatresultsisshownin(c).Now,´istheleft
childofitsparent,andcase3applies. Recoloringandrightrotationyieldthetreein(d),whichisa
legalred-blacktree.
318 Chapter13 Red-BlackTrees
The whileloop in lines 1–15 maintains the following three-part invariant at the
startofeachiteration oftheloop:
a. Node´isred.
b. If´:pistheroot,then´:pisblack.
c. Ifthetreeviolatesanyofthered-blackproperties,thenitviolatesatmost
oneofthem,andtheviolationisofeitherproperty 2orproperty4. Ifthe
tree violates property 2, it is because ´ is the root and is red. If the tree
violates property 4,itisbecause both´and´:parered.
Part (c), which deals with violations of red-black properties, is more central to
showing that RB-INSERT-FIXUP restores the red-black properties than parts (a)
and(b),whichweusealongthewaytounderstand situations inthecode. Because
we’ll be focusing on node ´ and nodes near it in the tree, it helps to know from
part(a)that´isred. Weshallusepart(b)toshowthatthenode´:p:pexistswhen
wereference itinlines2,3,7,8,13,and14.
Recall that we need to show that a loop invariant is true prior to the first itera-
tion of the loop, that each iteration maintains the loop invariant, and that the loop
invariant givesusausefulproperty atlooptermination.
We start with the initialization and termination arguments. Then, as we exam-
ine how the body of the loop works in more detail, we shall argue that the loop
maintains the invariant upon each iteration. Along the way, we shall also demon-
stratethateachiterationoftheloophastwopossibleoutcomes: eitherthepointer´
movesupthetree,orweperform somerotations andthentheloopterminates.
Initialization: Prior to the first iteration of the loop, we started with a red-black
tree withno violations, and weadded ared node ´. Weshow that each part of
theinvariant holdsatthetimeRB-INSERT-FIXUP iscalled:
a. When RB-INSERT-FIXUP iscalled,´istherednodethatwasadded.
b. If ´:p is the root, then ´:p started out black and did not change prior to the
callof RB-INSERT-FIXUP.
c. We have already seen that properties 1, 3, and 5 hold when RB-INSERT-
FIXUP iscalled.
If the tree violates property 2, then the red root must be the newly added
node ´, which is the only internal node in the tree. Because the parent and
both children of ´ are the sentinel, which is black, the tree does not also
violate property 4. Thus, thisviolation ofproperty 2istheonly violation of
red-black properties intheentiretree.
Ifthetreeviolatesproperty 4,then,becausethechildrenofnode´areblack
sentinels and the tree had no other violations prior to ´ being added, the
13.3 Insertion 319
violationmustbebecauseboth´and´:parered. Moreover,thetreeviolates
nootherred-black properties.
Termination: When the loop terminates, it does so because ´:p is black. (If ´ is
the root, then ´:pisthesentinel T:nil,which isblack.) Thus, thetreedoes not
violate property 4atloop termination. Bytheloop invariant, theonly property
that mightfail tohold isproperty 2. Line16restores this property, too, sothat
when RB-INSERT-FIXUP terminates, allthered-black properties hold.
Maintenance: Weactually need toconsider sixcases inthewhileloop, butthree
of them are symmetric to the other three, depending on whether line 2 deter-
mines´’sparent´:ptobealeftchildorarightchildof´’sgrandparent ´:p:p.
Wehave given the code only for the situation in which ´:p is aleft child. The
node ´:p:p exists, since by part (b) of the loop invariant, if ´:p is the root,
then ´:p is black. Since we enter a loop iteration only if ´:p is red, we know
that´:pcannotbetheroot. Hence,´:p:pexists.
We distinguish case 1 from cases 2 and 3 by the color of ´’s parent’s sibling,
or “uncle.” Line 3 makes y point to ´’s uncle ´:p:p:right, and line 4 tests y’s
color. If y isred, then weexecute case 1. Otherwise, control passes to cases 2
and 3. In all three cases, ´’s grandparent ´:p:pis black, since itsparent ´:pis
red,andproperty 4isviolated onlybetween´and´:p.
Case1: ´’suncley isred
Figure 13.5 shows the situation for case 1 (lines 5–8), which occurs when
both ´:p and y are red. Because ´:p:p is black, we can color both ´:p and y
black, thereby fixing the problem of ´ and ´:p both being red, and we can
color´:p:pred,therebymaintaining property5. Wethenrepeatthewhileloop
with´:p:pasthenewnode´. Thepointer´movesuptwolevelsinthetree.
Now, we show that case 1 maintains the loop invariant at the start of the next
iteration. We use ´ to denote node ´ in the current iteration, and ´ ´:p:p
0
D
to denote the node that will be called node ´at the test in line 1 upon the next
iteration.
a. Because thisiteration colors ´:p:pred, node ´ isredatthestartofthenext
0
iteration.
b. Thenode´:pis´:p:p:pinthisiteration, andthecolorofthisnodedoesnot
0
change. If this node is the root, it was black prior to this iteration, and it
remainsblackatthestartofthenextiteration.
c. We have already argued that case 1 maintains property 5, and it does not
introduce aviolation ofproperties 1or3.
320 Chapter13 Red-BlackTrees
C new z C
(a) A D y A D
a B z d e a B d e
b g b g
C new z C
(b) B D y B D
z A g d e A g d e
a b a b
Figure13.5 Case1oftheprocedure RB-INSERT-FIXUP. Property4isviolated, since´andits
parent ´:p are both red. We take the same action whether (a) ´ is a right child or (b) ´ isa left
child. Eachofthesubtrees˛,ˇ, ,ı,and"hasablackroot,andeachhasthesameblack-height.
Thecodeforcase1changesthecolorsofsomenodes,preservingproperty5: alldownwardsimple
pathsfromanodetoaleafhavethesamenumberofblacks.Thewhileloopcontinueswithnode´’s
grandparent´:p:pasthenew´.Anyviolationofproperty4cannowoccuronlybetweenthenew´,
whichisred,anditsparent,ifitisredaswell.
If node ´ is the root at the start of the next iteration, then case 1 corrected
0
the lone violation of property 4 in this iteration. Since ´ is red and it is the
0
root, property 2 becomes the only one that is violated, and this violation is
dueto´.
0
If node ´ is not the root at the start of the next iteration, then case 1 has
0
not created a violation of property 2. Case 1 corrected the lone violation
of property 4 that existed at the start of this iteration. It then made ´ red
0
and left ´:p alone. If ´:p was black, there is no violation of property 4.
0 0
If´:pwasred,coloring´ redcreatedoneviolationofproperty4between´
0 0 0
and´:p.
0
Case2: ´’suncley isblackand´isarightchild
Case3: ´’suncley isblackand´isaleftchild
Incases2and3,thecolorof´’suncley isblack. Wedistinguish thetwocases
according to whether ´ is a right or left child of ´:p. Lines 10–11 constitute
case 2, which is shown in Figure 13.6 together with case 3. In case 2, node ´
is a right child of its parent. We immediately use a left rotation to transform
thesituation intocase3(lines 12–14), inwhichnode´isaleftchild. Because
13.3 Insertion 321
C C B
A d y B d y z A C
a B z z A g a b g d
b g a b
Case 2 Case 3
Figure13.6 Cases2and3oftheprocedureRB-INSERT-FIXUP.Asincase1,property4isviolated
ineithercase2orcase3because´anditsparent´:parebothred.Eachofthesubtrees˛,ˇ, ,andı
hasablackroot(˛,ˇ,and fromproperty4,andıbecauseotherwisewewouldbeincase1),and
eachhasthesameblack-height. Wetransformcase2intocase3byaleftrotation,whichpreserves
property5:alldownwardsimplepathsfromanodetoaleafhavethesamenumberofblacks.Case3
causessomecolorchangesandarightrotation,whichalsopreserveproperty5.Thewhileloopthen
terminates,becauseproperty4issatisfied:therearenolongertworednodesinarow.
both ´ and ´:p are red, the rotation affects neither the black-height of nodes
norproperty5. Whetherweentercase3directlyorthroughcase2,´’suncley
is black, since otherwise we would have executed case 1. Additionally, the
node ´:p:p exists, since we have argued that this node existed at the time that
lines2and3wereexecuted,andaftermoving´uponelevelinline10andthen
down one level in line 11, the identity of ´:p:p remains unchanged. In case 3,
weexecutesomecolorchangesandarightrotation,whichpreserveproperty5,
and then, since we no longer have two red nodes in a row, we are done. The
whileloopdoesnotiterateanothertime,since´:pisnowblack.
Wenow show that cases 2and 3 maintain the loop invariant. (Aswehave just
argued,´:pwillbeblackuponthenexttestinline1,andtheloopbodywillnot
executeagain.)
a. Case2makes´pointto´:p,whichisred. Nofurtherchangeto´oritscolor
occursincases2and3.
b. Case 3 makes ´:p black, so that if ´:p is the root at the start of the next
iteration, itisblack.
c. Asincase1,properties 1,3,and5aremaintained incases2and3.
Sincenode´isnottherootincases2and3,weknowthatthereisnoviola-
tionofproperty 2. Cases 2and 3donot introduce aviolation ofproperty 2,
since theonly node thatismaderedbecomes achild ofablack node bythe
rotationincase3.
Cases2and3correcttheloneviolation ofproperty 4,andtheydonotintro-
duceanother violation.
322 Chapter13 Red-BlackTrees
Having shown that each iteration of the loop maintains the invariant, we have
shownthat RB-INSERT-FIXUP correctly restores thered-black properties.
Analysis
WhatistherunningtimeofRB-INSERT? Sincetheheightofared-blacktreeonn
nodes is O.lgn/, lines 1–16 of RB-INSERT take O.lgn/ time. In RB-INSERT-
FIXUP, thewhileloop repeats only ifcase 1occurs, and then thepointer ´moves
two levels up the tree. The total number of times the while loop can be executed
istherefore O.lgn/. Thus, RB-INSERT takesatotalofO.lgn/time. Moreover, it
never performs more than two rotations, since the while loop terminates if case 2
orcase3isexecuted.
Exercises
13.3-1
In line 16 of RB-INSERT, we set the color of the newly inserted node ´ to red.
Observe that if we had chosen to set ´’s color to black, then property 4 of a red-
blacktreewouldnotbeviolated. Whydidn’t wechoosetoset´’scolortoblack?
13.3-2
Showthered-blacktreesthatresultaftersuccessivelyinsertingthekeys41;38;31;
12;19;8 intoaninitiallyemptyred-black tree.
13.3-3
Suppose that the black-height of each of the subtrees ˛;ˇ; ;ı;" in Figures 13.5
and 13.6 is k. Label each node in each figure with its black-height to verify that
theindicated transformation preserves property5.
13.3-4
Professor Teach is concerned that RB-INSERT-FIXUP might set T:nil:color to
RED,inwhichcasethetestinline1wouldnotcausethelooptoterminatewhen´
is the root. Show that the professor’s concern is unfounded by arguing that RB-
INSERT-FIXUP neversetsT:nil:color toRED.
13.3-5
Consider a red-black tree formed by inserting n nodes with RB-INSERT. Argue
thatifn > 1,thetreehasatleastonerednode.
13.3-6
Suggest how to implement RB-INSERT efficiently if the representation for red-
blacktreesincludes nostorageforparentpointers.
13.4 Deletion 323
13.4 Deletion
Liketheotherbasicoperationsonann-nodered-blacktree,deletionofanodetakes
timeO.lgn/. Deletinganodefromared-black treeisabitmorecomplicatedthan
inserting anode.
The procedure for deleting a node from a red-black tree is based on the TREE-
DELETE procedure (Section 12.3). First, weneed tocustomize the TRANSPLANT
subroutine that TREE-DELETE callssothatitappliestoared-black tree:
RB-TRANSPLANT.T;u;/
1 ifu:p == T:nil
2 T:root 
D
3 elseifu== u:p:left
4 u:p:left 
D
5 elseu:p:right 
D
6 :p u:p
D
The procedure RB-TRANSPLANT differs from TRANSPLANT in two ways. First,
line1referencesthesentinelT:nilinsteadofNIL. Second,theassignmentto:pin
line6occursunconditionally: wecanassignto:pevenif pointstothesentinel.
Infact,weshallexploittheabilitytoassignto:pwhen T:nil.
D
The procedure RB-DELETE is like the TREE-DELETE procedure, but with ad-
ditional lines of pseudocode. Some of the additional lines keep track of a node y
that might cause violations of the red-black properties. When we want to delete
node´and´hasfewerthantwochildren, then´isremovedfromthetree,andwe
want y to be ´. When ´ has two children, then y should be ´’s successor, and y
moves into ´’s position in the tree. We also remember y’s color before it is re-
movedfromormovedwithinthetree,andwekeeptrackofthenodex thatmoves
into y’s original position in the tree, because node x might also cause violations
ofthe red-black properties. After deleting node ´, RB-DELETE calls anauxiliary
procedure RB-DELETE-FIXUP, which changes colors and performs rotations to
restorethered-black properties.
324 Chapter13 Red-BlackTrees
RB-DELETE.T;´/
1 y ´
D
2 y-original-color y:color
D
3 if´:left ==T:nil
4 x ´:right
D
5 RB-TRANSPLANT.T;´;´:right/
6 elseif´:right ==T:nil
7 x ´:left
D
8 RB-TRANSPLANT.T;´;´:left/
9 elsey TREE-MINIMUM.´:right/
D
10 y-original-color y:color
D
11 x y:right
D
12 ify:p==´
13 x:p y
D
14 else RB-TRANSPLANT.T;y;y:right/
15 y:right ´:right
D
16 y:right:p y
D
17 RB-TRANSPLANT.T;´;y/
18 y:left ´:left
D
19 y:left:p y
D
20 y:color ´:color
D
21 ify-original-color == BLACK
22 RB-DELETE-FIXUP.T;x/
Although RB-DELETE contains almost twice as many lines of pseudocode as
TREE-DELETE, the two procedures have the same basic structure. You can find
each line of TREE-DELETE within RB-DELETE (with the changes of replacing
NIL byT:nil and replacing calls to TRANSPLANT by calls to RB-TRANSPLANT),
executedunderthesameconditions.
Herearetheotherdifferences betweenthetwoprocedures:
 Wemaintain node y asthenodeeither removedfromthetreeormovedwithin
the tree. Line 1 sets y to point to node ´ when ´ has fewer than two children
andisthereforeremoved. When´hastwochildren,line9setsy topointto´’s
successor, just as in TREE-DELETE, and y will move into ´’s position in the
tree.
 Because node y’s color might change, the variable y-original-color stores y’s
color before any changes occur. Lines 2 and 10 set this variable immediately
after assignments to y. When ´ has two children, then y ´ and node y
¤
moves into node ´’s original position in the red-black tree; line 20 gives y the
same color as ´. We need to save y’s original color in order to test it at the
13.4 Deletion 325
end of RB-DELETE; if it was black, then removing or moving y could cause
violations ofthered-black properties.
 As discussed, we keep track of the node x that moves into node y’s original
position. Theassignments inlines4, 7,and 11setx topoint toeither y’sonly
child or, if y has no children, the sentinel T:nil. (Recall from Section 12.3
thaty hasnoleftchild.)
 Sincenodex movesintonode y’soriginal position, theattribute x:pisalways
settopointtotheoriginalpositioninthetreeofy’sparent,evenifx is,infact,
thesentinelT:nil. Unless´isy’soriginalparent(whichoccursonlywhen´has
twochildren anditssuccessor y is´’srightchild), theassignment tox:ptakes
place in line 6 of RB-TRANSPLANT. (Observe that when RB-TRANSPLANT
iscalledinlines5,8,or14,thethirdparameterpassedisthesameasx.)
Wheny’soriginalparentis´,however,wedonotwantx:ptopointtoy’sorig-
inalparent,sinceweareremovingthatnodefromthetree. Becausenodey will
move up to take ´’s position in the tree, setting x:p to y in line 13 causes x:p
topointtotheoriginalposition ofy’sparent,evenifx T:nil.
D
 Finally, ifnodey wasblack, wemighthave introduced oneormoreviolations
of the red-black properties, and so we call RB-DELETE-FIXUP in line 22 to
restorethered-blackproperties. Ify wasred,thered-blackpropertiesstillhold
wheny isremovedormoved,forthefollowingreasons:
1. Noblack-heights inthetreehavechanged.
2. No red nodes have been made adjacent. Because y takes ´’s place in the
tree,alongwith´’scolor, wecannothavetwoadjacentrednodesaty’snew
positioninthetree. Inaddition,ify wasnot´’srightchild,theny’soriginal
right childx replaces y inthetree. Ify isred, then x mustbeblack, and so
replacing y byx cannotcausetworednodestobecomeadjacent.
3. Sincey couldnothavebeentherootifitwasred,therootremainsblack.
Ifnode y wasblack, three problems mayarise, whichthe call of RB-DELETE-
FIXUP willremedy. First, ify hadbeen the root andaredchild of y becomes the
new root, we have violated property 2. Second, if both x and x:p are red, then
we have violated property 4. Third, moving y within the tree causes any simple
path that previously contained y to have one fewer black node. Thus, property 5
is now violated by any ancestor of y in the tree. We can correct the violation
of property 5 by saying that node x, now occupying y’s original position, has an
“extra” black. Thatis, ifweadd 1to thecount of black nodes on any simple path
thatcontains x,then under thisinterpretation, property 5holds. Whenweremove
or move the black node y, we “push” its blackness onto node x. The problem is
that now node x is neither red nor black, thereby violating property 1. Instead,
326 Chapter13 Red-BlackTrees
nodex iseither“doublyblack”or“red-and-black,” anditcontributes either2or1,
respectively, to the count of black nodes on simple paths containing x. The color
attribute of x will still be either RED (if x is red-and-black) or BLACK (if x is
doublyblack). Inotherwords,theextrablackonanodeisreflectedinx’spointing
tothenoderatherthaninthecolor attribute.
Wecannowseetheprocedure RB-DELETE-FIXUP andexaminehowitrestores
thered-black properties tothesearchtree.
RB-DELETE-FIXUP.T;x/
1 whilex T:root andx:color == BLACK
¤
2 ifx == x:p:left
3 w x:p:right
D
4 ifw:color == RED
5 w:color BLACK //case1
D
6 x:p:color RED //case1
D
7 LEFT-ROTATE.T;x:p/ //case1
8 w x:p:right //case1
D
9 ifw:left:color == BLACK andw:right:color == BLACK
10 w:color RED //case2
D
11 x x:p //case2
D
12 elseifw:right:color == BLACK
13 w:left:color BLACK //case3
D
14 w:color RED //case3
D
15 RIGHT-ROTATE.T;w/ //case3
16 w x:p:right //case3
D
17 w:color x:p:color //case4
D
18 x:p:color BLACK //case4
D
19 w:right:color BLACK //case4
D
20 LEFT-ROTATE.T;x:p/ //case4
21 x T:root //case4
D
22 else(sameasthenclausewith“right”and“left”exchanged)
23 x:color BLACK
D
The procedure RB-DELETE-FIXUP restores properties 1, 2, and 4. Exercises
13.4-1 and 13.4-2 ask you to show that the procedure restores properties 2 and 4,
and so in the remainder of this section, weshall focus on property 1. The goal of
thewhileloopinlines1–22istomovetheextrablackupthetreeuntil
1. x points to a red-and-black node, in which case we color x (singly) black in
line23;
2. x pointstotheroot,inwhichcasewesimply“remove”theextrablack;or
3. havingperformed suitablerotations andrecolorings, weexittheloop.
13.4 Deletion 327
Within the while loop, x always points to a nonroot doubly black node. We
determine in line 2 whether x is aleft child or aright child ofits parent x:p. (We
have given the code for the situation in which x is a left child; the situation in
which x is a right child—line 22—is symmetric.) We maintain a pointer w to
the sibling of x. Since node x is doubly black, node w cannot be T:nil, because
otherwise, thenumber ofblacks onthe simple path from x:pto the(singly black)
leafw wouldbesmallerthanthenumberonthesimplepathfromx:ptox.
Thefour cases2 in the code appear in Figure 13.7. Before examining each case
in detail, let’s look more generally at how we can verify that the transformation
in each of the cases preserves property 5. The key idea is that in each case, the
transformation applied preserves the number of black nodes (including x’s extra
black) from (and including) the root of the subtree shown to each of the subtrees
˛;ˇ;:::;. Thus, if property 5 holds prior to the transformation, it continues to
hold afterward. Forexample, inFigure 13.7(a), which illustrates case 1, the num-
berofblacknodesfromtheroottoeithersubtree˛ orˇ is3,bothbeforeandafter
thetransformation. (Again, rememberthatnodex addsanextrablack.) Similarly,
the number of black nodes from the root to any of , ı, ", and  is 2, both be-
foreandafterthetransformation. InFigure13.7(b), thecounting mustinvolve the
valuec ofthecolor attribute oftheroot ofthesubtree shown, whichcan beeither
RED or BLACK. If we define count.RED/ 0 and count.BLACK/ 1, then the
D D
number of black nodes from the root to ˛ is 2 count.c/, both before and after
C
thetransformation. Inthiscase,afterthetransformation, thenewnodex hascolor
attributec,butthisnodeisreallyeitherred-and-black(ifc RED)ordoublyblack
D
(ifc BLACK). Youcanverifytheothercasessimilarly(seeExercise13.4-5).
D
Case1: x’ssiblingwisred
Case1(lines5–8ofRB-DELETE-FIXUP andFigure13.7(a))occurswhennodew,
thesibling ofnodex,isred. Sincew musthaveblack children, wecanswitchthe
colors ofw and x:pand then perform aleft-rotation on x:pwithout violating any
of the red-black properties. The new sibling of x, which is one of w’s children
prior to the rotation, is now black, and thus we have converted case 1 into case 2,
3,or4.
Cases 2, 3, and 4 occur when node w is black; they are distinguished by the
colorsofw’schildren.
2AsinRB-INSERT-FIXUP,thecasesinRB-DELETE-FIXUParenotmutuallyexclusive.
328 Chapter13 Red-BlackTrees
Case2: x’ssiblingwisblack, andbothofw’schildrenareblack
In case 2 (lines 10–11 of RB-DELETE-FIXUP and Figure 13.7(b)), both of w’s
children are black. Since w is also black, we take one black off both x and w,
leaving x with only one black and leaving w red. To compensate for removing
one black from x and w, we would like to add an extra black to x:p, which was
originally either red or black. We do so by repeating the while loop with x:p as
the new node x. Observe that if we enter case 2 through case 1, the new node x
is red-and-black, since the original x:p was red. Hence, the value c of the color
attribute of the new node x is RED, and the loop terminates when it tests the loop
condition. Wethencolorthenewnodex (singly) blackinline23.
Case3: x’ssiblingwisblack, w’sleftchildisred,andw’srightchildisblack
Case 3 (lines 13–16 and Figure 13.7(c)) occurs when w is black, its left child
is red, and its right child is black. We can switch the colors of w and its left
child w:left and then perform a right rotation on w without violating any of the
red-black properties. Thenew sibling w ofx isnow ablack node witharedright
child, andthuswehavetransformed case3intocase4.
Case4: x’ssiblingwisblack, andw’srightchildisred
Case 4 (lines 17–21 and Figure 13.7(d)) occurs when node x’s sibling w is black
andw’srightchildisred. Bymakingsomecolorchangesandperformingaleftro-
tationonx:p,wecanremovetheextrablackonx,makingitsinglyblack,without
violating anyofthered-black properties. Settingx tobetherootcauses thewhile
looptoterminatewhenitteststheloopcondition.
Analysis
WhatistherunningtimeofRB-DELETE? Sincetheheightofared-blacktreeofn
nodes is O.lgn/, the total cost of the procedure without the call to RB-DELETE-
FIXUP takes O.lgn/ time. Within RB-DELETE-FIXUP, each of cases 1, 3, and 4
lead to termination after performing a constant number of color changes and at
most three rotations. Case 2 is the only case in which the while loop can be re-
peated,andthenthepointerx movesupthetreeatmostO.lgn/times,performing
norotations. Thus,theprocedure RB-DELETE-FIXUP takesO.lgn/timeandper-
forms at most three rotations, and the overall time for RB-DELETE is therefore
alsoO.lgn/.
13.4 Deletion 329
Case 1
B D
(a) x A D w B E
a b C E x A new w C e z
g d e z a b g d
Case 2
B c new x B c
(b) x A D w A D
a b C E a b C E
g d e z g d e z
Case 3
B c B c
(c) x A D w x A C new w
a b C E a b g D
g d e z d E
e z
Case 4
B c D c
(d) x A D w B E
a b C c¢ E A C c¢ e z
g d e z a b g d newx T:root
D
Figure13.7 Thecasesinthewhileloopoftheprocedure RB-DELETE-FIXUP. Darkenednodes
have color attributes BLACK, heavily shaded nodes have color attributes RED, and lightly shaded
nodeshavecolorattributesrepresentedbycandc 0,whichmaybeeitherREDorBLACK.Theletters
˛;ˇ;:::; representarbitrarysubtrees. Eachcasetransformstheconfigurationontheleftintothe
configurationontherightbychangingsomecolorsand/orperformingarotation. Anynodepointed
tobyxhasanextrablackandiseitherdoublyblackorred-and-black.Onlycase2causestheloopto
repeat. (a)Case1istransformedtocase2,3,or4byexchangingthecolorsofnodesB andDand
performingaleftrotation. (b)Incase2,theextrablackrepresentedbythepointerx movesupthe
treebycoloringnodeDredandsettingxtopointtonodeB. Ifweentercase2throughcase1,the
whileloopterminatesbecausethenewnodexisred-and-black,andthereforethevaluecofitscolor
attributeisRED.(c)Case3istransformedtocase4byexchangingthecolorsofnodesC andDand
performingarightrotation. (d)Case4removestheextrablackrepresentedbyxbychangingsome
colorsandperformingaleftrotation(withoutviolatingthered-blackproperties),andthentheloop
terminates.
330 Chapter13 Red-BlackTrees
Exercises
13.4-1
Arguethatafterexecuting RB-DELETE-FIXUP, therootofthetreemustbeblack.
13.4-2
ArguethatifinRB-DELETE bothx andx:parered,thenproperty4isrestoredby
thecallto RB-DELETE-FIXUP.T;x/.
13.4-3
In Exercise 13.3-2, you found the red-black tree that results from successively
inserting the keys 41;38;31;12;19;8 into an initially empty tree. Now show the
red-black trees that result from the successive deletion of the keys in the order
8;12;19;31;38;41.
13.4-4
In which lines of the code for RB-DELETE-FIXUP might we examine or modify
thesentinel T:nil?
13.4-5
Ineach ofthecases ofFigure 13.7, givethe count ofblack nodes from the rootof
the subtree shown to each of the subtrees ˛;ˇ;:::;, and verify that each count
remains the same after the transformation. When a node has a color attribute c
orc ,usethenotation count.c/orcount.c /symbolically inyourcount.
0 0
13.4-6
Professors Skelton and Baron are concerned that at the start of case 1 of RB-
DELETE-FIXUP, the node x:p might not be black. If the professors are correct,
thenlines5–6arewrong. Showthatx:pmustbeblackatthestartofcase1,sothat
theprofessors havenothingtoworryabout.
13.4-7
Suppose that a node x is inserted into a red-black tree with RB-INSERT and then
isimmediately deleted with RB-DELETE. Isthe resulting red-black treethesame
astheinitialred-black tree? Justifyyouranswer.
ProblemsforChapter13 331
Problems
13-1 Persistentdynamicsets
Duringthecourseofanalgorithm,wesometimesfindthatweneedtomaintainpast
versionsofadynamicsetasitisupdated. Wecallsuchasetpersistent. Onewayto
implementapersistent setistocopytheentiresetwheneveritismodified,butthis
approachcanslowdownaprogramandalsoconsumemuchspace. Sometimes,we
candomuchbetter.
ConsiderapersistentsetS withtheoperations INSERT, DELETE, and SEARCH,
which we implement using binary search trees as shown in Figure 13.8(a). We
maintain a separate root for every version of the set. In order to insert the key 5
into the set, we create a new node with key 5. This node becomes the left child
of a new node with key 7, since we cannot modify the existing node with key 7.
Similarly, the new node with key 7 becomes the left child of a new node with
key8whoserightchildistheexistingnodewithkey10. Thenewnodewithkey8
becomes, inturn, therightchildofanewrootr withkey4whoseleftchildisthe
0
existingnodewithkey3. Wethuscopyonlypartofthetreeandsharesomeofthe
nodeswiththeoriginaltree,asshowninFigure13.8(b).
Assume that each tree node has the attributes key, left, and right but no parent.
(SeealsoExercise13.3-6.)
4 r r 4 4 r¢
3 8 3 8 8
2 7 10 2 7 7 10
5
(a) (b)
Figure13.8 (a) A binary search treewithkeys 2;3;4;7;8;10. (b) Thepersistent binary search
treethatresultsfromtheinsertionofkey5. Themostrecentversionofthesetconsistsofthenodes
reachablefromtherootr ,andthepreviousversionconsistsofthenodesreachablefromr.Heavily
0
shadednodesareaddedwhenkey5isinserted.
332 Chapter13 Red-BlackTrees
a. For a general persistent binary search tree, identify the nodes that we need to
changetoinsertakeyk ordeleteanodey.
b. Write a procedure PERSISTENT-TREE-INSERT that, given a persistent tree T
andakeyk toinsert,returnsanewpersistent treeT thatistheresultofinsert-
0
ingk intoT.
c. If the height of the persistent binary search tree T is h, what are the time and
space requirements of your implementation of PERSISTENT-TREE-INSERT?
(Thespacerequirement isproportional tothenumberofnewnodesallocated.)
d. Suppose that we had included the parent attribute in each node. In this case,
PERSISTENT-TREE-INSERT wouldneed toperform additional copying. Prove
that PERSISTENT-TREE-INSERT would then require .n/ time and space,
wherenisthenumberofnodesinthetree.
e. Showhow touse red-black trees toguarantee that the worst-case running time
andspaceareO.lgn/perinsertion ordeletion.
13-2 Joinoperation onred-black trees
The join operation takes two dynamic sets S and S and an element x such that
1 2
for any x S and x S , we have x :key x:key x :key. It returns a set
1 1 2 2 1 2
2 2  
S S x S . In this problem, we investigate how to implement the join
1 2
D [ f g[
operation onred-black trees.
a. Givenared-blacktreeT,letusstoreitsblack-height asthenewattributeT:bh.
Argue that RB-INSERT and RB-DELETE can maintain the bh attribute with-
out requiring extra storage in the nodes of the tree and without increasing the
asymptotic running times. Show that while descending through T, we can de-
terminetheblack-height ofeachnodewevisitinO.1/timepernodevisited.
WewishtoimplementtheoperationRB-JOIN.T 1;x;T 2/,whichdestroysT 1andT
2
andreturnsared-blacktreeT T x T . Letnbethetotalnumberofnodes
1 2
D [f g[
inT andT .
1 2
b. Assume that T :bh T :bh. Describe an O.lgn/-time algorithm that finds a
1 2

black node y inT withthe largest key from among those nodes whose black-
1
heightisT :bh.
2
c. LetT bethe subtree rooted aty. Describe howT x T can replace T
y y 2 y
[f g[
inO.1/timewithoutdestroying thebinary-search-tree property.
d. Whatcolorshouldwemakex sothatred-blackproperties1,3,and5aremain-
tained? Describehowtoenforceproperties 2and4inO.lgn/time.
ProblemsforChapter13 333
e. Arguethatnogenerality islostbymakingtheassumption inpart(b). Describe
thesymmetricsituation thatariseswhenT :bh T :bh.
1 2

f. Arguethattherunning timeof RB-JOIN isO.lgn/.
13-3 AVLtrees
An AVL tree is a binary search tree that is height balanced: for each node x, the
heightsoftheleftandrightsubtreesofxdifferbyatmost1. ToimplementanAVL
tree, we maintain an extra attribute in each node: x:his the height of node x. As
foranyotherbinarysearchtreeT,weassumethatT:root pointstotherootnode.
a. Prove that an AVL tree with n nodes has height O.lgn/. (Hint: Prove that
an AVL tree of height h has at least F nodes, where F is the hth Fibonacci
h h
number.)
b. ToinsertintoanAVLtree,wefirstplaceanodeintotheappropriateplaceinbi-
narysearch treeorder. Afterward, thetreemightnolonger beheight balanced.
Specifically,theheightsoftheleftandrightchildrenofsomenodemightdiffer
by 2. Describe a procedure BALANCE.x/, which takes a subtree rooted at x
whose left and right children are height balanced and have heights that differ
by atmost2, i.e., x:right:h x:left:h 2, and alters thesubtree rooted atx
j (cid:0) j 
tobeheightbalanced. (Hint:Userotations.)
c. Using part (b), describe a recursive procedure AVL-INSERT.x;´/ that takes
a node x within an AVL tree and a newly created node ´ (whose key has al-
ready been filled in), and adds ´ to the subtree rooted at x, maintaining the
property that x is the root of an AVL tree. As in TREE-INSERT from Sec-
tion 12.3, assume that ´:key has already been filled in and that ´:left NIL
D
and ´:right NIL; also assume that ´:h 0. Thus, to insert the node ´ into
D D
theAVLtreeT,wecall AVL-INSERT.T:root;´/.
d. Show that AVL-INSERT, run on an n-node AVL tree, takes O.lgn/ time and
performsO.1/rotations.
13-4 Treaps
If we insert a set of n items into a binary search tree, the resulting tree may be
horribly unbalanced, leading to long search times. As we saw in Section 12.4,
however, randomly built binary search trees tend to be balanced. Therefore, one
strategythat,onaverage,buildsabalancedtreeforafixedsetofitemswouldbeto
randomlypermutetheitemsandtheninserttheminthatorderintothetree.
What if we do not have all the items at once? If we receive the items one at a
time,canwestillrandomlybuildabinarysearchtreeoutofthem?
334 Chapter13 Red-BlackTrees
G: 4
B: 7 H: 5
A: 10 E: 23 K: 65
I: 73
Figure13.9 A treap. Eachnode x islabeledwithx:key: x:priority. For example, theroot has
keyGandpriority4.
Wewillexamineadatastructurethatanswersthisquestionintheaffirmative. A
treapisabinarysearchtreewithamodifiedwayoforderingthenodes. Figure13.9
shows an example. As usual, each node x in the tree has a key value x:key. In
addition, we assign x:priority, which is a random number chosen independently
for each node. We assume that all priorities are distinct and also that all keys are
distinct. Thenodesofthetreapareorderedsothatthekeysobeythebinary-search-
treeproperty andthepriorities obeythemin-heaporderproperty:
 If isaleftchildofu,then:key <u:key.
 If isarightchildofu,then:key > u:key.
 If isachildofu,then:priority > u:priority.
(This combination of properties is why the tree is called a “treap”: it has features
ofbothabinarysearchtreeandaheap.)
It helps to think of treaps in the following way. Suppose that we insert nodes
x ;x ;:::;x , with associated keys, into a treap. Then the resulting treap is the
1 2 n
tree that would have been formed if the nodes had been inserted into a normal
binary search tree in the order given by their (randomly chosen) priorities, i.e.,
x :priority <x :prioritymeansthatwehadinserted x beforex .
i j i j
a. Show that given aset of nodes x ;x ;:::;x , with associated keys and priori-
1 2 n
ties,alldistinct, thetreapassociated withthesenodesisunique.
b. Showthattheexpectedheightofatreapis‚.lgn/,andhencetheexpectedtime
tosearchforavalueinthetreapis‚.lgn/.
Let us see how to insert a new node into an existing treap. The first thing we do
is assign tothe new node arandom priority. Thenwecall the insertion algorithm,
whichwecall TREAP-INSERT, whoseoperation isillustrated inFigure13.10.
ProblemsforChapter13 335
G: 4 G: 4
B: 7 H: 5 C: 25 B: 7 H: 5
A: 10 E: 23 K: 65 A: 10 E: 23 K: 65
I: 73 C: 25 I: 73
(a) (b)
G: 4 G: 4
D: 9 B: 7 H: 5 B: 7 H: 5
A: 10 E: 23 K: 65 A: 10 E: 23 K: 65
C: 25 I: 73 D: 9 I: 73
D: 9 C: 25
(c) (d)
G: 4 F: 2
B: 7 H: 5 F: 2 B: 7 G: 4
…
A: 10 D: 9 K: 65 A: 10 D: 9 H: 5
C: 25 E: 23 I: 73 C: 25 E: 23 K: 65
I: 73
(e) (f)
Figure13.10 TheoperationofTREAP-INSERT.(a)Theoriginaltreap,priortoinsertion. (b)The
treapafterinsertinganodewithkeyC andpriority25.(c)–(d)Intermediatestageswheninsertinga
nodewithkeyDandpriority9. (e)Thetreapaftertheinsertionofparts(c)and(d)isdone. (f)The
treapafterinsertinganodewithkeyF andpriority2.
336 Chapter13 Red-BlackTrees
15 15
9 18 9 18
3 12 25 3 12 25
6 21 6 21
(a) (b)
Figure13.11 Spinesofabinarysearchtree. Theleftspineisshadedin(a),andtherightspineis
shadedin(b).
c. ExplainhowTREAP-INSERT works. ExplaintheideainEnglishandgivepseu-
docode. (Hint: Execute the usual binary-search-tree insertion procedure and
thenperform rotations torestorethemin-heaporderproperty.)
d. Showthattheexpected runningtimeofTREAP-INSERT is‚.lgn/.
TREAP-INSERT performs a search and then a sequence of rotations. Although
these two operations have the same expected running time, they have different
costs inpractice. A search reads information from the treap without modifying it.
Incontrast, arotation changes parent andchild pointers within thetreap. Onmost
computers, read operations aremuch faster than writeoperations. Thuswewould
like TREAP-INSERT to perform few rotations. We will show that the expected
numberofrotations performedisbounded byaconstant.
In order to do so, we will need some definitions, which Figure 13.11 depicts.
TheleftspineofabinarysearchtreeT isthesimplepathfromtheroottothenode
with the smallest key. In other words, the left spine is the simple path from the
root that consists of only left edges. Symmetrically, the right spine of T is the
simple path from the root consisting of only right edges. The length of a spine is
thenumberofnodesitcontains.
e. Consider the treap T immediately after TREAP-INSERT has inserted node x.
Let C be the length of the right spine of the left subtree of x. Let D be the
length oftheleftspine oftherightsubtree ofx. Provethatthetotal numberof
rotations thatwereperformed duringtheinsertion ofx isequaltoC D.
C
WewillnowcalculatetheexpectedvaluesofC andD. Withoutlossofgenerality,
weassume that the keys are1;2;:::;n, since wearecomparing them only to one
another.
NotesforChapter13 337
Fornodes x and y intreap T, where y x,let k x:keyand i y:key. We
¤ D D
defineindicator randomvariables
X I y isintherightspineoftheleftsubtree ofx :
ik
D f g
f. Showthat X 1ifand onlyify:priority > x:priority, y:key < x:key, and,
ik
D
forevery´suchthaty:key < ´:key <x:key,wehavey:priority < ´:priority.
g. Showthat
.k i 1/Š
Pr X 1 (cid:0) (cid:0)
ik
f D g D .k i 1/Š
(cid:0) C
1
:
D .k i 1/.k i/
(cid:0) C (cid:0)
h. Showthat
k 1
(cid:0) 1
EŒC
D j.j 1/
j 1 C
XD
1
1 :
D (cid:0) k
i. Useasymmetryargumenttoshowthat
1
EŒD 1 :
D (cid:0) n k 1
(cid:0) C
j. Conclude that the expected number of rotations performed when inserting a
nodeintoatreapislessthan2.
Chapter notes
TheideaofbalancingasearchtreeisduetoAdel’son-Vel’ski˘ıandLandis[2],who
introducedaclassofbalancedsearchtreescalled“AVLtrees”in1962,describedin
Problem13-3. Anotherclassofsearch trees,called “2-3trees,”wasintroduced by
J.E.Hopcroft(unpublished)in1970. A2-3treemaintainsbalancebymanipulating
the degrees of nodes in the tree. Chapter 18 covers a generalization of 2-3 trees
introduced byBayerandMcCreight[35],called“B-trees.”
Red-blacktreeswereinventedbyBayer[34]underthename“symmetricbinary
B-trees.” Guibas and Sedgewick [155] studied their properties at length and in-
troduced the red/black color convention. Andersson [15] gives a simpler-to-code
338 Chapter13 Red-BlackTrees
variant of red-black trees. Weiss [351] calls this variant AA-trees. An AA-tree is
similartoared-black treeexceptthatleftchildrenmayneverbered.
Treaps,thesubjectofProblem13-4,wereproposedbySeidelandAragon[309].
They are the default implementation of a dictionary in LEDA [253], which is a
well-implemented collection ofdatastructures andalgorithms.
There are many other variations on balanced binary trees, including weight-
balanced trees [264], k-neighbor trees [245], and scapegoat trees [127]. Perhaps
the most intriguing are the “splay trees” introduced by Sleator and Tarjan [320],
whichare“self-adjusting.” (SeeTarjan[330]foragooddescriptionofsplaytrees.)
Splay trees maintain balance without anyexplicit balance condition such ascolor.
Instead,“splayoperations” (whichinvolverotations)areperformedwithinthetree
every timeanaccess ismade. Theamortized cost (seeChapter 17) ofeach opera-
tiononann-nodetreeisO.lgn/.
Skip lists [286] provide an alternative to balanced binary trees. A skip list is a
linkedlistthatisaugmentedwithanumberofadditional pointers. Eachdictionary
operation runsinexpected timeO.lgn/onaskiplistofnitems.
14 Augmenting Data Structures
Some engineering situations require no more than a “textbook” data struc-
ture—such asadoubly linked list, ahashtable, orabinary search tree—but many
others require a dash of creativity. Only in rare situations will you need to cre-
ate an entirely new type of data structure, though. More often, it will suffice to
augment atextbook datastructure bystoring additional information init. Youcan
then program new operations for the data structure tosupport the desired applica-
tion. Augmentingadatastructureisnotalwaysstraightforward,however,sincethe
added information must be updated and maintained by the ordinary operations on
thedatastructure.
Thischapterdiscusses twodatastructures thatweconstruct byaugmenting red-
black trees. Section 14.1 describes a data structure that supports general order-
statistic operations on a dynamic set. We can then quickly find the ith smallest
number in a set or the rank of a given element in the total ordering of the set.
Section 14.2 abstracts the process of augmenting a data structure and provides a
theorem thatcansimplify theprocess ofaugmenting red-black trees. Section14.3
uses this theorem to help design adata structure for maintaining a dynamic set of
intervals, such as time intervals. Given a query interval, we can then quickly find
anintervalinthesetthatoverlapsit.
14.1 Dynamicorderstatistics
Chapter 9 introduced the notion of an order statistic. Specifically, the ith order
statisticofasetofnelements,wherei 1;2;:::;n ,issimplytheelementinthe
2 f g
setwiththeithsmallestkey. WesawhowtodetermineanyorderstatisticinO.n/
timefrom an unordered set. In this section, weshall see how to modify red-black
treessothatwecandetermineanyorderstatisticforadynamicsetinO.lgn/time.
Weshallalsoseehowtocomputetherankofanelement—itspositioninthelinear
orderoftheset—inO.lgn/time.
340 Chapter14 AugmentingDataStructures
26
20
17 41
12 7
14 21 30 47
7 4 5 1
10 16 19 21 28 38
4 2 2 1 1 3
7 12 14 20 key 35 39
2 1 1 1 1 1
3 size
1
Figure14.1 Anorder-statistictree,whichisanaugmentedred-blacktree. Shadednodesarered,
anddarkenednodesareblack.Inadditiontoitsusualattributes,eachnodexhasanattributex:size,
whichisthenumberofnodes,otherthanthesentinel,inthesubtreerootedatx.
Figure14.1showsadatastructurethatcansupportfastorder-statisticoperations.
An order-statistic tree T is simply a red-black tree with additional information
storedineachnode. Besidestheusualred-blacktreeattributesx:key,x:color,x:p,
x:left, and x:right in a node x, we have another attribute, x:size. This attribute
contains the number of (internal) nodes in the subtree rooted at x (including x
itself), thatis, thesize ofthesubtree. Ifwedefinethesentinel’s sizetobe0—that
is,wesetT:nil:sizetobe0—thenwehavetheidentity
x:size x:left:size x:right:size 1:
D C C
Wedonotrequirekeystobedistinctinanorder-statistic tree. (Forexample,the
treeinFigure14.1hastwokeyswithvalue14andtwokeyswithvalue21.) Inthe
presence of equal keys, the above notion of rank is not well defined. We remove
this ambiguity for an order-statistic tree by defining the rank of an element as the
positionatwhichitwouldbeprintedinaninorderwalkofthetree. InFigure14.1,
forexample,thekey14storedinablacknodehasrank5,andthekey14storedin
arednodehasrank6.
Retrievinganelementwithagivenrank
Before we show how to maintain this size information during insertion and dele-
tion, let us examine the implementation of two order-statistic queries that use this
additional information. Webegin withan operation that retrieves an element with
a given rank. The procedure OS-SELECT.x;i/ returns a pointer to the node con-
tainingtheithsmallestkeyinthesubtreerootedatx. Tofindthenodewiththeith
smallestkeyinanorder-statistic treeT,wecall OS-SELECT.T:root;i/.
14.1 Dynamicorderstatistics 341
OS-SELECT.x;i/
1 r x:left:size 1
D C
2 ifi ==r
3 returnx
4 elseifi < r
5 return OS-SELECT.x:left;i/
6 elsereturn OS-SELECT.x:right;i r/
(cid:0)
In line 1 of OS-SELECT, we compute r, the rank of node x within the subtree
rooted at x. The value of x:left:size is the number of nodes that come before x
in an inorder tree walk of the subtree rooted at x. Thus, x:left:size 1 is the
C
rank of x within the subtree rooted at x. If i r, then node x is the ith smallest
D
element, and so we return x in line 3. If i < r, then the ith smallest element
resides in x’s left subtree, and so we recurse on x:left in line 5. If i > r, then
the ith smallest element resides in x’sright subtree. Since the subtree rooted atx
contains r elements that comebefore x’sright subtree inaninorder treewalk, the
ithsmallest element in thesubtree rooted atx isthe .i r/thsmallest element in
(cid:0)
thesubtreerootedatx:right. Line6determines thiselementrecursively.
To see how OS-SELECT operates, consider a search for the 17th smallest ele-
mentintheorder-statistic tree ofFigure14.1. Webegin withx astheroot, whose
keyis26, and withi 17. Since thesize of26’s leftsubtree is12, itsrank is13.
D
Thus, we know that the node with rank 17 is the 17 13 4th smallest element
(cid:0) D
in26’srightsubtree. Aftertherecursivecall,x isthenodewithkey41,andi 4.
D
Since the size of 41’s left subtree is 5, its rank within its subtree is 6. Thus, we
knowthatthenodewithrank4isthe4thsmallestelementin41’sleftsubtree. Af-
tertherecursivecall,x isthenodewithkey30,anditsrankwithinitssubtreeis2.
Thus,werecurseonceagaintofindthe4 2 2ndsmallestelementinthesubtree
(cid:0) D
rooted at thenode withkey 38. Wenow findthat itsleft subtree hassize 1, which
means it is the second smallest element. Thus, the procedure returns a pointer to
thenodewithkey38.
Because each recursive call goes down one level in the order-statistic tree, the
total time for OS-SELECT is at worst proportional tothe height of the tree. Since
the tree is a red-black tree, its height is O.lgn/, where n is the number of nodes.
Thus,therunning timeofOS-SELECT isO.lgn/foradynamicsetofnelements.
Determiningtherankofanelement
Given a pointer to a node x in an order-statistic tree T, the procedure OS-RANK
returns the position of x in the linear order determined by an inorder tree walk
ofT.
342 Chapter14 AugmentingDataStructures
OS-RANK.T;x/
1 r x:left:size 1
D C
2 y x
D
3 whiley T:root
¤
4 ify ==y:p:right
5 r r y:p:left:size 1
D C C
6 y y:p
D
7 returnr
Theprocedureworksasfollows. Wecanthinkofnodex’srankasthenumberof
nodesprecedingx inaninordertreewalk,plus1forx itself. OS-RANK maintains
thefollowingloopinvariant:
At the start of each iteration of the while loop of lines 3–6, r is the rank
ofx:keyinthesubtreerootedatnodey.
Weusethisloopinvariant toshowthat OS-RANK workscorrectly asfollows:
Initialization: Priortothefirstiteration,line1setsr tobetherankofx:keywithin
the subtree rooted at x. Setting y x in line 2 makes the invariant true the
D
firsttimethetestinline3executes.
Maintenance: At the end of each iteration of the while loop, we set y y:p.
D
Thuswemustshowthatifr istherankofx:keyinthesubtreerootedaty atthe
start of the loop body, then r is the rank of x:key in the subtree rooted at y:p
at the end of the loop body. In each iteration of the while loop, we consider
thesubtree rooted aty:p. Wehavealready counted thenumberofnodes inthe
subtreerootedatnodey thatprecedex inaninorderwalk,andsowemustadd
thenodesinthesubtree rootedaty’ssibling thatprecede x inaninorder walk,
plus1fory:pifit,too,precedesx. Ify isaleftchild,thenneithery:pnorany
nodeiny:p’srightsubtreeprecedesx,andsoweleaver alone. Otherwise,y is
arightchildandallthenodesiny:p’sleftsubtreeprecedex,asdoesy:pitself.
Thus,inline5,weaddy:p:left:size 1tothecurrentvalueofr.
C
Termination: The loop terminates when y T:root, so that the subtree rooted
D
aty istheentiretree. Thus,thevalueofr istherankofx:keyintheentiretree.
Asanexample,whenwerunOS-RANKontheorder-statistictreeofFigure14.1
to find the rank of the node with key 38, we get the following sequence of values
ofy:keyandr atthetopofthewhileloop:
iteration y:key r
1 38 2
2 30 4
3 41 4
4 26 17
14.1 Dynamicorderstatistics 343
Theprocedure returnstherank17.
SinceeachiterationofthewhilelooptakesO.1/time,andygoesuponelevelin
thetreewitheachiteration, therunningtimeofOS-RANK isatworstproportional
totheheightofthetree: O.lgn/onann-nodeorder-statistic tree.
Maintainingsubtreesizes
Given the size attribute in each node, OS-SELECT and OS-RANK can quickly
compute order-statistic information. But unless we can efficiently maintain these
attributes within the basic modifying operations on red-black trees, our work will
have been for naught. We shall now show how to maintain subtree sizes for both
insertion and deletion without affecting the asymptotic running time of either op-
eration.
We noted in Section 13.3 that insertion into a red-black tree consists of two
phases. The first phase goes down the tree from the root, inserting the new node
asachildofanexisting node. Thesecond phase goesupthetree, changing colors
andperformingrotations tomaintainthered-black properties.
Tomaintain the subtree sizes in the first phase, wesimply increment x:size for
eachnodexonthesimplepathtraversedfromtherootdowntowardtheleaves. The
new node added gets a size of 1. Since there are O.lgn/ nodes on the traversed
path,theadditional costofmaintaining thesizeattributes isO.lgn/.
Inthesecondphase,theonlystructuralchangestotheunderlying red-blacktree
are caused by rotations, of which there are at most two. Moreover, a rotation is
a local operation: only two nodes have their size attributes invalidated. The link
around which the rotation is performed is incident on these two nodes. Referring
tothecodefor LEFT-ROTATE.T;x/inSection13.2,weaddthefollowinglines:
13 y:size x:size
D
14 x:size x:left:size x:right:size 1
D C C
Figure 14.2 illustrates how the attributes are updated. The change to RIGHT-
ROTATE issymmetric.
Sinceatmosttworotationsareperformedduringinsertion intoared-black tree,
we spend only O.1/ additional time updating size attributes in the second phase.
Thus, the total time for insertion into an n-node order-statistic tree is O.lgn/,
whichisasymptotically thesameasforanordinary red-black tree.
Deletion from a red-black tree also consists of two phases: the first operates
on the underlying search tree, and the second causes at most three rotations and
otherwise performs no structural changes. (See Section 13.4.) The first phase
either removes one node y from the tree or moves upward it within the tree. To
update the subtree sizes, we simply traverse a simple path from node y (starting
from its original position within the tree) up to the root, decrementing the size
344 Chapter14 AugmentingDataStructures
LEFT-ROTATE(T, x)
93 42
y x
19 19
42 93
x y
11 12
RIGHT-ROTATE(T, y)
7 6
6 4 4 7
Figure14.2 Updatingsubtreesizesduringrotations. Thelinkaroundwhichwerotateisincident
onthetwonodeswhosesizeattributesneedtobeupdated. Theupdatesarelocal,requiringonlythe
sizeinformationstoredinx,y,andtherootsofthesubtreesshownastriangles.
attribute of each node on the path. Since this path has length O.lgn/ in an n-
nodered-blacktree,theadditionaltimespentmaintainingsizeattributesinthefirst
phase is O.lgn/. We handle the O.1/ rotations in the second phase of deletion
in the same manner as for insertion. Thus, both insertion and deletion, including
maintaining thesizeattributes, takeO.lgn/timeforann-nodeorder-statistic tree.
Exercises
14.1-1
Show how OS-SELECT.T:root;10/ operates on the red-black tree T of Fig-
ure14.1.
14.1-2
Show how OS-RANK.T;x/ operates on the red-black tree T of Figure 14.1 and
thenodex withx:key 35.
D
14.1-3
Writeanonrecursive versionofOS-SELECT.
14.1-4
Write a recursive procedure OS-KEY-RANK.T;k/ that takes as input an order-
statistictreeT andakeyk andreturnstherankofkinthedynamicsetrepresented
byT. AssumethatthekeysofT aredistinct.
14.1-5
Given an element x in an n-node order-statistic tree and a natural number i, how
can we determine the ith successor of x in the linear order of the tree in O.lgn/
time?
14.2 Howtoaugmentadatastructure 345
14.1-6
Observe that whenever we reference the size attribute of a node in either OS-
SELECT or OS-RANK, we use it only to compute a rank. Accordingly, suppose
we store in each node its rank in the subtree of which it is the root. Show how to
maintainthisinformationduringinsertionanddeletion. (Rememberthatthesetwo
operations cancauserotations.)
14.1-7
Show how to use an order-statistic tree to count the number of inversions (see
Problem2-4)inanarrayofsizenintimeO.nlgn/.
14.1-8 ?
Considernchordsonacircle,eachdefinedbyitsendpoints. DescribeanO.nlgn/-
timealgorithm todetermine thenumber ofpairs ofchords thatintersect insidethe
circle. (Forexample, ifthen chords are alldiameters that meetatthe center, then
n
thecorrectansweris .) Assumethatnotwochordsshareanendpoint.
2
(cid:0) 
14.2 Howto augment adata structure
Theprocessofaugmentingabasicdatastructuretosupportadditionalfunctionality
occursquitefrequentlyinalgorithmdesign. Weshalluseitagaininthenextsection
todesign adata structure that supports operations onintervals. Inthis section, we
examine the steps involved in such augmentation. We shall also prove a theorem
thatallowsustoaugmentred-black treeseasilyinmanycases.
Wecanbreaktheprocessofaugmenting adatastructure intofoursteps:
1. Chooseanunderlying datastructure.
2. Determineadditional information tomaintainintheunderlying datastructure.
3. Verifythat wecanmaintain theadditional information forthebasic modifying
operations ontheunderlying datastructure.
4. Developnewoperations.
As with any prescriptive design method, you should not blindly follow the steps
in the order given. Most design work contains an element of trial and error, and
progressonallstepsusuallyproceedsinparallel. Thereisnopoint,forexample,in
determining additional information anddeveloping newoperations (steps 2and4)
ifwewillnotbeabletomaintain theadditional information efficiently. Neverthe-
less, this four-step method provides a good focus for your efforts in augmenting
a data structure, and it is also a good way to organize the documentation of an
augmenteddatastructure.
346 Chapter14 AugmentingDataStructures
We followed these steps in Section 14.1 to design our order-statistic trees. For
step 1, we chose red-black trees as the underlying data structure. A clue to the
suitability of red-black trees comes from their efficient support of other dynamic-
set operations on a total order, such as MINIMUM, MAXIMUM, SUCCESSOR, and
PREDECESSOR.
Forstep2,weaddedthesizeattribute,inwhicheachnodexstoresthesizeofthe
subtree rooted at x. Generally, the additional information makes operations more
efficient. For example, we could have implemented OS-SELECT and OS-RANK
using just thekeysstored inthetree, but theywouldnothave runinO.lgn/time.
Sometimes, the additional information is pointer information rather than data, as
inExercise14.2-1.
For step 3, we ensured that insertion and deletion could maintain the size at-
tributes whilestillrunninginO.lgn/time. Ideally, weshouldneedtoupdateonly
afewelementsofthedatastructureinordertomaintaintheadditionalinformation.
Forexample,ifwesimplystoredineachnodeitsrankinthetree,theOS-SELECT
and OS-RANK procedures would run quickly, but inserting a new minimum ele-
mentwouldcauseachangetothisinformationineverynodeofthetree. Whenwe
store subtree sizes instead, inserting a new element causes information to change
inonlyO.lgn/nodes.
Forstep 4,wedeveloped theoperations OS-SELECT and OS-RANK. Afterall,
theneedfornewoperationsiswhywebothertoaugmentadatastructureinthefirst
place. Occasionally, rather than developing new operations, weuse the additional
information toexpediteexistingones,asinExercise14.2-1.
Augmentingred-blacktrees
When red-black trees underlie an augmented data structure, wecan prove that in-
sertion and deletion can always efficiently maintain certain kinds of additional in-
formation, therebymakingstep3veryeasy. Theproofofthefollowingtheoremis
similar to the argument from Section 14.1 that we can maintain the size attribute
fororder-statistic trees.
Theorem14.1(Augmentingared-black tree)
Letf beanattributethataugmentsared-blacktreeT ofnnodes,andsupposethat
thevalueoff foreachnodex dependsononlytheinformationinnodesx,x:left,
and x:right, possibly including x:left:f and x:right:f. Then, wecan maintain the
valuesoff inallnodesofT duringinsertion anddeletion withoutasymptotically
affecting theO.lgn/performance oftheseoperations.
Proof The main idea of the proof is that a change to an f attribute in a node x
propagates only to ancestors of x in the tree. That is, changing x:f may re-
14.2 Howtoaugmentadatastructure 347
quire x:p:f to be updated, but nothing else; updating x:p:f may require x:p:p:f
to be updated, but nothing else; and so on up the tree. Once we have updated
T:root:f, no other node will depend on the new value, and so the process termi-
nates. Since theheight ofared-black tree isO.lgn/,changing anf attribute ina
nodecostsO.lgn/timeinupdating allnodesthatdependonthechange.
Insertion of a node x into T consists of two phases. (See Section 13.3.) The
firstphase inserts x as achild of an existing node x:p. Wecan compute the value
of x:f in O.1/ time since, by supposition, it depends only on information in the
other attributes ofx itself and theinformation inx’schildren, but x’schildren are
both the sentinel T:nil. Once we have computed x:f, the change propagates up
thetree. Thus, thetotal time forthe firstphase of insertion is O.lgn/. During the
second phase, the only structural changes to the tree come from rotations. Since
only two nodes change in a rotation, the total time for updating the f attributes
is O.lgn/ per rotation. Since the number of rotations during insertion is at most
two,thetotaltimeforinsertion isO.lgn/.
Like insertion, deletion has two phases. (See Section 13.4.) In the first phase,
changes to the tree occur when the deleted node is removed from the tree. If the
deletednodehadtwochildrenatthetime,thenitssuccessormovesintotheposition
of the deleted node. Propagating the updates to f caused by these changes costs
atmostO.lgn/,sincethechangesmodifythetreelocally. Fixingupthered-black
tree during the second phase requires at most three rotations, and each rotation
requires at most O.lgn/ time to propagate the updates to f. Thus, like insertion,
thetotaltimefordeletion isO.lgn/.
Inmanycases, suchasmaintaining thesizeattributes inorder-statistic trees,the
costofupdatingafterarotationisO.1/,ratherthantheO.lgn/derivedintheproof
ofTheorem14.1. Exercise14.2-3givesanexample.
Exercises
14.2-1
Show, by adding pointers to the nodes, how to support each of the dynamic-set
queries MINIMUM, MAXIMUM, SUCCESSOR, and PREDECESSOR inO.1/worst-
case time on an augmented order-statistic tree. The asymptotic performance of
otheroperations onorder-statistic treesshouldnotbeaffected.
14.2-2
Canwemaintain the black-heights of nodes in ared-black tree asattributes inthe
nodes of the tree without affecting the asymptotic performance of any of the red-
black tree operations? Show how, or argue why not. How about maintaining the
depthsofnodes?
348 Chapter14 AugmentingDataStructures
14.2-3 ?
Let beanassociativebinaryoperator,andletabeanattributemaintainedineach
˝
node ofared-black tree. Suppose thatwewanttoinclude ineachnode x anaddi-
tionalattributef suchthatx:f x :a x :a x :a,wherex ;x ;:::;x
1 2 m 1 2 m
D ˝ ˝˝
istheinorderlistingofnodesinthesubtreerootedatx. Showhowtoupdatethef
attributes in O.1/ time after a rotation. Modify your argument slightly to apply it
tothesizeattributes inorder-statistic trees.
14.2-4 ?
We wish to augment red-black trees with an operation RB-ENUMERATE.x;a;b/
that outputs all the keys k such that a k b in a red-black tree rooted at x.
 
DescribehowtoimplementRB-ENUMERATE in‚.m lgn/time,wheremisthe
C
number of keys that are output and n is the number of internal nodes in the tree.
(Hint:Youdonotneedtoaddnewattributes tothered-black tree.)
14.3 Interval trees
Inthissection, weshallaugmentred-black treestosupportoperations ondynamic
sets ofintervals. Aclosed interval isan ordered pair of real numbers Œt ;t , with
1 2
t t . The interval Œt ;t  represents the set t R t t t . Open and
1 2 1 2 1 2
 f 2 W   g
half-openintervals omitbothoroneoftheendpoints fromtheset,respectively. In
thissection,weshallassumethatintervalsareclosed;extendingtheresultstoopen
andhalf-open intervals isconceptually straightforward.
Intervals are convenient for representing events that each occupy a continuous
period of time. Wemight, for example, wish toquery adatabase of timeintervals
tofindout whatevents occurred during agiveninterval. Thedata structure inthis
section providesanefficientmeansformaintaining suchanintervaldatabase.
We can represent an interval Œt ;t  as an object i, with attributes i:low t
1 2 1
D
(the low endpoint) and i:high t (the high endpoint). We say that intervals i
2
D
and i overlap if i i , that is, if i:low i :high and i :low i:high. As
0 0 0 0
\ ¤ ;  
Figure 14.3 shows, any two intervals i and i satisfy the interval trichotomy; that
0
is,exactlyoneofthefollowingthreeproperties holds:
a. i andi overlap,
0
b. i istotheleftofi (i.e.,i:high <i :low),
0 0
c. i istotherightofi (i.e.,i :high <i:low).
0 0
Anintervaltreeisared-blacktreethatmaintainsadynamicsetofelements,with
each element x containing an interval x:int. Interval trees support the following
operations:
14.3 Intervaltrees 349
i i i i
i¢ i¢ i¢ i¢
(a)
i i¢ i¢ i
(b) (c)
Figure14.3 Theintervaltrichotomyfortwoclosedintervalsi andi .(a)Ifi andi overlap,there
0 0
arefoursituations;ineach,i:low i :highandi :low i:high. (b)Theintervalsdonotoverlap,
 0 0 
andi:high<i 0:low.(c)Theintervalsdonotoverlap,andi 0:high<i:low.
INTERVAL-INSERT.T;x/ adds the element x, whose int attribute is assumed to
containaninterval,totheintervaltreeT.
INTERVAL-DELETE.T;x/removestheelementx fromtheintervaltreeT.
INTERVAL-SEARCH.T;i/ returns a pointer to an element x in the interval tree T
such that x:int overlaps interval i, or a pointer to the sentinel T:nil if no such
elementisintheset.
Figure14.4showshowanintervaltreerepresents asetofintervals. Weshalltrack
thefour-step methodfromSection14.2aswereviewthedesign ofanintervaltree
andtheoperations thatrunonit.
Step1: Underlyingdatastructure
Wechooseared-blacktreeinwhicheachnodex containsanintervalx:intandthe
keyofx isthelowendpoint, x:int:low,oftheinterval. Thus,aninorder treewalk
ofthedatastructure liststheintervals insortedorderbylowendpoint.
Step2: Additionalinformation
Inadditiontotheintervalsthemselves,eachnodex containsavaluex:max,which
isthemaximumvalueofanyintervalendpoint storedinthesubtreerootedatx.
Step3: Maintainingtheinformation
We must verify that insertion and deletion take O.lgn/ time on an interval tree
of n nodes. We can determine x:max given interval x:int and the max values of
nodex’schildren:
350 Chapter14 AugmentingDataStructures
26 26
25 30
19 20
17 19
(a) 16 21
15 23
8 9
6 10
5 8
0 3
0 5 10 15 20 25 30
[16,21]
30
int
[8,9] [25,30]
(b) 23 30 max
[5,8] [15,23] [17,19] [26,26]
10 23 20 26
[0,3] [6,10] [19,20]
3 10 20
Figure14.4 Anintervaltree.(a)Asetof10intervals,shownsortedbottomtotopbyleftendpoint.
(b)Theintervaltreethatrepresentsthem.Eachnodexcontainsaninterval,shownabovethedashed
line, andthemaximumvalueofanyintervalendpoint inthesubtreerootedatx,shownbelowthe
dashedline.Aninordertreewalkofthetreeliststhenodesinsortedorderbyleftendpoint.
x:max max.x:int:high;x:left:max;x:right:max/:
D
Thus, by Theorem 14.1, insertion and deletion run in O.lgn/ time. In fact, we
can update the max attributes after a rotation in O.1/ time, as Exercises 14.2-3
and14.3-1show.
Step4: Developingnewoperations
Theonly newoperation weneed is INTERVAL-SEARCH.T;i/, whichfindsanode
intreeT whoseintervaloverlapsintervali. Ifthereisnointervalthatoverlapsi in
thetree,theprocedure returnsapointertothesentinelT:nil.
14.3 Intervaltrees 351
INTERVAL-SEARCH.T;i/
1 x T:root
D
2 whilex T:nilandi doesnotoverlapx:int
¤
3 ifx:left T:nilandx:left:max i:low
¤ 
4 x x:left
D
5 elsex x:right
D
6 returnx
The search for an interval that overlaps i starts with x at the root of the tree and
proceedsdownward. Itterminateswheneitheritfindsanoverlapping intervalorx
pointstothesentinel T:nil. Sinceeachiteration ofthebasiclooptakesO.1/time,
andsincetheheightofann-nodered-blacktreeisO.lgn/,theINTERVAL-SEARCH
procedure takesO.lgn/time.
Before we see why INTERVAL-SEARCH is correct, let’s examine how it works
ontheintervaltreeinFigure14.4. Supposewewishtofindanintervalthatoverlaps
theinterval i Œ22;25. Webegin withx astheroot, whichcontains Œ16;21 and
D
does not overlap i. Since x:left:max 23 is greater than i:low 22, the loop
D D
continueswithxastheleftchildoftheroot—thenodecontainingŒ8;9,whichalso
doesnotoverlapi. Thistime,x:left:max 10islessthani:low 22,andsothe
D D
loopcontinues withtherightchildofx asthenewx. Becausetheinterval Œ15;23
storedinthisnodeoverlapsi,theprocedure returnsthisnode.
As an example of an unsuccessful search, suppose we wish to find an interval
that overlaps i Œ11;14 in the interval tree of Figure 14.4. We once again be-
D
gin with x as the root. Since the root’s interval Œ16;21 does not overlap i, and
since x:left:max 23 is greater than i:low 11, we go left to the node con-
D D
taining Œ8;9. Interval Œ8;9 does not overlap i, and x:left:max 10 is less than
D
i:low 11, and so we go right. (Note that no interval in the left subtree over-
D
laps i.) Interval Œ15;23 does not overlap i, and its left child is T:nil, so again we
goright, theloopterminates, andwereturnthesentinel T:nil.
Tosee why INTERVAL-SEARCH is correct, we must understand why it suffices
to examine a single path from the root. The basic idea is that at any node x,
if x:int does not overlap i, the search always proceeds in a safe direction: the
search will definitely find an overlapping interval if the tree contains one. The
followingtheoremstatesthisproperty moreprecisely.
Theorem14.2
Any execution of INTERVAL-SEARCH.T;i/ either returns a node whose interval
overlapsi,oritreturns T:nilandthetreeT contains nonodewhoseinterval over-
lapsi.
352 Chapter14 AugmentingDataStructures
i¢¢
i¢¢
i¢ i¢¢
i¢ i i i¢
(a) (b)
Figure14.5 IntervalsintheproofofTheorem14.2.Thevalueofx:left:maxisshownineachcase
asadashedline. (a)Thesearchgoesright. Nointervali inx’sleftsubtreecanoverlapi. (b)The
0
search goes left. The left subtree of x contains an interval that overlaps i (situation not shown),
orx’sleftsubtreecontainsanintervali suchthati :high x:left:max.Sinceidoesnotoverlapi ,
0 0 D 0
neitherdoesitoverlapanyintervali inx’srightsubtree,sincei :low i :low.
00 0  00
Proof The while loop of lines 2–5 terminates either when x T:nil or i over-
D
lapsx:int. Inthelattercase,itiscertainly correcttoreturnx. Therefore,wefocus
ontheformercase,inwhichthewhileloopterminates becausex T:nil.
D
Weusethefollowinginvariant forthewhileloopoflines2–5:
If tree T contains an interval that overlaps i, then the subtree rooted at x
contains suchaninterval.
Weusethisloopinvariant asfollows:
Initialization: Prior to the first iteration, line 1 sets x to be the root of T, so that
theinvariant holds.
Maintenance: Eachiterationofthewhileloopexecuteseitherline4orline5. We
shallshowthatbothcasesmaintaintheloopinvariant.
If line 5 is executed, then because of the branch condition in line 3, we
have x:left T:nil, or x:left:max < i:low. If x:left T:nil, the subtree
D D
rooted at x:left clearly contains no interval that overlaps i, and so setting x
to x:right maintains the invariant. Suppose, therefore, that x:left T:nil and
¤
x:left:max < i:low. As Figure 14.5(a) shows, for each interval i in x’s left
0
subtree, wehave
i :high x:left:max
0

< i:low:
By the interval trichotomy, therefore, i and i do not overlap. Thus, the left
0
subtree of x contains no intervals that overlap i, so that setting x to x:right
maintainstheinvariant.
14.3 Intervaltrees 353
If, on the other hand, line 4is executed, then we will show that the contrapos-
itive of the loop invariant holds. That is, if the subtree rooted at x:left con-
tainsnointervaloverlappingi,thennointervalanywhereinthetreeoverlapsi.
Since line 4 is executed, then because of the branch condition in line 3, we
have x:left:max i:low. Moreover, bydefinition ofthemaxattribute, x’sleft

subtree mustcontainsomeintervali suchthat
0
i :high x:left:max
0
D
i:low:

(Figure 14.5(b) illustrates the situation.) Since i and i do not overlap, and
0
since it is not true that i :high < i:low, it follows by the interval trichotomy
0
thati:high < i :low. Intervaltreesarekeyedonthelowendpoints ofintervals,
0
and thus the search-tree property implies that for any interval i in x’s right
00
subtree,
i:high < i :low
0
i :low:
00

Bythe interval trichotomy, i and i do not overlap. Weconclude that whether
00
or not any interval in x’s left subtree overlaps i, setting x to x:left maintains
theinvariant.
Termination: Iftheloopterminates whenx T:nil,thenthesubtreerooted atx
D
contains no interval overlapping i. The contrapositive of the loop invariant
implies thatT contains nointerval thatoverlaps i. Henceitiscorrect toreturn
x T:nil.
D
Thus,the INTERVAL-SEARCH procedure workscorrectly.
Exercises
14.3-1
Writepseudocode forLEFT-ROTATE thatoperatesonnodesinanintervaltreeand
updatesthemaxattributes inO.1/time.
14.3-2
Rewritethecodefor INTERVAL-SEARCH sothat itworksproperly whenallinter-
valsareopen.
14.3-3
Describe an efficient algorithm that, given an interval i, returns an interval over-
lappingi thathastheminimumlowendpoint, orT:nilifnosuchintervalexists.
354 Chapter14 AugmentingDataStructures
14.3-4
Given an interval tree T and an interval i, describe how to list all intervals in T
that overlap i inO.min.n;klgn// time, where k is thenumber ofintervals inthe
output list. (Hint: One simple method makes several queries, modifying the tree
betweenqueries. Aslightlymorecomplicated methoddoesnotmodifythetree.)
14.3-5
Suggest modifications to the interval-tree procedures to support the new opera-
tion INTERVAL-SEARCH-EXACTLY.T;i/, where T is an interval tree and i is
an interval. The operation should return a pointer to a node x in T such that
x:int:low i:low and x:int:high i:high, or T:nil if T contains no such node.
D D
All operations, including INTERVAL-SEARCH-EXACTLY, should run in O.lgn/
timeonann-nodeinterval tree.
14.3-6
Show how to maintain a dynamic set Q of numbers that supports the operation
MIN-GAP, which gives the magnitude of the difference of the two closest num-
bers in Q. For example, if Q 1;5;9;15;18;22 , then MIN-GAP.Q/ returns
D f g
18 15 3, since 15 and 18 are the two closest numbers in Q. Make the op-
(cid:0) D
erations INSERT, DELETE, SEARCH, and MIN-GAP as efficient as possible, and
analyze theirrunning times.
14.3-7 ?
VLSI databases commonly represent an integrated circuit as a list of rectan-
gles. Assume that each rectangle is rectilinearly oriented (sides parallel to the
x-andy-axes),sothatwerepresent arectangle byitsminimumandmaximum x-
andy-coordinates. GiveanO.nlgn/-timealgorithmtodecidewhetherornotaset
ofnrectanglessorepresentedcontainstworectanglesthatoverlap. Youralgorithm
neednotreportallintersecting pairs,butitmustreportthatanoverlapexistsifone
rectangleentirelycoversanother,eveniftheboundarylinesdonotintersect. (Hint:
Movea“sweep”lineacrossthesetofrectangles.)
Problems
14-1 Pointofmaximumoverlap
Suppose that we wish to keep track of a point of maximum overlap in a set of
intervals—a pointwiththelargestnumberofintervals inthesetthatoverlapit.
a. Showthattherewillalwaysbeapointofmaximumoverlapthatisanendpoint
ofoneofthesegments.
NotesforChapter14 355
b. Design a data structure that efficiently supports the operations INTERVAL-
INSERT, INTERVAL-DELETE, and FIND-POM, which returns a point of max-
imum overlap. (Hint: Keep a red-black tree of all the endpoints. Associate
a value of 1 with each left endpoint, and associate a value of 1 with each
C (cid:0)
right endpoint. Augment each node of the tree with some extra information to
maintainthepointofmaximumoverlap.)
14-2 Josephuspermutation
We define the Josephus problem as follows. Suppose that n people form a circle
and that we are given a positive integer m n. Beginning with a designated

firstperson, weproceed around thecircle, removing everymthperson. Aftereach
personisremoved,countingcontinuesaroundthecirclethatremains. Thisprocess
continues until we have removed all n people. The order in which the people are
removed from the circle defines the .n;m/-Josephus permutation of the integers
1;2;:::;n. Forexample,the.7;3/-Josephus permutation is 3;6;2;7;5;1;4 .
h i
a. Suppose that m is a constant. Describe an O.n/-time algorithm that, given an
integern,outputs the.n;m/-Josephus permutation.
b. Suppose that m is not a constant. Describe an O.nlgn/-time algorithm that,
givenintegers nandm,outputs the.n;m/-Josephus permutation.
Chapter notes
In their book, Preparata and Shamos [282] describe several of the interval trees
that appear in the literature, citing work by H. Edelsbrunner (1980) and E. M.
McCreight (1981). The book details an interval tree that, given a static database
of n intervals, allows us to enumerate all k intervals that overlap a given query
intervalinO.k lgn/time.
C
IV Advanced Design and Analysis Techniques
Introduction
This part covers three important techniques used in designing and analyzing effi-
cient algorithms: dynamic programming (Chapter 15), greedy algorithms (Chap-
ter 16), and amortized analysis (Chapter 17). Earlier parts have presented other
widelyapplicabletechniques,suchasdivide-and-conquer, randomization,andhow
tosolverecurrences. Thetechniques inthispartaresomewhatmoresophisticated,
buttheyhelpustoattackmanycomputationalproblems. Thethemesintroducedin
thispartwillrecurlaterinthisbook.
Dynamic programming typically applies to optimization problems in which we
make a set of choices in order to arrive at an optimal solution. As we make
each choice, subproblems of the same form often arise. Dynamic programming
is effective when a given subproblem may arise from more than one partial set of
choices;thekeytechniqueistostorethesolutiontoeachsuchsubproblemincaseit
should reappear. Chapter 15showshowthissimpleideacansometimestransform
exponential-time algorithms intopolynomial-time algorithms.
Like dynamic-programming algorithms, greedy algorithms typically apply to
optimization problems in which we make a set of choices in order to arrive at an
optimalsolution. Theideaofagreedyalgorithmistomakeeachchoiceinalocally
optimal manner. A simple example is coin-changing: to minimize the number of
U.S. coins needed to make change for a given amount, we can repeatedly select
the largest-denomination coin that is not larger than the amount that remains. A
greedyapproachprovidesanoptimalsolutionformanysuchproblemsmuchmore
quickly than would a dynamic-programming approach. We cannot always easily
tell whether a greedy approach will be effective, however. Chapter 16 introduces
358 PartIV AdvancedDesignandAnalysisTechniques
matroid theory, whichprovides amathematical basis thatcanhelp ustoshow that
agreedyalgorithm yieldsanoptimalsolution.
Weuseamortizedanalysistoanalyzecertainalgorithmsthatperformasequence
of similar operations. Instead of bounding the cost of the sequence of operations
by bounding the actual cost of each operation separately, an amortized analysis
provides a bound on the actual cost of the entire sequence. One advantage of this
approach isthatalthough someoperations mightbeexpensive, manyothersmight
becheap. Inotherwords,manyoftheoperationsmightruninwellundertheworst-
casetime. Amortizedanalysisisnotjustananalysistool,however;itisalsoaway
ofthinkingaboutthedesignofalgorithms,sincethedesignofanalgorithmandthe
analysis of its running time are often closely intertwined. Chapter 17 introduces
threewaystoperformanamortizedanalysis ofanalgorithm.
15 Dynamic Programming
Dynamic programming, like the divide-and-conquer method, solves problems by
combining the solutions to subproblems. (“Programming” in this context refers
to a tabular method, not to writing computer code.) As we saw in Chapters 2
and 4, divide-and-conquer algorithms partition the problem into disjoint subprob-
lems,solvethesubproblems recursively, andthencombinetheirsolutions tosolve
theoriginalproblem. Incontrast,dynamicprogrammingapplieswhenthesubprob-
lems overlap—that is, when subproblems share subsubproblems. In this context,
a divide-and-conquer algorithm does more work than necessary, repeatedly solv-
ingthecommonsubsubproblems. Adynamic-programming algorithmsolveseach
subsubproblem justonceandthensavesitsanswerinatable, therebyavoiding the
workofrecomputing theanswereverytimeitsolveseachsubsubproblem.
Wetypicallyapplydynamicprogrammingtooptimizationproblems. Suchprob-
lemscanhavemanypossible solutions. Eachsolution hasavalue, andwewishto
find a solution with the optimal (minimum or maximum) value. We call such a
solution an optimal solution to the problem, as opposed to the optimal solution,
sincetheremaybeseveralsolutions thatachievetheoptimalvalue.
When developing a dynamic-programming algorithm, we follow a sequence of
foursteps:
1. Characterize thestructure ofanoptimalsolution.
2. Recursively definethevalueofanoptimalsolution.
3. Computethevalueofanoptimalsolution, typically inabottom-upfashion.
4. Construct anoptimalsolutionfromcomputedinformation.
Steps 1–3 form the basis ofa dynamic-programming solution to aproblem. If we
need only the value of an optimal solution, and not the solution itself, then we
can omit step 4. When we do perform step 4, we sometimes maintain additional
information duringstep3sothatwecaneasilyconstruct anoptimalsolution.
The sections that follow use the dynamic-programming method to solve some
optimization problems. Section 15.1 examines the problem of cutting a rod into
360 Chapter15 DynamicProgramming
rods of smaller length in way that maximizes their total value. Section 15.2 asks
how we can multiply a chain of matrices while performing the fewest total scalar
multiplications. Giventheseexamplesofdynamicprogramming,Section15.3dis-
cussestwokeycharacteristics thataproblemmusthavefordynamicprogramming
tobeaviable solution technique. Section 15.4then showshow tofindthelongest
common subsequence of two sequences via dynamic programming. Finally, Sec-
tion15.5usesdynamicprogrammingtoconstruct binarysearchtreesthatareopti-
mal,givenaknowndistribution ofkeystobelookedup.
15.1 Rodcutting
Ourfirstexample uses dynamic programming tosolve asimple problem indecid-
ingwheretocutsteelrods. SerlingEnterprises buys longsteelrodsandcutsthem
intoshorter rods, whichitthensells. Eachcutisfree. ThemanagementofSerling
Enterprises wantstoknowthebestwaytocutuptherods.
We assume that we know, for i 1;2;:::, the price p in dollars that Serling
i
D
Enterpriseschargesforarodoflengthi inches. Rodlengthsarealwaysanintegral
numberofinches. Figure15.1givesasamplepricetable.
Therod-cutting problem isthefollowing. Givenarodoflength ninches and a
table of prices p for i 1;2;:::;n, determine the maximum revenue r obtain-
i n
D
ablebycuttinguptherodandsellingthepieces. Notethatifthepricep forarod
n
oflengthnislargeenough, anoptimalsolutionmayrequirenocuttingatall.
Consider the case when n 4. Figure 15.2 shows all the ways to cut up a rod
D
of 4 inches in length, including the way with no cuts at all. We see that cutting a
4-inchrodintotwo2-inchpiecesproducesrevenuep p 5 5 10,which
2 2
C D C D
isoptimal.
We can cut up a rod of length n in 2n 1 different ways, since we have an in-
(cid:0)
dependent option of cutting, or not cutting, at distance i inches from the left end,
lengthi 1 2 3 4 5 6 7 8 9 10
pricep i 1 5 8 9 10 17 17 20 24 30
Figure 15.1 A sample price table for rods. Each rod of length i inches earns the company p i
dollarsofrevenue.
15.1 Rodcutting 361
9 1 8 5 5 8 1
(a) (b) (c) (d)
1 1 5 1 5 1 5 1 1 1 1 1 1
(e) (f) (g) (h)
Figure 15.2 The 8 possible ways of cutting up a rod of length 4. Above each piece is the
value of that piece, according to the sample price chart of Figure 15.1. The optimal strategy is
part(c)—cuttingtherodintotwopiecesoflength2—whichhastotalvalue10.
for i 1;2;:::;n 1.1 We denote a decomposition into pieces using ordinary
D (cid:0)
additive notation, sothat7 2 2 3indicates thatarodoflength 7iscutinto
D C C
three pieces—two of length 2 and one of length 3. If an optimal solution cuts the
rodintok pieces, forsome1 k n,thenanoptimaldecomposition
 
n i i i
1 2 k
D C CC
of the rod into pieces of lengths i , i , ..., i provides maximum corresponding
1 2 k
revenue
r p p p :
n
D
i1
C
i2
CC
ik
For our sample problem, we can determine the optimal revenue figures r , for
i
i 1;2;:::;10,byinspection, withthecorresponding optimaldecompositions
D
1If we required the pieces to be cut in order of nondecreasing size, there would be fewer ways
to consider. For n 4, we would consider only 5 such ways: parts (a), (b), (c), (e), and (h)
D
inFigure15.2. Thenumber of ways iscalled thepartitionfunction; it isapproximately equal to
ep2n=3=4np3. Thisquantityislessthan2n 1,butstillmuchgreaterthananypolynomialinn.
(cid:0)
Weshallnotpursuethislineofinquiryfurther,however.
362 Chapter15 DynamicProgramming
r 1 fromsolution1 1 (nocuts);
1
D D
r 5 fromsolution2 2 (nocuts);
2
D D
r 8 fromsolution3 3 (nocuts);
3
D D
r 10 fromsolution4 2 2;
4
D D C
r 13 fromsolution5 2 3;
5
D D C
r 17 fromsolution6 6 (nocuts);
6
D D
r 18 fromsolution7 1 6 or 7 2 2 3;
7
D D C D C C
r 22 fromsolution8 2 6;
8
D D C
r 25 fromsolution9 3 6;
9
D D C
r 30 fromsolution10 10 (nocuts):
10
D D
More generally, we can frame the values r for n 1 in terms of optimal rev-
n

enuesfromshorter rods:
r max.p ;r r ;r r ;:::;r r / : (15.1)
n n 1 n 1 2 n 2 n 1 1
D C (cid:0) C (cid:0) (cid:0) C
Thefirstargument, p ,corresponds tomakingnocuts atallandselling therodof
n
lengthnasis. Theothern 1arguments tomaxcorrespond tothemaximumrev-
(cid:0)
enueobtainedbymakinganinitialcutoftherodintotwopiecesofsizei andn i,
(cid:0)
for each i 1;2;:::;n 1, and then optimally cutting up those pieces further,
D (cid:0)
obtaining revenues r andr fromthose twopieces. Sincewedon’t knowahead
i n i
oftimewhichvalueofi opti(cid:0) mizesrevenue,wehavetoconsiderallpossiblevalues
fori andpicktheonethatmaximizesrevenue. Wealsohavetheoption ofpicking
noi atallifwecanobtainmorerevenuebysellingtheroduncut.
Note that to solve the original problem of size n, wesolve smaller problems of
the same type, but of smaller sizes. Once we make the first cut, we may consider
the two pieces as independent instances of the rod-cutting problem. The overall
optimal solution incorporates optimal solutions to the two related subproblems,
maximizing revenue from each of those two pieces. We say that the rod-cutting
problemexhibitsoptimalsubstructure: optimalsolutionstoaproblemincorporate
optimalsolutions torelatedsubproblems, whichwemaysolveindependently.
Inarelated,butslightlysimpler,waytoarrangearecursivestructurefortherod-
cutting problem,weviewadecomposition asconsisting ofafirstpieceoflengthi
cut off the left-hand end, and then a right-hand remainder of length n i. Only
(cid:0)
theremainder, and notthefirstpiece, maybefurther divided. Wemayview every
decomposition of a length-n rod in this way: as a first piece followed by some
decomposition of the remainder. When doing so, we can couch the solution with
no cuts at all as saying that the firstpiece has size i n and revenue p and that
n
D
the remainder has size 0 with corresponding revenue r 0. We thus obtain the
0
D
followingsimplerversionofequation (15.1):
r max .p r / : (15.2)
n i n i
D 1 i n C (cid:0)
 
15.1 Rodcutting 363
In this formulation, an optimal solution embodies the solution to only one related
subproblem—the remainder—rather thantwo.
Recursivetop-downimplementation
The following procedure implements the computation implicit in equation (15.2)
inastraightforward, top-down, recursivemanner.
CUT-ROD.p;n/
1 ifn ==0
2 return0
3 q
D (cid:0)1
4 fori 1ton
D
5 q max.q;pŒi CUT-ROD.p;n i//
D C (cid:0)
6 returnq
Procedure CUT-ROD takesasinputanarraypŒ1::nofpricesandanintegern,
and it returns the maximum revenue possible for a rod of length n. If n 0, no
D
revenue is possible, and so CUT-ROD returns 0 in line 2. Line 3 initializes the
maximum revenue q to , so that the for loop in lines 4–5 correctly computes
(cid:0)1
q max
1 i
n.p
i
CUT-ROD.p;n i//;line6thenreturnsthisvalue. Asimple
indD uction  onn proC ves that this answ(cid:0) er is equal to the desired answer r , using
n
equation (15.2).
IfyouweretocodeupCUT-RODinyourfavoriteprogramminglanguageandrun
it on your computer, you would find that once the input size becomes moderately
large,yourprogramwouldtakealongtimetorun. Forn 40,youwouldfindthat
D
yourprogram takesatleastseveralminutes, andmostlikelymorethananhour. In
fact, you would find that each time you increase n by 1, your program’s running
timewouldapproximately double.
Why is CUT-ROD so inefficient? The problem is that CUT-ROD calls itself
recursively over and over again with the same parameter values; it solves the
same subproblems repeatedly. Figure 15.3 illustrates what happens for n 4:
D
CUT-ROD.p;n/ calls CUT-ROD.p;n i/ for i 1;2;:::;n. Equivalently,
(cid:0) D
CUT-ROD.p;n/ calls CUT-ROD.p;j/ for each j 0;1;:::;n 1. When this
D (cid:0)
process unfolds recursively, the amount of work done, as a function of n, grows
explosively.
To analyze the running time of CUT-ROD, let T.n/ denote the total number of
calls made to CUT-ROD when called with its second parameter equal to n. This
expression equals the number of nodes in a subtree whose root is labeled n in the
recursion tree. Thecountincludestheinitialcallatitsroot. Thus,T.0/ 1and
D
364 Chapter15 DynamicProgramming
4
3 2 1 0
2 1 0 1 0 0
1 0 0 0
0
Figure15.3 Therecursiontreeshowingrecursivecallsresultingfromacall CUT-ROD.p;n/for
n 4. Each node label gives the sizen of thecorresponding subproblem, so that an edge from
D
aparentwithlabels toachildwithlabelt correspondstocuttingoffaninitialpieceofsizes t
(cid:0)
andleavingaremainingsubproblemofsizet. Apathfromtheroottoaleafcorrespondstooneof
the2n (cid:0)1waysofcuttinguparodoflengthn.Ingeneral,thisrecursiontreehas2nnodesand2n (cid:0)1
leaves.
n 1
(cid:0)
T.n/ 1 T.j/: (15.3)
D C
j 0
XD
Theinitial1isforthecallattheroot,andthetermT.j/countsthenumberofcalls
(including recursive calls) due to the call CUT-ROD.p;n i/, where j n i.
(cid:0) D (cid:0)
AsExercise15.1-1asksyoutoshow,
T.n/ 2n ; (15.4)
D
andsotherunningtimeofCUT-ROD isexponential inn.
In retrospect, this exponential running time is not so surprising. CUT-ROD ex-
plicitly considers all the 2n 1 possible ways of cutting up a rod of length n. The
(cid:0)
tree ofrecursive calls has 2n 1 leaves, onefor eachpossible wayofcutting upthe
(cid:0)
rod. The labels on the simple path from the root to a leaf give the sizes of each
remaining right-hand piece before making each cut. That is, the labels give the
corresponding cutpoints, measuredfromtheright-hand endoftherod.
Usingdynamicprogrammingforoptimalrodcutting
WenowshowhowtoconvertCUT-ROD intoanefficientalgorithm,usingdynamic
programming.
The dynamic-programming method works as follows. Having observed that a
naive recursive solution is inefficient because it solves the same subproblems re-
peatedly, we arrange for each subproblem to be solved only once, saving its solu-
tion. Ifweneedtorefertothissubproblem’ssolutionagainlater,wecanjustlookit
15.1 Rodcutting 365
up, ratherthan recompute it. Dynamicprogramming thus usesadditional memory
to save computation time; it serves an example of a time-memory trade-off. The
savings maybedramatic: anexponential-time solution maybe transformed into a
polynomial-time solution. A dynamic-programming approach runs in polynomial
timewhenthenumberofdistinct subproblems involved ispolynomial intheinput
sizeandwecansolveeachsuchsubproblem inpolynomial time.
There are usually two equivalent ways to implement a dynamic-programming
approach. Weshallillustrate bothofthemwithourrod-cutting example.
The first approach is top-down with memoization.2 In this approach, we write
the procedure recursively in a natural manner, but modified to save the result of
eachsubproblem(usuallyinanarrayorhashtable). Theprocedurenowfirstchecks
tosee whether ithas previously solved this subproblem. Ifso, itreturns the saved
value, saving further computation at this level; if not, the procedure computes the
valueintheusualmanner. Wesaythattherecursiveprocedurehasbeenmemoized;
it“remembers”whatresultsithascomputedpreviously.
Thesecondapproachisthebottom-upmethod. Thisapproachtypicallydepends
on some natural notion of the “size” of a subproblem, such that solving any par-
ticular subproblem depends only on solving “smaller” subproblems. We sort the
subproblems by size and solve them in size order, smallest first. When solving a
particular subproblem, we have already solved all of the smaller subproblems its
solution depends upon, and we have saved their solutions. We solve each sub-
problem only once, and when we first see it, we have already solved all of its
prerequisite subproblems.
Thesetwoapproaches yield algorithms withthe sameasymptotic running time,
except in unusual circumstances where the top-down approach does not actually
recurse to examine all possible subproblems. The bottom-up approach often has
muchbetterconstant factors, sinceithaslessoverheadforprocedure calls.
Hereisthethe pseudocode forthe top-down CUT-ROD procedure, withmemo-
izationadded:
MEMOIZED-CUT-ROD.p;n/
1 letrŒ0::nbeanewarray
2 fori 0ton
D
3 rŒi
D (cid:0)1
4 return MEMOIZED-CUT-ROD-AUX.p;n;r/
2Thisisnotamisspelling. Thewordreallyismemoization,notmemorization. Memoizationcomes
frommemo,sincethetechniqueconsistsofrecordingavaluesothatwecanlookituplater.
366 Chapter15 DynamicProgramming
MEMOIZED-CUT-ROD-AUX.p;n;r/
1 ifrŒn 0

2 returnrŒn
3 ifn == 0
4 q 0
D
5 elseq
D (cid:0)1
6 fori 1ton
D
7 q max.q;pŒi MEMOIZED-CUT-ROD-AUX.p;n i;r//
D C (cid:0)
8 rŒn q
D
9 returnq
Here, the mainprocedure MEMOIZED-CUT-ROD initializes anew auxiliary ar-
ray rŒ0::n with the value , a convenient choice with which to denote “un-
(cid:0)1
known.” (Known revenue values are always nonnegative.) It then calls its helper
routine, MEMOIZED-CUT-ROD-AUX.
TheprocedureMEMOIZED-CUT-ROD-AUX isjustthememoizedversionofour
previous procedure, CUT-ROD. It first checks in line 1 to see whether the desired
value is already known and, if it is, then line 2 returns it. Otherwise, lines 3–7
compute thedesired valueq intheusual manner, line 8savesitinrŒn,andline 9
returnsit.
Thebottom-up versionisevensimpler:
BOTTOM-UP-CUT-ROD.p;n/
1 letrŒ0::nbeanewarray
2 rŒ0 0
D
3 forj 1ton
D
4 q
D (cid:0)1
5 fori 1toj
D
6 q max.q;pŒi rŒj i/
D C (cid:0)
7 rŒj q
D
8 returnrŒn
For the bottom-up dynamic-programming approach, BOTTOM-UP-CUT-ROD
uses the natural ordering of the subproblems: a subproblem of size i is “smaller”
than a subproblem of size j if i < j. Thus, the procedure solves subproblems of
sizesj 0;1;:::;n,inthatorder.
D
Line 1 of procedure BOTTOM-UP-CUT-ROD creates a new array rŒ0::n in
which to save the results of the subproblems, and line 2 initializes rŒ0 to 0, since
arodoflength0earnsnorevenue. Lines3–6solveeachsubproblem ofsizej,for
j 1;2;:::;n,inorderofincreasing size. Theapproachusedtosolveaproblem
D
ofaparticular sizej isthesameasthatusedby CUT-ROD,exceptthatline6now
15.1 Rodcutting 367
4
3
2
1
0
Figure15.4 The subproblem graph for the rod-cutting problem with n 4. The vertex labels
D
givethesizes of thecorresponding subproblems. A directededge .x;y/ indicatesthat weneed a
solutiontosubproblemywhensolvingsubproblemx.Thisgraphisareducedversionofthetreeof
Figure15.3,inwhichallnodeswiththesamelabelarecollapsedintoasinglevertexandalledges
gofromparenttochild.
directly references array entry rŒj iinstead ofmaking arecursive call tosolve
(cid:0)
the subproblem of size j i. Line 7 saves in rŒjthe solution to the subproblem
(cid:0)
ofsizej. Finally,line8returns rŒn,whichequalstheoptimalvaluer .
n
The bottom-up and top-down versions have the same asymptotic running time.
The running time of procedure BOTTOM-UP-CUT-ROD is ‚.n2/, due to its
doubly-nested loop structure. The number of iterations of its inner for loop, in
lines5–6,formsanarithmeticseries. Therunningtimeofitstop-downcounterpart,
MEMOIZED-CUT-ROD, is also ‚.n2/, although this running time may be a little
harder to see. Because a recursive call to solve a previously solved subproblem
returns immediately, MEMOIZED-CUT-ROD solves each subproblem just once. It
solves subproblems for sizes 0;1;:::;n. Tosolve asubproblem of size n, the for
loop of lines 6–7 iterates n times. Thus, the total number of iterations of this for
loop,overallrecursivecallsofMEMOIZED-CUT-ROD, formsanarithmeticseries,
giving a total of ‚.n2/ iterations, just like the inner for loop of BOTTOM-UP-
CUT-ROD. (Weactually areusing aform ofaggregate analysis here. Weshallsee
aggregateanalysis indetailinSection17.1.)
Subproblemgraphs
Whenwethinkaboutadynamic-programming problem,weshouldunderstandthe
setofsubproblems involved andhowsubproblems dependononeanother.
Thesubproblem graphfortheproblem embodiesexactlythisinformation. Fig-
ure 15.4 shows the subproblem graph for the rod-cutting problem with n 4. It
D
is a directed graph, containing one vertex for each distinct subproblem. The sub-
368 Chapter15 DynamicProgramming
problem graph hasadirected edge from the vertex for subproblem x tothe vertex
for subproblem y if determining an optimal solution for subproblem x involves
directly considering an optimal solution for subproblem y. For example, the sub-
problem graphcontains anedgefromx toy ifatop-downrecursive procedure for
solving x directly calls itself to solve y. We can think of the subproblem graph
asa“reduced” or“collapsed” version oftherecursion treeforthetop-downrecur-
sivemethod,inwhichwecoalesceallnodesforthesamesubproblem intoasingle
vertexanddirectalledgesfromparenttochild.
The bottom-up method for dynamic programming considers the vertices of the
subproblem graph in such an order that we solve the subproblems y adjacent to
a given subproblem x before we solve subproblem x. (Recall from Section B.4
that the adjacency relation is not necessarily symmetric.) Using the terminology
fromChapter22,inabottom-updynamic-programmingalgorithm,weconsiderthe
vertices ofthesubproblem graphinanorderthatisa“reversetopological sort,”or
a“topologicalsortofthetranspose”(seeSection22.4)ofthesubproblemgraph. In
other words, no subproblem is considered until all of the subproblems it depends
upon have been solved. Similarly, using notions from the same chapter, we can
view the top-down method (with memoization) for dynamic programming as a
“depth-first search”ofthesubproblem graph(seeSection22.3).
ThesizeofthesubproblemgraphG .V;E/canhelpusdeterminetherunning
D
timeofthedynamicprogrammingalgorithm. Sincewesolveeachsubproblemjust
once, the running time is the sum of the times needed to solve each subproblem.
Typically, the time to compute the solution to a subproblem is proportional to the
degree (number ofoutgoing edges) ofthecorresponding vertex inthesubproblem
graph,andthenumberofsubproblemsisequaltothenumberofverticesinthesub-
problem graph. In this common case, the running time of dynamic programming
islinearinthenumberofverticesandedges.
Reconstructingasolution
Ourdynamic-programmingsolutionstotherod-cuttingproblemreturnthevalueof
an optimal solution, but theydo not return an actual solution: alist ofpiece sizes.
Wecanextendthedynamic-programming approach torecordnotonlytheoptimal
value computed for each subproblem, but also a choice that led to the optimal
value. Withthisinformation, wecanreadilyprintanoptimalsolution.
HereisanextendedversionofBOTTOM-UP-CUT-ROD thatcomputes, foreach
rod size j, not only the maximum revenue r , but also s , the optimal size of the
j j
firstpiecetocutoff:
15.1 Rodcutting 369
EXTENDED-BOTTOM-UP-CUT-ROD.p;n/
1 letrŒ0::nandsŒ0::nbenewarrays
2 rŒ0 0
D
3 forj 1ton
D
4 q
D (cid:0)1
5 fori 1toj
D
6 ifq <pŒi rŒj i
C (cid:0)
7 q pŒi rŒj i
D C (cid:0)
8 sŒj i
D
9 rŒj q
D
10 returnr ands
Thisprocedure issimilar to BOTTOM-UP-CUT-ROD, except that itcreates thear-
ray s in line 1, and it updates sŒj in line 8 to hold the optimal size i of the first
piecetocutoffwhensolvingasubproblem ofsizej.
The following procedure takes a price table p and a rod size n, and it calls
EXTENDED-BOTTOM-UP-CUT-ROD to compute the array sŒ1::n of optimal
first-piece sizes and then prints out the complete list of piece sizes in an optimal
decomposition ofarodoflengthn:
PRINT-CUT-ROD-SOLUTION.p;n/
1 .r;s/ EXTENDED-BOTTOM-UP-CUT-ROD.p;n/
D
2 whilen > 0
3 printsŒn
4 n n sŒn
D (cid:0)
In our rod-cutting example, the call EXTENDED-BOTTOM-UP-CUT-ROD.p;10/
wouldreturnthefollowingarrays:
i 0 1 2 3 4 5 6 7 8 9 10
rŒi 0 1 5 8 10 13 17 18 22 25 30
sŒi 0 1 2 3 2 2 6 1 2 3 10
A call to PRINT-CUT-ROD-SOLUTION.p;10/ would print just 10, but a call with
n 7would print thecuts1and 6,corresponding tothe firstoptimal decomposi-
D
tionforr givenearlier.
7
Exercises
15.1-1
Show that equation (15.4) follows from equation (15.3) and the initial condition
T.0/ 1.
D
370 Chapter15 DynamicProgramming
15.1-2
Show, by means of a counterexample, that the following “greedy” strategy does
not always determine an optimal way to cut rods. Define the density of a rod of
length i to be p =i, that is, its value per inch. The greedy strategy for a rod of
i
length n cuts off a first piece of length i, where 1 i n, having maximum
 
density. Itthencontinuesbyapplyingthegreedystrategytotheremainingpieceof
lengthn i.
(cid:0)
15.1-3
Consider a modification of the rod-cutting problem in which, in addition to a
pricep foreachrod,eachcutincursafixedcostofc. Therevenueassociatedwith
i
asolution isnowthesumofthepricesofthepiecesminusthecostsofmakingthe
cuts. Giveadynamic-programming algorithm tosolvethismodifiedproblem.
15.1-4
ModifyMEMOIZED-CUT-ROD toreturnnotonlythevaluebuttheactualsolution,
too.
15.1-5
The Fibonacci numbers are defined by recurrence (3.22). Give an O.n/-time
dynamic-programming algorithm tocompute thenthFibonaccinumber. Drawthe
subproblem graph. Howmanyverticesandedgesareinthegraph?
15.2 Matrix-chain multiplication
Ournextexampleofdynamicprogrammingisanalgorithmthatsolvestheproblem
ofmatrix-chain multiplication. Wearegiven asequence (chain) A ;A ;:::;A
1 2 n
h i
ofnmatricestobemultiplied, andwewishtocomputetheproduct
A A A : (15.5)
1 2 n

We can evaluate the expression (15.5) using the standard algorithm for multiply-
ing pairs of matrices as a subroutine once we have parenthesized it to resolve all
ambiguities in how the matrices are multiplied together. Matrix multiplication is
associative, andsoallparenthesizations yieldthe sameproduct. Aproduct ofma-
tricesisfullyparenthesizedifitiseitherasinglematrixortheproductoftwofully
parenthesized matrix products, surrounded by parentheses. For example, if the
chain of matrices is A ;A ;A ;A , then we can fully parenthesize the product
1 2 3 4
h i
A A A A infivedistinctways:
1 2 3 4
15.2 Matrix-chainmultiplication 371
.A .A .A A ///;
1 2 3 4
.A ..A A /A //;
1 2 3 4
..A A /.A A //;
1 2 3 4
..A .A A //A /;
1 2 3 4
...A A /A /A /:
1 2 3 4
Howweparenthesizeachainofmatricescanhaveadramaticimpactonthecost
ofevaluating theproduct. Considerfirstthecostofmultiplying twomatrices. The
standard algorithm is given by the following pseudocode, which generalizes the
SQUARE-MATRIX-MULTIPLY procedure from Section 4.2. The attributes rows
andcolumnsarethenumbersofrowsandcolumnsinamatrix.
MATRIX-MULTIPLY.A;B/
1 ifA:columns B:rows
¤
2 error“incompatible dimensions”
3 elseletC beanewA:rows B:columnsmatrix

4 fori 1toA:rows
D
5 forj 1toB:columns
D
6 c 0
ij
D
7 fork 1toA:columns
D
8 c c a b
ij ij ik kj
D C 
9 returnC
WecanmultiplytwomatricesAandB onlyiftheyarecompatible: thenumberof
columnsofAmustequalthenumberofrowsofB. IfAisap q matrixandB is

aq r matrix,theresultingmatrixC isap r matrix. ThetimetocomputeC is
 
dominatedbythenumberofscalarmultiplications inline8,whichispqr. Inwhat
follows,weshallexpresscostsintermsofthenumberofscalarmultiplications.
Toillustratethedifferentcostsincurredbydifferentparenthesizationsofamatrix
product, consider the problem of a chain A ;A ;A of three matrices. Suppose
1 2 3
h i
that the dimensions of the matrices are 10 100, 100 5, and 5 50, respec-
  
tively. If we multiply according to the parenthesization ..A A /A /, we perform
1 2 3
10 100 5 5000 scalar multiplications to compute the 10 5 matrix prod-
  D 
uct A A , plus another 10 5 50 2500 scalar multiplications to multiply this
1 2
  D
matrix by A , for a total of 7500 scalar multiplications. If instead we multiply
3
according to the parenthesization .A .A A //, we perform 100 5 50 25,000
1 2 3
  D
scalarmultiplications tocompute the100 50matrixproduct A A ,plusanother
2 3

10 100 50 50,000 scalar multiplications to multiply A by this matrix, for a
1
  D
total of 75,000 scalar multiplications. Thus, computing the product according to
thefirstparenthesization is10timesfaster.
We state the matrix-chain multiplication problem as follows: given a chain
A ;A ;:::;A ofnmatrices,wherefori 1;2;:::;n,matrixA hasdimension
1 2 n i
h i D
372 Chapter15 DynamicProgramming
p p ,fully parenthesize theproduct A A A inawaythatminimizes the
i 1 i 1 2 n
(cid:0)  
numberofscalarmultiplications.
Notethatinthematrix-chain multiplication problem, wearenotactually multi-
plying matrices. Our goal is only to determine an order for multiplying matrices
that has the lowest cost. Typically, the time invested in determining this optimal
orderismorethanpaidforbythetimesavedlateronwhenactuallyperformingthe
matrixmultiplications(suchasperformingonly7500scalarmultiplicationsinstead
of75,000).
Countingthenumberofparenthesizations
Beforesolvingthematrix-chainmultiplicationproblembydynamicprogramming,
letusconvince ourselves thatexhaustively checking allpossible parenthesizations
does not yield an efficient algorithm. Denote the number of alternative parenthe-
sizations of a sequence of n matrices by P.n/. When n 1, we have just one
D
matrixandtherefore onlyonewaytofullyparenthesize thematrixproduct. When
n 2, a fully parenthesized matrix product is the product of two fully parenthe-

sized matrix subproducts, and the split between the two subproducts may occur
between the kth and .k 1/st matrices for any k 1;2;:::;n 1. Thus, we
C D (cid:0)
obtaintherecurrence
1 ifn 1;
D
n 1
P.n/ (cid:0) (15.6)
D P.k/P.n k/ ifn 2:
(cid:0) 
 Xk D1
Problem 12-4 asked you to show that the solution to a similar recurrence is the
sequence of Catalan numbers, which grows as .4n=n3=2/. A simpler exercise
(seeExercise15.2-3)istoshowthatthesolution totherecurrence (15.6)is.2n/.
The number of solutions is thus exponential in n, and the brute-force method of
exhaustive search makes for a poor strategy when determining how to optimally
parenthesize amatrixchain.
Applyingdynamicprogramming
We shall use the dynamic-programming method to determine how to optimally
parenthesize a matrix chain. In so doing, we shall follow the four-step sequence
thatwestatedatthebeginning ofthischapter:
1. Characterize thestructure ofanoptimalsolution.
2. Recursivelydefinethevalueofanoptimalsolution.
3. Computethevalueofanoptimalsolution.
15.2 Matrix-chainmultiplication 373
4. Construct anoptimalsolutionfromcomputedinformation.
Weshallgothroughthesestepsinorder,demonstrating clearlyhowweapplyeach
steptotheproblem.
Step1: Thestructureofanoptimalparenthesization
Forourfirststepinthedynamic-programming paradigm, wefindtheoptimalsub-
structureandthenuseittoconstruct anoptimalsolution totheproblemfromopti-
malsolutions tosubproblems. Inthematrix-chain multiplication problem, wecan
performthisstepasfollows. Forconvenience,letusadoptthenotationA ,where
i::j
i j, for the matrix that results from evaluating the product A A A . Ob-
i i 1 j
se rvethat iftheproblem isnontrivial, i.e.,i < j,then toparenthesizC e  th e product
A A A ,wemustsplittheproductbetweenA andA forsomeintegerk
i i 1 j k k 1
intheC ra n g ei k < j. Thatis,forsomevalueofk,wefirstC computethematrices

A andA andthenmultiplythemtogethertoproducethefinalproductA .
i::k k 1::j i::j
Thecost ofpC arenthesizing this wayisthecost ofcomputing thematrix A ,plus
i::k
thecostofcomputing A ,plusthecostofmultiplying themtogether.
k 1::j
C
The optimal substructure of this problem is as follows. Suppose that to op-
timally parenthesize A A A , we split the product between A and A .
i i 1 j k k 1
Then the way we parenthC esi z e the “prefix” subchain A A A within tC his
i i 1 k
optimal parenthesization of A A A must be an optimaC l p a r enthesization of
i i 1 j
A A A . Why? IftherewerC ea l e sscostlywaytoparenthesizeA A A ,
i i 1 k i i 1 k
C  C 
then we could substitute that parenthesization in the optimal parenthesization
ofA A A toproduceanotherwaytoparenthesize A A A whosecost
i i 1 j i i 1 j
C  C 
waslowerthantheoptimum: acontradiction. Asimilarobservation holdsforhow
we parenthesize the subchain A A A in the optimal parenthesization of
k 1 k 2 j
A A A : itmustbeanoptimC alpaC ren th esization ofA A A .
i i 1 j k 1 k 2 j
C  C C 
Nowweuse our optimal substructure toshow that wecan construct an optimal
solution totheproblem fromoptimalsolutions tosubproblems. Wehaveseenthat
any solution to a nontrivial instance of the matrix-chain multiplication problem
requiresustosplittheproduct,andthatanyoptimalsolutioncontainswithinitop-
timalsolutions tosubproblem instances. Thus,wecanbuildanoptimalsolutionto
aninstanceofthematrix-chainmultiplicationproblembysplittingtheprobleminto
two subproblems (optimally parenthesizing A A A and A A A ),
i i 1 k k 1 k 2 j
C  C C 
finding optimal solutions to subproblem instances, and then combining these op-
timal subproblem solutions. We must ensure that when we search for the correct
place to split the product, we have considered all possible places, so that we are
sureofhaving examinedtheoptimalone.
374 Chapter15 DynamicProgramming
Step2: Arecursive solution
Next, wedefine thecostofanoptimal solution recursively intermsoftheoptimal
solutions tosubproblems. Forthematrix-chain multiplication problem,wepickas
oursubproblemstheproblemsofdeterminingtheminimumcostofparenthesizing
A A A for 1 i j n. Let mŒi;j be the minimum number of scalar
i i 1 j
multiC plic a tionsneede dto comp utethematrixA ;forthefullproblem,thelowest-
i::j
costwaytocomputeA wouldthusbemŒ1;n.
1::n
We can define mŒi;j recursively as follows. If i j, the problem is trivial;
D
the chain consists of just one matrix A A , so that no scalar multiplications
i::i i
D
are necessary to compute the product. Thus, mŒi;i 0 for i 1;2;:::;n. To
D D
compute mŒi;j when i < j, we take advantage of the structure of an optimal
solution from step 1. Let us assume that to optimally parenthesize, we split the
product A A A between A and A , where i k < j. Then, mŒi;j
i i 1 j k k 1
equalstheminC im u m costforcomputingtheC subproductsA andA ,plusthe
i::k k 1::j
cost of multiplying these two matrices together. Recalling that eachC matrix A is
i
p p , wesee that computing the matrix product A A takes p p p
i 1 i i::k k 1::j i 1 k j
(cid:0)  C (cid:0)
scalarmultiplications. Thus,weobtain
mŒi;j mŒi;k mŒk 1;j p p p :
i 1 k j
D C C C (cid:0)
Thisrecursive equation assumes thatweknowthevalueofk,whichwedonot.
Thereareonlyj i possiblevaluesfork,however,namelyk i;i 1;:::;j 1.
(cid:0) D C (cid:0)
Sincetheoptimalparenthesizationmustuseoneofthesevaluesfork,weneedonly
checkthemalltofindthebest. Thus,ourrecursivedefinitionfortheminimumcost
ofparenthesizing theproduct A A A becomes
i i 1 j
C 
0 ifi j ;
mŒi;j D (15.7)
D ( im ki <n jfmŒi;k CmŒk C1;j Cp i (cid:0)1p kp j g ifi < j :

The mŒi;j values give the costs of optimal solutions to subproblems, but they
do not provide all the information we need to construct an optimal solution. To
help us do so, we define sŒi;j to be a value of k at which we split the product
A A A inanoptimalparenthesization. Thatis,sŒi;jequalsavaluek such
i i 1 j
thatmCŒi; j
 mŒi;k mŒk 1;j p p p .
i 1 k j
D C C C (cid:0)
Step3: Computingtheoptimalcosts
Atthispoint,wecouldeasilywritearecursivealgorithmbasedonrecurrence(15.7)
tocompute theminimumcostmŒ1;nformultiplying A A A . Aswesawfor
1 2 n

the rod-cutting problem, and as we shall see in Section 15.3, this recursive algo-
rithm takes exponential time, which is no better than the brute-force method of
checking eachwayofparenthesizing theproduct.
15.2 Matrix-chainmultiplication 375
Observe that we have relatively few distinct subproblems: one subproblem for
each choice of i and j satisfying 1 i j n, or n n ‚.n2/ in all.
   2 C D
A recursive algorithm may encounter each subproblem many times in different
(cid:0) 
branches of its recursion tree. This property of overlapping subproblems is the
second hallmark of when dynamic programming applies (the first hallmark being
optimalsubstructure).
Instead of computing the solution to recurrence (15.7) recursively, wecompute
the optimal cost by using a tabular, bottom-up approach. (We present the corre-
sponding top-downapproach usingmemoization inSection15.3.)
We shall implement the tabular, bottom-up method in the procedure MATRIX-
CHAIN-ORDER, which appears below. This procedure assumes that matrix A
i
has dimensions p p for i 1;2;:::;n. Its input is a sequence p
i 1 i
p ;p ;:::;p , w(cid:0) he re p:length D n 1. The procedure uses an auxiliarD y
0 1 n
h i D C
table mŒ1::n;1::n for storing the mŒi;j costs and another auxiliary table
sŒ1::n 1;2::nthatrecordswhichindexofk achievedtheoptimalcostincom-
(cid:0)
putingmŒi;j. Weshallusethetables toconstruct anoptimalsolution.
Inordertoimplementthebottom-upapproach,wemustdeterminewhichentries
of the table we refer to when computing mŒi;j. Equation (15.7) shows that the
costmŒi;jofcomputingamatrix-chainproductofj i 1matricesdependsonly
(cid:0) C
onthecostsofcomputingmatrix-chainproductsoffewerthanj i 1matrices.
(cid:0) C
That is, for k i;i 1;:::;j 1, the matrix A is a product of k i 1 <
i::k
D C (cid:0) (cid:0) C
j i 1 matrices and the matrix A is a product of j k < j i 1
k 1::j
ma(cid:0) triceC
s.
Thus,thealgorithmshouldfillC inthetableminamann(cid:0) erthatcorr(cid:0) espoC
nds
tosolving theparenthesization problem onmatrixchainsofincreasing length. For
the subproblem of optimally parenthesizing the chain A A A , we consider
i i 1 j
thesubproblem sizetobethelengthj i 1ofthechain.C 
(cid:0) C
MATRIX-CHAIN-ORDER.p/
1 n p:length 1
D (cid:0)
2 letmŒ1::n;1::nandsŒ1::n 1;2::nbenewtables
(cid:0)
3 fori 1ton
D
4 mŒi;i 0
D
5 forl 2ton //l isthechainlength
D
6 fori 1ton l 1
D (cid:0) C
7 j i l 1
D C (cid:0)
8 mŒi;j
D 1
9 fork i toj 1
D (cid:0)
10 q mŒi;k mŒk 1;j p p p
i 1 k j
11 ifqD <mŒi;jC  C C (cid:0)
12 mŒi;j q
D
13 sŒi;j k
D
14 returnmands
376 Chapter15 DynamicProgramming
m s
6 1 6 1
15,125 3
5 2 5 2
j i
j i
11,875 10,500 3 3
4 3 4 3
9,375 7,125 5,375 3 3 3
3 4 3 4
7,875 4,375 2,500 3,500 1 3 3 5
2 5 2 5
15,750 2,625 750 1,000 5,000 1 2 3 4 5
1 6
0 0 0 0 0 0
A A A A A A
1 2 3 4 5 6
Figure15.5 ThemandstablescomputedbyMATRIX-CHAIN-ORDERforn 6andthefollow-
D
ingmatrixdimensions:
matrix A1 A2 A3 A4 A5 A6
dimension 30 35 35 15 15 5 5 10 10 20 20 25
     
Thetablesarerotatedsothatthemaindiagonalrunshorizontally. Themtableusesonlythemain
diagonalanduppertriangle,andthes tableusesonlytheuppertriangle. Theminimumnumberof
scalarmultiplicationstomultiplythe6matricesismŒ1;6 15,125.Ofthedarkerentries,thepairs
D
thathavethesameshadingaretakentogetherinline10whencomputing
mŒ2;2 mŒ3;5 p1p2p5 0 2500 35 15 20 13,000;
C C D C C   D
mŒ2;5 min8mŒ2;3 mŒ4;5 p1p3p5 2625 1000 35 5 20 7125;
D C C D C C   D
ˆ<mŒ2;4 mŒ5;5 p1p4p5 4375 0 35 10 20 11,375
C C D C C   D
7125:
D
ˆ:
The algorithm first computes mŒi;i 0 for i 1;2;:::;n (the minimum
D D
costsforchainsoflength1)inlines3–4. Itthenusesrecurrence(15.7)tocompute
mŒi;i 1fori 1;2;:::;n 1(theminimumcostsforchainsoflengthl 2)
C D (cid:0) D
duringthefirstexecutionoftheforloopinlines5–13. Thesecondtimethroughthe
loop,itcomputesmŒi;i 2fori 1;2;:::;n 2(theminimumcostsforchainsof
C D (cid:0)
lengthl 3),andsoforth. Ateachstep,themŒi;jcostcomputedinlines10–13
D
depends onlyontableentriesmŒi;kandmŒk 1;jalready computed.
C
Figure 15.5 illustrates this procedure on a chain of n 6 matrices. Since
D
we have defined mŒi;j only for i j, only the portion of the table m strictly

above the main diagonal is used. The figure shows the table rotated to make the
main diagonal run horizontally. The matrix chain is listed along the bottom. Us-
ing this layout, we can find the minimum cost mŒi;j for multiplying a subchain
A A A ofmatricesattheintersectionoflinesrunningnortheastfromA and
i i 1 j i
C 
15.2 Matrix-chainmultiplication 377
northwestfromA . Eachhorizontalrowinthetablecontainstheentriesformatrix
j
chainsofthesamelength. MATRIX-CHAIN-ORDER computestherowsfrombot-
tom to top and from left to right within each row. It computes each entry mŒi;j
usingtheproducts p p p fork i;i 1;:::;j 1andallentries southwest
i 1 k j
andsoutheast
fromm(cid:0)Œi;j. D C (cid:0)
A simple inspection of the nested loop structure of MATRIX-CHAIN-ORDER
yieldsarunning timeofO.n3/forthealgorithm. Theloopsarenested threedeep,
andeachloopindex(l,i,andk)takesonatmostn 1values. Exercise15.2-5asks
(cid:0)
you to show that the running time of this algorithm is in fact also .n3/. The al-
gorithmrequires ‚.n2/spacetostorethemands tables. Thus, MATRIX-CHAIN-
ORDER is much more efficient than the exponential-time method of enumerating
allpossible parenthesizations andchecking eachone.
Step4: Constructinganoptimalsolution
Although MATRIX-CHAIN-ORDER determines theoptimalnumberofscalarmul-
tiplications needed to compute a matrix-chain product, it does not directly show
how to multiply the matrices. The table sŒ1::n 1;2::n gives us the informa-
(cid:0)
tion we need to do so. Each entry sŒi;j records a value of k such that an op-
timal parenthesization of A A A splits the product between A and A .
i i 1 j k k 1
Thus, we know that the final mC at ri x multiplication in computing A optimaC lly
1::n
is A A . We can determine the earlier matrix multiplications recur-
1::sŒ1;n sŒ1;n 1::n
sively,sincesŒ1;sCŒ1;ndeterminesthelastmatrixmultiplication
whencomputing
A andsŒsŒ1;n 1;ndetermines thelastmatrix multiplication whencom-
1::sŒ1;n
C
puting A . The following recursive procedure prints an optimal parenthe-
sŒ1;n 1::n
sization of AC i;A
i
1;:::;A
j
, given the s table computed by MATRIX-CHAIN-
ORDERandh theindC icesi andji . TheinitialcallPRINT-OPTIMAL-PARENS.s;1;n/
printsanoptimalparenthesization of A ;A ;:::;A .
1 2 n
h i
PRINT-OPTIMAL-PARENS.s;i;j/
1 ifi ==j
2 print“A”
i
3 elseprint“(”
4 PRINT-OPTIMAL-PARENS.s;i;sŒi;j/
5 PRINT-OPTIMAL-PARENS.s;sŒi;j 1;j/
C
6 print“)”
In the example of Figure 15.5, the call PRINT-OPTIMAL-PARENS.s;1;6/ prints
theparenthesization ..A .A A //..A A /A //.
1 2 3 4 5 6
378 Chapter15 DynamicProgramming
Exercises
15.2-1
Find an optimal parenthesization of a matrix-chain product whose sequence of
dimensions is 5;10;3;12;5;50;6 .
h i
15.2-2
Give a recursive algorithm MATRIX-CHAIN-MULTIPLY.A;s;i;j/ that actually
performs the optimal matrix-chain multiplication, given the sequence of matrices
A 1;A 2;:::;A
n
, the s table computed by MATRIX-CHAIN-ORDER, and the in-
h i
dicesi andj. (TheinitialcallwouldbeMATRIX-CHAIN-MULTIPLY.A;s;1;n/.)
15.2-3
Use the substitution method to show that the solution to the recurrence (15.6)
is.2n/.
15.2-4
Describethesubproblemgraphformatrix-chainmultiplicationwithaninputchain
of length n. How many vertices does it have? How many edges does it have, and
whichedgesarethey?
15.2-5
Let R.i;j/ be the number of times that table entry mŒi;j is referenced while
computing other tableentriesinacallof MATRIX-CHAIN-ORDER. Showthatthe
totalnumberofreferences fortheentiretableis
n n n3 n
R.i;j/ (cid:0) :
D 3
i 1 j i
XD XD
(Hint:Youmayfindequation (A.3)useful.)
15.2-6
Showthatafullparenthesizationofann-elementexpressionhasexactlyn 1pairs
(cid:0)
ofparentheses.
15.3 Elements ofdynamicprogramming
Althoughwehavejustworkedthroughtwoexamplesofthedynamic-programming
method, you might still be wondering just when the method applies. From an en-
gineering perspective, when should welook for a dynamic-programming solution
to a problem? In this section, we examine the two key ingredients that an opti-
15.3 Elementsofdynamicprogramming 379
mization problem musthaveinorderfordynamic programming toapply: optimal
substructure and overlapping subproblems. Wealso revisit and discuss morefully
how memoization might help us take advantage of the overlapping-subproblems
propertyinatop-downrecursiveapproach.
Optimalsubstructure
The first step in solving an optimization problem by dynamic programming is to
characterize the structure of an optimal solution. Recall that a problem exhibits
optimalsubstructureifanoptimalsolution totheproblem contains withinitopti-
malsolutions tosubproblems. Whenever aproblem exhibits optimalsubstructure,
wehaveagood cluethat dynamic programming mightapply. (AsChapter16dis-
cusses, it also might mean that a greedy strategy applies, however.) In dynamic
programming, webuildanoptimalsolution totheproblem fromoptimalsolutions
to subproblems. Consequently, wemust take care to ensure that the range of sub-
problemsweconsiderincludes thoseusedinanoptimalsolution.
Wediscovered optimal substructure in both of the problems wehave examined
in this chapter so far. In Section 15.1, we observed that the optimal way of cut-
ting up a rod of length n (if we make any cuts at all) involves optimally cutting
up the two pieces resulting from the first cut. In Section 15.2, we observed that
an optimal parenthesization of A A A that splits the product between A
i i 1 j k
and A contains within it optimalC so lu t ions to the problems of parenthesizing
k 1
A A C A andA A A .
i i 1 k k 1 k 2 j
C  C C 
You will find yourself following a common pattern in discovering optimal sub-
structure:
1. You show that a solution to the problem consists of making a choice, such as
choosinganinitialcutinarodorchoosinganindexatwhichtosplitthematrix
chain. Makingthischoiceleavesoneormoresubproblems tobesolved.
2. Yousuppose thatforagivenproblem, youaregiventhechoicethatleadstoan
optimal solution. You do not concern yourself yet with how to determine this
choice. Youjustassumethatithasbeengiventoyou.
3. Given this choice, you determine which subproblems ensue and how to best
characterize theresulting spaceofsubproblems.
4. Youshowthatthesolutionstothesubproblemsusedwithinanoptimalsolution
to the problem must themselves be optimal by using a “cut-and-paste” tech-
nique. You do so by supposing that each of the subproblem solutions is not
optimal and then deriving a contradiction. In particular, by “cutting out” the
nonoptimal solution toeachsubproblem and“pasting in”the optimal one, you
showthatyoucangetabettersolutiontotheoriginalproblem,thuscontradict-
ing your supposition that you already had an optimal solution. If an optimal
380 Chapter15 DynamicProgramming
solution gives rise to more than one subproblem, they are typically so similar
that you can modify the cut-and-paste argument for one to apply to the others
withlittleeffort.
To characterize the space of subproblems, a good rule of thumb says to try to
keepthespaceassimpleaspossibleandthenexpanditasnecessary. Forexample,
thespaceofsubproblemsthatweconsideredfortherod-cuttingproblemcontained
the problems of optimally cutting up a rod of length i for each size i. This sub-
problem space worked well, and we had no need to try a more general space of
subproblems.
Conversely, suppose that we had tried to constrain our subproblem space for
matrix-chainmultiplicationtomatrixproductsoftheformA A A . Asbefore,
1 2 j

anoptimalparenthesization mustsplitthisproductbetweenA andA forsome
k k 1
1 k < j. Unlesswecouldguarantee thatk alwaysequalsj 1,weC wouldfind
 (cid:0)
that we had subproblems of the form A A A and A A A , and that
1 2 k k 1 k 2 j
thelatter subproblem isnotoftheformA A A . ForthC is prC obl e m , weneeded
1 2 j

to allow our subproblems to vary at “both ends,” that is, to allow both i and j to
varyinthesubproblem A A A .
i i 1 j
C 
Optimalsubstructure variesacrossproblem domainsintwoways:
1. howmanysubproblems anoptimalsolutiontotheoriginalproblem uses,and
2. how many choices we have in determining which subproblem(s) to use in an
optimalsolution.
In the rod-cutting problem, an optimal solution for cutting up a rod of size n
uses just one subproblem (of size n i), but we must consider n choices for i
(cid:0)
in order to determine which one yields an optimal solution. Matrix-chain mul-
tiplication for the subchain A A A serves as an example with two sub-
i i 1 j
problems and j i choices. ForC a g i v en matrix A at which we split the prod-
k
(cid:0)
uct, we have two subproblems—parenthesizing A A A and parenthesizing
i i 1 k
A A A —andwemustsolvebothofthemopC tim a lly. Oncewedetermine
k 1 k 2 j
theC optimC al s o lutionstosubproblems, wechoosefromamongj i candidates for
(cid:0)
theindexk.
Informally, the running time of a dynamic-programming algorithm depends on
the product of two factors: the number of subproblems overall and how many
choiceswelookatforeachsubproblem. Inrodcutting,wehad‚.n/subproblems
overall,andatmostnchoicestoexamineforeach,yieldinganO.n2/runningtime.
Matrix-chainmultiplication had‚.n2/subproblemsoverall,andineachwehadat
mostn 1choices,givinganO.n3/runningtime(actually,a‚.n3/runningtime,
(cid:0)
byExercise15.2-5).
Usually, the subproblem graph gives an alternative way to perform the same
analysis. Each vertex corresponds to a subproblem, and the choices for a sub-
15.3 Elementsofdynamicprogramming 381
problem are the edges incident to that subproblem. Recall that in rod cutting,
the subproblem graph had n vertices and at most n edges per vertex, yielding an
O.n2/ running time. Formatrix-chain multiplication, if wewereto draw the sub-
problemgraph,itwouldhave‚.n2/verticesandeachvertexwouldhavedegreeat
mostn 1,givingatotalofO.n3/verticesandedges.
(cid:0)
Dynamic programming often uses optimal substructure in abottom-up fashion.
Thatis,wefirstfindoptimalsolutions tosubproblems and, having solvedthesub-
problems, we find an optimal solution to the problem. Finding an optimal solu-
tion to the problem entails making a choice among subproblems as to which we
will use in solving the problem. The cost of the problem solution is usually the
subproblem costs plus a cost that is directly attributable to the choice itself. In
rod cutting, for example, first we solved the subproblems of determining optimal
ways to cut up rods of length i for i 0;1;:::;n 1, and then we determined
D (cid:0)
which such subproblem yielded an optimal solution for a rod of length n, using
equation (15.2). The cost attributable to the choice itself is the term p in equa-
i
tion (15.2). In matrix-chain multiplication, we determined optimal parenthesiza-
tions of subchains of A A A , and then we chose the matrix A at which to
i i 1 j k
splittheproduct. ThecostaC ttr i b utable tothechoiceitselfisthetermp p p .
i 1 k j
(cid:0)
InChapter16,weshallexamine“greedyalgorithms,”whichhavemanysimilar-
itiestodynamicprogramming. Inparticular, problemstowhichgreedyalgorithms
applyhaveoptimalsubstructure. Onemajordifferencebetweengreedyalgorithms
anddynamic programming isthatinstead offirstfindingoptimal solutions tosub-
problems and then making an informed choice, greedy algorithms first make a
“greedy”choice—thechoicethatlooksbestatthetime—andthensolvearesulting
subproblem, without bothering to solve all possible related smaller subproblems.
Surprisingly, insomecasesthisstrategyworks!
Subtleties
Youshouldbecarefulnottoassumethatoptimalsubstructure applieswhenitdoes
not. Consider the following two problems in which weare given a directed graph
G .V;E/andverticesu; V.
D 2
Unweightedshortestpath:3 Find a path from u to  consisting of the fewest
edges. Such a path must be simple, since removing a cycle from a path pro-
ducesapathwithfeweredges.
3Weusetheterm“unweighted”todistinguishthisproblemfromthatoffindingshortestpathswith
weighted edges, which we shall see in Chapters 24 and 25. We can use the breadth-first search
techniqueofChapter22tosolvetheunweightedproblem.
382 Chapter15 DynamicProgramming
q r
s t
Figure 15.6 A directed graph showing that the problem of finding a longest simple path in an
unweighteddirectedgraphdoesnothaveoptimalsubstructure. Thepathq r t isalongest
! !
simplepathfromq tot,butthesubpathq r isnotalongestsimplepathfromq tor,noristhe
!
subpathr t alongestsimplepathfromr tot.
!
Unweightedlongestsimplepath: Find a simple path from u to  consisting of
themostedges. Weneedtoincludetherequirementofsimplicitybecauseother-
wise we can traverse a cycle as many times as we like to create paths with an
arbitrarily largenumberofedges.
Theunweightedshortest-pathproblemexhibitsoptimalsubstructure,asfollows.
Suppose that u , so that the problem is nontrivial. Then, any path p from u
¤
to  must contain an intermediate vertex, say w. (Note that w may be u or .)
Thus, wecan decompose the path u
;p
 intosubpaths u
;p1
w
;p2
. Clearly, the
number of edges in p equals the number of edges in p plus the number of edges
1
in p . We claim that if p is an optimal (i.e., shortest) path from u to , then p
2 1
must be a shortest path from u to w. Why? We use a “cut-and-paste” argument:
ifthere wereanother path, sayp ,from utow withfeweredges thanp ,thenwe
10 1
could cut out p and paste in p to produce apath u
;p 10
w
;p2
 withfewer edges
1 10
than p, thus contradicting p’s optimality. Symmetrically, p must be a shortest
2
path from w to . Thus, we can find a shortest path from u to  by considering
allintermediate vertices w,findingashortest pathfromutow andashortest path
fromw to,andchoosinganintermediate vertexw thatyieldstheoverallshortest
path. In Section 25.2, weuse avariant of this observation of optimal substructure
tofindashortestpathbetweeneverypairofverticesonaweighted,directedgraph.
You might be tempted to assume that the problem of finding an unweighted
longest simple path exhibits optimal substructure as well. After all, if wedecom-
pose a longest simple path u
;p
 into subpaths u
;p1
w
;p2
, then mustn’t p
1
be a longest simple path from u to w, and mustn’t p be a longest simple path
2
from w to ? The answer is no! Figure 15.6 supplies an example. Consider the
path q r t, which is a longest simple path from q to t. Is q r a longest
! ! !
simple path from q to r? No, for the path q s t r is a simple path
! ! !
that is longer. Is r t a longest simple path from r to t? No again, for the path
!
r q s t isasimplepaththatislonger.
! ! !
15.3 Elementsofdynamicprogramming 383
This example shows that for longest simple paths, not only does the problem
lack optimal substructure, but we cannot necessarily assemble a “legal” solution
to the problem from solutions to subproblems. If we combine the longest simple
pathsq s t r andr q s t,wegetthepathq s t r
! ! ! ! ! ! ! ! ! !
q s t, which is not simple. Indeed, the problem of finding an unweighted
! !
longest simple path does not appear to have any sort of optimal substructure. No
efficientdynamic-programmingalgorithmforthisproblemhaseverbeenfound. In
fact, this problem is NP-complete, which—as weshall see inChapter 34—means
thatweareunlikely tofindawaytosolveitinpolynomialtime.
Whyisthesubstructureofalongestsimplepathsodifferentfromthatofashort-
estpath? Althoughasolutiontoaproblemforbothlongestandshortestpathsuses
twosubproblems, thesubproblems infindingthelongestsimplepatharenotinde-
pendent, whereas for shortest paths they are. What do we mean by subproblems
being independent? We mean that the solution to one subproblem does not affect
the solution to another subproblem of the same problem. Forthe example of Fig-
ure15.6,wehavetheproblemoffindingalongestsimplepathfromqtot withtwo
subproblems: findinglongestsimplepathsfromq tor andfromr tot. Forthefirst
of these subproblems, we choose the path q s t r, and so we have also
! ! !
used the vertices s and t. We can no longer use these vertices in the second sub-
problem, sincethecombination ofthetwosolutions tosubproblems wouldyielda
path that is not simple. If we cannot use vertex t in the second problem, then we
cannot solve it at all, since t is required to be on the path that we find, and it is
not the vertex at which we are “splicing” together the subproblem solutions (that
vertex being r). Because we use vertices s and t in one subproblem solution, we
cannotusethemintheothersubproblemsolution. Wemustuseatleastoneofthem
to solve the other subproblem, however, and wemust use both of them to solve it
optimally. Thus, we say that these subproblems are not independent. Looked at
another way, using resources in solving one subproblem (those resources being
vertices)rendersthemunavailable fortheothersubproblem.
Why, then, are the subproblems independent for finding a shortest path? The
answer is that by nature, the subproblems do not share resources. We claim that
if a vertex w is on a shortest path p from u to , then we can splice together any
shortestpathu
;p1
wandanyshortestpathw
;p2
toproduceashortestpathfromu
to . We are assured that, other than w, no vertex can appear in both paths p
1
andp . Why? Supposethatsomevertexx w appearsinbothp andp ,sothat
2 1 2
we can decompose p as u p;ux x ; w and¤ p as w ; x p;x . By the optimal
1 2
substructureofthisproblem,pathphasasmanyedgesasp andp together;let’s
1 2
saythatphaseedges. Nowletusconstructapathp u
p;ux
x
p;x
 fromuto.
0
D
Because we have excised the paths from x to w and from w to x, each of which
contains atleastoneedge, pathp contains atmoste 2edges, whichcontradicts
0
(cid:0)
384 Chapter15 DynamicProgramming
theassumptionthatpisashortestpath. Thus,weareassuredthatthesubproblems
fortheshortest-path problem areindependent.
Both problems examined in Sections 15.1 and 15.2 have independent subprob-
lems. In matrix-chain multiplication, the subproblems are multiplying subchains
A A A and A A A . These subchains are disjoint, so that no ma-
i i 1 k k 1 k 2 j
C  C C 
trix could possibly be included in both of them. In rod cutting, to determine the
best way to cut up a rod of length n, we look at the best ways of cutting up rods
of length i for i 0;1;:::;n 1. Because an optimal solution to the length-n
D (cid:0)
problem includes justoneofthesesubproblem solutions (after wehavecutoffthe
firstpiece), independence ofsubproblems isnotanissue.
Overlappingsubproblems
The second ingredient that an optimization problem must have for dynamic pro-
gramming to apply is that the space of subproblems must be “small” in the sense
that a recursive algorithm for the problem solves the same subproblems over and
over, rather than always generating new subproblems. Typically, the total number
of distinct subproblems is a polynomial in the input size. When a recursive algo-
rithm revisits the same problem repeatedly, we say that the optimization problem
has overlapping subproblems.4 In contrast, a problem for which a divide-and-
conquer approach is suitable usually generates brand-new problems at each step
of the recursion. Dynamic-programming algorithms typically take advantage of
overlapping subproblems by solving each subproblem once and then storing the
solutioninatablewhereitcanbelookedupwhenneeded,usingconstanttimeper
lookup.
In Section 15.1, we briefly examined how a recursive solution to rod cut-
ting makes exponentially many calls to find solutions of smaller subproblems.
Ourdynamic-programming solutiontakesanexponential-time recursivealgorithm
downtoquadratic time.
To illustrate the overlapping-subproblems property in greater detail, let us re-
examine the matrix-chain multiplication problem. Referring back to Figure 15.5,
observethatMATRIX-CHAIN-ORDER repeatedlylooksupthesolutiontosubprob-
lems in lower rows when solving subproblems in higher rows. For example, it
references entry mŒ3;4 four times: during the computations of mŒ2;4, mŒ1;4,
4It may seem strange that dynamic programming relies on subproblems being both independent
andoverlapping. Althoughtheserequirementsmaysoundcontradictory,theydescribetwodifferent
notions, ratherthantwopointsonthesameaxis. Twosubproblemsofthesameproblemareinde-
pendentiftheydonotshareresources. Twosubproblemsareoverlappingiftheyarereallythesame
subproblemthatoccursasasubproblemofdifferentproblems.
15.3 Elementsofdynamicprogramming 385
1..4
1..1 2..4 1..2 3..4 1..3 4..4
2..2 3..4 2..3 4..4 1..1 2..2 3..3 4..4 1..1 2..3 1..2 3..3
3..3 4..4 2..2 3..3 2..2 3..3 1..1 2..2
Figure 15.7 The recursion tree for the computation of RECURSIVE-MATRIX-CHAIN.p;1;4/.
Eachnode containstheparameters i andj. Thecomputations performed inashaded subtreeare
replacedbyasingletablelookupinMEMOIZED-MATRIX-CHAIN.
mŒ3;5, and mŒ3;6. If we were to recompute mŒ3;4 each time, rather than just
looking itup, the running timewould increase dramatically. Tosee how, consider
the following (inefficient) recursive procedure that determines mŒi;j, the mini-
mumnumberofscalarmultiplicationsneededtocomputethematrix-chainproduct
A A A A . Theprocedure isbaseddirectlyontherecurrence (15.7).
i::j i i 1 j
D C 
RECURSIVE-MATRIX-CHAIN.p;i;j/
1 ifi ==j
2 return0
3 mŒi;j
D 1
4 fork i toj 1
D (cid:0)
5 q RECURSIVE-MATRIX-CHAIN.p;i;k/
D
RECURSIVE-MATRIX-CHAIN.p;k 1;j/
C C
p p p
i 1 k j
6 ifq <C mŒi(cid:0);j
7 mŒi;j q
D
8 returnmŒi;j
Figure 15.7 shows the recursion tree produced by the call RECURSIVE-MATRIX-
CHAIN.p;1;4/. Each node is labeled by the values of the parameters i and j.
Observethatsomepairsofvaluesoccurmanytimes.
In fact, we can show that the time to compute mŒ1;n by this recursive proce-
dure is at least exponential in n. LetT.n/ denote the time taken by RECURSIVE-
MATRIX-CHAIN tocomputeanoptimalparenthesization ofachainofnmatrices.
Becausetheexecution oflines1–2andoflines6–7eachtakeatleastunittime,as
386 Chapter15 DynamicProgramming
doesthemultiplication inline5,inspection oftheprocedure yieldstherecurrence
T.1/ 1;

n 1
(cid:0)
T.n/ 1 .T.k/ T.n k/ 1/ forn >1:
 C C (cid:0) C
k 1
XD
Notingthatfori 1;2;:::;n 1,eachtermT.i/appearsonceasT.k/andonce
D (cid:0)
asT.n k/,andcollecting then 11sinthe summation together withthe1out
(cid:0) (cid:0)
front,wecanrewritetherecurrence as
n 1
(cid:0)
T.n/ 2 T.i/ n: (15.8)
 C
i 1
XD
We shall prove that T.n/ .2n/ using the substitution method. Specifi-
D
cally, we shall show that T.n/ 2n 1 for all n 1. The basis is easy, since
(cid:0)
 
T.1/ 1 20. Inductively, forn 2wehave
 D 
n 1
(cid:0)
T.n/ 2 2i 1 n
(cid:0)
 C
i 1
XD
n 2
(cid:0)
2 2i n
D C
i 0
XD
2.2n 1 1/ n (byequation (A.5))
(cid:0)
D (cid:0) C
2n 2 n
D (cid:0) C
2n 1 ;
(cid:0)

which completes the proof. Thus, the total amount of work performed by the call
RECURSIVE-MATRIX-CHAIN.p;1;n/isatleastexponential inn.
Compare this top-down, recursive algorithm (without memoization) with the
bottom-up dynamic-programming algorithm. The latter is more efficient because
it takes advantage of the overlapping-subproblems property. Matrix-chain mul-
tiplication has only ‚.n2/ distinct subproblems, and the dynamic-programming
algorithm solves each exactly once. The recursive algorithm, on the other hand,
must again solve each subproblem every time it reappears in the recursion tree.
Whenever arecursion treefor the natural recursive solution to aproblem contains
the same subproblem repeatedly, and the total number of distinct subproblems is
small,dynamicprogrammingcanimproveefficiency,sometimesdramatically.
15.3 Elementsofdynamicprogramming 387
Reconstructinganoptimalsolution
Asapracticalmatter,weoftenstorewhichchoicewemadeineachsubproblem in
atablesothatwedonothavetoreconstructthisinformationfromthecoststhatwe
stored.
Formatrix-chainmultiplication,thetablesŒi;jsavesusasignificantamountof
work when reconstructing an optimal solution. Suppose that we did not maintain
thesŒi;jtable,havingfilledinonlythetablemŒi;jcontainingoptimalsubprob-
lem costs. We choose from among j i possibilities when we determine which
(cid:0)
subproblems to use in an optimal solution to parenthesizing A A A , and
i i 1 j
j i isnot aconstant. Therefore, it would take ‚.j i/ !.1/ tC im e to recon-
(cid:0) (cid:0) D
struct which subproblems we chose for a solution to a given problem. By storing
in sŒi;j the index of the matrix at which we split the product A A A , we
i i 1 j
canreconstruct eachchoiceinO.1/time. C 
Memoization
As we saw for the rod-cutting problem, there is an alternative approach to dy-
namic programming that often offers the efficiency of the bottom-up dynamic-
programming approach while maintaining a top-down strategy. The idea is to
memoize the natural, but inefficient, recursive algorithm. As inthe bottom-up ap-
proach, we maintain a table with subproblem solutions, but the control structure
forfillinginthetableismoreliketherecursivealgorithm.
Amemoizedrecursivealgorithmmaintainsanentryinatableforthesolutionto
eachsubproblem. Eachtableentryinitiallycontainsaspecialvaluetoindicatethat
the entry has yet to be filled in. When the subproblem is first encountered as the
recursive algorithm unfolds, its solution is computed and then stored in the table.
Each subsequent time that we encounter this subproblem, we simply look up the
valuestoredinthetableandreturnit.5
Here is a memoized version of RECURSIVE-MATRIX-CHAIN. Note where it
resemblesthememoizedtop-downmethodfortherod-cutting problem.
5Thisapproachpresupposesthatweknowthesetofallpossiblesubproblemparametersandthatwe
haveestablishedtherelationshipbetweentablepositionsandsubproblems. Another,moregeneral,
approachistomemoizebyusinghashingwiththesubproblemparametersaskeys.
388 Chapter15 DynamicProgramming
MEMOIZED-MATRIX-CHAIN.p/
1 n p:length 1
D (cid:0)
2 letmŒ1::n;1::nbeanewtable
3 fori 1ton
D
4 forj i ton
D
5 mŒi;j
D 1
6 return LOOKUP-CHAIN.m;p;1;n/
LOOKUP-CHAIN.m;p;i;j/
1 ifmŒi;j <
1
2 returnmŒi;j
3 ifi ==j
4 mŒi;j 0
D
5 elsefork i toj 1
D (cid:0)
6 q LOOKUP-CHAIN.m;p;i;k/
D
LOOKUP-CHAIN.m;p;k 1;j/ p
i
1p kp
j
7 ifq <C mŒi;j C C (cid:0)
8 mŒi;j q
D
9 returnmŒi;j
The MEMOIZED-MATRIX-CHAIN procedure, like MATRIX-CHAIN-ORDER,
maintainsatablemŒ1::n;1::nofcomputedvaluesofmŒi;j,theminimumnum-
ber of scalar multiplications needed to compute the matrix A . Each table entry
i::j
initiallycontainsthevalue toindicatethattheentryhasyettobefilledin. Upon
1
calling LOOKUP-CHAIN.m;p;i;j/,ifline1findsthatmŒi;j < ,thenthepro-
1
cedure simply returns the previously computed cost mŒi;j in line 2. Otherwise,
the cost is computed as in RECURSIVE-MATRIX-CHAIN, stored in mŒi;j, and
returned. Thus, LOOKUP-CHAIN.m;p;i;j/ always returns the value of mŒi;j,
but it computes it only upon the first call of LOOKUP-CHAIN with these specific
valuesofi andj.
Figure 15.7 illustrates how MEMOIZED-MATRIX-CHAIN saves time compared
withRECURSIVE-MATRIX-CHAIN. Shadedsubtreesrepresentvaluesthatitlooks
upratherthanrecomputes.
Likethebottom-updynamic-programmingalgorithmMATRIX-CHAIN-ORDER,
the procedure MEMOIZED-MATRIX-CHAIN runs in O.n3/ time. Line 5 of
MEMOIZED-MATRIX-CHAIN executes ‚.n2/ times. We can categorize the calls
of LOOKUP-CHAIN intotwotypes:
1. callsinwhichmŒi;j ,sothatlines3–9execute, and
D1
2. callsinwhichmŒi;j < ,sothat LOOKUP-CHAIN simplyreturnsinline2.
1
15.3 Elementsofdynamicprogramming 389
There are ‚.n2/ calls of the first type, one per table entry. All calls of the sec-
ond type are made as recursive calls by calls of the first type. Whenever a given
call of LOOKUP-CHAIN makes recursive calls, it makes O.n/ of them. There-
fore, there are O.n3/ calls of the second type in all. Each call of the second type
takesO.1/time,andeachcallofthefirsttypetakesO.n/timeplusthetimespent
in its recursive calls. The total time, therefore, is O.n3/. Memoization thus turns
an.2n/-timealgorithm intoanO.n3/-timealgorithm.
In summary, we can solve the matrix-chain multiplication problem by either a
top-down, memoized dynamic-programming algorithm or a bottom-up dynamic-
programming algorithm in O.n3/ time. Both methods take advantage of the
overlapping-subproblems property. There are only ‚.n2/ distinct subproblems in
total, and either of these methods computes the solution to each subproblem only
once. Without memoization, the natural recursive algorithm runs in exponential
time,sincesolvedsubproblems arerepeatedly solved.
Ingeneralpractice, ifallsubproblems mustbesolvedatleastonce,abottom-up
dynamic-programmingalgorithmusuallyoutperformsthecorrespondingtop-down
memoizedalgorithm byaconstantfactor, becausethebottom-up algorithm hasno
overhead for recursion and less overhead for maintaining the table. Moreover, for
someproblemswecanexploittheregularpatternoftableaccessesinthedynamic-
programming algorithm to reduce time or space requirements even further. Alter-
natively, if some subproblems in the subproblem space need not be solved at all,
the memoized solution has the advantage of solving only those subproblems that
aredefinitely required.
Exercises
15.3-1
Whichisamoreefficientwaytodetermine theoptimal number ofmultiplications
inamatrix-chainmultiplicationproblem: enumeratingallthewaysofparenthesiz-
ing the product and computing the number of multiplications for each, or running
RECURSIVE-MATRIX-CHAIN? Justify youranswer.
15.3-2
Drawtherecursion treeforthe MERGE-SORT procedure fromSection2.3.1onan
array of 16 elements. Explain why memoization fails to speed up a good divide-
and-conquer algorithm suchas MERGE-SORT.
15.3-3
Consider avariant ofthematrix-chain multiplication problem inwhichthegoalis
toparenthesize the sequence ofmatrices soastomaximize, rather than minimize,
390 Chapter15 DynamicProgramming
the number of scalar multiplications. Does this problem exhibit optimal substruc-
ture?
15.3-4
Asstated,indynamicprogrammingwefirstsolvethesubproblemsandthenchoose
which of them to use in an optimal solution to the problem. Professor Capulet
claims that wedo notalways need tosolve allthe subproblems inorder tofindan
optimal solution. Shesuggests that wecan findan optimal solution tothe matrix-
chain multiplication problem by always choosing the matrix A at which to split
k
the subproduct A A A (by selecting k to minimize the quantity p p p )
i i 1 j i 1 k j
C  (cid:0)
before solving the subproblems. Find an instance of the matrix-chain multiplica-
tionproblem forwhichthisgreedyapproach yieldsasuboptimal solution.
15.3-5
Supposethatintherod-cuttingproblemofSection15.1,wealsohadlimitl onthe
i
number of pieces of length i that we are allowed to produce, for i 1;2;:::;n.
D
Show that the optimal-substructure property described in Section 15.1 no longer
holds.
15.3-6
Imagine that you wish to exchange one currency for another. You realize that
instead of directly exchanging one currency for another, you might be better off
making a series of trades through other currencies, winding up with the currency
youwant. Supposethatyoucantradendifferentcurrencies, numbered1;2;:::;n,
where you start with currency 1 and wish to wind up with currency n. You are
given, for each pair of currencies i and j, an exchange rate r , meaning that if
ij
you start with d units of currency i, you can trade for dr units of currency j.
ij
A sequence of trades may entail a commission, which depends on the number of
tradesyoumake. Letc bethecommissionthatyouarechargedwhenyoumakek
k
trades. Showthat,ifc 0forallk 1;2;:::;n,thentheproblemoffindingthe
k
D D
best sequence of exchanges from currency 1 to currency n exhibits optimal sub-
structure. Thenshowthatifcommissionsc arearbitraryvalues, thentheproblem
k
of finding the best sequence of exchanges from currency 1to currency n does not
necessarily exhibitoptimalsubstructure.
15.4 Longest commonsubsequence
Biological applications often need to compare the DNA of two (or more) dif-
ferent organisms. A strand of DNA consists of a string of molecules called
15.4 Longestcommonsubsequence 391
bases, where the possible bases are adenine, guanine, cytosine, and thymine.
Representing each of these bases by its initial letter, we can express a strand
of DNA as a string over the finite set A;C;G;T . (See Appendix C for
f g
the definition of a string.) For example, the DNA of one organism may be
S ACCGGTCGAGTGCGCGGAAGCCGGCCGAA,andthe DNAofanother organ-
1
D
ism may be S GTCGTTCGGAATGCCGTTGCTCTGTAAA. One reason to com-
2
D
paretwostrandsofDNAistodeterminehow“similar”thetwostrandsare,assome
measureofhowcloselyrelatedthetwoorganismsare. Wecan,anddo,definesim-
ilarity inmanydifferent ways. Forexample, wecansaythattwoDNAstrands are
similar if one isasubstring ofthe other. (Chapter 32 explores algorithms to solve
thisproblem.) Inourexample, neither S norS isasubstring oftheother. Alter-
1 2
natively,wecouldsaythattwostrandsaresimilarifthenumberofchangesneeded
toturnoneintotheotherissmall. (Problem15-5looksatthisnotion.) Yetanother
way to measure the similarity of strands S and S is by finding a third strand S
1 2 3
in which the bases in S appear in each of S and S ; these bases must appear
3 1 2
in the same order, but not necessarily consecutively. The longer the strand S we
3
can find, the more similar S and S are. In our example, the longest strand S is
1 2 3
GTCGTCGGAAGCCGGCCGAA.
Weformalize this last notion of similarity as the longest-common-subsequence
problem. Asubsequenceofagivensequenceisjustthegivensequencewithzeroor
moreelementsleftout. Formally,givenasequenceX x ;x ;:::;x ,another
1 2 m
D h i
sequence Z ´ ; ´ ; :::; ´ is a subsequence of X if there exists a strictly
1 2 k
D h i
increasingsequence i ;i ;:::;i ofindicesofX suchthatforallj 1;2;:::;k,
1 2 k
h i D
we have x ´ . For example, Z B;C;D;B is a subsequence of X
ij
D
j
D h i D
A;B;C;B;D;A;B withcorresponding indexsequence 2;3;5;7 .
h i h i
Given two sequences X and Y, we say that a sequence Z is a common sub-
sequence of X and Y if Z is a subsequence of both X and Y. For example, if
X A;B;C;B;D;A;B andY B;D;C;A;B;A ,thesequence B;C;A is
Dh i Dh i h i
acommonsubsequence ofbothX andY. Thesequence B;C;A isnotalongest
h i
common subsequence (LCS) of X and Y, however, since it has length 3 and the
sequence B;C;B;A ,whichisalsocommontobothX andY,haslength 4. The
h i
sequence B;C;B;A is an LCS of X and Y, as is the sequence B;D;A;B ,
h i h i
sinceX andY havenocommonsubsequence oflength5orgreater.
In the longest-common-subsequence problem, we are given two sequences
X x ;x ;:::;x and Y y ;y ;:::;y and wish to find a maximum-
1 2 m 1 2 n
D h i D h i
length common subsequence of X and Y. This section shows how to efficiently
solvetheLCSproblem usingdynamicprogramming.
392 Chapter15 DynamicProgramming
Step1: Characterizingalongestcommonsubsequence
In a brute-force approach to solving the LCS problem, we would enumerate all
subsequences of X and check each subsequence to see whether it is also a subse-
quenceofY,keepingtrackofthelongestsubsequence wefind. Eachsubsequence
ofX corresponds toasubset oftheindices 1;2;:::;m ofX. BecauseX has2m
f g
subsequences, this approach requires exponential time, making it impractical for
longsequences.
TheLCSproblem hasanoptimal-substructure property, however,asthefollow-
ing theorem shows. As we shall see, the natural classes of subproblems corre-
spond to pairs of “prefixes” of the two input sequences. To be precise, given a
sequenceX x ;x ;:::;x ,wedefinetheithprefixofX,fori 0;1;:::;m,
1 2 m
Dh i D
as X x ; x ; :::; x . For example, if X A; B; C; B; D; A; B , then
i 1 2 i
D h i D h i
X A;B;C;B andX istheemptysequence.
4 0
D h i
Theorem15.1(OptimalsubstructureofanLCS)
Let X x ;x ;:::;x and Y y ;y ;:::;y be sequences, and let Z
1 2 m 1 2 n
D h i D h i D
´ ;´ ;:::;´ beanyLCSofX andY.
1 2 k
h i
1. Ifx y ,then´ x y andZ isanLCSofX andY .
m n k m n k 1 m 1 n 1
D D D (cid:0) (cid:0) (cid:0)
2. Ifx y ,then´ x impliesthatZ isanLCSofX andY.
m n k m m 1
¤ ¤ (cid:0)
3. Ifx y ,then´ y impliesthatZ isanLCSofX andY .
m n k n n 1
¤ ¤ (cid:0)
Proof (1)If´ x ,thenwecould append x y toZ toobtain acommon
k m m n
¤ D
subsequence of X and Y of length k 1, contradicting the supposition that Z is
C
a longest common subsequence of X and Y. Thus, wemust have ´ x y .
k m n
D D
Now,theprefixZ isalength-.k 1/commonsubsequence ofX andY .
k 1 m 1 n 1
(cid:0) (cid:0) (cid:0) (cid:0)
We wish to show that it is an LCS. Suppose for the purpose of contradiction
that there exists acommon subsequence W of X and Y withlength greater
m 1 n 1
than k 1. Then, appending x y to W produ(cid:0) ces a com(cid:0) mon subsequence of
m n
(cid:0) D
X andY whoselengthisgreaterthank,whichisacontradiction.
(2)If´ x ,thenZisacommonsubsequenceofX andY. Iftherewerea
k m m 1
commonsu¤ bsequenceW ofX andY withlengthgreat(cid:0) erthank,thenW would
m 1
alsobeacommonsubsequence(cid:0) ofX andY,contradicting theassumption thatZ
m
isanLCSofX andY.
(3)Theproofissymmetricto(2).
The way that Theorem 15.1 characterizes longest common subsequences tells
us that an LCS of two sequences contains within it an LCS of prefixes of the two
sequences. Thus,theLCSproblem hasanoptimal-substructure property. Arecur-
15.4 Longestcommonsubsequence 393
sive solution also has the overlapping-subproblems property, as we shall see in a
moment.
Step2: Arecursive solution
Theorem15.1impliesthatweshouldexamineeitheroneortwosubproblemswhen
finding an LCS of X x ;x ;:::;x and Y y ;y ;:::;y . If x y ,
1 2 m 1 2 n m n
D h i D h i D
we must find an LCS of X and Y . Appending x y to this LCSyields
m 1 n 1 m n
anLCSofX andY. Ifx (cid:0)y ,then(cid:0) wemustsolvetwoD subproblems: findingan
m n
¤
LCSof X and Y and finding an LCSof X and Y . Whichever of these two
m 1 n 1
LCSsislon(cid:0) gerisanLCSofX andY. Becausetheseca(cid:0) sesexhaustallpossibilities,
weknowthatoneoftheoptimalsubproblem solutions mustappearwithinanLCS
ofX andY.
We can readily see the overlapping-subproblems property in the LCS problem.
To find an LCS of X and Y, we may need to find the LCSs of X and Y and
n 1
of X and Y. Buteach of these subproblems has the subsubproblem of fi(cid:0) nding
m 1
anLC(cid:0) SofX andY . Manyothersubproblems sharesubsubproblems.
m 1 n 1
(cid:0) (cid:0)
Asinthematrix-chainmultiplicationproblem,ourrecursivesolutiontotheLCS
problem involves establishing a recurrence for the value of an optimal solution.
Let us define cŒi;j to be the length of an LCS of the sequences X and Y . If
i j
either i 0 or j 0, one of the sequences has length 0, and so the LCS has
D D
length0. Theoptimalsubstructure oftheLCSproblemgivestherecursiveformula
0 ifi 0orj 0;
D D
cŒi;j cŒi 1;j 1 1 ifi;j > 0andx y ; (15.9)
i j
D (cid:0) (cid:0) C D
max.cŒi;j 1;cŒi 1;j/ ifi;j > 0andx y :
i j
 (cid:0) (cid:0) ¤
Observe that in this recursive formulation, a condition in the problem restricts
whichsubproblems wemayconsider. Whenx y ,wecanandshould consider
i j
D
the subproblem of finding an LCS of X and Y . Otherwise, we instead con-
i 1 j 1
siderthetwosubproblemsoffindinganL(cid:0) CSofX (cid:0) andY andofX andY . In
i j 1 i 1 j
(cid:0) (cid:0)
thepreviousdynamic-programmingalgorithmswehaveexamined—forrodcutting
and matrix-chain multiplication—we ruled out no subproblems due to conditions
in the problem. Finding an LCS is not the only dynamic-programming algorithm
that rules out subproblems based on conditions in the problem. For example, the
edit-distance problem (seeProblem15-5)hasthischaracteristic.
Step3: ComputingthelengthofanLCS
Based on equation (15.9), we could easily write an exponential-time recursive al-
gorithmtocomputethelengthofanLCSoftwosequences. SincetheLCSproblem
394 Chapter15 DynamicProgramming
hasonly‚.mn/distinctsubproblems,however,wecanusedynamicprogramming
tocomputethesolutions bottom up.
Procedure LCS-LENGTH takes two sequences X x 1; x 2; :::; x
m
and
D h i
Y y ;y ;:::;y asinputs. ItstoresthecŒi;jvaluesinatablecŒ0::m;0::n,
1 2 n
Dh i
and itcomputes the entries in row-major order. (That is, the procedure fillsin the
firstrowofcfromlefttoright,thenthesecondrow,andsoon.) Theprocedurealso
maintains the table bŒ1::m;1::n to help us construct an optimal solution. Intu-
itively, bŒi;j points to the table entry corresponding to the optimal subproblem
solution chosenwhencomputing cŒi;j. Theprocedure returnstheb andc tables;
cŒm;ncontains thelengthofanLCSofX andY.
LCS-LENGTH.X;Y/
1 m X:length
D
2 n Y:length
D
3 letbŒ1::m;1::nandcŒ0::m;0::nbenewtables
4 fori 1tom
D
5 cŒi;0 0
D
6 forj 0ton
D
7 cŒ0;j 0
D
8 fori 1tom
D
9 forj 1ton
D
10 ifx == y
i j
11 cŒi;j cŒi 1;j 1 1
D (cid:0) (cid:0) C
12 bŒi;j “ ”
D -
13 elseifcŒi 1;j cŒi;j 1
(cid:0)  (cid:0)
14 cŒi;j cŒi 1;j
D (cid:0)
15 bŒi;j “ ”
D "
16 elsecŒi;j cŒi;j 1
D (cid:0)
17 bŒi;j “ ”
D
18 returnc andb
Figure 15.8 shows the tables produced by LCS-LENGTH on the sequences X
D
A;B;C;B; D;A;B and Y B;D;C;A;B;A . The running time of the
h i D h i
procedure is‚.mn/,sinceeachtableentrytakes‚.1/timetocompute.
Step4: ConstructinganLCS
The b table returned by LCS-LENGTH enables us to quickly construct an LCSof
X x ;x ;:::;x and Y y ;y ;:::;y . Wesimply begin atbŒm;nand
1 2 m 1 2 n
D h i D h i
tracethroughthetablebyfollowingthearrows. Wheneverweencounter a“ ”in
-
entry bŒi;j, it implies that x
i
y
j
is anelement of the LCSthat LCS-LENGTH
D
15.4 Longestcommonsubsequence 395
j 0 1 2 3 4 5 6
i y B D C A B A
j
0 x
i 0 0 0 0 0 0 0
1 A
0 0 0 0 1 1 1
2 B
0 1 1 1 1 2 2
3 C
0 1 1 2 2 2 2
4 B
0 1 1 2 2 3 3
5 D
0 1 2 2 2 3 3
6 A
0 1 2 2 3 3 4
7 B
0 1 2 2 3 4 4
Figure15.8 Thec andbtablescomputedbyLCS-LENGTHonthesequencesX A;B;C;B;
D h
D;A;B andY B;D;C;A;B;A .Thesquareinrowiandcolumnj containsthevalueofcŒi;j
i Dh i
andtheappropriatearrowforthevalueofbŒi;j.Theentry4incŒ7;6—thelowerright-handcorner
ofthetable—isthelengthofanLCS B;C;B;A ofX andY. Fori;j >0,entrycŒi;jdepends
h i
onlyonwhetherx
i
y
j
andthevaluesinentriescŒi 1;j,cŒi;j 1,andcŒi 1;j 1,which
D (cid:0) (cid:0) (cid:0) (cid:0)
arecomputedbeforecŒi;j. ToreconstructtheelementsofanLCS,followthebŒi;jarrowsfrom
thelowerright-handcorner;thesequenceisshaded.Each“ ”ontheshadedsequencecorresponds
-
toanentry(highlighted)forwhichx i y j isamemberofanLCS.
D
found. With this method, weencounter the elements of this LCSin reverse order.
The following recursive procedure prints out an LCS of X and Y in the proper,
forwardorder. TheinitialcallisPRINT-LCS.b;X;X:length;Y:length/.
PRINT-LCS.b;X;i;j/
1 ifi ==0orj == 0
2 return
3 ifbŒi;j ==“ ”
-
4 PRINT-LCS.b;X;i 1;j 1/
(cid:0) (cid:0)
5 printx
i
6 elseifbŒi;j ==“ ”
"
7 PRINT-LCS.b;X;i 1;j/
(cid:0)
8 else PRINT-LCS.b;X;i;j 1/
(cid:0)
For the b table in Figure 15.8, this procedure prints BCBA. The procedure takes
timeO.m n/,sinceitdecrements atleastoneofi andj ineachrecursivecall.
C
396 Chapter15 DynamicProgramming
Improvingthecode
Once you have developed an algorithm, you will often find that you can improve
on the time or space it uses. Some changes can simplify the code and improve
constant factors but otherwise yield no asymptotic improvement in performance.
Otherscanyieldsubstantial asymptotic savings intimeandspace.
IntheLCSalgorithm,forexample,wecaneliminatethebtablealtogether. Each
cŒi;jentrydependsononlythreeotherc tableentries: cŒi 1;j 1,cŒi 1;j,
(cid:0) (cid:0) (cid:0)
andcŒi;j 1. GiventhevalueofcŒi;j,wecandetermineinO.1/timewhichof
(cid:0)
thesethreevalueswasusedtocomputecŒi;j,withoutinspectingtableb. Thus,we
canreconstructanLCSinO.m n/timeusingaproceduresimilartoPRINT-LCS.
C
(Exercise15.4-2asksyoutogivethepseudocode.) Althoughwesave‚.mn/space
by this method, the auxiliary space requirement for computing an LCS does not
asymptotically decrease, sinceweneed‚.mn/spaceforthec tableanyway.
Wecan,however,reducetheasymptoticspacerequirements forLCS-LENGTH,
since itneeds only two rowsoftable c at atime: the row being computed and the
previousrow. (Infact,asExercise15.4-4asksyoutoshow,wecanuseonlyslightly
more than the space for one row of c to compute the length of an LCS.) This
improvementworksifweneedonlythelengthofanLCS;ifweneedtoreconstruct
the elements of an LCS, the smaller table does not keep enough information to
retraceourstepsinO.m n/time.
C
Exercises
15.4-1
DetermineanLCSof 1;0;0;1;0;1;0;1 and 0;1;0;1;1;0;1;1;0 .
h i h i
15.4-2
GivepseudocodetoreconstructanLCSfromthecompletedctableandtheoriginal
sequences X x ;x ;:::;x and Y y ;y ;:::;y in O.m n/ time,
1 2 m 1 2 n
D h i D h i C
withoutusingtheb table.
15.4-3
GiveamemoizedversionofLCS-LENGTH thatrunsinO.mn/time.
15.4-4
ShowhowtocomputethelengthofanLCSusingonly2 min.m;n/entriesinthec

table plus O.1/ additional space. Then show how to do the same thing, but using
min.m;n/entries plusO.1/additional space.
15.5 Optimalbinarysearchtrees 397
15.4-5
GiveanO.n2/-timealgorithm tofindthelongest monotonically increasing subse-
quenceofasequence ofnnumbers.
15.4-6 ?
GiveanO.nlgn/-timealgorithmtofindthelongestmonotonicallyincreasingsub-
sequence of a sequence of n numbers. (Hint: Observe that the last element of a
candidate subsequence of length i is at least as large as the last element of a can-
didate subsequence of length i 1. Maintain candidate subsequences by linking
(cid:0)
themthrough theinputsequence.)
15.5 Optimalbinary search trees
Suppose thatwearedesigning aprogram totranslate textfrom English toFrench.
ForeachoccurrenceofeachEnglishwordinthetext,weneedtolookupitsFrench
equivalent. Wecouldperform theselookup operations bybuilding abinarysearch
tree with n English words as keys and their French equivalents as satellite data.
Because we will search the tree for each individual word in the text, we want the
total time spent searching to be as low as possible. We could ensure an O.lgn/
search timeper occurrence by using ared-black tree or anyother balanced binary
search tree. Words appear with different frequencies, however, and a frequently
usedwordsuchasthemayappear farfromtherootwhileararelyusedwordsuch
asmachicolationappearsneartheroot. Suchanorganizationwouldslowdownthe
translation, sincethenumberofnodesvisitedwhensearchingforakeyinabinary
search tree equals one plus the depth of the node containing the key. We want
words that occur frequently in the text to be placed nearer the root.6 Moreover,
some words in the text might have no French translation,7 and such words would
notappearinthebinarysearchtreeatall. Howdoweorganizeabinarysearchtree
soastominimize thenumber ofnodes visited inallsearches, given thatweknow
howofteneachwordoccurs?
What we need is known as an optimal binary search tree. Formally, we are
given a sequence K k ;k ;:::;k of n distinct keys in sorted order (so that
1 2 n
D h i
k <k < <k ), and we wish to build a binary search tree from these keys.
1 2 n

For each key k , we have a probability p that a search will be for k . Some
i i i
searches may be for values not in K, and so we also have n 1 “dummy keys”
C
6Ifthesubjectofthetextiscastlearchitecture,wemightwantmachicolationtoappearneartheroot.
7Yes,machicolationhasaFrenchcounterpart: maˆchicoulis.
398 Chapter15 DynamicProgramming
k k
2 2
k k k k
1 4 1 5
d d k k d d k d
0 1 3 5 0 1 4 5
d d d d k d
2 3 4 5 3 4
d d
2 3
(a) (b)
Figure15.9 Twobinarysearchtreesforasetofn 5keyswiththefollowingprobabilities:
D
i 0 1 2 3 4 5
p i 0.15 0.10 0.05 0.10 0.20
q i 0.05 0.10 0.05 0.05 0.05 0.10
(a)Abinarysearchtreewithexpectedsearchcost2.80.(b)Abinarysearchtreewithexpectedsearch
cost2.75.Thistreeisoptimal.
d ;d ;d ;:::;d representing valuesnotinK. Inparticular, d represents allval-
0 1 2 n 0
ueslessthank ,d representsallvaluesgreaterthank ,andfori 1;2;:::;n 1,
1 n n
D (cid:0)
the dummy key d represents all values between k and k . For each dummy
i i i 1
key d , we have a probability q that a search will correspoC nd to d . Figure 15.9
i i i
shows two binary search trees for a set of n 5 keys. Each key k is an internal
i
D
node, and each dummy key d is aleaf. Every search is either successful (finding
i
somekeyk )orunsuccessful (findingsomedummykeyd ),andsowehave
i i
n n
p q 1: (15.10)
i i
C D
i 1 i 0
XD XD
Because we have probabilities of searches for each key and each dummy key,
wecandeterminetheexpectedcostofasearchinagivenbinarysearchtreeT. Let
us assume that the actual cost of a search equals the number of nodes examined,
i.e.,thedepthofthenodefoundbythesearchinT,plus1. Thentheexpectedcost
ofasearchinT is
n n
EŒsearchcostinT .depth .k / 1/ p .depth .d / 1/ q
D T i C  i C T i C  i
i 1 i 0
XD XD
n n
1 depth .k / p depth .d / q ; (15.11)
D C T i  i C T i  i
i 1 i 0
XD XD
15.5 Optimalbinarysearchtrees 399
wheredepth denotes anode’s depth inthetreeT. Thelastequality followsfrom
T
equation(15.10). InFigure15.9(a),wecancalculatetheexpectedsearchcostnode
bynode:
node depth probability contribution
k1 1 0.15 0.30
k2 0 0.10 0.10
k3 2 0.05 0.15
k4 1 0.10 0.20
k5 2 0.20 0.60
d0 2 0.05 0.15
d1 2 0.10 0.30
d2 3 0.05 0.20
d3 3 0.05 0.20
d4 3 0.05 0.20
d5 3 0.10 0.40
Total 2.80
Foragivensetofprobabilities, wewishtoconstruct abinarysearch treewhose
expectedsearchcostissmallest. Wecallsuchatreeanoptimalbinarysearchtree.
Figure 15.9(b) shows an optimal binary search tree for the probabilities given in
the figure caption; its expected cost is 2.75. This example shows that an optimal
binary search tree is not necessarily a tree whose overall height is smallest. Nor
can we necessarily construct an optimal binary search tree by always putting the
key with the greatest probability at the root. Here, key k has the greatest search
5
probability of any key, yet the root of the optimal binary search tree shown is k .
2
(Thelowestexpectedcostofanybinarysearchtreewithk attherootis2.85.)
5
Aswithmatrix-chainmultiplication,exhaustivecheckingofallpossibilitiesfails
to yield an efficient algorithm. We can label the nodes of any n-node binary tree
with the keys k ;k ;:::;k to construct a binary search tree, and then add in the
1 2 n
dummy keys as leaves. In Problem 12-4, we saw that the number of binary trees
with n nodes is .4n=n3=2/, and so we would have to examine an exponential
number of binary search trees in an exhaustive search. Not surprisingly, we shall
solvethisproblem withdynamicprogramming.
Step1: Thestructureofanoptimalbinarysearch tree
To characterize the optimal substructure of optimal binary search trees, we start
with an observation about subtrees. Consider any subtree of a binary search tree.
It must contain keys in a contiguous range k ;:::;k , for some 1 i j n.
i j
  
Inaddition, asubtree thatcontains keysk ;:::;k mustalso haveasitsleavesthe
i j
dummykeysd ;:::;d .
i 1 j
Now we can(cid:0) state the optimal substructure: if an optimal binary search tree T
hasasubtreeT containingkeysk ;:::;k ,thenthissubtreeT mustbeoptimalas
0 i j 0
400 Chapter15 DynamicProgramming
well for the subproblem with keys k ;:::;k and dummy keys d ;:::;d . The
i j i 1 j
usual cut-and-paste argument applies. If there were a subtree T w(cid:0) hose expected
00
cost is lower than that of T , then we could cut T out of T and paste in T ,
0 0 00
resulting in a binary search tree of lower expected cost than T, thus contradicting
theoptimality ofT.
We need to use the optimal substructure to show that wecan construct an opti-
mal solution to the problem from optimal solutions to subproblems. Given keys
k ;:::;k , one of these keys, say k (i r j), is the root of an optimal
i j r
 
subtree containing these keys. The left subtree of the root k contains the keys
r
k ;:::;k (and dummy keys d ;:::;d ), and the right subtree contains the
i r 1 i 1 r 1
keysk (cid:0);:::;k (anddummykey(cid:0) sd ;:::;(cid:0)d ). Aslongasweexamineallcandi-
r 1 j r j
date rooC ts k , where i r j, and wedetermine all optimal binary search trees
r
 
containing k ;:::;k and those containing k ;:::;k , weare guaranteed that
i r 1 r 1 j
(cid:0) C
wewillfindanoptimalbinarysearchtree.
There is one detail worth noting about “empty” subtrees. Suppose that in a
subtree withkeysk ;:::;k ,weselectk astheroot. Bytheaboveargument, k ’s
i j i i
leftsubtreecontainsthekeysk ;:::;k . Weinterpretthissequenceascontaining
i i 1
(cid:0)
nokeys. Bearinmind,however,thatsubtreesalsocontaindummykeys. Weadopt
the convention that a subtree containing keys k ;:::;k has no actual keys but
i i 1
doescontainthesingledummykeyd . Symmetrically,(cid:0) ifweselectk astheroot,
i 1 j
then k ’s right subtree contains the k(cid:0) eys k ;:::;k ; this right subtree contains
j j 1 j
noactualkeys,butitdoescontain thedummC ykeyd .
j
Step2: Arecursive solution
We are ready to define the value of an optimal solution recursively. We pick our
subproblem domain as finding an optimal binary search tree containing the keys
k ;:::;k , where i 1, j n, and j i 1. (When j i 1, there
i j
   (cid:0) D (cid:0)
are no actual keys; we have just the dummy key d .) Let us define eŒi;j as
i 1
(cid:0)
the expected cost of searching an optimal binary search tree containing the keys
k ;:::;k . Ultimately,wewishtocomputeeŒ1;n.
i j
Theeasycaseoccurs whenj i 1. Thenwehavejustthedummykeyd .
i 1
Theexpected searchcostiseŒi;iD 1(cid:0)  q . (cid:0)
i 1
Whenj i,weneedtoselecta(cid:0) rootD k fr(cid:0) omamongk ;:::;k andthenmakean
r i j

optimalbinary searchtreewithkeysk ;:::;k asitsleftsubtree andanoptimal
i r 1
binary searchtreewithkeysk ;:::;k asits(cid:0) rightsubtree. Whathappens tothe
r 1 j
C
expected search costofasubtree whenitbecomes asubtree ofanode? Thedepth
ofeachnodeinthesubtreeincreasesby1. Byequation(15.11),theexpectedsearch
costofthissubtree increases bythesumofalltheprobabilities inthesubtree. For
asubtree withkeysk ;:::;k ,letusdenotethissumofprobabilities as
i j
15.5 Optimalbinarysearchtrees 401
j j
w.i;j/ p q : (15.12)
l l
D C
l i l i 1
XD DX(cid:0)
Thus,ifk istherootofanoptimalsubtreecontaining keysk ;:::;k ,wehave
r i j
eŒi;j p .eŒi;r 1 w.i;r 1// .eŒr 1;j w.r 1;j//:
r
D C (cid:0) C (cid:0) C C C C
Notingthat
w.i;j/ w.i;r 1/ p w.r 1;j/;
r
D (cid:0) C C C
werewriteeŒi;jas
eŒi;j eŒi;r 1 eŒr 1;j w.i;j/: (15.13)
D (cid:0) C C C
The recursive equation (15.13) assumes that we know which node k to use as
r
the root. We choose the root that gives the lowest expected search cost, giving us
ourfinalrecursiveformulation:
q ifj i 1;
i 1
eŒi;j (cid:0) D (cid:0) (15.14)
D min eŒi;r 1 eŒr 1;j w.i;j/ ifi j :
( i r jf (cid:0) C C C g 
 
TheeŒi;j values give theexpected search costs inoptimal binary search trees.
To help us keep track of the structure of optimal binary search trees, we define
rootŒi;j, for 1 i j n, to be the index r for which k is the root of an
r
  
optimal binary search tree containing keys k ;:::;k . Although we will see how
i j
tocompute thevaluesofrootŒi;j,weleave theconstruction ofanoptimalbinary
searchtreefromthesevaluesasExercise15.5-1.
Step3: Computingtheexpectedsearch costofanoptimalbinarysearch tree
Atthispoint,youmayhavenoticedsomesimilaritiesbetweenourcharacterizations
of optimal binary search trees and matrix-chain multiplication. For both problem
domains, oursubproblems consist ofcontiguous index subranges. Adirect, recur-
sive implementation of equation (15.14) would be as inefficient as adirect, recur-
sivematrix-chainmultiplication algorithm. Instead,westoretheeŒi;jvaluesina
tableeŒ1::n 1;0::n. Thefirstindexneedstorunton 1ratherthannbecause
C C
inordertohaveasubtree containing onlythedummykeyd ,weneedtocompute
n
and store eŒn 1;n. The second index needs to start from 0 because in order to
C
have a subtree containing only the dummy key d , we need to compute and store
0
eŒ1;0. We use only the entries eŒi;j for which j i 1. We also use a table
 (cid:0)
rootŒi;j, for recording the root of the subtree containing keys k ;:::;k . This
i j
tableusesonlytheentriesforwhich1 i j n.
  
We will need one other table for efficiency. Rather than compute the value
of w.i;j/ from scratch every time we are computing eŒi;j—which would take
402 Chapter15 DynamicProgramming
‚.j i/ additions—we store these values in a table wŒ1::n 1;0::n. Forthe
(cid:0) C
base case, we compute wŒi;i 1 q for 1 i n 1. For j i, we
i 1
(cid:0) D (cid:0)   C 
compute
wŒi;j wŒi;j 1 p q : (15.15)
j j
D (cid:0) C C
Thus,wecancomputethe‚.n2/valuesofwŒi;jin‚.1/timeeach.
The pseudocode that follows takes as inputs the probabilities p ;:::;p and
1 n
q ;:::;q andthesizen,anditreturns thetablese androot.
0 n
OPTIMAL-BST.p;q;n/
1 leteŒ1::n 1;0::n,wŒ1::n 1;0::n,
C C
androotŒ1::n;1::nbenewtables
2 fori 1ton 1
D C
3 eŒi;i 1 q
i 1
4
wŒi;i(cid:0) 1D q(cid:0)
i 1
5 forl 1t(cid:0) on D (cid:0)
D
6 fori 1ton l 1
D (cid:0) C
7 j i l 1
D C (cid:0)
8 eŒi;j
D 1
9 wŒi;j wŒi;j 1 p q
j j
D (cid:0) C C
10 forr i toj
D
11 t eŒi;r 1 eŒr 1;j wŒi;j
D (cid:0) C C C
12 ift < eŒi;j
13 eŒi;j t
D
14 rootŒi;j r
D
15 returne androot
FromthedescriptionaboveandthesimilaritytotheMATRIX-CHAIN-ORDER pro-
cedure inSection15.2,youshouldfindtheoperation ofthisprocedure tobefairly
straightforward. The for loop of lines 2–4 initializes the values of eŒi;i 1
(cid:0)
and wŒi;i 1. The for loop of lines 5–14 then uses the recurrences (15.14)
(cid:0)
and(15.15)tocomputeeŒi;jandwŒi;jforall1 i j n. Inthefirstitera-
  
tion,whenl 1,theloopcomputeseŒi;iandwŒi;ifori 1;2;:::;n. Thesec-
D D
onditeration,withl 2,computeseŒi;i 1andwŒi;i 1fori 1;2;:::;n 1,
D C C D (cid:0)
and so forth. Theinnermost for loop, in lines 10–14, tries each candidate index r
todetermine whichkey k touse asthe root ofanoptimal binary search tree con-
r
taining keys k ;:::;k . This for loop saves the current value of the index r in
i j
rootŒi;jwheneveritfindsabetterkeytouseastheroot.
Figure 15.10 shows the tables eŒi;j, wŒi;j, and rootŒi;j computed by the
procedure OPTIMAL-BST onthekeydistribution showninFigure15.9. Asinthe
matrix-chain multiplication exampleofFigure15.5, thetablesarerotated tomake
15.5 Optimalbinarysearchtrees 403
e w
5 1 5 1
j 4 2.75 2 i j 4 1.00 2 i
1.75 2.00 0.70 0.80
3 3 3 3
1.25 1.20 1.30 0.55 0.50 0.60
2 4 2 4
0.90 0.70 0.60 0.90 0.45 0.35 0.30 0.50
1 5 1 5
0.45 0.40 0.25 0.30 0.50 0.30 0.25 0.15 0.20 0.35
0 6 0 6
0.05 0.10 0.05 0.05 0.05 0.10 0.05 0.10 0.05 0.05 0.05 0.10
root
5 1
j 4 2 2 i
2 4
3 3
2 2 5
2 4
1 2 4 5
1 5
1 2 3 4 5
Figure15.10 The tables eŒi;j, wŒi;j, and rootŒi;j computed by OPTIMAL-BSTon the key
distributionshowninFigure15.9.Thetablesarerotatedsothatthediagonalsrunhorizontally.
thediagonals runhorizontally. OPTIMAL-BST computestherowsfrombottomto
topandfromlefttorightwithineachrow.
The OPTIMAL-BST procedure takes ‚.n3/ time, just like MATRIX-CHAIN-
ORDER. We can easily see that its running time is O.n3/, since its for loops are
nestedthreedeepandeachloopindextakesonatmostnvalues. Theloopindicesin
OPTIMAL-BST donothaveexactlythesameboundsasthoseinMATRIX-CHAIN-
ORDER,buttheyarewithinatmost1inalldirections. Thus,likeMATRIX-CHAIN-
ORDER,the OPTIMAL-BST procedure takes.n3/time.
Exercises
15.5-1
Write pseudocode for the procedure CONSTRUCT-OPTIMAL-BST.root/ which,
giventhetableroot,outputs thestructure ofanoptimalbinary search tree. Forthe
exampleinFigure15.10,yourprocedure shouldprintoutthestructure
404 Chapter15 DynamicProgramming
k istheroot
2
k istheleftchildofk
1 2
d istheleftchildofk
0 1
d istherightchildofk
1 1
k istherightchildofk
5 2
k istheleftchildofk
4 5
k istheleftchildofk
3 4
d istheleftchildofk
2 3
d istherightchildofk
3 3
d istherightchildofk
4 4
d istherightchildofk
5 5
corresponding totheoptimalbinarysearchtreeshowninFigure15.9(b).
15.5-2
Determinethecostandstructureofanoptimalbinarysearchtreeforasetofn 7
D
keyswiththefollowingprobabilities:
i 0 1 2 3 4 5 6 7
p i 0.04 0.06 0.08 0.02 0.10 0.12 0.14
q i 0.06 0.06 0.06 0.06 0.05 0.05 0.05 0.05
15.5-3
Suppose that instead of maintaining the table wŒi;j, we computed the value
ofw.i;j/directlyfromequation(15.12)inline9ofOPTIMAL-BST andusedthis
computed value in line 11. How would this change affect the asymptotic running
timeofOPTIMAL-BST?
15.5-4 ?
Knuth [212] has shown that there are always roots of optimal subtrees such that
rootŒi;j 1 rootŒi;j rootŒi 1;j forall1 i < j n. Usethisfactto
(cid:0)   C  
modifytheOPTIMAL-BST procedure torunin‚.n2/time.
Problems
15-1 Longestsimplepathinadirected acyclicgraph
Suppose that we are given a directed acyclic graph G .V;E/ with real-
D
valued edge weights and two distinguished vertices s and t. Describe a dynamic-
programming approach for finding a longest weighted simple path from s to t.
What does the subproblem graph look like? What is the efficiency of your algo-
rithm?
ProblemsforChapter15 405
(a) (b)
Figure15.11 Sevenpointsintheplane, shownonaunitgrid. (a)Theshortestclosedtour, with
lengthapproximately24:89.Thistourisnotbitonic.(b)Theshortestbitonictourforthesamesetof
points.Itslengthisapproximately25:58.
15-2 Longestpalindromesubsequence
A palindrome is a nonempty string over some alphabet that reads the same for-
wardand backward. Examples ofpalindromes areall strings of length 1, civic,
racecar,andaibohphobia(fearofpalindromes).
Giveanefficient algorithm to findthe longest palindrome that isasubsequence
ofagiveninputstring. Forexample,giventheinputcharacter,youralgorithm
shouldreturncarac. Whatistherunning timeofyouralgorithm?
15-3 Bitoniceuclideantraveling-salesman problem
In the euclidean traveling-salesman problem, we are given a set of n points in
the plane, and we wish to find the shortest closed tour that connects all n points.
Figure 15.11(a) shows the solution to a 7-point problem. The general problem is
NP-hard, and its solution is therefore believed to require more than polynomial
time(seeChapter34).
J. L. Bentley has suggested that we simplify the problem by restricting our at-
tention to bitonic tours, that is, tours that start at the leftmost point, go strictly
rightward to the rightmost point, and then go strictly leftward back to the starting
point. Figure15.11(b) showstheshortest bitonic tourofthesame7points. Inthis
case,apolynomial-time algorithm ispossible.
DescribeanO.n2/-timealgorithm fordetermining anoptimalbitonictour. You
mayassumethatnotwopoints havethesamex-coordinate andthatalloperations
onreal numbers takeunittime. (Hint:Scanlefttoright, maintaining optimal pos-
sibilities forthetwopartsofthetour.)
15-4 Printingneatly
Consider the problem of neatly printing a paragraph with a monospaced font (all
characters having the same width) on a printer. The input text is a sequence of n
406 Chapter15 DynamicProgramming
words oflengths l ;l ;:::;l ,measured incharacters. Wewanttoprint thispara-
1 2 n
graphneatlyonanumberoflinesthatholdamaximumofM characterseach. Our
criterion of “neatness” is as follows. If a given line contains words i through j,
wherei j,andweleave exactly onespace between words, thenumber ofextra
space ch aracters at the end of the line is M j i j l , which must be
(cid:0) C (cid:0) k i k
nonnegative so that the words fit on the line. We wish to minDimize the sum, over
P
alllinesexceptthelast,ofthecubesofthenumbersofextraspacecharactersatthe
ends of lines. Give a dynamic-programming algorithm to print a paragraph of n
words neatly on a printer. Analyze the running time and space requirements of
youralgorithm.
15-5 Editdistance
In order to transform one source string of textxŒ1::mto atarget string yŒ1::n,
we can perform various transformation operations. Our goal is, given x and y,
to produce a series of transformations that change x to y. We use an ar-
ray´—assumedtobelargeenough toholdallthecharacters itwillneed—tohold
the intermediate results. Initially, ´ is empty, and at termination, we should have
´Œj yŒjforj 1;2;:::;n. Wemaintaincurrentindicesi intoxandj into´,
D D
and the operations are allowed to alter ´ and these indices. Initially, i j 1.
D D
Wearerequired to examine everycharacter inx during thetransformation, which
means that at the end of the sequence of transformation operations, we must have
i m 1.
D C
Wemaychoosefromamongsixtransformation operations:
Copyacharacterfromx to´bysetting´Œj xŒiandthenincrementing bothi
D
andj. Thisoperation examinesxŒi.
Replaceacharacter from x byanother character c,by setting ´Œj c,and then
D
incrementing bothi andj. Thisoperation examinesxŒi.
Deletea character from x by incrementing i but leaving j alone. This operation
examinesxŒi.
Insertthe character c into ´ by setting ´Œj c and then incrementing j, but
D
leavingi alone. Thisoperation examinesnocharacters ofx.
Twiddle(i.e., exchange) the nexttwocharacters bycopying them from x to´but
intheoppositeorder;wedosobysetting´Œj xŒi 1and´Œj 1 xŒi
D C C D
and then setting i i 2 and j j 2. This operation examines xŒi
D C D C
andxŒi 1.
C
Killthe remainder of x by setting i m 1. This operation examines all char-
D C
actersinx thathavenotyetbeenexamined. Thisoperation, ifperformed,must
bethefinaloperation.
ProblemsforChapter15 407
Asanexample,onewaytotransformthesourcestringalgorithmtothetarget
string altruistic is to use the following sequence of operations, where the
underlined characters arexŒiand´Œjaftertheoperation:
Operation x ´
initialstrings algorithm
copy algorithm a
copy algorithm al
replacebyt algorithm alt
delete algorithm alt
copy algorithm altr
insertu algorithm altru
inserti algorithm altrui
inserts algorithm altruis
twiddle algorithm altruisti
insertc algorithm altruistic
kill algorithm altruistic
Notethatthereareseveralothersequences oftransformation operationsthattrans-
formalgorithmtoaltruistic.
Each of the transformation operations has an associated cost. The cost of an
operationdepends onthespecificapplication, butweassumethateachoperation’s
cost is aconstant that is known to us. Wealso assume that the individual costs of
thecopy and replace operations areless than thecombined costs ofthedelete and
insert operations; otherwise, the copy and replace operations would not be used.
The cost of a given sequence of transformation operations is the sum of the costs
of the individual operations in the sequence. For the sequence above, the cost of
transforming algorithmtoaltruisticis
.3 cost.copy// cost.replace/ cost.delete/ .4 cost.insert//
 C C C 
cost.twiddle/ cost.kill/:
C C
a. GiventwosequencesxŒ1::mandyŒ1::nandsetoftransformation-operation
costs, the edit distance from x toy is the cost ofthe least expensive operation
sequence that transforms x to y. Describe a dynamic-programming algorithm
that findsthe edit distance from xŒ1::mtoyŒ1::n andprints anoptimal op-
eration sequence. Analyze the running time and space requirements of your
algorithm.
Theedit-distanceproblemgeneralizestheproblemofaligningtwoDNAsequences
(see, for example, Setubal and Meidanis [310, Section 3.2]). There are several
methods for measuring the similarity of two DNA sequences by aligning them.
One such method to align two sequences x and y consists of inserting spaces at
408 Chapter15 DynamicProgramming
arbitrary locations inthetwosequences (including ateitherend)sothattheresult-
ing sequences x and y have the same length but donot have a space in the same
0 0
position(i.e.,fornopositionj arebothx Œjandy Œjaspace). Thenweassigna
0 0
“score” toeachposition. Positionj receivesascoreasfollows:
 1ifx Œj y Œjandneitherisaspace,
0 0
C D
 1ifx Œj y Œjandneitherisaspace,
0 0
(cid:0) ¤
 2ifeitherx Œjory Œjisaspace.
0 0
(cid:0)
Thescoreforthealignmentisthesumofthescoresoftheindividualpositions. For
example, given the sequences x GATCGGCATand y CAATGTGAATC, one
D D
alignment is
G ATCG GCAT
CAAT GTGAATC
-*++*+*+-++*
A+underapositionindicatesascoreof 1forthatposition,a-indicatesascore
C
of 1, and a * indicates a score of 2, so that this alignment has a total score of
(cid:0) (cid:0)
6 1 2 1 4 2 4.
 (cid:0)  (cid:0)  D(cid:0)
b. Explain how to cast the problem of finding an optimal alignment as an edit
distanceproblemusingasubsetofthetransformation operations copy,replace,
delete,insert, twiddle,andkill.
15-6 Planningacompanyparty
Professor Stewart is consulting for the president of a corporation that is planning
acompanyparty. Thecompany hasahierarchical structure; that is,thesupervisor
relation forms atree rooted atthepresident. Thepersonnel officehas ranked each
employee with a conviviality rating, which is a real number. In order to make the
party fun for all attendees, the president does not want both an employee and his
orherimmediatesupervisor toattend.
ProfessorStewartisgiventhetreethatdescribesthestructureofthecorporation,
using the left-child, right-sibling representation described in Section 10.4. Each
node of the tree holds, in addition to the pointers, the name of an employee and
that employee’s conviviality ranking. Describe an algorithm to make up a guest
list that maximizes the sum of the conviviality ratings of the guests. Analyze the
running timeofyouralgorithm.
15-7 Viterbialgorithm
We can use dynamic programming on a directed graph G .V;E/ for speech
D
recognition. Each edge .u;/ E is labeled with a sound .u;/ from a fi-
2
nite set † of sounds. The labeled graph is a formal model of a person speaking
ProblemsforChapter15 409
a restricted language. Each path in the graph starting from a distinguished ver-
tex  V corresponds to a possible sequence of sounds produced by the model.
0
2
We define the label of a directed path to be the concatenation of the labels of the
edgesonthatpath.
a. Describe an efficient algorithm that, given an edge-labeled graph G with dis-
tinguished vertex  and a sequence s  ; ;:::; of sounds from †,
0 1 2 k
D h i
returnsapathinGthatbeginsat andhassasitslabel,ifanysuchpathexists.
0
Otherwise, the algorithm should return NO-SUCH-PATH. Analyze the running
timeofyouralgorithm. (Hint:YoumayfindconceptsfromChapter22useful.)
Now, suppose that every edge .u;/ E has an associated nonnegative proba-
2
bility p.u;/ of traversing the edge .u;/ from vertex u and thus producing the
corresponding sound. Thesumoftheprobabilities oftheedgesleavinganyvertex
equals 1. The probability of a path is defined to be the product of the probabil-
ities of its edges. We can view the probability of a path beginning at  as the
0
probability that a “random walk” beginning at  will follow the specified path,
0
wherewerandomlychoosewhichedgetotakeleaving avertexuaccording tothe
probabilities oftheavailable edgesleavingu.
b. Extend your answer to part (a) so that if a path is returned, it is a most prob-
able path starting at  and having label s. Analyze the running time of your
0
algorithm.
15-8 Imagecompression byseamcarving
Wearegivenacolorpictureconsistingofanm narrayAŒ1::m;1::nofpixels,

where each pixel specifies atriple of red, green, and blue (RGB)intensities. Sup-
posethatwewishtocompressthispictureslightly. Specifically,wewishtoremove
one pixel from each of the m rows, so that the whole picture becomes one pixel
narrower. To avoid disturbing visual effects, however, we require that the pixels
removed in two adjacent rows be in the same or adjacent columns; the pixels re-
movedforma“seam”fromthetoprowtothebottom rowwheresuccessive pixels
intheseamareadjacent vertically ordiagonally.
a. Showthatthenumberofsuchpossibleseamsgrowsatleastexponentiallyinm,
assumingthatn >1.
b. Suppose now that along with each pixel AŒi;j, we have calculated a real-
valued disruption measure dŒi;j, indicating how disruptive it would be to
remove pixel AŒi;j. Intuitively, the lower a pixel’s disruption measure, the
more similar the pixel is to its neighbors. Suppose further that we define the
disruption measure of a seam to be the sum of the disruption measures of its
pixels.
410 Chapter15 DynamicProgramming
Give an algorithm to find a seam with the lowest disruption measure. How
efficientisyouralgorithm?
15-9 Breakingastring
A certain string-processing language allows a programmer to break a string into
two pieces. Because this operation copies the string, itcosts ntime units tobreak
a string of n characters into two pieces. Suppose a programmer wants to break
a string into many pieces. The order in which the breaks occur can affect the
total amount of time used. For example, suppose that the programmer wants to
break a20-character string aftercharacters 2,8,and 10(numbering thecharacters
in ascending order from the left-hand end, starting from 1). If she programs the
breaks to occur in left-to-right order, then the first break costs 20 time units, the
second break costs 18 time units (breaking the string from characters 3 to 20 at
character 8), and the third break costs 12 time units, totaling 50 time units. If she
programs the breaks to occur in right-to-left order, however, then the first break
costs 20timeunits, thesecond break costs10timeunits, andthethird breakcosts
8time units, totaling 38time units. Inyet another order, she could break firstat 8
(costing 20), then break the left piece at 2 (costing 8), and finally the right piece
at10(costing 12),foratotalcostof40.
Design analgorithm that, given thenumbers ofcharacters after whichtobreak,
determines a least-cost way to sequence those breaks. More formally, given a
stringS withncharactersandanarrayLŒ1::mcontainingthebreakpoints,com-
putethelowestcostforasequence ofbreaks,alongwithasequence ofbreaksthat
achievesthiscost.
15-10 Planninganinvestmentstrategy
Your knowledge of algorithms helps you obtain an exciting job with the Acme
Computer Company, along with a $10,000 signing bonus. You decide to invest
this money with the goal of maximizing your return at the end of 10 years. You
decidetousetheAmalgamatedInvestmentCompanytomanageyourinvestments.
Amalgamated Investments requires you toobserve the following rules. Itoffers n
differentinvestments,numbered1throughn. Ineachyearj,investmenti provides
areturnrateofr . Inotherwords,ifyouinvestd dollarsininvestmenti inyearj,
ij
then at the end of year j, you have dr dollars. The return rates are guaranteed,
ij
that is, you aregivenall thereturn rates forthe next10years for eachinvestment.
You make investment decisions only once per year. At the end of each year, you
can leave the money made in the previous year in the same investments, or you
can shift money to other investments, by either shifting money between existing
investments or moving money to a new investement. If you do not move your
moneybetweentwoconsecutive years,youpayafeeoff dollars, whereasifyou
1
switchyourmoney,youpayafeeoff dollars, wheref > f .
2 2 1
ProblemsforChapter15 411
a. Theproblem,asstated,allowsyoutoinvestyourmoneyinmultipleinvestments
in each year. Prove that there exists an optimal investment strategy that, in
each year, puts all the money into a single investment. (Recall that an optimal
investment strategy maximizes the amount of money after 10 years and is not
concerned withanyotherobjectives, suchasminimizingrisk.)
b. Prove that the problem of planning your optimal investment strategy exhibits
optimalsubstructure.
c. Design an algorithm that plans your optimal investment strategy. What is the
running timeofyouralgorithm?
d. SupposethatAmalgamatedInvestmentsimposedtheadditionalrestrictionthat,
atanypoint, youcanhave nomorethan$15,000 inanyoneinvestment. Show
that the problem of maximizing your income at the end of 10 years no longer
exhibits optimalsubstructure.
15-11 Inventoryplanning
The Rinky Dink Company makes machines that resurface ice rinks. The demand
for such products varies from month to month, and so the company needs to de-
velop a strategy to plan its manufacturing given the fluctuating, but predictable,
demand. The company wishes to design a plan for the next n months. For each
month i, the company knows the demand d , that is, the number of machines that
i
it will sell. Let D n d be the total demand over the next n months. The
D i 1 i
company keeps a full-timDe staff who provide labor to manufacture up to m ma-
P
chinespermonth. Ifthecompanyneedstomakemorethanmmachinesinagiven
month, it can hire additional, part-time labor, at a cost that works out to c dollars
per machine. Furthermore, if, at the end of a month, the company is holding any
unsold machines, it must pay inventory costs. The cost for holding j machines is
givenasafunction h.j/forj 1;2;:::;D,whereh.j/ 0for1 j D and
D   
h.j/ h.j 1/for1 j D 1.
 C   (cid:0)
Giveanalgorithmthatcalculatesaplanforthecompanythatminimizesitscosts
whilefulfillingallthedemand. TherunningtimeshouldbepolynomialinnandD.
15-12 Signingfree-agentbaseball players
Supposethatyouarethegeneralmanagerforamajor-leaguebaseballteam. During
the off-season, you need to sign somefree-agent players for your team. The team
owner has given you a budget of $X to spend on free agents. You are allowed to
spend less than $X altogether, but the owner will fire you if you spend any more
than$X.
412 Chapter15 DynamicProgramming
You are considering N different positions, and for each position, P free-agent
playerswhoplaythatpositionareavailable.8 Becauseyoudonotwanttooverload
your roster with too many players at any position, for each position you may sign
atmostonefreeagentwhoplaysthatposition. (Ifyoudonotsignanyplayersata
particular position, thenyouplantostickwiththeplayersyoualreadyhaveatthat
position.)
Todeterminehowvaluableaplayerisgoingtobe,youdecidetouseasabermet-
ricstatistic9 knownas“VORP,”or“value overreplacement player.” Aplayerwith
ahigherVORPismorevaluablethanaplayerwithalowerVORP. Aplayerwitha
higher VORPisnotnecessarily moreexpensive tosignthan aplayer withalower
VORP,because factors other than aplayer’s valuedetermine how muchitcosts to
signhim.
Foreachavailable free-agent player, youhavethreepiecesofinformation:
 theplayer’sposition,
 theamountofmoneyitwillcosttosigntheplayer, and
 theplayer’sVORP.
DeviseanalgorithmthatmaximizesthetotalVORPoftheplayersyousignwhile
spendingnomorethan$X altogether. Youmayassumethateachplayersignsfora
multipleof$100,000. Youralgorithm should outputthetotalVORPoftheplayers
you sign, the total amount of money you spend, and a list of which players you
sign. Analyzetherunning timeandspacerequirement ofyouralgorithm.
Chapter notes
R. Bellman began the systematic study of dynamic programming in 1955. The
word “programming,” both here and in linear programming, refers to using a tab-
ularsolutionmethod. Althoughoptimizationtechniquesincorporating elementsof
dynamicprogrammingwereknownearlier,Bellmanprovidedtheareawithasolid
mathematical basis[37].
8Althoughthereareninepositionsonabaseballteam,N isnotnecesarilyequalto9becausesome
generalmanagershaveparticularwaysofthinkingaboutpositions. Forexample,ageneralmanager
mightconsiderright-handedpitchersandleft-handedpitcherstobeseparate“positions,”aswellas
startingpitchers,longreliefpitchers(reliefpitcherswhocanpitchseveralinnings),andshortrelief
pitchers(reliefpitcherswhonormallypitchatmostonlyoneinning).
9Sabermetricsistheapplicationofstatisticalanalysistobaseballrecords. Itprovidesseveralways
tocomparetherelativevaluesofindividualplayers.
NotesforChapter15 413
GalilandPark[125]classifydynamic-programmingalgorithmsaccordingtothe
sizeofthetableandthenumberofothertableentrieseachentrydependson. They
call a dynamic-programming algorithm tD=eD if its table size is O.nt/ and each
entrydependsonO.ne/otherentries. Forexample,thematrix-chainmultiplication
algorithminSection15.2wouldbe2D=1D,andthelongest-common-subsequence
algorithm inSection15.4wouldbe2D=0D.
Huand Shing [182,183]give anO.nlgn/-time algorithm for the matrix-chain
multiplication problem.
The O.mn/-time algorithm for the longest-common-subsequence problem ap-
pearstobeafolkalgorithm. Knuth[70]posedthequestionofwhethersubquadratic
algorithms for the LCS problem exist. Masek and Paterson [244] answered this
question in the affirmative by giving an algorithm that runs in O.mn=lgn/ time,
where n m and the sequences are drawn from a set of bounded size. For the

special case in which no element appears more than once in an input sequence,
Szymanski[326]showshowtosolvetheproblem inO..n m/lg.n m//time.
C C
Many of these results extend to the problem of computing string edit distances
(Problem15-5).
Anearlypaperonvariable-length binaryencodings byGilbertandMoore[133]
hadapplicationstoconstructingoptimalbinarysearchtreesforthecaseinwhichall
probabilitiesp are0;thispapercontainsanO.n3/-timealgorithm. Aho,Hopcroft,
i
andUllman[5]present thealgorithm from Section15.5. Exercise15.5-4isdueto
Knuth [212]. Hu and Tucker [184] devised an algorithm for the case in which all
probabilities p are 0 that uses O.n2/ time and O.n/ space; subsequently, Knuth
i
[211]reduced thetimetoO.nlgn/.
Problem 15-8 isdue toAvidan andShamir[27], whohave posted onthe Weba
wonderfulvideoillustrating thisimage-compression technique.
16 Greedy Algorithms
Algorithms for optimization problems typically go through a sequence of steps,
withasetofchoicesateachstep. Formanyoptimizationproblems,usingdynamic
programming to determine the best choices is overkill; simpler, more efficient al-
gorithms will do. A greedy algorithm always makes the choice that looks best at
themoment. Thatis,itmakesalocallyoptimal choiceinthehopethatthischoice
will lead to a globally optimal solution. This chapter explores optimization prob-
lems for which greedy algorithms provide optimal solutions. Before reading this
chapter, you should read about dynamic programming in Chapter 15, particularly
Section15.3.
Greedyalgorithmsdonotalwaysyieldoptimalsolutions,butformanyproblems
they do. We shall first examine, in Section 16.1, a simple but nontrivial problem,
the activity-selection problem, for which a greedy algorithm efficiently computes
an optimal solution. We shall arrive at the greedy algorithm by first consider-
ingadynamic-programming approach andthenshowingthatwecanalwaysmake
greedy choices to arrive at an optimal solution. Section 16.2 reviews the basic
elements of the greedy approach, giving a direct approach for proving greedy al-
gorithms correct. Section 16.3 presents an important application of greedy tech-
niques: designing data-compression (Huffman) codes. In Section 16.4, we inves-
tigate some of the theory underlying combinatorial structures called “matroids,”
for which a greedy algorithm always produces an optimal solution. Finally, Sec-
tion 16.5 applies matroids to solve a problem of scheduling unit-time tasks with
deadlines andpenalties.
The greedy method is quite powerful and works well for a wide range of prob-
lems. Later chapters will present many algorithms that we can view as applica-
tions of the greedy method, including minimum-spanning-tree algorithms (Chap-
ter 23), Dijkstra’s algorithm for shortest paths from a single source (Chapter 24),
andChva´tal’sgreedyset-coveringheuristic(Chapter35). Minimum-spanning-tree
algorithmsfurnishaclassicexampleofthegreedymethod. Althoughyoucanread
16.1 Anactivity-selectionproblem 415
this chapter and Chapter 23 independently of each other, you might find it useful
toreadthemtogether.
16.1 Anactivity-selectionproblem
Ourfirstexampleistheproblemofschedulingseveralcompetingactivitiesthatre-
quireexclusiveuseofacommonresource,withagoalofselectingamaximum-size
setofmutually compatible activities. SupposewehaveasetS a ;a ;:::;a
1 2 n
D f g
of n proposed activities that wish to use a resource, such as a lecture hall, which
canserveonlyoneactivityatatime. Eachactivitya hasastarttimes andafinish
i i
time f , where 0 s < f < . If selected, activity a takes place during the
i i i i
 1
half-open timeinterval Œs ;f /. Activities a anda arecompatible iftheintervals
i i i j
Œs ;f / and Œs ;f / do not overlap. That is, a and a are compatible if s f
i i j j i j i j

or s f . In the activity-selection problem, we wish to select a maximum-size
j i

subset of mutually compatible activities. We assume that the activities are sorted
inmonotonically increasing orderoffinishtime:
f f f f f : (16.1)
1 2 3 n 1 n
     (cid:0) 
(We shall see later the advantage that this assumption provides.) For example,
considerthefollowingsetS ofactivities:
i 1 2 3 4 5 6 7 8 9 10 11
s i 1 3 0 5 3 5 6 8 8 2 12
f i 4 5 6 7 9 9 10 11 12 14 16
Forthisexample,thesubset a ;a ;a consistsofmutuallycompatibleactivities.
3 9 11
f g
Itisnotamaximumsubset, however,sincethesubset a ;a ;a ;a islarger. In
1 4 8 11
f g
fact, a ;a ;a ;a is a largest subset of mutually compatible activities; another
1 4 8 11
f g
largestsubsetis a ;a ;a ;a .
2 4 9 11
f g
We shall solve this problem in several steps. We start by thinking about a
dynamic-programming solution,inwhichweconsiderseveralchoiceswhendeter-
miningwhichsubproblemstouseinanoptimalsolution. Weshallthenobservethat
weneedtoconsideronlyonechoice—thegreedychoice—andthatwhenwemake
thegreedychoice,onlyonesubproblem remains. Basedontheseobservations, we
shall develop a recursive greedy algorithm to solve the activity-scheduling prob-
lem. Weshallcomplete theprocess ofdeveloping agreedy solution byconverting
therecursivealgorithm toaniterativeone. Althoughthestepsweshallgothrough
inthissectionareslightly moreinvolved thanistypicalwhendeveloping agreedy
algorithm, theyillustrate therelationship between greedy algorithms anddynamic
programming.
416 Chapter16 GreedyAlgorithms
Theoptimalsubstructureoftheactivity-selection problem
Wecaneasily verify that theactivity-selection problem exhibits optimal substruc-
ture. LetusdenotebyS thesetofactivitiesthatstartafteractivitya finishesand
ij i
thatfinishbeforeactivitya starts. Supposethatwewishtofindamaximumsetof
j
mutuallycompatibleactivitiesinS ,andsupposefurtherthatsuchamaximumset
ij
isA ,whichincludessomeactivitya . Byincludinga inanoptimalsolution,we
ij k k
areleftwithtwosubproblems: findingmutuallycompatibleactivitiesinthesetS
ik
(activitiesthatstartafteractivitya finishesandthatfinishbeforeactivitya starts)
i k
and finding mutually compatible activities in the set S (activities that start after
kj
activity a finishes and that finish before activity a starts). Let A A S
k j ik ij ik
D \
andA A S ,sothatA containstheactivitiesinA thatfinishbeforea
kj ij kj ik ij k
D \
starts and A contains the activities in A that start after a finishes. Thus, we
kj ij k
haveA A a A ,andsothemaximum-sizesetA ofmutuallycom-
ij ik k kj ij
D [f g[
patibleactivities inS consistsof A A A 1activities.
ij ij ik kj
j j D j jCj jC
Theusualcut-and-paste argumentshowsthattheoptimalsolutionA mustalso
ij
include optimal solutions to the two subproblems for S and S . If we could
ik kj
find a set A of mutually compatible activities in S where A > A , then
0kj kj
j
0kjj
j
kj
j
we could use A , rather than A , in a solution to the subproblem for S . We
0kj kj ij
would have constructed a set of A A 1 > A A 1 A
j
ik
jCj
0kjjC
j
ik
jCj
kj
jC D j
ij
j
mutually compatible activities, which contradicts the assumption that A is an
ij
optimalsolution. AsymmetricargumentappliestotheactivitiesinS .
ik
This way of characterizing optimal substructure suggests that we might solve
the activity-selection problem by dynamic programming. If we denote the size of
anoptimalsolution forthesetS bycŒi;j,thenwewouldhavetherecurrence
ij
cŒi;j cŒi;k cŒk;j 1:
D C C
Of course, if we did not know that an optimal solution for the set S includes
ij
activity a , we would have to examine all activities in S to find which one to
k ij
choose, sothat
0 ifS ;
ij
cŒi;j D; (16.2)
D ( am
k
a Sx
ij
fcŒi;k CcŒk;j C1 g ifS ij ¤;:
2
We could then develop a recursive algorithm and memoize it, or we could work
bottom-up and fill in table entries as we go along. But we would be overlooking
another important characteristic of the activity-selection problem that we can use
togreatadvantage.
16.1 Anactivity-selectionproblem 417
Makingthegreedychoice
Whatifwecould choose anactivity toaddtoouroptimal solution without having
to first solve all the subproblems? That could save us from having to consider all
thechoicesinherentinrecurrence(16.2). Infact,fortheactivity-selectionproblem,
weneedconsider onlyonechoice: thegreedy choice.
Whatdowemeanbythegreedychoicefortheactivity-selection problem? Intu-
ition suggests that we should choose an activity that leaves the resource available
for as many other activities as possible. Now, of the activities we end up choos-
ing, one of them must be the first one to finish. Our intuition tells us, therefore,
to choose the activity in S with the earliest finish time, since that would leave the
resource available for as many of the activities that follow it as possible. (If more
than one activity in S has the earliest finish time, then we can choose any such
activity.) Inotherwords,sincetheactivitiesaresortedinmonotonicallyincreasing
order by finish time, the greedy choice is activity a . Choosing the first activity
1
to finish is not the only way to think of making a greedy choice for this problem;
Exercise16.1-3asksyoutoexploreotherpossibilities.
Ifwemakethegreedychoice,wehaveonlyoneremainingsubproblemtosolve:
finding activities that start after a finishes. Why don’t we have to consider ac-
1
tivities that finish before a starts? We have that s < f , and f is the earliest
1 1 1 1
finishtimeofanyactivity,andthereforenoactivitycanhaveafinishtimelessthan
or equal to s . Thus, all activities that are compatible with activity a must start
1 1
aftera finishes.
1
Furthermore,wehavealreadyestablishedthattheactivity-selection problemex-
hibitsoptimalsubstructure. LetS a S s f bethesetofactivitiesthat
k i i k
D f 2 W  g
startafteractivitya finishes. Ifwemakethegreedychoiceofactivitya ,thenS
k 1 1
remains as the only subproblem to solve.1 Optimal substructure tells us that if a
1
isintheoptimalsolution, thenanoptimalsolutiontotheoriginalproblemconsists
ofactivitya andalltheactivities inanoptimalsolutiontothesubproblem S .
1 1
One big question remains: is our intuition correct? Is the greedy choice—in
whichwechoosethefirstactivitytofinish—alwayspartofsomeoptimalsolution?
Thefollowingtheoremshowsthatitis.
1WesometimesrefertothesetsS kassubproblemsratherthanasjustsetsofactivities.Itwillalways
beclear fromthe context whether wearereferringtoS k asaset of activitiesor as asubproblem
whoseinputisthatset.
418 Chapter16 GreedyAlgorithms
Theorem16.1
Consider any nonempty subproblem S , and let a be an activity in S with the
k m k
earliestfinishtime. Thena isincludedinsomemaximum-sizesubsetofmutually
m
compatible activities ofS .
k
Proof LetA beamaximum-sizesubsetofmutuallycompatibleactivitiesinS ,
k k
and let a be the activity in A with the earliest finish time. If a a , we are
j k j m
D
done, since we have shown that a is in some maximum-size subset of mutually
m
compatible activities ofS . Ifa a ,letthesetA A a a beA
k j
¤
m 0k
D
k
(cid:0)f
j
g[f
m
g
k
butsubstituting a fora . TheactivitiesinA aredisjoint, whichfollowsbecause
m j 0k
theactivitiesinA aredisjoint, a isthefirstactivityinA tofinish,andf f .
k j k m j

Since A A , we conclude that A is a maximum-size subset of mutually
j
0kj
D j
k
j
0k
compatible activities ofS ,anditincludesa .
k m
Thus,weseethatalthoughwemightbeabletosolvetheactivity-selection prob-
lem with dynamic programming, we don’t need to. (Besides, we have not yet
examined whether the activity-selection problem even has overlapping subprob-
lems.) Instead, we can repeatedly choose the activity that finishes first, keep only
the activities compatible with this activity, and repeat until no activities remain.
Moreover, because wealways choose the activity with the earliest finish time, the
finish times of the activities we choose must strictly increase. We can consider
eachactivityjustonceoverall,inmonotonically increasing orderoffinishtimes.
An algorithm to solve the activity-selection problem does not need to work
bottom-up, like a table-based dynamic-programming algorithm. Instead, it can
worktop-down,choosinganactivitytoputintotheoptimalsolutionandthensolv-
ingthesubproblemofchoosingactivitiesfromthosethatarecompatiblewiththose
already chosen. Greedy algorithms typically have this top-down design: make a
choiceandthensolveasubproblem,ratherthanthebottom-uptechniqueofsolving
subproblems beforemakingachoice.
Arecursivegreedyalgorithm
Nowthatwehaveseenhowtobypassthedynamic-programming approachandin-
stead use a top-down, greedy algorithm, wecan writea straightforward, recursive
procedure to solve the activity-selection problem. The procedure RECURSIVE-
ACTIVITY-SELECTOR takesthestartandfinishtimesoftheactivities, represented
as arrays s and f,2 the index k that defines the subproblem S it is to solve, and
k
2Becausethepseudocodetakess andf asarrays,itindexesintothemwithsquarebracketsrather
thansubscripts.
16.1 Anactivity-selectionproblem 419
thesizenoftheoriginalproblem. Itreturnsamaximum-sizesetofmutuallycom-
patible activities in S . We assume that the n input activities are already ordered
k
by monotonically increasing finish time, according to equation (16.1). If not, we
can sort them into this order in O.nlgn/ time, breaking ties arbitrarily. In order
to start, we add the fictitious activity a with f 0, so that subproblem S is
0 0 0
D
the entire set of activities S. The initial call, which solves the entire problem, is
RECURSIVE-ACTIVITY-SELECTOR.s;f;0;n/.
RECURSIVE-ACTIVITY-SELECTOR.s;f;k;n/
1 m k 1
D C
2 whilem nandsŒm <fŒk //findthefirstactivityinS tofinish
k

3 m m 1
D C
4 ifm n

5 return a
m
RECURSIVE-ACTIVITY-SELECTOR.s;f;m;n/
f g[
6 elsereturn
;
Figure 16.1 shows the operation of the algorithm. In a given recursive call
RECURSIVE-ACTIVITY-SELECTOR.s;f;k;n/, the while loop of lines 2–3 looks
for the first activity in S to finish. The loop examines a ;a ;:::;a , un-
k k 1 k 2 n
til it finds the first activity a that is compatible with a ;C suchC an activity has
m k
s f . If the loop terminates because it finds such an activity, line 5 returns
m k

the union of a and the maximum-size subset of S returned by the recursive
m m
f g
call RECURSIVE-ACTIVITY-SELECTOR.s;f;m;n/. Alternatively, the loop may
terminate because m > n, in which case we have examined all activities in S
k
without finding one that is compatible with a . In this case, S , and so the
k k
D ;
procedure returns inline6.
;
Assumingthattheactivitieshavealreadybeensortedbyfinishtimes,therunning
time of the call RECURSIVE-ACTIVITY-SELECTOR.s;f;0;n/ is ‚.n/, which we
canseeasfollows. Overallrecursive calls,eachactivityisexaminedexactlyonce
inthewhilelooptestofline2. Inparticular, activitya isexaminedinthelastcall
i
madeinwhichk < i.
Aniterative greedyalgorithm
We easily can convert our recursive procedure to an iterative one. The procedure
RECURSIVE-ACTIVITY-SELECTOR is almost “tail recursive” (see Problem 7-4):
itends with arecursive call to itself followed by aunion operation. It isusually a
straightforward tasktotransform atail-recursive procedure toaniterativeform;in
fact,somecompilersforcertainprogramminglanguagesperformthistaskautomat-
ically. Aswritten, RECURSIVE-ACTIVITY-SELECTOR worksforsubproblemsS k,
i.e.,subproblems thatconsistofthelastactivities tofinish.
420 Chapter16 GreedyAlgorithms
k s f
k k
0 – 0
a
0
a
1
1 1 4
a 0 m = 1
RECURSIVE-ACTIVITY-SELECTOR(s, f, 0, 11)
a
2
2 3 5
a
RECURSIVE-ACTIVITY-SELECTOR(s, f, 1, 11)
1
a
3
3 0 6
a
1
a
4
4 5 7
a 1 m = 4
RECURSIVE-ACTIVITY-SELECTOR(s, f, 4, 11)
a
5
5 3 9
a a
1 4
a
6
6 5 9
a a
1 4
a
7
7 6 10
a a
1 4
a
8
8 8 11
a 1 a 4 m = 8
RECURSIVE-ACTIVITY-SELECTOR(s, f, 8, 11) a 9
9 8 12
a a a
1 4 8
a
10
10 2 14
a a a
1 4 8
a
11
11 12 16
a 1 a 4 a 8 m = 11
RECURSIVE-ACTIVITY-SELECTOR(s, f, 11, 11)
a a a a
1 4 8 11
time
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Figure16.1 Theoperationof RECURSIVE-ACTIVITY-SELECTORonthe11activitiesgivenear-
lier. Activities considered in each recursive call appear between horizontal lines. The fictitious
activitya0finishesattime0,andtheinitialcallRECURSIVE-ACTIVITY-SELECTOR.s;f;0;11/,se-
lects activity a1. In each recursive call, the activities that have already been selected are shaded,
andtheactivityshowninwhiteisbeingconsidered. Ifthestartingtimeofanactivityoccursbefore
the finish time of the most recently added activity (the arrow between them points left), it is re-
jected. Otherwise(thearrowpointsdirectlyuportotheright),itisselected. Thelastrecursivecall,
RECURSIVE-ACTIVITY-SELECTOR.s;f;11;11/,returns .Theresultingsetofselectedactivitiesis
;
a1;a4;a8;a11 .
f g
16.1 Anactivity-selectionproblem 421
TheprocedureGREEDY-ACTIVITY-SELECTOR isaniterativeversionofthepro-
cedure RECURSIVE-ACTIVITY-SELECTOR. It also assumes that the input activi-
tiesareordered bymonotonically increasing finishtime. Itcollects selected activ-
itiesintoasetAandreturns thissetwhenitisdone.
GREEDY-ACTIVITY-SELECTOR.s;f/
1 n s:length
D
2 A a
1
D f g
3 k 1
D
4 form 2ton
D
5 ifsŒm fŒk

6 A A a
m
D [f g
7 k m
D
8 returnA
Theprocedureworksasfollows. Thevariablekindexesthemostrecentaddition
to A, corresponding to the activity a in the recursive version. Since we consider
k
the activities in order of monotonically increasing finish time, f is always the
k
maximumfinishtimeofanyactivity inA. Thatis,
f max f a A : (16.3)
k i i
D f W 2 g
Lines2–3selectactivitya ,initializeAtocontainjustthisactivity,andinitializek
1
to index this activity. The for loop of lines 4–7 finds the earliest activity in S to
k
finish. Theloopconsiderseachactivitya inturnandaddsa toAifitiscompat-
m m
ible with all previously selected activities; such an activity is the earliest in S to
k
finish. Toseewhetheractivitya iscompatible witheveryactivitycurrently inA,
m
itsufficesbyequation (16.3)tocheck(inline5)thatitsstarttimes isnotearlier
m
than the finish time f of the activity most recently added to A. If activity a is
k m
compatible, thenlines6–7addactivitya toAandsetk tom. ThesetAreturned
m
by the call GREEDY-ACTIVITY-SELECTOR.s;f/ is precisely the set returned by
thecall RECURSIVE-ACTIVITY-SELECTOR.s;f;0;n/.
Liketherecursiveversion, GREEDY-ACTIVITY-SELECTOR schedulesasetofn
activitiesin‚.n/time,assumingthattheactivitieswerealreadysortedinitiallyby
theirfinishtimes.
Exercises
16.1-1
Give a dynamic-programming algorithm for the activity-selection problem, based
on recurrence (16.2). Have your algorithm compute the sizes cŒi;j as defined
aboveandalsoproducethemaximum-sizesubsetofmutuallycompatibleactivities.
422 Chapter16 GreedyAlgorithms
Assumethattheinputshavebeensortedasinequation(16.1). Comparetherunning
timeofyoursolution totherunning timeof GREEDY-ACTIVITY-SELECTOR.
16.1-2
Supposethatinsteadofalwaysselectingthefirstactivitytofinish,weinsteadselect
thelastactivitytostartthatiscompatiblewithallpreviouslyselectedactivities. De-
scribe howthisapproach isagreedy algorithm, andprovethatityieldsanoptimal
solution.
16.1-3
Not just any greedy approach to the activity-selection problem produces a max-
imum-size set of mutually compatible activities. Give an example to show that
the approach of selecting the activity of least duration from among those that are
compatible with previously selected activities does not work. Do the same for
theapproaches ofalwaysselecting thecompatibleactivitythatoverlapsthefewest
other remaining activities and always selecting the compatible remaining activity
withtheearliest starttime.
16.1-4
Supposethatwehaveasetofactivitiestoscheduleamongalargenumberoflecture
halls, where any activity can take place in any lecture hall. We wish to schedule
all the activities using as few lecture halls as possible. Give an efficient greedy
algorithm todeterminewhichactivityshould usewhichlecturehall.
(This problem is also known as the interval-graph coloring problem. We can
create an interval graph whose vertices are the given activities and whose edges
connect incompatible activities. The smallest number of colors required to color
every vertex so that no two adjacent vertices have the same color corresponds to
findingthefewestlecturehallsneededtoschedule allofthegivenactivities.)
16.1-5
Consideramodificationtotheactivity-selection probleminwhicheachactivitya
i
has, in addition to a start and finish time, a value  . The objective is no longer
i
to maximize the number of activities scheduled, but instead to maximize the total
valueoftheactivities scheduled. Thatis,wewishtochoose asetAofcompatible
activities suchthat  ismaximized. Giveapolynomial-time algorithm for
ak A k
thisproblem. 2
P
16.2 Elementsofthegreedystrategy 423
16.2 Elements ofthe greedy strategy
Agreedyalgorithmobtainsanoptimalsolutiontoaproblembymakingasequence
ofchoices. Ateachdecisionpoint, thealgorithm makesthechoicethatseemsbest
atthemoment. Thisheuristicstrategydoesnotalwaysproduceanoptimalsolution,
but as we saw in the activity-selection problem, sometimes it does. This section
discusses someofthegeneralproperties ofgreedymethods.
TheprocessthatwefollowedinSection16.1todevelopagreedyalgorithmwas
abitmoreinvolved thanistypical. Wewentthrough thefollowingsteps:
1. Determinetheoptimalsubstructure oftheproblem.
2. Develop a recursive solution. (For the activity-selection problem, we formu-
latedrecurrence(16.2),butwebypasseddevelopingarecursivealgorithmbased
onthisrecurrence.)
3. Showthatifwemakethegreedy choice,thenonlyonesubproblem remains.
4. Provethatitisalwayssafetomakethegreedychoice. (Steps3and4canoccur
ineitherorder.)
5. Developarecursive algorithm thatimplementsthegreedystrategy.
6. Converttherecursivealgorithm toaniterativealgorithm.
Ingoingthroughthesesteps,wesawingreatdetailthedynamic-programming un-
derpinnings ofagreedyalgorithm. Forexample, intheactivity-selection problem,
we first defined the subproblems S , where both i and j varied. We then found
ij
thatifwealwaysmadethegreedy choice, wecould restrict thesubproblems tobe
oftheformS .
k
Alternatively, we could have fashioned our optimal substructure with a greedy
choice in mind, so that the choice leaves just one subproblem to solve. In the
activity-selection problem,wecouldhavestartedbydroppingthesecondsubscript
anddefiningsubproblemsoftheformS . Then,wecouldhaveproventhatagreedy
k
choice (the firstactivity a to finish in S ), combined with an optimal solution to
m k
the remaining set S of compatible activities, yields an optimal solution to S .
m k
Moregenerally, wedesigngreedy algorithms according tothefollowingsequence
ofsteps:
1. Cast the optimization problem as one in which we make a choice and are left
withonesubproblem tosolve.
2. Provethatthereisalwaysanoptimalsolutiontotheoriginalproblemthatmakes
thegreedychoice, sothatthegreedychoiceisalwayssafe.
424 Chapter16 GreedyAlgorithms
3. Demonstrate optimal substructure by showing that, having made the greedy
choice, what remains is asubproblem with the property that if we combine an
optimal solution to the subproblem with the greedy choice we have made, we
arriveatanoptimalsolution totheoriginal problem.
We shall use this more direct process in later sections of this chapter. Neverthe-
less, beneath every greedy algorithm, there is almost always a more cumbersome
dynamic-programming solution.
Howcanwetellwhetheragreedyalgorithmwillsolveaparticularoptimization
problem? No way works all the time, but the greedy-choice property and optimal
substructure are the two key ingredients. If we can demonstrate that the problem
hastheseproperties, thenwearewellonthewaytodevelopingagreedyalgorithm
forit.
Greedy-choice property
Thefirstkeyingredient isthegreedy-choice property: wecanassembleaglobally
optimalsolutionbymakinglocallyoptimal(greedy)choices. Inotherwords,when
we are considering which choice to make, we make the choice that looks best in
thecurrentproblem, withoutconsidering resultsfromsubproblems.
Hereiswheregreedyalgorithmsdifferfromdynamicprogramming. Indynamic
programming,wemakeachoiceateachstep,butthechoiceusuallydependsonthe
solutionstosubproblems. Consequently,wetypicallysolvedynamic-programming
problems in a bottom-up manner, progressing from smaller subproblems to larger
subproblems. (Alternatively, we can solve them top down, but memoizing. Of
course, even though the code works top down, we still must solve the subprob-
lems before making a choice.) In a greedy algorithm, we make whatever choice
seemsbestatthemomentandthensolvethesubproblem thatremains. Thechoice
madebyagreedyalgorithmmaydependonchoicessofar,butitcannotdependon
any future choices oron the solutions tosubproblems. Thus, unlike dynamic pro-
gramming, whichsolves thesubproblems before making thefirstchoice, agreedy
algorithm makes its first choice before solving any subproblems. A dynamic-
programming algorithm proceeds bottom up, whereas a greedy strategy usually
progresses in a top-down fashion, making one greedy choice after another, reduc-
ingeachgivenprobleminstance toasmallerone.
Of course, we must prove that a greedy choice at each step yields a globally
optimal solution. Typically, as in the case of Theorem 16.1, the proof examines
a globally optimal solution to some subproblem. It then shows how to modify
thesolution tosubstitute thegreedy choiceforsomeotherchoice, resulting inone
similar, butsmaller,subproblem.
We can usually make the greedy choice more efficiently than when we have to
considerawidersetofchoices. Forexample,intheactivity-selection problem,as-
16.2 Elementsofthegreedystrategy 425
sumingthatwehadalready sorted theactivities inmonotonically increasing order
offinishtimes,weneededtoexamineeachactivityjustonce. Bypreprocessingthe
input or by using an appropriate data structure (often a priority queue), we often
canmakegreedychoices quickly, thusyielding anefficientalgorithm.
Optimalsubstructure
A problem exhibits optimal substructure if an optimal solution to the problem
contains within it optimal solutions to subproblems. This property is a key in-
gredient of assessing the applicability ofdynamic programming aswell asgreedy
algorithms. Asanexampleofoptimalsubstructure, recallhowwedemonstratedin
Section 16.1 that ifan optimal solution tosubproblem S includes anactivity a ,
ij k
thenitmustalsocontain optimal solutions tothesubproblems S andS . Given
ik kj
thisoptimalsubstructure,wearguedthatifweknewwhichactivitytouseasa ,we
k
could construct an optimal solution to S by selecting a along with all activities
ij k
inoptimal solutions tothesubproblems S andS . Based onthisobservation of
ik kj
optimal substructure, we were able to devise the recurrence (16.2) that described
thevalueofanoptimalsolution.
We usually use a more direct approach regarding optimal substructure when
applying it to greedy algorithms. As mentioned above, we have the luxury of
assuming that we arrived at a subproblem by having made the greedy choice in
the original problem. Allwereally need todo isargue that anoptimal solution to
thesubproblem, combinedwiththegreedychoicealreadymade,yieldsanoptimal
solution to the original problem. This scheme implicitly uses induction on the
subproblems to prove that making the greedy choice at every step produces an
optimalsolution.
Greedyversusdynamicprogramming
Becauseboththegreedyanddynamic-programmingstrategiesexploitoptimalsub-
structure, youmightbetempted togenerate adynamic-programming solution toa
problemwhenagreedysolutionsufficesor,conversely,youmightmistakenlythink
that a greedy solution works when in fact a dynamic-programming solution is re-
quired. To illustrate the subtleties between the two techniques, let us investigate
twovariantsofaclassicaloptimization problem.
The 0-1 knapsack problem is the following. A thief robbing a store finds n
items. Theithitemisworth dollarsandweighsw pounds,where andw are
i i i i
integers. Thethiefwantstotakeasvaluable aloadaspossible, buthecancarryat
mostW poundsinhisknapsack,forsomeintegerW. Whichitemsshouldhetake?
(Wecallthisthe0-1knapsackproblembecauseforeachitem,thethiefmusteither
426 Chapter16 GreedyAlgorithms
take itor leave it behind; hecannot take afractional amount of an item or take an
itemmorethanonce.)
Inthefractionalknapsackproblem,thesetupisthesame,butthethiefcantake
fractions ofitems, rather than having to make abinary (0-1) choice for each item.
You can think of an item in the 0-1 knapsack problem as being like a gold ingot
andaniteminthefractional knapsack problemasmorelikegolddust.
Both knapsack problems exhibit the optimal-substructure property. For the 0-1
problem, consider the most valuable load that weighs at most W pounds. If we
remove item j from this load, the remaining load must be the most valuable load
weighing at most W w that the thief can take from the n 1 original items
j
(cid:0) (cid:0)
excluding j. For the comparable fractional problem, consider that if we remove
a weight w of one item j from the optimal load, the remaining load must be the
mostvaluableloadweighingatmostW w thatthethiefcantakefromthen 1
(cid:0) (cid:0)
original itemsplusw w poundsofitemj.
j
(cid:0)
Althoughtheproblemsaresimilar,wecansolvethefractionalknapsackproblem
by a greedy strategy, but we cannot solve the 0-1 problem by such a strategy. To
solve thefractional problem, wefirstcompute the valueper pound  =w for each
i i
item. Obeying agreedy strategy, thethiefbegins bytaking asmuchaspossible of
the item withthe greatest value per pound. Ifthe supply of that item is exhausted
and hecanstill carry more, he takes asmuchaspossible ofthe item withthenext
greatest value per pound, and so forth, until he reaches his weight limit W. Thus,
by sorting the items by value per pound, the greedy algorithm runs in O.nlgn/
time. We leave the proof that the fractional knapsack problem has the greedy-
choiceproperty asExercise16.2-1.
To see that this greedy strategy does not work for the 0-1 knapsack problem,
consider the problem instance illustrated in Figure 16.2(a). This example has 3
items and a knapsack that can hold 50 pounds. Item 1 weighs 10 pounds and
is worth 60 dollars. Item 2 weighs 20 pounds and is worth 100 dollars. Item 3
weighs30pounds andisworth120dollars. Thus,thevalueperpoundofitem1is
6 dollars per pound, which is greater than the value per pound of either item 2 (5
dollars perpound) oritem3(4dollars perpound). Thegreedy strategy, therefore,
would take item 1 first. As you can see from the case analysis in Figure 16.2(b),
however, theoptimalsolution takesitems2and3,leaving item1behind. Thetwo
possible solutions thattakeitem1arebothsuboptimal.
For the comparable fractional problem, however, the greedy strategy, which
takes item1first,doesyield anoptimalsolution, asshowninFigure16.2(c). Tak-
ing item 1 doesn’t work in the 0-1 problem because the thief is unable to fill his
knapsack tocapacity, andtheempty spacelowersthe effective valueperpound of
his load. In the 0-1 problem, when weconsider whether to include an item in the
knapsack, we must compare the solution to the subproblem that includes the item
withthesolutiontothesubproblem thatexcludestheitembeforewecanmakethe
16.2 Elementsofthegreedystrategy 427
20
$80
30
30 $120
item 3
+
50 30 $120
item 2 +
20 $100 20 $100
30 +
item 1 + +
20 20 $100
10 10 $60 10 $60 10 $60
$60 $100 $120 knapsack = $220 = $160 = $180 = $240
(a) (b) (c)
Figure 16.2 An example showing that the greedy strategy does not work for the 0-1 knapsack
problem. (a)Thethiefmustselectasubsetofthethreeitemsshownwhoseweightmustnotexceed
50pounds. (b)Theoptimalsubsetincludesitems2and3. Anysolutionwithitem1issuboptimal,
eventhoughitem1hasthegreatestvalueperpound. (c)Forthefractionalknapsackproblem,taking
theitemsinorderofgreatestvalueperpoundyieldsanoptimalsolution.
choice. The problem formulated in this way gives rise to many overlapping sub-
problems—a hallmark of dynamic programming, and indeed, as Exercise 16.2-2
asksyoutoshow,wecanusedynamicprogramming tosolvethe0-1problem.
Exercises
16.2-1
Provethatthefractional knapsack problem hasthegreedy-choice property.
16.2-2
Give a dynamic-programming solution to the 0-1 knapsack problem that runs in
O.nW/ time, where n is the number of items and W is the maximum weight of
itemsthatthethiefcanputinhisknapsack.
16.2-3
Suppose that in a 0-1 knapsack problem, the order of the items when sorted by
increasingweightisthesameastheirorderwhensortedbydecreasingvalue. Give
an efficient algorithm to find an optimal solution to this variant of the knapsack
problem,andarguethatyouralgorithm iscorrect.
16.2-4
Professor Gekko has always dreamed of inline skating across North Dakota. He
plans to cross the state on highway U.S. 2, which runs from Grand Forks, on the
easternborderwithMinnesota,toWilliston,nearthewesternborderwithMontana.
428 Chapter16 GreedyAlgorithms
Theprofessorcancarrytwolitersofwater,andhecanskatemmilesbeforerunning
out of water. (Because North Dakota isrelatively flat, theprofessor does not have
to worry about drinking water at a greater rate on uphill sections than on flat or
downhill sections.) The professor will start in Grand Forks with two full liters of
water. His official North Dakota state map shows all the places along U.S. 2 at
whichhecanrefillhiswaterandthedistances betweentheselocations.
The professor’s goal is to minimize the number of water stops along his route
across thestate. Giveanefficient methodbywhich hecandetermine whichwater
stopsheshouldmake. Provethatyourstrategyyieldsanoptimalsolution,andgive
itsrunningtime.
16.2-5
Describe an efficient algorithm that, given a set x ;x ;:::;x of points on the
1 2 n
f g
real line, determines the smallest set of unit-length closed intervals that contains
allofthegivenpoints. Arguethatyouralgorithm iscorrect.
16.2-6 ?
Showhowtosolvethefractional knapsack problem inO.n/time.
16.2-7
SupposeyouaregiventwosetsAandB,eachcontaining npositiveintegers. You
canchoosetoreordereachsethoweveryoulike. Afterreordering, leta betheith
i
element ofsetA,andletb betheithelement ofsetB. Youthenreceive apayoff
i
of n a bi. Give an algorithm that will maximize your payoff. Prove that your
i 1 i
algoritDhm maximizesthepayoff,andstateitsrunning time.
Q
16.3 Huffman codes
Huffmancodescompressdataveryeffectively: savingsof20%to90%aretypical,
depending on the characteristics of the data being compressed. We consider the
datatobeasequenceofcharacters. Huffman’sgreedyalgorithmusesatablegiving
howoften eachcharacter occurs (i.e.,itsfrequency) tobuild upanoptimalwayof
representing eachcharacterasabinarystring.
Suppose wehave a100,000-character data filethatwewishtostore compactly.
Weobserve that thecharacters inthefileoccur withthefrequencies givenby Fig-
ure 16.3. That is, only 6 different characters appear, and the character a occurs
45,000times.
We have many options for how to represent such a file of information. Here,
weconsider the problem of designing abinary character code (or code for short)
16.3 Huffmancodes 429
a b c d e f
Frequency(inthousands) 45 13 12 16 9 5
Fixed-lengthcodeword 000 001 010 011 100 101
Variable-lengthcodeword 0 101 100 111 1101 1100
Figure16.3 Acharacter-codingproblem.Adatafileof100,000characterscontainsonlythechar-
actersa–f, withthefrequencies indicated. Ifwe assign each character a3-bit codeword, wecan
encodethefilein300,000bits.Usingthevariable-lengthcodeshown,wecanencodethefileinonly
224,000bits.
in which each character is represented by a unique binary string, which we call a
codeword. Ifweuseafixed-length code, weneed 3bitstorepresent 6characters:
a = 000, b = 001, ..., f=101. This method requires 300,000 bits to code the
entirefile. Canwedobetter?
A variable-length code can do considerably better than a fixed-length code, by
giving frequent characters short codewords and infrequent characters long code-
words. Figure16.3showssuchacode;herethe1-bitstring0representsa,andthe
4-bitstring1100represents f. Thiscoderequires
.45 1 13 3 12 3 16 3 9 4 5 4/ 1,000 224,000bits
 C  C  C  C  C   D
to represent the file, a savings of approximately 25%. In fact, this is an optimal
character codeforthisfile,asweshallsee.
Prefixcodes
Weconsider here only codes in which no codeword is also a prefix of some other
codeword. Suchcodesarecalledprefixcodes.3 Althoughwewon’tproveithere,a
prefixcodecanalwaysachievetheoptimaldatacompression amonganycharacter
code, and so we suffer no loss of generality by restricting our attention to prefix
codes.
Encodingisalwayssimpleforanybinarycharactercode;wejustconcatenatethe
codewords representing each character ofthe file. Forexample, withthe variable-
lengthprefixcodeofFigure16.3,wecodethe3-characterfileabcas0 101 100
  D
0101100, where“”denotesconcatenation.

Prefix codes are desirable because they simplify decoding. Since no codeword
isaprefix of anyother, the codeword that begins an encoded fileis unambiguous.
Wecan simply identify the initial codeword, translate it back to the original char-
3Perhaps“prefix-freecodes”wouldbeabettername,buttheterm“prefixcodes”isstandardinthe
literature.
430 Chapter16 GreedyAlgorithms
100 100
0 1
0 1
86 14 a:45 55
0 1 0 0 1
58 28 14 25 30
0 1 0 1 0 1 0 1 0 1
a:45 b:13 c:12 d:16 e:9 f:5 c:12 b:13 14 d:16
0 1
f:5 e:9
(a) (b)
Figure16.4 TreescorrespondingtothecodingschemesinFigure16.3. Eachleafislabeledwith
acharacteranditsfrequencyofoccurrence. Eachinternalnodeislabeledwiththesumofthefre-
quenciesoftheleavesinitssubtree.(a)Thetreecorrespondingtothefixed-lengthcodea=000,...,
f=101.(b)Thetreecorrespondingtotheoptimalprefixcodea=0,b=101,...,f=1100.
acter, andrepeatthedecoding process ontheremainderoftheencoded file. Inour
example,thestring001011101 parsesuniquelyas0 0 101 1101,whichdecodes
  
toaabe.
The decoding process needs a convenient representation for the prefix code so
that we can easily pick off the initial codeword. A binary tree whose leaves are
the given characters provides one such representation. We interpret the binary
codewordforacharacterasthesimplepathfromtheroottothatcharacter,where0
means“gototheleftchild”and1means“gototherightchild.” Figure16.4shows
the trees for the two codes of our example. Note that these are not binary search
trees, since the leaves need not appear in sorted order and internal nodes do not
contain characterkeys.
An optimal code for a file is always represented by a full binary tree, in which
every nonleaf node has twochildren (see Exercise 16.3-2). The fixed-length code
inourexampleisnotoptimalsinceitstree,showninFigure16.4(a),isnotafullbi-
narytree: itcontainscodewordsbeginning 10...,butnonebeginning11.... Since
we can now restrict our attention to full binary trees, we can say that if C is the
alphabetfromwhichthecharactersaredrawnandallcharacterfrequenciesarepos-
itive, then the tree for an optimal prefix code has exactly C leaves, one for each
j j
letterofthealphabet, andexactly C 1internalnodes(seeExerciseB.5-3).
j j(cid:0)
GivenatreeT correspondingtoaprefixcode,wecaneasilycomputethenumber
of bits required to encode a file. For each character c in the alphabet C, let the
attribute c:freqdenotethefrequency ofc inthefileandletd .c/denotethedepth
T
16.3 Huffmancodes 431
of c’s leaf in the tree. Note that d .c/ is also the length of the codeword for
T
character c. Thenumberofbitsrequired toencodeafileisthus
B.T/ c:freq d .c/; (16.4)
T
D 
c C
X2
whichwedefineasthecostofthetreeT.
ConstructingaHuffmancode
Huffmaninventedagreedyalgorithm thatconstructs anoptimalprefixcodecalled
a Huffman code. In line with our observations in Section 16.2, its proof of cor-
rectness relies on the greedy-choice property and optimal substructure. Rather
thandemonstrating thatthesepropertiesholdandthendevelopingpseudocode, we
present the pseudocode first. Doing so will help clarify how the algorithm makes
greedychoices.
In the pseudocode that follows, we assume that C is a set of n characters and
thateachcharacter c C isanobjectwithanattribute c:freqgivingitsfrequency.
2
Thealgorithm builds thetree T corresponding totheoptimal code inabottom-up
manner. It begins with a set of C leaves and performs a sequence of C 1
j j j j (cid:0)
“merging” operations to create the final tree. The algorithm uses a min-priority
queue Q, keyed on the freq attribute, to identify the two least-frequent objects to
merge together. When we merge two objects, the result is a new object whose
frequency isthesumofthefrequencies ofthetwoobjects thatweremerged.
HUFFMAN.C/
1 n C
D j j
2 Q C
D
3 fori 1ton 1
D (cid:0)
4 allocate anewnode´
5 ´:left x EXTRACT-MIN.Q/
D D
6 ´:right y EXTRACT-MIN.Q/
D D
7 ´:freq x:freq y:freq
D C
8 INSERT.Q;´/
9 return EXTRACT-MIN.Q/ //returntherootofthetree
For our example, Huffman’s algorithm proceeds as shown in Figure 16.5. Since
the alphabet contains 6 letters, the initial queue size is n 6, and 5 merge steps
D
build thetree. Thefinaltreerepresents theoptimalprefix code. Thecodeword for
aletteristhesequenceofedgelabelsonthesimplepathfromtheroottotheletter.
Line 2 initializes the min-priority queue Q with the characters in C. The for
loop in lines 3–8 repeatedly extracts the two nodes x and y of lowest frequency
432 Chapter16 GreedyAlgorithms
(a) f:5 e:9 c:12 b:13 d:16 a:45 (b) c:12 b:13 14 d:16 a:45
0 1
f:5 e:9
(c) 14 d:16 25 a:45 (d) 25 30 a:45
0 1 0 1 0 1 0 1
f:5 e:9 c:12 b:13 c:12 b:13 14 d:16
0 1
f:5 e:9
(e) a:45 55 (f) 100
0 1 0 1
25 30 a:45 55
0 1 0 1 0 1
c:12 b:13 14 d:16 25 30
0 1 0 1 0 1
f:5 e:9 c:12 b:13 14 d:16
0 1
f:5 e:9
Figure16.5 ThestepsofHuffman’salgorithmforthefrequenciesgiveninFigure16.3. Eachpart
shows the contents of the queue sorted into increasing order by frequency. At each step, the two
trees with lowest frequencies are merged. Leaves are shown as rectangles containing a character
anditsfrequency. Internalnodesareshownascirclescontainingthesumofthefrequenciesoftheir
children. Anedgeconnectinganinternalnodewithitschildrenislabeled0ifitisanedgetoaleft
childand1ifitisanedgetoarightchild. Thecodewordforaletteristhesequenceoflabelsonthe
edgesconnectingtheroottotheleafforthatletter. (a)Theinitialsetofn 6nodes,oneforeach
D
letter.(b)–(e)Intermediatestages.(f)Thefinaltree.
from the queue, replacing them in the queue withanew node ´representing their
merger. Thefrequency of ´iscomputed as the sum of the frequencies of x and y
in line 7. The node ´has x as its left child and y as its right child. (This order is
arbitrary; switching the left and right child of any node yields a different code of
the same cost.) After n 1 mergers, line 9returns the one node left inthe queue,
(cid:0)
whichistherootofthecodetree.
Although the algorithm would produce the sameresult if wewereto excise the
variables x and y—assigning directly to ´:left and ´:right in lines 5 and 6, and
changing line 7 to ´:freq ´:left:freq ´:right:freq—we shall use the node
D C
16.3 Huffmancodes 433
names x and y in the proof of correctness. Therefore, we find it convenient to
leavethemin.
To analyze the running time of Huffman’s algorithm, we assume that Q is im-
plemented as a binary min-heap (see Chapter 6). For a set C of n characters, we
caninitializeQinline2inO.n/timeusingtheBUILD-MIN-HEAP proceduredis-
cussed inSection6.3. Theforloop inlines 3–8executes exactly n 1times, and
(cid:0)
since each heap operation requires timeO.lgn/, theloop contributes O.nlgn/to
the running time. Thus, the total running time of HUFFMAN on aset of n charac-
ters is O.nlgn/. Wecan reduce the running time to O.nlglgn/ by replacing the
binarymin-heapwithavanEmdeBoastree(seeChapter20).
CorrectnessofHuffman’salgorithm
To prove that the greedy algorithm HUFFMAN is correct, we show that the prob-
lemofdetermininganoptimalprefixcodeexhibitsthegreedy-choice andoptimal-
substructure properties. The next lemma shows that the greedy-choice property
holds.
Lemma16.2
Let C be an alphabet in which each character c C has frequency c:freq. Let
2
x and y be two characters in C having the lowest frequencies. Then there exists
an optimal prefix code for C in which the codewords for x and y have the same
lengthanddifferonlyinthelastbit.
Proof TheideaoftheproofistotakethetreeT representinganarbitraryoptimal
prefix code and modify it to make a tree representing another optimal prefix code
suchthatthecharacters x andy appearassibling leavesofmaximumdepth inthe
newtree. Ifwecanconstructsuchatree,thenthecodewordsforx andy willhave
thesamelengthanddifferonlyinthelastbit.
Let a and b be two characters that are sibling leaves of maximum depth in T.
Without loss of generality, we assume that a:freq b:freq and x:freq y:freq.
 
Since x:freq and y:freq are the two lowest leaf frequencies, in order, and a:freq
and b:freq are two arbitrary frequencies, in order, we have x:freq a:freq and

y:freq b:freq.

Inthe remainder of theproof, itis possible that wecould have x:freq a:freq
D
ory:freq b:freq. However,ifwehadx:freq b:freq,thenwewouldalsohave
D D
a:freq b:freq x:freq y:freq (see Exercise 16.3-1), and the lemma would
D D D
be trivially true. Thus, we will assume that x:freq b:freq, which means that
¤
x b.
¤
AsFigure 16.6 shows, weexchange the positions in T of a and x to produce a
tree T , and then weexchange thepositions inT of b and y to produce atree T
0 0 00
434 Chapter16 GreedyAlgorithms
T
T¢ T¢¢
x a a
y y b
a b x b x y
Figure16.6 An illustration of the key step in the proof of Lemma 16.2. In the optimal tree T,
leavesaandbaretwosiblingsofmaximumdepth. Leavesx andy arethetwocharacterswiththe
lowestfrequencies;theyappearinarbitrarypositionsinT.Assumingthatx b,swappingleavesa
¤
andxproducestreeT ,andthenswappingleavesbandyproducestreeT . Sinceeachswapdoes
0 00
notincreasethecost,theresultingtreeT 00isalsoanoptimaltree.
in which x and y are sibling leaves of maximum depth. (Note that if x b but
D
y a, then tree T does not have x and y as sibling leaves of maximum depth.
00
¤
Because we assume that x b, this situation cannot occur.) By equation (16.4),
¤
thedifference incostbetweenT andT is
0
B.T/ B.T /
0
(cid:0)
c:freq d T.c/ c:freq d T0.c/
D  (cid:0) 
c C c C
X2 X2
x:freq d T.x/ a:freq d T.a/ x:freq d T0.x/ a:freq d T0.a/
D  C  (cid:0)  (cid:0) 
x:freq d .x/ a:freq d .a/ x:freq d .a/ a:freq d .x/
T T T T
D  C  (cid:0)  (cid:0) 
.a:freq x:freq/.d .a/ d .x//
T T
D (cid:0) (cid:0)
0;

because both a:freq x:freq and d .a/ d .x/ are nonnegative. More specifi-
T T
(cid:0) (cid:0)
cally, a:freq x:freqisnonnegative because x is aminimum-frequency leaf, and
(cid:0)
d .a/ d .x/isnonnegativebecauseaisaleafofmaximumdepthinT. Similarly,
T T
(cid:0)
exchanging y andb doesnotincreasethecost,andsoB.T / B.T /isnonnega-
0 00
(cid:0)
tive. Therefore, B.T / B.T/,andsinceT isoptimal, wehaveB.T/ B.T /,
00 00
 
which implies B.T / B.T/. Thus, T is an optimal tree in which x and y
00 00
D
appearassiblingleavesofmaximumdepth,fromwhichthelemmafollows.
Lemma16.2 implies that the process of building up anoptimal tree by mergers
can, without loss of generality, begin with the greedy choice of merging together
those two characters of lowest frequency. Why is this a greedy choice? We can
viewthecostofasinglemergerasbeingthesumofthefrequenciesofthetwoitems
being merged. Exercise 16.3-4 shows that the total cost of the tree constructed
equals the sum of the costs of its mergers. Of all possible mergers at each step,
HUFFMAN choosestheonethatincurstheleastcost.
16.3 Huffmancodes 435
Thenextlemmashowsthattheproblemofconstructingoptimalprefixcodeshas
theoptimal-substructure property.
Lemma16.3
LetC beagivenalphabet withfrequencyc:freqdefinedforeachcharacterc C.
2
Let x and y be two characters in C with minimum frequency. Let C be the
0
alphabet C with the characters x and y removed and a new character ´ added,
so that C C x;y ´ . Define freq for C as for C, except that
0 0
D (cid:0) f g [ f g
´:freq x:freq y:freq. Let T be any tree representing an optimal prefix code
0
D C
for the alphabet C . Then the tree T, obtained from T by replacing the leaf node
0 0
for´withaninternalnodehavingxandy aschildren,representsanoptimalprefix
codeforthealphabetC.
Proof We first show how to express the cost B.T/ of tree T in terms of the
cost B.T / of tree T , by considering the component costs in equation (16.4).
0 0
For each character c C x;y , we have that d T.c/ d T0.c/, and hence
2 (cid:0) f g D
c:freq d T.c/ c:freq d T0.c/. Sinced T.x/ d T.y/ d T0.´/ 1,wehave
 D  D D C
x:freq d T.x/ y:freq d T.y/ .x:freq y:freq/.d T0.´/ 1/
 C  D C C
´:freq d T0.´/ .x:freq y:freq/;
D  C C
fromwhichweconclude that
B.T/ B.T / x:freq y:freq
0
D C C
or,equivalently,
B.T / B.T/ x:freq y:freq:
0
D (cid:0) (cid:0)
We now prove the lemma by contradiction. Suppose that T does not repre-
sent an optimal prefix code for C. Then there exists an optimal tree T such that
00
B.T / < B.T/. Without loss of generality (by Lemma 16.2), T has x and y as
00 00
siblings. LetT bethetree T withthecommon parent ofx andy replaced bya
000 00
leaf´withfrequency ´:freq x:freq y:freq. Then
D C
B.T / B.T / x:freq y:freq
000 00
D (cid:0) (cid:0)
< B.T/ x:freq y:freq
(cid:0) (cid:0)
B.T /;
0
D
yieldingacontradictiontotheassumptionthatT representsanoptimalprefixcode
0
forC . Thus,T mustrepresentanoptimalprefixcodeforthealphabet C.
0
Theorem16.4
Procedure HUFFMAN produces anoptimalprefixcode.
Proof ImmediatefromLemmas16.2and16.3.
436 Chapter16 GreedyAlgorithms
Exercises
16.3-1
Explain why, in the proof of Lemma 16.2, if x:freq b:freq, then we must have
D
a:freq b:freq x:freq y:freq.
D D D
16.3-2
Provethatabinarytreethatisnotfullcannotcorrespondtoanoptimalprefixcode.
16.3-3
What is an optimal Huffman code for the following set of frequencies, based on
thefirst8Fibonacci numbers?
a:1 b:1 c:2 d:3 e:5 f:8 g:13 h:21
Canyougeneralize youranswertofindtheoptimalcodewhenthefrequencies are
thefirstnFibonaccinumbers?
16.3-4
Prove that we can also express the total cost of a tree for a code as the sum, over
allinternalnodes, ofthecombinedfrequencies ofthetwochildrenofthenode.
16.3-5
Prove that if we order the characters in an alphabet so that their frequencies
are monotonically decreasing, then there exists an optimal code whose codeword
lengths aremonotonically increasing.
16.3-6
SupposewehaveanoptimalprefixcodeonasetC 0;1;:::;n 1 ofcharac-
D f (cid:0) g
ters and wewish to transmit this code using asfew bits as possible. Show how to
represent any optimal prefix code on C using only 2n 1 n lgn bits. (Hint:
(cid:0) C d e
Use2n 1bits tospecify the structure of thetree, asdiscovered by awalk ofthe
(cid:0)
tree.)
16.3-7
Generalize Huffman’s algorithm to ternary codewords (i.e., codewords using the
symbols0,1,and2),andprovethatityieldsoptimalternarycodes.
16.3-8
Suppose that a data file contains a sequence of 8-bit characters such that all 256
characters are about equally common: the maximum character frequency is less
than twice the minimum character frequency. Prove that Huffman coding in this
caseisnomoreefficientthanusinganordinary 8-bitfixed-length code.
16.4 Matroidsandgreedymethods 437
16.3-9
Showthatnocompression schemecanexpect tocompress afileofrandomly cho-
sen 8-bit characters by even a single bit. (Hint: Compare the number of possible
fileswiththenumberofpossible encodedfiles.)
? 16.4 Matroids and greedy methods
In this section, wesketch a beautiful theory about greedy algorithms. This theory
describes manysituations inwhichthe greedy method yields optimal solutions. It
involves combinatorial structures knownas“matroids.” Although thistheory does
not cover all cases for which a greedy method applies (for example, it does not
cover the activity-selection problem of Section 16.1 or the Huffman-coding prob-
lem of Section 16.3), it does cover many cases of practical interest. Furthermore,
this theory has been extended tocover manyapplications; see thenotes attheend
ofthischapter forreferences.
Matroids
Amatroid isanordered pairM .S;(cid:9)/satisfying thefollowingconditions.
D
1. S isafiniteset.
2. (cid:9) is a nonempty family of subsets of S, called the independent subsets of S,
such that if B (cid:9) and A B, then A (cid:9) . We say that (cid:9) is hereditary if it
2  2 (cid:9)
satisfiesthisproperty. Notethattheemptyset isnecessarily amemberof .
;
3. If A (cid:9) , B (cid:9) , and A < B , then there exists some element x B A
such2 thatA 2 x (cid:9) .j Wj esayj thj atM satisfiestheexchangepropert2 y. (cid:0)
[f g 2
The word “matroid” is due to Hassler Whitney. He was studying matric ma-
troids,inwhichtheelementsofS aretherowsofagivenmatrixandasetofrowsis
independentiftheyarelinearlyindependent intheusualsense. AsExercise16.4-2
asksyoutoshow,thisstructure definesamatroid.
Asanotherexampleofmatroids,considerthegraphicmatroidM .S ;(cid:9) /
G G G
D
definedintermsofagivenundirected graphG .V;E/asfollows:
D
 ThesetS isdefinedtobeE,thesetofedgesofG.
G
 If Aisa subset ofE, then A (cid:9) ifand only if Ais acyclic. Thatis, aset of
G
2
edgesAisindependent ifandonlyifthesubgraphG .V;A/formsaforest.
A
D
ThegraphicmatroidM iscloselyrelatedtotheminimum-spanning-tree problem,
G
whichChapter23coversindetail.
438 Chapter16 GreedyAlgorithms
Theorem16.5
IfG .V;E/isanundirected graph,thenM .S ;(cid:9) /isamatroid.
G G G
D D
Proof Clearly, S E is a finite set. Furthermore, (cid:9) is hereditary, since a
G G
D
subset of a forest is a forest. Putting it another way, removing edges from an
acyclicsetofedgescannotcreatecycles.
Thus, itremains to show that M satisfies the exchange property. Suppose that
G
G .V;A/ and G .V;B/ are forests of G and that B > A . That is, A
A B
D D j j j j
andB areacyclicsetsofedges, andB containsmoreedgesthanAdoes.
We claim that a forest F .V ;E / contains exactly V E trees. To
F F F F
D j j (cid:0) j j
see why,suppose thatF consists oft trees, wherethe ithtree contains  vertices
i
ande edges. Then,wehave
i
t
E e
F i
j j D
i 1
XD
t
. 1/ (byTheoremB.2)
i
D (cid:0)
i 1
XD
t
 t
i
D (cid:0)
i 1
XD
V t ;
F
D j j(cid:0)
whichimpliesthatt V E . Thus,forestG contains V A trees,and
F F A
D j j(cid:0)j j j j(cid:0)j j
forestG contains V B trees.
B
j j(cid:0)j j
Since forest G has fewer trees than forest G does, forest G must contain
B A B
some tree T whose vertices are in two different trees in forest G . Moreover,
A
since T is connected, it must contain an edge .u;/ such that vertices u and 
are in different trees in forest G . Since the edge .u;/ connects vertices in two
A
differenttreesinforestG ,wecanaddtheedge.u;/toforestG withoutcreating
A A
a cycle. Therefore, M satisfies the exchange property, completing the proof that
G
M isamatroid.
G
GivenamatroidM .S;(cid:9)/,wecallanelementx Aanextension ofA (cid:9)
D … 2
if we can add x to A while preserving independence; that is, x is an extension
of A if A x (cid:9) . As an example, consider a graphic matroid M . If A is an
G
[f g 2
independent set of edges, then edge e is an extension of A if and only if e is not
inAandtheaddition ofe toAdoesnotcreateacycle.
IfAisanindependent subsetinamatroidM,wesaythatAismaximalifithas
noextensions. Thatis,Aismaximalifitisnotcontainedinanylargerindependent
subsetofM. Thefollowingproperty isoftenuseful.
16.4 Matroidsandgreedymethods 439
Theorem16.6
Allmaximalindependent subsetsinamatroidhavethesamesize.
Proof Suppose to the contrary that A is a maximal independent subset of M
and there exists another larger maximal independent subset B of M. Then, the
exchange property implies that for some x B A, wecan extend A to a larger
2 (cid:0)
independent setA x ,contradicting theassumption thatAismaximal.
[f g
As an illustration of this theorem, consider a graphic matroid M for a con-
G
nected, undirected graph G. Every maximal independent subset of M must be a
G
freetreewithexactly V 1edgesthatconnectsalltheverticesofG. Suchatree
j j(cid:0)
iscalledaspanningtreeofG.
Wesay that amatroid M .S;(cid:9)/ isweighted if itis associated with aweight
D
functionw thatassignsastrictlypositiveweightw.x/toeachelementx S. The
2
weightfunction w extendstosubsets ofS bysummation:
w.A/ w.x/
D
x A
X2
for any A S. For example, if we let w.e/ denote the weight of an edge e in a

graphicmatroidM ,thenw.A/isthetotalweightoftheedgesinedgesetA.
G
Greedyalgorithmsonaweightedmatroid
Manyproblemsforwhichagreedyapproachprovidesoptimalsolutionscanbefor-
mulated in terms of finding a maximum-weight independent subset in a weighted
matroid. That is, we are given a weighted matroid M .S;(cid:9)/, and we wish to
findan independent set A (cid:9) such that w.A/is maximD ized. Wecall such a sub-
2
setthatisindependent andhasmaximumpossibleweightanoptimal subsetofthe
matroid. Because the weight w.x/ of any element x S is positive, an optimal
2
subsetisalwaysamaximalindependentsubset—italwayshelpstomakeAaslarge
aspossible.
Forexample,intheminimum-spanning-treeproblem,wearegivenaconnected
undirected graph G .V;E/andalength function w suchthatw.e/isthe(posi-
D
tive)length ofedgee. (Weuse theterm“length” here torefertotheoriginal edge
weights for the graph, reserving the term “weight” to refer to the weights in the
associated matroid.) We wish to find a subset of the edges that connects all of
the vertices together and has minimum total length. To view this as a problem of
finding an optimal subset of a matroid, consider the weighted matroid M with
G
weightfunctionw ,wherew .e/ w w.e/andw islargerthanthemaximum
0 0 0 0
D (cid:0)
length ofany edge. In this weighted matroid, allweights are positive and an opti-
malsubset isaspanning treeofminimum total length inthe original graph. More
specifically, each maximal independent subset A corresponds to a spanning tree
440 Chapter16 GreedyAlgorithms
with V 1edges, andsince
j j(cid:0)
w .A/ w.e/
0 0
D
e A
X2
.w w.e//
0
D (cid:0)
e A
X2
. V 1/w w.e/
0
D j j(cid:0) (cid:0)
e A
X2
. V 1/w w.A/
0
D j j(cid:0) (cid:0)
for any maximal independent subset A, an independent subset that maximizes the
quantityw.A/mustminimizew.A/. Thus,anyalgorithmthatcanfindanoptimal
0
subsetAinanarbitrarymatroidcansolvetheminimum-spanning-tree problem.
Chapter 23 gives algorithms for the minimum-spanning-tree problem, but here
we give a greedy algorithm that works for any weighted matroid. The algorithm
takes asinput aweighted matroid M .S;(cid:9)/ withanassociated positive weight
D
function w, and it returns an optimal subset A. In our pseudocode, we denote the
components ofM byM:SandM:(cid:9) andtheweightfunction byw. Thealgorithm
is greedy because it considers in turn each element x S, in order of monotoni-
2
callydecreasing weight,andimmediatelyaddsittothesetAbeingaccumulated if
A x isindependent.
[f g
GREEDY.M;w/
1 A
D ;
2 sortM:Sintomonotonically decreasing orderbyweightw
3 foreachx M:S,takeninmonotonically decreasing orderbyweightw.x/
4 ifA 2 x M:(cid:9)
[f g2
5 A A x
D [f g
6 returnA
Line 4 checks whether adding each element x to A would maintain A as an inde-
pendentset. IfAwouldremainindependent, thenline5addsx toA. Otherwise,x
isdiscarded. Sincetheemptysetisindependent, andsinceeachiterationofthefor
loop maintains A’s independence, the subset A is always independent, by induc-
tion. Therefore, GREEDY alwaysreturns anindependent subsetA. Weshallseein
a moment that A isa subset of maximum possible weight, so that A isan optimal
subset.
Therunning timeof GREEDY iseasy to analyze. Letn denote S . The sorting
j j
phase of GREEDY takes timeO.nlgn/. Line4executes exactly n times, once for
each element of S. Each execution of line 4 requires a check on whether or not
the setA x isindependent. Ifeach such check takes timeO.f.n//,theentire
[f g
algorithm runsintimeO.nlgn nf.n//.
C
16.4 Matroidsandgreedymethods 441
Wenowprovethat GREEDY returnsanoptimalsubset.
Lemma16.7(Matroids exhibitthegreedy-choice property)
SupposethatM .S;(cid:9)/isaweightedmatroidwithweightfunctionwandthatS
D
issortedintomonotonically decreasing orderbyweight. Letx bethefirstelement
ofS suchthat x isindependent, ifanysuchx exists. Ifx exists,thenthereexists
f g
anoptimalsubsetAofS thatcontains x.
Proof Ifno such x exists, then the only independent subset isthe empty set and
the lemma is vacuously true. Otherwise, let B be any nonempty optimal subset.
Assume that x B; otherwise, letting A B gives an optimal subset of S that
… D
containsx.
NoelementofB hasweightgreaterthanw.x/. Toseewhy,observethaty B
impliesthat y isindependent, since B (cid:9) and(cid:9) ishereditary. Ourchoice 2 ofx
f g 2
therefore ensuresthatw.x/ w.y/foranyy B.
 2
ConstructthesetAasfollows. BeginwithA x . Bythechoiceofx,setAis
D f g
independent. Usingtheexchangeproperty,repeatedlyfindanewelementofB that
wecanaddtoAuntil A B ,whilepreserving theindependence ofA. Atthat
j j D j j
point, AandB arethesameexceptthatAhasx andB hassomeotherelementy.
Thatis,A B y x forsomey B,andso
D (cid:0)f g[f g 2
w.A/ w.B/ w.y/ w.x/
D (cid:0) C
w.B/:

BecausesetB isoptimal, setA,whichcontains x,mustalsobeoptimal.
We next show that if an element is not an option initially, then it cannot be an
optionlater.
Lemma16.8
Let M .S;(cid:9)/ be any matroid. If x is an element of S that is an extension of
D
someindependent subsetAofS,thenx isalsoanextensionof .
;
Proof SincexisanextensionofA,wehavethatA x isindependent. Since(cid:9)
[f g
ishereditary, x mustbeindependent. Thus,x isanextension of .
f g ;
Corollary16.9
Let M .S;(cid:9)/ be any matroid. If x is an element of S such that x is not an
D
extension of ,thenx isnotanextensionofanyindependent subsetAofS.
;
Proof Thiscorollary issimplythecontrapositive ofLemma16.8.
442 Chapter16 GreedyAlgorithms
Corollary16.9saysthatanyelementthatcannotbeusedimmediatelycannever
be used. Therefore, GREEDY cannot make an error by passing over any initial
elementsinS thatarenotanextension of ,sincetheycanneverbeused.
;
Lemma16.10(Matroids exhibittheoptimal-substructure property)
Let x be the first element of S chosen by GREEDY for the weighted matroid
M .S;(cid:9)/. The remaining problem of finding a maximum-weight indepen-
D
dentsubsetcontainingxreducestofindingamaximum-weightindependentsubset
oftheweightedmatroidM .S ;(cid:9) /,where
0 0 0
D
S y S x;y (cid:9) ;
0
D f 2 W f g 2 g
(cid:9) B S x B x (cid:9) ;
0
D f  (cid:0)f gW [f g 2 g
andtheweightfunction forM istheweightfunction forM,restricted toS . (We
0 0
callM thecontraction ofM bytheelementx.)
0
Proof IfAisanymaximum-weight independent subset ofM containing x,then
A A x is an independent subset of M . Conversely, any independent sub-
0 0
D (cid:0)f g
set A of M yields anindependent subset A A x of M. Since wehave in
0 0 0
D [f g
bothcasesthatw.A/ w.A/ w.x/,amaximum-weightsolutioninM contain-
0
D C
ingx yieldsamaximum-weightsolution inM ,andviceversa.
0
Theorem16.11(Correctnessofthegreedy algorithmonmatroids)
IfM .S;(cid:9)/isaweightedmatroidwithweightfunctionw,thenGREEDY.M;w/
D
returnsanoptimalsubset.
Proof By Corollary 16.9, any elements that GREEDY passes over initially be-
cause they are not extensions of can be forgotten about, since they can never
;
be useful. Once GREEDY selects the first element x, Lemma 16.7 implies that
the algorithm does not err by adding x to A, since there exists an optimal subset
containing x. Finally, Lemma 16.10 implies that the remaining problem is one of
finding an optimal subset in the matroid M that is the contraction of M by x.
0
After the procedure GREEDY sets A to x , we can interpret all of its remaining
steps as acting in the matroid M
.Sf ;(cid:9)g
/, because B is independent in M if
0 0 0 0
andonlyifB x isindependentD inM,forallsetsB (cid:9) . Thus,thesubsequent
0
[f g 2
operationofGREEDYwillfindamaximum-weightindependentsubsetforM 0,and
theoveralloperation of GREEDY willfindamaximum-weight independent subset
forM.
16.5 Atask-schedulingproblemasamatroid 443
Exercises
16.4-1
Show that .S;(cid:9) / is a matroid, where S is any finite set and (cid:9) is the set of all
k k
subsetsofS ofsizeatmostk,wherek S .
 j j
16.4-2 ?
Givenanm nmatrixT oversomefield(suchasthereals), showthat.S;(cid:9)/isa
matroid, whe reS isthesetofcolumnsofT andA (cid:9) ifandonly ifthecolumns
2
inAarelinearly independent.
16.4-3 ?
Showthatif.S;(cid:9)/isamatroid,then.S;(cid:9) /isamatroid,where
0
(cid:9) A S A contains somemaximalA (cid:9) :
0 0 0
D f W (cid:0) 2 g
That is, the maximal independent sets of .S;(cid:9) / are just the complements of the
0
maximalindependent
setsof.S;(cid:9)/.
16.4-4 ?
LetS beafinitesetandletS ;S ;:::;S beapartitionofS intononemptydisjoint
1 2 k
subsets. Define the structure .S;(cid:9)/ by the condition that (cid:9) A A S 1
i
for i 1;2;:::;k . Show that .S;(cid:9)/ is a matroid. That is,D thf e seW tj of\ all sj et s A
D g
that contain at most one member of each subset in the partition determines the
independent setsofamatroid.
16.4-5
Showhowtotransform theweightfunctionofaweightedmatroidproblem,where
thedesired optimal solution isaminimum-weight maximal independent subset, to
make itastandard weighted-matroid problem. Argue carefully that your transfor-
mationiscorrect.
? 16.5 A task-scheduling problem as a matroid
An interesting problem that we can solve using matroids is the problem of op-
timally scheduling unit-time tasks on a single processor, where each task has a
deadline, along with a penalty paid if the task misses its deadline. The problem
looks complicated, but we can solve itin a surprisingly simple manner by casting
itasamatroidandusingagreedyalgorithm.
Aunit-timetaskisajob,suchasaprogramtoberunonacomputer,thatrequires
exactly one unit of time to complete. Given a finite set S of unit-time tasks, a
444 Chapter16 GreedyAlgorithms
schedule for S is a permutation of S specifying the order in which to perform
these tasks. The first task in the schedule begins at time 0 and finishes at time 1,
thesecondtaskbeginsattime1andfinishesattime2,andsoon.
The problem of scheduling unit-time tasks with deadlines and penalties for a
singleprocessor hasthefollowinginputs:
 asetS a ;a ;:::;a ofnunit-timetasks;
1 2 n
D f g
 asetofnintegerdeadlinesd ;d ;:::;d ,suchthateachd satisfies1 d n
1 2 n i i
 
andtaska issupposed tofinishbytimed ;and
i i
 asetofnnonnegative weightsorpenalties w ;w ;:::;w ,such thatweincur
1 2 n
apenalty of w iftask a is not finished by time d , and weincur no penalty if
i i i
ataskfinishesbyitsdeadline.
We wish to find a schedule for S that minimizes the total penalty incurred for
misseddeadlines.
Consideragivenschedule. Wesaythatataskislateinthisscheduleifitfinishes
afteritsdeadline. Otherwise,thetaskisearlyintheschedule. Wecanalwaystrans-
form an arbitrary schedule into early-first form, in which the early tasks precede
thelatetasks. Toseewhy,notethatifsomeearlytaska followssomelatetaska ,
i j
thenwecanswitchthepositions ofa anda ,anda willstillbeearlyanda will
i j i j
stillbelate.
Furthermore, weclaim that wecan always transform an arbitrary schedule into
canonical form, in which the early tasks precede the late tasks and we schedule
the early tasks in order of monotonically increasing deadlines. To do so, we put
the schedule into early-first form. Then, as long as there exist two early tasks a
i
anda finishingatrespectivetimesk andk 1intheschedulesuchthatd < d ,
j j i
C
weswapthepositions ofa anda . Sincea isearlybeforetheswap,k 1 d .
i j j j
C 
Therefore, k 1 < d , and so a is still early after the swap. Because task a is
i i j
C
movedearlierintheschedule, itremainsearlyaftertheswap.
The search for an optimal schedule thus reduces to finding a set A of tasks that
weassigntobeearlyintheoptimalschedule. HavingdeterminedA,wecancreate
theactualschedule bylistingtheelementsofAinorderofmonotonically increas-
ing deadlines, then listing the late tasks (i.e., S A) in any order, producing a
(cid:0)
canonical ordering oftheoptimalschedule.
We say that a set A of tasks is independent if there exists a schedule for these
taskssuchthatnotasksarelate. Clearly,thesetofearlytasksforascheduleforms
(cid:9)
anindependent setoftasks. Let denotethesetofallindependent setsoftasks.
Consider theproblem ofdetermining whether agivensetAoftasks isindepen-
dent. For t 0;1;2;:::;n, let N .A/ denote the number of tasks in A whose
t
D
deadline ist orearlier. NotethatN .A/ 0foranysetA.
0
D
16.5 Atask-schedulingproblemasamatroid 445
Lemma16.12
ForanysetoftasksA,thefollowingstatementsareequivalent.
1. ThesetAisindependent.
2. Fort 0;1;2;:::;n,wehaveN .A/ t.
t
D 
3. If thetasks inA arescheduled inorder ofmonotonically increasing deadlines,
thennotaskislate.
Proof Toshowthat(1)implies(2),weprovethecontrapositive: ifN .A/ >t for
t
somet,thenthereisnowaytomakeaschedulewithnolatetasksforsetA,because
morethant tasksmustfinishbeforetimet. Therefore,(1)implies(2). If(2)holds,
then (3) must follow: there is no way to “get stuck” when scheduling the tasks in
order of monotonically increasing deadlines, since (2) implies that the ith largest
deadlineisatleasti. Finally,(3)trivially implies(1).
Usingproperty2ofLemma16.12,wecaneasilycomputewhetherornotagiven
setoftasksisindependent (seeExercise16.5-2).
Theproblemofminimizingthesumofthepenaltiesofthelatetasksisthesame
as the problem of maximizing the sum of the penalties of the early tasks. The
following theorem thus ensures that we can use the greedy algorithm to find an
independent setAoftaskswiththemaximumtotalpenalty.
Theorem16.13
If S is a set of unit-time tasks with deadlines, and (cid:9) is the set of all independent
setsoftasks, thenthecorresponding system
.S;(cid:9)/isamatroid.
Proof Every subset of an independent set of tasks is certainly independent. To
prove the exchange property, suppose that B and A are independent sets of tasks
andthat B > A . Letk bethelargestt suchthatN .B/ N .A/. (Suchavalue
t t
j j j j 
of t exists, since N .A/ N .B/ 0.) Since N .B/ B and N .A/ A ,
0 0 n n
D D D j j D j j
but B > A , we must have that k < n and that N .B/ > N .A/ for all j in
j j
j j j j
the range k 1 j n. Therefore, B contains more tasks withdeadline k 1
C   C
thanAdoes. Leta beataskinB Awithdeadline k 1. LetA A a .
i 0 i
(cid:0) C D [f g
WenowshowthatA mustbeindependentbyusingproperty2ofLemma16.12.
0
For 0 t k, we have N .A/ N .A/ t, since A is independent. For
t 0 t
  D 
k < t n, wehave N .A/ N .B/ t, since B is independent. Therefore, A
t 0 t 0
isindep
endent, completing
ou rproofth at.S;(cid:9)/isamatroid.
By Theorem 16.11, we can use a greedy algorithm to find a maximum-weight
independent set of tasks A. We can then create an optimal schedule having the
tasks in A as its early tasks. This method is an efficient algorithm for scheduling
446 Chapter16 GreedyAlgorithms
Task
a i 1 2 3 4 5 6 7
d i 4 2 4 3 1 4 6
w i 70 60 50 40 30 20 10
Figure16.7 Aninstanceoftheproblemofschedulingunit-timetaskswithdeadlinesandpenalties
forasingleprocessor.
unit-time tasks with deadlines and penalties for a single processor. The running
time is O.n2/ using GREEDY, since each of the O.n/ independence checks made
by that algorithm takes time O.n/ (see Exercise 16.5-2). Problem 16-4 gives a
fasterimplementation.
Figure 16.7 demonstrates an example of the problem of scheduling unit-time
tasks with deadlines and penalties for a single processor. In this example, the
greedyalgorithmselects,inorder,tasksa ,a ,a ,anda ,thenrejectsa (because
1 2 3 4 5
N . a ;a ;a ;a ;a / 5) and a (because N . a ;a ;a ;a ;a / 5), and
4 1 2 3 4 5 6 4 1 2 3 4 6
f g D f g D
finallyaccepts a . Thefinaloptimalschedule is
7
a ;a ;a ;a ;a ;a ;a ;
2 4 1 3 7 5 6
h i
whichhasatotalpenaltyincurred ofw w 50.
5 6
C D
Exercises
16.5-1
Solve the instance of the scheduling problem given in Figure 16.7, but with each
penalty w replacedby80 w .
i i
(cid:0)
16.5-2
Showhowtouseproperty2ofLemma16.12todetermineintimeO. A /whether
j j
ornotagivensetAoftasksisindependent.
Problems
16-1 Coinchanging
Consider the problem of making change for n cents using the fewest number of
coins. Assumethateachcoin’svalueisaninteger.
a. Describe a greedy algorithm to make change consisting of quarters, dimes,
nickels, andpennies. Provethatyouralgorithm yieldsanoptimalsolution.
ProblemsforChapter16 447
b. Suppose that theavailable coins areinthedenominations thatarepowersofc,
i.e., the denominations are c0;c1;:::;ck for some integers c > 1 and k 1.

Showthatthegreedyalgorithm alwaysyieldsanoptimalsolution.
c. Giveasetofcoindenominationsforwhichthegreedyalgorithmdoesnotyield
anoptimal solution. Yoursetshould include apenny sothatthere isasolution
foreveryvalueofn.
d. GiveanO.nk/-timealgorithmthatmakeschangeforanysetofkdifferentcoin
denominations, assumingthatoneofthecoinsisapenny.
16-2 Schedulingtominimizeaveragecompletion time
Suppose you are given a set S a ;a ;:::;a of tasks, where task a re-
1 2 n i
D f g
quires p units of processing time to complete, once it has started. You have one
i
computeronwhichtorunthesetasks, andthecomputer canrunonlyonetaskata
time. Letc bethecompletiontimeoftaska ,thatis,thetimeatwhichtaska com-
i i i
pletes processing. Your goal is to minimize the average completion time, that is,
to minimize .1=n/ n c . For example, suppose there are two tasks, a and a ,
i 1 i 1 2
withp 3andp D5,andconsiderthescheduleinwhicha runsfirst,followed
1 2 2
D PD
bya . Thenc 5,c 8,andtheaveragecompletion timeis.5 8/=2 6:5.
1 2 1
D D C D
If task a runs first, however, then c 3, c 8, and the average completion
1 1 2
D D
timeis.3 8/=2 5:5.
C D
a. Givean algorithm that schedules the tasks so asto minimize the average com-
pletiontime. Eachtaskmustrunnon-preemptively,thatis,oncetaska starts,it
i
mustruncontinuouslyforp unitsoftime. Provethatyouralgorithmminimizes
i
theaveragecompletion time,andstatetherunning timeofyouralgorithm.
b. Suppose now that the tasks are not all available at once. That is, each task
cannotstartuntilitsreleasetimer . Supposealsothatweallowpreemption,so
i
thatataskcanbesuspendedandrestartedatalatertime. Forexample,ataska
i
with processing time p 6 and release time r 1 might start running at
i i
D D
time 1 and be preempted at time 4. It might then resume at time 10 but be
preempted at time 11, and it might finally resume at time 13 and complete at
time15. Taska hasrunforatotalof6timeunits,butitsrunningtimehasbeen
i
divided into three pieces. In this scenario, a ’s completion time is 15. Give
i
analgorithm thatschedules thetaskssoastominimizetheaveragecompletion
time in this new scenario. Prove that your algorithm minimizes the average
completion time,andstatetherunning timeofyouralgorithm.
448 Chapter16 GreedyAlgorithms
16-3 Acyclicsubgraphs
a. The incidence matrix for an undirected graph G .V;E/ is a V E ma-
D j jj j
trixM suchthatM 1ifedgeeisincidentonvertex,andM 0other-
e e
D D
wise. ArguethatasetofcolumnsofM islinearlyindependentoverthefieldof
integersmodulo2ifandonlyifthecorresponding setofedgesisacyclic.
b. Suppose that we associate a nonnegative weight w.e/ with each edge in an
undirected graph G .V;E/. Give an efficient algorithm to find an acyclic
D
subsetofE ofmaximumtotalweight.
c. Let G.V;E/ be an arbitrary directed graph, and let .E;(cid:9)/ be defined so that
A (cid:9) ifand only ifA does not contain any directed cycles. Give anexample
of2 a directed graph G such that the associated system .E;(cid:9)/ is not a matroid.
Specifywhichdefiningcondition foramatroidfailstohold.
d. The incidence matrix for a directed graph G .V;E/ with no self-loops is a
D
V E matrixM suchthatM 1ifedgee leavesvertex,M 1if
e e
j jj j D (cid:0) D
edge e enters vertex ,and M 0otherwise. Argue thatifasetofcolumns
e
D
of M is linearly independent, then the corresponding set of edges does not
containadirected cycle.
e. Exercise 16.4-2 tells us that the set of linearly independent sets of columns of
any matrix M forms a matroid. Explain carefully why the results of parts (c)
and (e) are not contradictory. How can there fail to be a perfect correspon-
dence between the notion of a set of edges being acyclic and the notion of the
associated setofcolumnsoftheincidence matrixbeinglinearly independent?
16-4 Schedulingvariations
Considerthefollowingalgorithm fortheproblemfromSection16.5ofscheduling
unit-timetaskswithdeadlinesandpenalties. Letallntimeslotsbeinitiallyempty,
wheretimesloti istheunit-length slotoftimethatfinishesattimei. Weconsider
thetasksinorderofmonotonically decreasing penalty. Whenconsidering taska ,
j
ifthere exists atimeslot atorbefore a ’sdeadline d thatisstill empty, assign a
j j j
to the latest such slot, filling it. If there is nosuch slot, assign task a tothe latest
j
oftheasyetunfilledslots.
a. Arguethatthisalgorithm alwaysgivesanoptimalanswer.
b. Usethefastdisjoint-set forestpresentedinSection21.3toimplementthealgo-
rithmefficiently. Assumethatthesetofinputtaskshasalreadybeensortedinto
monotonically decreasing order by penalty. Analyze the running time of your
implementation.
ProblemsforChapter16 449
16-5 Off-linecaching
Modern computers use a cache to store a small amount of data in a fast memory.
Eventhoughaprogrammayaccesslargeamountsofdata,bystoringasmallsubset
ofthemainmemoryinthecache—asmallbutfastermemory—overallaccesstime
can greatly decrease. When a computer program executes, it makes a sequence
r ;r ;:::;r of n memory requests, where each request is for a particular data
1 2 n
h i
element. For example, a program that accesses 4 distinct elements a;b;c;d
f g
mightmakethesequence ofrequests d;b;d;b;d;a;c;d;b;a;c;b . Letk bethe
h i
sizeofthecache. Whenthecachecontainskelementsandtheprogramrequeststhe
.k 1/st element, the system must decide, for this and each subsequent request,
C
which k elements to keep in the cache. More precisely, for each request r , the
i
cache-management algorithmcheckswhetherelementr isalreadyinthecache. If
i
it is, then we have a cache hit; otherwise, we have a cache miss. Upon a cache
miss, the system retrieves r from the main memory, and the cache-management
i
algorithm mustdecide whether tokeep r inthecache. Ifitdecides tokeepr and
i i
the cache already holds k elements, then it must evict one element to make room
for r . The cache-management algorithm evicts data with the goal of minimizing
i
thenumberofcachemissesovertheentiresequence ofrequests.
Typically, caching is an on-line problem. That is, we have to make decisions
about which data to keep in the cache without knowing the future requests. Here,
however, we consider the off-line version of this problem, in which we are given
inadvance the entire sequence of n requests and the cache size k, and wewish to
minimizethetotalnumberofcachemisses.
Wecansolvethisoff-lineproblembyagreedystrategycalledfurthest-in-future,
which chooses to evict the item in the cache whose next access in the request
sequence comesfurthestinthefuture.
a. Writepseudocode foracachemanagerthatusesthefurthest-in-future strategy.
Theinput should beasequence r ;r ;:::;r of requests and acache size k,
1 2 n
h i
and the output should be asequence of decisions about which data element (if
any)toevictuponeachrequest. Whatistherunningtimeofyouralgorithm?
b. Showthattheoff-linecachingproblem exhibitsoptimalsubstructure.
c. Prove that furthest-in-future produces the minimum possible number of cache
misses.
450 Chapter16 GreedyAlgorithms
Chapter notes
Much more material on greedy algorithms and matroids can be found in Lawler
[224]andPapadimitriouandSteiglitz[271].
Thegreedy algorithm firstappeared inthecombinatorial optimization literature
in a 1971 article by Edmonds [101], though the theory of matroids dates back to
a1935articlebyWhitney[355].
Our proof of the correctness of the greedy algorithm for the activity-selection
problem is based on that of Gavril [131]. The task-scheduling problem is studied
inLawler[224];Horowitz,Sahni,andRajasekaran[181];andBrassardandBratley
[54].
Huffmancodeswereinventedin1952[185];LelewerandHirschberg[231]sur-
veysdata-compression techniques knownasof1987.
An extension of matroid theory to greedoid theory was pioneered by Korte and
Lova´sz[216,217,218,219],whogreatlygeneralize thetheorypresented here.
17 Amortized Analysis
In an amortized analysis, we average the time required to perform a sequence of
data-structure operationsoveralltheoperationsperformed. Withamortizedanaly-
sis,wecanshowthattheaveragecostofanoperationissmall,ifweaverageovera
sequence ofoperations, even though asingle operation withinthe sequence might
be expensive. Amortized analysis differs from average-case analysis in that prob-
ability isnot involved; anamortized analysis guarantees the average performance
ofeachoperation intheworstcase.
Thefirstthree sections ofthischapter coverthethree mostcommon techniques
used in amortized analysis. Section 17.1 starts with aggregate analysis, in which
wedetermineanupperboundT.n/onthetotalcostofasequenceofnoperations.
The average cost per operation is then T.n/=n. We take the average cost as the
amortized cost of each operation, so that all operations have the same amortized
cost.
Section17.2coverstheaccountingmethod,inwhichwedetermineanamortized
costofeachoperation. Whenthereismorethanonetypeofoperation,eachtypeof
operationmayhaveadifferentamortizedcost. Theaccountingmethodovercharges
some operations early in the sequence, storing the overcharge as “prepaid credit”
on specific objects in the data structure. Later in the sequence, the credit pays for
operations thatarecharged lessthantheyactually cost.
Section17.3discussesthepotentialmethod,whichisliketheaccountingmethod
inthatwedetermine theamortizedcostofeachoperation andmayovercharge op-
erationsearlyontocompensateforunderchargeslater. Thepotentialmethodmain-
tainsthecreditasthe“potential energy”ofthedatastructureasawholeinsteadof
associating thecreditwithindividual objectswithinthedatastructure.
We shall use two examples to examine these three methods. One is a stack
withtheadditional operation MULTIPOP, whichpops severalobjects atonce. The
other is a binary counter that counts up from 0 by means of the single operation
INCREMENT.
452 Chapter17 AmortizedAnalysis
While reading this chapter, bear in mind that the charges assigned during an
amortized analysis are for analysis purposes only. They need not—and should
not—appear in the code. If, for example, we assign a credit to an object x when
using theaccounting method, wehave noneedtoassign anappropriate amountto
someattribute, suchasx:credit,inthecode.
Whenweperform anamortized analysis, weoften gaininsight intoaparticular
data structure, and this insight can help us optimize the design. In Section 17.4,
forexample,weshallusethepotentialmethodtoanalyzeadynamicallyexpanding
andcontracting table.
17.1 Aggregateanalysis
In aggregate analysis, we show that for all n, a sequence of n operations takes
worst-case time T.n/ in total. In the worst case, the average cost, or amortized
cost, per operation is therefore T.n/=n. Note that this amortized cost applies to
each operation, even when there are several types of operations in the sequence.
The other two methods we shall study in this chapter, the accounting method and
the potential method, may assign different amortized costs to different types of
operations.
Stackoperations
In our first example of aggregate analysis, we analyze stacks that have been aug-
mented with a new operation. Section 10.1 presented the two fundamental stack
operations, eachofwhichtakesO.1/time:
PUSH.S;x/pushes objectx ontostackS.
POP.S/pops thetopofstack S andreturns thepopped object. Calling POP onan
emptystackgenerates anerror.
Since each of these operations runs in O.1/ time, let us consider the cost of each
tobe1. Thetotalcostofasequence ofn PUSH and POP operations istherefore n,
andtheactualrunningtimefornoperations istherefore ‚.n/.
Nowweaddthestackoperation MULTIPOP.S;k/,whichremovesthek topob-
jectsofstackS,poppingtheentirestackifthestackcontainsfewerthank objects.
Ofcourse,weassumethatk ispositive;otherwisetheMULTIPOP operationleaves
the stack unchanged. In the following pseudocode, the operation STACK-EMPTY
returns TRUE iftherearenoobjectscurrently onthestack,and FALSE otherwise.
17.1 Aggregateanalysis 453
top 23
17
6
39
10 top 10
47 47
(a) (b) (c)
Figure17.1 Theactionof MULTIPOPonastackS,showninitiallyin(a). Thetop4objectsare
poppedbyMULTIPOP.S;4/,whoseresultisshownin(b). ThenextoperationisMULTIPOP.S;7/,
whichemptiesthestack—shownin(c)—sincetherewerefewerthan7objectsremaining.
MULTIPOP.S;k/
1 whilenot STACK-EMPTY.S/andk > 0
2 POP.S/
3 k k 1
D (cid:0)
Figure17.1showsanexampleof MULTIPOP.
What is the running time of MULTIPOP.S;k/ on a stack of s objects? The
actual running time is linear in the number of POP operations actually executed,
and thus we can analyze MULTIPOP in terms of the abstract costs of 1 each for
PUSHandPOP. Thenumberofiterationsofthewhileloopisthenumbermin.s;k/
ofobjectspoppedoffthestack. EachiterationoftheloopmakesonecalltoPOPin
line2. Thus, thetotal costof MULTIPOP ismin.s;k/, andthe actual running time
isalinearfunction ofthiscost.
LetusanalyzeasequenceofnPUSH,POP,andMULTIPOP operationsonanini-
tially empty stack. Theworst-case cost ofa MULTIPOP operation in thesequence
isO.n/, sincethe stacksize isatmostn. Theworst-case timeofanystackopera-
tionisthereforeO.n/,andhenceasequenceofnoperationscostsO.n2/,sincewe
may have O.n/ MULTIPOP operations costing O.n/ each. Although this analysis
iscorrect, theO.n2/result, whichweobtained byconsidering theworst-case cost
ofeachoperation individually, isnottight.
Usingaggregate analysis, wecanobtainabetter upperbound thatconsiders the
entire sequence of n operations. In fact, although a single MULTIPOP operation
canbeexpensive, anysequence ofn PUSH, POP,and MULTIPOP operations onan
initiallyemptystackcancostatmostO.n/. Why? Wecanpopeachobjectfromthe
stack at most once for each time we have pushed it onto the stack. Therefore, the
numberoftimesthatPOPcanbecalledonanonemptystack,includingcallswithin
MULTIPOP, isatmostthenumberofPUSHoperations, whichisatmostn. Forany
valueofn,anysequence ofn PUSH, POP,and MULTIPOP operations takes atotal
of O.n/ time. The average cost of an operation is O.n/=n O.1/. In aggregate
D
454 Chapter17 AmortizedAnalysis
analysis, weassign theamortized costofeachoperation tobetheaverage cost. In
thisexample,therefore, allthreestackoperations haveanamortizedcostofO.1/.
Weemphasizeagainthatalthoughwehavejustshownthattheaveragecost,and
hence the running time, of a stack operation is O.1/, we did not use probabilistic
reasoning. We actually showed a worst-case bound of O.n/ on a sequence of n
operations. Dividing this total costbynyielded theaverage cost peroperation, or
theamortizedcost.
Incrementingabinarycounter
As another example of aggregate analysis, consider the problem of implementing
ak-bitbinarycounter thatcountsupwardfrom0. WeuseanarrayAŒ0::k 1of
(cid:0)
bits, where A:length k, as the counter. A binary number x that is stored in the
D
counterhasitslowest-orderbitinAŒ0anditshighest-order bitinAŒk 1,sothat
(cid:0)
x
D
k i(cid:0)01AŒi 2i. Initially,x
D
0,andthusAŒi
D
0fori
D
0;1;:::;k (cid:0)1. To
add1(mDodulo2k)tothevalueinthecounter, weusethefollowingprocedure.
P
INCREMENT.A/
1 i 0
D
2 whilei < A:lengthandAŒi ==1
3 AŒi 0
D
4 i i 1
D C
5 ifi < A:length
6 AŒi 1
D
Figure 17.2 shows what happens to a binary counter as we increment it 16 times,
starting with the initial value 0 and ending with the value 16. At the start of
each iteration of the while loop in lines 2–4, we wish to add a 1 into position i.
If AŒi 1, then adding 1 flips the bit to 0 in position i and yields a carry of 1,
D
to be added into position i 1 on the next iteration of the loop. Otherwise, the
C
loop ends, and then, if i < k, we know that AŒi 0, so that line 6 adds a 1 into
D
position i, flipping the 0 to a 1. The cost of each INCREMENT operation is linear
inthenumberofbitsflipped.
Aswith the stack example, acursory analysis yields abound that is correct but
nottight. Asingleexecution of INCREMENT takestime‚.k/intheworstcase,in
which array A contains all 1s. Thus, a sequence of n INCREMENT operations on
aninitiallyzerocountertakestimeO.nk/intheworstcase.
Wecantightenouranalysistoyieldaworst-casecostofO.n/forasequenceofn
INCREMENT operations by observing that not all bits flip each time INCREMENT
is called. As Figure 17.2 shows, AŒ0 does flip each time INCREMENT is called.
The next bit up, AŒ1, flips only every other time: a sequence of n INCREMENT
17.1 Aggregateanalysis 455
Counter Total
value
A[7] A[6] A[5] A[4] A[3] A[2] A[1] A[0]
cost
0 0 0 0 0 0 0 0 0 0
1 0 0 0 0 0 0 0 1 1
2 0 0 0 0 0 0 1 0 3
3 0 0 0 0 0 0 1 1 4
4 0 0 0 0 0 1 0 0 7
5 0 0 0 0 0 1 0 1 8
6 0 0 0 0 0 1 1 0 10
7 0 0 0 0 0 1 1 1 11
8 0 0 0 0 1 0 0 0 15
9 0 0 0 0 1 0 0 1 16
10 0 0 0 0 1 0 1 0 18
11 0 0 0 0 1 0 1 1 19
12 0 0 0 0 1 1 0 0 22
13 0 0 0 0 1 1 0 1 23
14 0 0 0 0 1 1 1 0 25
15 0 0 0 0 1 1 1 1 26
16 0 0 0 1 0 0 0 0 31
Figure17.2 An8-bitbinarycounterasitsvaluegoesfrom0to16byasequenceof16INCREMENT
operations. Bitsthatfliptoachievethenextvalueareshaded. Therunningcostforflippingbitsis
shownattheright.NoticethatthetotalcostisalwayslessthantwicethetotalnumberofINCREMENT
operations.
operations on an initially zero counter causes AŒ1 to flip n=2 times. Similarly,
b c
bitAŒ2flipsonlyeveryfourthtime,or n=4 timesinasequenceofnINCREMENT
b c
operations. In general, for i 0;1;:::;k 1, bit AŒi flips n=2i times in a
D (cid:0) b c
sequence of n INCREMENT operations on an initially zero counter. For i k,

bit AŒi does not exist, and so it cannot flip. The total number of flips in the
sequence isthus
k 1
(cid:0) n 1 1
< n
2i 2i
Xi D0j k Xi D0
2n;
D
byequation(A.6). Theworst-casetimeforasequenceofnINCREMENToperations
onaninitially zerocounter istherefore O.n/. Theaverage costofeachoperation,
andtherefore theamortized costperoperation, isO.n/=n O.1/.
D
456 Chapter17 AmortizedAnalysis
Exercises
17.1-1
If the set of stack operations included a MULTIPUSH operation, which pushes k
items onto the stack, would the O.1/ bound on the amortized cost ofstack opera-
tionscontinue tohold?
17.1-2
ShowthatifaDECREMENT operationwereincludedinthek-bitcounterexample,
noperations couldcostasmuchas‚.nk/time.
17.1-3
Supposeweperformasequenceofnoperationsonadatastructureinwhichtheith
operationcostsi ifi isanexactpowerof2,and1otherwise. Useaggregateanalysis
todeterminetheamortizedcostperoperation.
17.2 The accounting method
In the accounting method of amortized analysis, we assign differing charges to
different operations, with some operations charged more or less than they actu-
ally cost. We call the amount we charge an operation its amortized cost. When
an operation’s amortized cost exceeds its actual cost, we assign the difference to
specific objects in the data structure as credit. Credit can help pay for later oper-
ations whose amortized cost is less than their actual cost. Thus, we can view the
amortizedcostofanoperation asbeingsplitbetweenitsactualcostandcreditthat
is either deposited or used up. Different operations may have different amortized
costs. This method differs from aggregate analysis, in which all operations have
thesameamortizedcost.
Wemustchoosetheamortizedcostsofoperations carefully. Ifwewanttoshow
that in the worst case the average cost per operation is small by analyzing with
amortizedcosts,wemustensurethatthetotalamortizedcostofasequenceofoper-
ationsprovidesanupperboundonthetotalactualcostofthesequence. Moreover,
as in aggregate analysis, this relationship must hold for all sequences of opera-
tions. Ifwedenotetheactualcostoftheithoperationbyc andtheamortizedcost
i
oftheithoperation byc ,werequire
i
y
n n
c c (17.1)
i i
y 
i 1 i 1
XD XD
for all sequences of n operations. The total credit stored in the data structure
is the difference between the total amortized cost and the total actual cost, or
17.2 Theaccountingmethod 457
n c n c . By inequality (17.1), the total credit associated with the data
i 1yi (cid:0) i 1 i
struDcture mustDbenonnegativeatalltimes. Ifweeverweretoallowthetotalcredit
P P
tobecome negative (theresult ofundercharging early operations withthe promise
of repaying the account later on), then the total amortized costs incurred at that
timewouldbebelowthetotalactualcostsincurred;forthesequence ofoperations
up to that time, the total amortized cost would not be an upper bound on the total
actualcost. Thus,wemusttakecarethatthetotalcreditinthedatastructure never
becomesnegative.
Stackoperations
Toillustratetheaccountingmethodofamortizedanalysis, letusreturntothestack
example. Recallthattheactualcostsoftheoperations were
PUSH 1,
POP 1,
MULTIPOP min.k;s/ ,
where k is the argument supplied to MULTIPOP and s is the stack size when it is
called. Letusassignthefollowingamortizedcosts:
PUSH 2,
POP 0,
MULTIPOP 0.
NotethattheamortizedcostofMULTIPOP isaconstant(0),whereastheactualcost
isvariable. Here, all three amortized costs are constant. In general, the amortized
costs of the operations under consideration may differ from each other, and they
mayevendifferasymptotically.
We shall now show that we can pay for any sequence of stack operations by
charging the amortized costs. Suppose we use a dollar bill to represent each unit
ofcost. Westartwithanemptystack. Recalltheanalogy ofSection10.1between
the stack data structure and a stack of plates in acafeteria. When wepush a plate
on the stack, weuse 1 dollar to pay the actual cost of the push and are left with a
creditof1dollar(outofthe2dollarscharged), whichweleaveontopoftheplate.
Atanypointintime,everyplateonthestackhasadollarofcreditonit.
The dollar stored on the plate serves as prepayment for the cost of popping it
fromthestack. WhenweexecuteaPOPoperation,wechargetheoperationnothing
andpay itsactual cost using the credit stored in thestack. Topop aplate, wetake
the dollar of credit off the plate and use it to pay the actual cost of the operation.
Thus, by charging the PUSH operation a little bit more, we can charge the POP
operation nothing.
458 Chapter17 AmortizedAnalysis
Moreover, we can also charge MULTIPOP operations nothing. To pop the first
plate, wetakethedollarofcreditofftheplateanduseittopaytheactualcostofa
POPoperation. Topopasecondplate,weagainhaveadollarofcreditontheplate
to pay for the POP operation, and so on. Thus, we have always charged enough
up front to pay for MULTIPOP operations. In other words, since each plate on the
stackhas1dollarofcreditonit,andthestackalwayshasanonnegativenumberof
plates, wehaveensured thattheamountofcreditisalwaysnonnegative. Thus,for
anysequenceofnPUSH,POP,andMULTIPOP operations, thetotalamortizedcost
is an upper bound on the total actual cost. Since the total amortized cost is O.n/,
soisthetotalactualcost.
Incrementingabinarycounter
Asanother illustration oftheaccounting method,weanalyze the INCREMENT op-
eration onabinary counter that starts atzero. Asweobserved earlier, therunning
timeofthisoperation isproportional tothenumberofbitsflipped, whichweshall
use as our cost for this example. Let us once again use a dollar bill to represent
eachunitofcost(theflippingofabitinthisexample).
For the amortized analysis, let us charge an amortized cost of 2 dollars to set a
bit to 1. When a bit is set, we use 1 dollar (out of the 2 dollars charged) to pay
fortheactualsettingofthebit,andweplacetheotherdollaronthebitascreditto
be used later when we flip the bit back to 0. At any point in time, every 1 in the
counter has a dollar of credit on it, and thus we can charge nothing to reset a bit
to0;wejustpayfortheresetwiththedollarbillonthebit.
NowwecandeterminetheamortizedcostofINCREMENT. Thecostofresetting
thebitswithinthewhileloopispaidforbythedollarsonthebitsthatarereset. The
INCREMENT procedure sets atmostonebit, inline6, and therefore the amortized
cost of an INCREMENT operation is at most 2 dollars. The number of 1s in the
counter never becomes negative, and thus the amount of credit stays nonnegative
atalltimes. Thus,forn INCREMENT operations, thetotalamortized cost isO.n/,
whichbounds thetotalactualcost.
Exercises
17.2-1
Suppose we perform a sequence of stack operations on a stack whose size never
exceedsk. Aftereveryk operations,wemakeacopyoftheentirestackforbackup
purposes. Show that the cost of n stack operations, including copying the stack,
isO.n/byassigning suitableamortizedcoststothevariousstackoperations.
17.3 Thepotentialmethod 459
17.2-2
RedoExercise17.1-3usinganaccounting methodofanalysis.
17.2-3
Suppose we wish not only to increment a counter but also to reset it to zero (i.e.,
make all bits in it 0). Counting the time to examine or modify a bit as ‚.1/,
show how to implement a counter as an array of bits so that any sequence of n
INCREMENT and RESET operations takes time O.n/ on an initially zero counter.
(Hint:Keepapointertothehigh-order 1.)
17.3 The potential method
Instead of representing prepaid work as credit stored with specific objects in the
data structure, the potential method of amortized analysis represents the prepaid
work as “potential energy,” or just “potential,” which can be released to pay for
future operations. We associate the potential with the data structure as a whole
ratherthanwithspecificobjectswithinthedatastructure.
The potential method works as follows. We will perform n operations, starting
with an initial data structure D . Foreach i 1;2;:::;n, welet c be the actual
0 i
D
cost of the ith operation and D be the data structure that results after applying
i
the ith operation to data structure D . A potential function ˆ maps each data
i 1
structure D to a real number ˆ.D /,(cid:0) which is the potential associated with data
i i
structure D . The amortized cost c of the ith operation with respect to potential
i i
y
functionˆisdefinedby
c c ˆ.D / ˆ.D /: (17.2)
i i i i 1
y D C (cid:0) (cid:0)
Theamortized costofeachoperation istherefore itsactualcostplusthechange in
potentialduetotheoperation. Byequation(17.2),thetotalamortizedcostofthen
operations is
n n
c .c ˆ.D / ˆ.D //
i i i i 1
y D C (cid:0) (cid:0)
i 1 i 1
XD XD
n
c ˆ.D / ˆ.D /: (17.3)
i n 0
D C (cid:0)
i 1
XD
The second equality follows from equation (A.9) because the ˆ.D / terms tele-
i
scope.
If we can define a potential function ˆ so that ˆ.D / ˆ.D /, then the total
n 0
amortized cost n c gives an upper bound on the to tal actual cost n c .
i 1yi i 1 i
D D
P P
460 Chapter17 AmortizedAnalysis
In practice, we do not always know how many operations might be performed.
Therefore, if we require that ˆ.D / ˆ.D / for all i, then we guarantee, as in
i 0

the accounting method, that we pay in advance. We usually just define ˆ.D / to
0
be0andthenshowthatˆ.D / 0foralli. (SeeExercise17.3-1foraneasyway
i

tohandlecasesinwhichˆ.D / 0.)
0
¤
Intuitively, if the potential difference ˆ.D / ˆ.D / of the ith operation is
i i 1
positive, then the amortized cost c represents a(cid:0) n overch(cid:0) arge to the ith operation,
i
y
and the potential of the data structure increases. If the potential difference is neg-
ative, then the amortized cost represents an undercharge to the ith operation, and
thedecrease inthepotential paysfortheactualcostoftheoperation.
Theamortizedcostsdefinedbyequations(17.2)and(17.3)dependonthechoice
ofthepotentialfunctionˆ. Differentpotentialfunctionsmayyielddifferentamor-
tized costs yet still be upper bounds on the actual costs. We often find trade-offs
that we can make in choosing a potential function; the best potential function to
usedepends onthedesired timebounds.
Stackoperations
Toillustratethepotentialmethod,wereturnonceagaintotheexampleofthestack
operations PUSH, POP, and MULTIPOP. We define the potential function ˆ on a
stack tobethenumberofobjects inthestack. FortheemptystackD withwhich
0
we start, we have ˆ.D / 0. Since the number of objects in the stack is never
0
D
negative,thestackD thatresultsaftertheithoperationhasnonnegativepotential,
i
andthus
ˆ.D / 0
i

ˆ.D /:
0
D
The total amortized cost of n operations with respect to ˆtherefore represents an
upperboundontheactualcost.
Letusnowcomputetheamortizedcostsofthevariousstackoperations. Iftheith
operation on a stack containing s objects is a PUSH operation, then the potential
difference is
ˆ.D / ˆ.D / .s 1/ s
i i 1
(cid:0) (cid:0) D C (cid:0)
1:
D
Byequation (17.2),theamortizedcostofthis PUSH operation is
c c ˆ.D / ˆ.D /
i i i i 1
y D C (cid:0) (cid:0)
1 1
D C
2:
D
17.3 Thepotentialmethod 461
Suppose that the ith operation on the stack is MULTIPOP.S;k/, which causes
k min.k;s/ objects to be popped off the stack. The actual cost of the opera-
0
D
tionisk ,andthepotential difference is
0
ˆ.D / ˆ.D / k :
i i 1 0
(cid:0) (cid:0) D(cid:0)
Thus,theamortizedcostofthe MULTIPOP operation is
c c ˆ.D / ˆ.D /
i i i i 1
y D C (cid:0) (cid:0)
k k
0 0
D (cid:0)
0:
D
Similarly,theamortizedcostofanordinary POPoperation is0.
The amortized cost of each of the three operations is O.1/, and thus the total
amortizedcostofasequenceofnoperationsisO.n/. Sincewehavealreadyargued
that ˆ.D / ˆ.D /, the total amortized cost of n operations is an upper bound
i 0

onthetotalactualcost. Theworst-casecostofnoperations istherefore O.n/.
Incrementingabinarycounter
Asanotherexampleofthepotentialmethod,weagainlookatincrementingabinary
counter. Thistime,wedefinethepotentialofthecounteraftertheithINCREMENT
operation tobeb ,thenumberof1sinthecounteraftertheithoperation.
i
Let us compute the amortized cost of an INCREMENT operation. Suppose that
the ith INCREMENT operation resets t
i
bits. The actual cost of the operation is
therefore at most t 1, since in addition to resetting t bits, it sets at most one
i i
C
bit to 1. If b 0, then the ith operation resets all k bits, and so b t k.
i i 1 i
If b > 0, thenD b b t 1. In either case, b b t (cid:0) D 1, andD the
i i i 1 i i i 1 i
D (cid:0) (cid:0) C  (cid:0) (cid:0) C
potential difference is
ˆ.D / ˆ.D / .b t 1/ b
i i 1 i 1 i i 1
(cid:0) (cid:0)  (cid:0) (cid:0) C (cid:0) (cid:0)
1 t :
i
D (cid:0)
Theamortizedcostistherefore
c c ˆ.D / ˆ.D /
i i i i 1
y D C (cid:0) (cid:0)
.t 1/ .1 t /
i i
 C C (cid:0)
2:
D
Ifthecounter starts atzero, then ˆ.D / 0. Since ˆ.D / 0for alli,the total
0 i
D 
amortizedcostofasequenceofnINCREMENToperationsisanupperboundonthe
totalactualcost,andsotheworst-case costofn INCREMENT operations isO.n/.
The potential method gives us an easy way to analyze the counter even when
it does not start at zero. The counter starts with b
0
1s, and after n INCREMENT
462 Chapter17 AmortizedAnalysis
operations ithasb 1s, where0 b ;b k. (Recallthatk isthenumberofbits
n 0 n
 
inthecounter.) Wecanrewriteequation (17.3)as
n n
c c ˆ.D / ˆ.D /: (17.4)
i i n 0
D y (cid:0) C
i 1 i 1
XD XD
Wehave c 2forall 1 i n. Since ˆ.D / b and ˆ.D / b , thetotal
i 0 0 n n
y    D D
actualcostofnINCREMENT operations is
n n
c 2 b b
i n 0
 (cid:0) C
i 1 i 1
XD XD
2n b b :
n 0
D (cid:0) C
Note in particular that since b k, as long as k O.n/, the total actual cost
0
 D
isO.n/. Inother words,ifweexecuteatleastn .k/ INCREMENT operations,
D
thetotalactualcostisO.n/,nomatterwhatinitialvaluethecountercontains.
Exercises
17.3-1
Suppose we have a potential function ˆ such that ˆ.D / ˆ.D / for all i, but
i 0

ˆ.D / 0. Showthatthere exists apotential function ˆ suchthat ˆ.D / 0,
0 0 0 0
¤ D
ˆ.D / 0 for all i 1, and the amortized costs using ˆ are the same as the
0 i 0
 
amortized costsusingˆ.
17.3-2
RedoExercise17.1-3usingapotential methodofanalysis.
17.3-3
Consider an ordinary binary min-heap data structure with n elements supporting
the instructions INSERT and EXTRACT-MIN in O.lgn/ worst-case time. Give a
potential function ˆ such that the amortized cost of INSERT is O.lgn/ and the
amortized costofEXTRACT-MIN isO.1/,andshowthatitworks.
17.3-4
What is the total cost of executing n of the stack operations PUSH, POP, and
MULTIPOP, assuming that the stack begins with s
0
objects and finishes with s
n
objects?
17.3-5
Suppose that a counter begins at a number with b 1s in its binary representa-
tion, rather than at 0. Show that the cost of performing n INCREMENT operations
isO.n/ifn .b/. (Donotassumethatb isconstant.)
D
17.4 Dynamictables 463
17.3-6
Showhowtoimplementaqueuewithtwoordinarystacks(Exercise10.1-6)sothat
theamortizedcostofeach ENQUEUE andeach DEQUEUE operation isO.1/.
17.3-7
Design a data structure to support the following two operations for a dynamic
multisetS ofintegers, whichallowsduplicate values:
INSERT.S;x/insertsx intoS.
DELETE-LARGER-HALF.S/deletesthelargest S =2 elementsfromS.
dj j e
Explain how to implement this data structure so that any sequence of m INSERT
andDELETE-LARGER-HALF operationsrunsinO.m/time. Yourimplementation
shouldalsoincludeawaytooutputtheelementsofS inO. S /time.
j j
17.4 Dynamictables
Wedonotalwaysknowinadvancehowmanyobjectssomeapplications willstore
in a table. We might allocate space for a table, only to find out later that it is not
enough. We must then reallocate the table with a larger size and copy all objects
storedintheoriginaltableoverintothenew,largertable. Similarly,ifmanyobjects
havebeendeleted fromthetable,itmaybeworthwhiletoreallocate thetablewith
asmallersize. Inthissection,westudythisproblemofdynamicallyexpandingand
contractingatable. Usingamortizedanalysis,weshallshowthattheamortizedcost
ofinsertion anddeletion isonly O.1/, eventhough theactual cost ofanoperation
islargewhenittriggersanexpansionoracontraction. Moreover,weshallseehow
to guarantee that the unused space in a dynamic table never exceeds a constant
fractionofthetotalspace.
We assume that the dynamic table supports the operations TABLE-INSERT and
TABLE-DELETE. TABLE-INSERT insertsintothetableanitemthatoccupiesasin-
gleslot, thatis, aspace forone item. Likewise, TABLE-DELETE removes anitem
from the table, thereby freeing a slot. The details of the data-structuring method
used to organize the table are unimportant; we might use a stack (Section 10.1),
a heap (Chapter 6), or a hash table (Chapter 11). We might also use an array or
collection ofarraystoimplementobjectstorage, aswedidinSection10.3.
Weshallfinditconvenienttouseaconceptintroducedinouranalysisofhashing
(Chapter 11). We define the load factor ˛.T/ of a nonempty table T to be the
number of items stored in the table divided by the size (number of slots) of the
table. Weassign anempty table (one withnoitems) size 0, and wedefine itsload
factortobe1. Iftheloadfactorofadynamictableisboundedbelowbyaconstant,
464 Chapter17 AmortizedAnalysis
the unused space in the table is never more than a constant fraction of the total
amountofspace.
We start by analyzing a dynamic table in which we only insert items. We then
consider themoregeneral caseinwhichwebothinsertanddeleteitems.
17.4.1 Tableexpansion
Letusassume thatstorage foratable isallocated asanarray ofslots. Atable fills
upwhenallslotshavebeenusedor,equivalently,whenitsloadfactoris1.1 Insome
softwareenvironments, uponattemptingtoinsertanitemintoafulltable,theonly
alternative is to abort with an error. We shall assume, however, that our software
environment,likemanymodernones,providesamemory-managementsystemthat
can allocate and free blocks of storage on request. Thus, upon inserting an item
intoafulltable,wecanexpandthetablebyallocatinganewtablewithmoreslots
than the old table had. Because we always need the table to reside in contiguous
memory,wemustallocateanewarrayforthelargertableandthencopyitemsfrom
theoldtableintothenewtable.
A common heuristic allocates a new table with twice as many slots as the old
one. If the only table operations are insertions, then the load factor of the table is
always at least 1=2, and thus the amount of wasted space never exceeds half the
totalspaceinthetable.
In the following pseudocode, we assume that T is an object representing the
table. Theattribute T:table contains apointer tothe block ofstorage representing
thetable,T:numcontainsthenumberofitemsinthetable,andT:sizegivesthetotal
numberofslotsinthetable. Initially, thetableisempty: T:num T:size 0.
D D
TABLE-INSERT.T;x/
1 ifT:size ==0
2 allocate T:tablewith1slot
3 T:size 1
D
4 ifT:num ==T:size
5 allocate new-tablewith2 T:sizeslots

6 insertallitemsinT:tableintonew-table
7 freeT:table
8 T:table new-table
D
9 T:size 2 T:size
D 
10 insertx intoT:table
11 T:num T:num 1
D C
1Insomesituations,suchasanopen-addresshashtable,wemaywishtoconsideratabletobefullif
itsloadfactorequalssomeconstantstrictlylessthan1.(SeeExercise17.4-1.)
17.4 Dynamictables 465
Notice that we have two “insertion” procedures here: the TABLE-INSERT proce-
dure itself and the elementary insertion into a table in lines 6 and 10. We can
analyze therunning timeof TABLE-INSERT intermsofthenumberofelementary
insertions by assigning a cost of 1 to each elementary insertion. We assume that
theactual running timeof TABLE-INSERT islinear inthetimetoinsert individual
items, so that the overhead for allocating an initial table in line 2 is constant and
the overhead for allocating and freeing storage in lines 5 and 7 is dominated by
the cost of transferring items in line 6. We call the event in which lines 5–9 are
executedanexpansion.
Letusanalyze asequence ofnTABLE-INSERT operations onaninitially empty
table. Whatisthecostc oftheithoperation? Ifthecurrenttablehasroomforthe
i
newitem(orifthisisthefirstoperation), thenc 1,sinceweneedonlyperform
i
D
theoneelementary insertion inline10. Ifthecurrenttableisfull,however,andan
expansion occurs, thenc i: thecostis1fortheelementary insertion inline 10
i
D
plus i 1 for the items that we must copy from the old table to the new table in
(cid:0)
line 6. If we perform n operations, the worst-case cost of an operation is O.n/,
whichleadstoanupperboundofO.n2/onthetotalrunningtimefornoperations.
This bound is not tight, because we rarely expand the table in the course of n
TABLE-INSERT operations. Specifically, the ith operation causes an expansion
only when i 1 is an exact power of 2. The amortized cost of an operation is in
(cid:0)
fact O.1/, as wecan show using aggregate analysis. Thecost ofthe ithoperation
is
i ifi 1isanexactpowerof2;
c (cid:0)
i
D
(
1 otherwise:
ThetotalcostofnTABLE-INSERT operations istherefore
n lgn
b c
c n 2j
i
 C
i 1 j 0
XD XD
< n 2n
C
3n;
D
becauseatmostnoperationscost1andthecostsoftheremainingoperations form
ageometricseries. Sincethetotalcostofn TABLE-INSERT operations isbounded
by3n,theamortizedcostofasingleoperation isatmost3.
By using the accounting method, we can gain some feeling for why the amor-
tized cost of a TABLE-INSERT operation should be 3. Intuitively, each item pays
for 3 elementary insertions: inserting itself into the current table, moving itself
when the table expands, and moving another item that has already been moved
once when thetable expands. Forexample, suppose that the size ofthe table is m
immediately after an expansion. Then the table holds m=2 items, and it contains
466 Chapter17 AmortizedAnalysis
no credit. We charge 3 dollars for each insertion. The elementary insertion that
occurs immediately costs 1 dollar. We place another dollar as credit on the item
inserted. Weplacethethirddollarascreditononeofthem=2itemsalready inthe
table. The table will not fill again until we have inserted another m=2 1 items,
(cid:0)
and thus, by the time the table contains m items and isfull, wewillhave placed a
dollaroneachitemtopaytoreinsert itduringtheexpansion.
We can use the potential method to analyze a sequence of n TABLE-INSERT
operations, and we shall use it in Section 17.4.2 to design a TABLE-DELETE op-
eration that has an O.1/ amortized cost as well. We start by defining a potential
function ˆthat is0immediately after an expansion but builds tothe table size by
thetimethetableisfull,sothatwecanpayforthenextexpansionbythepotential.
Thefunction
ˆ.T/ 2 T:num T:size (17.5)
D  (cid:0)
is one possibility. Immediately after an expansion, we have T:num T:size=2,
D
and thus ˆ.T/ 0, as desired. Immediately before an expansion, we have
D
T:num T:size, and thus ˆ.T/ T:num, as desired. The initial value of the
D D
potential is 0, and since the table is always at least half full, T:num T:size=2,

which implies that ˆ.T/ is always nonnegative. Thus, the sum of the amortized
costsofnTABLE-INSERT operationsgivesanupperboundonthesumoftheactual
costs.
Toanalyze theamortized costoftheith TABLE-INSERT operation, weletnum
i
denote the number ofitems stored inthe table after the ithoperation, size denote
i
the total size of the table after the ith operation, and ˆ denote the potential after
i
theithoperation. Initially, wehavenum 0,size 0,andˆ 0.
0 0 0
D D D
IftheithTABLE-INSERT operation doesnottriggeranexpansion, thenwehave
size size andtheamortized costoftheoperation is
i i 1
D (cid:0)
c c ˆ ˆ
i i i i 1
y D C (cid:0) (cid:0)
1 .2 num size / .2 num size /
i i i 1 i 1
D C  (cid:0) (cid:0)  (cid:0) (cid:0) (cid:0)
1 .2 num size / .2.num 1/ size /
i i i i
D C  (cid:0) (cid:0) (cid:0) (cid:0)
3:
D
Iftheithoperation doestrigger anexpansion, thenwehavesize 2 size and
i i 1
size num num 1, which implies that size 2 .nD um  1/.(cid:0) Thus,
i 1 i 1 i i i
(cid:0) D (cid:0) D (cid:0) D  (cid:0)
theamortizedcostoftheoperation is
c c ˆ ˆ
i i i i 1
y D C (cid:0) (cid:0)
num .2 num size / .2 num size /
i i i i 1 i 1
D C  (cid:0) (cid:0)  (cid:0) (cid:0) (cid:0)
num .2 num 2 .num 1// .2.num 1/ .num 1//
i i i i i
D C  (cid:0)  (cid:0) (cid:0) (cid:0) (cid:0) (cid:0)
num 2 .num 1/
i i
D C (cid:0) (cid:0)
3:
D
17.4 Dynamictables 467
32
size num
i i
24
16
F
i
8
0 i
0 8 16 24 32
Figure17.3 TheeffectofasequenceofnTABLE-INSERToperationsonthenumbernumi ofitems
inthetable, thenumber sizei of slotsinthetable, and thepotential ˆ i 2 numi sizei, each
D  (cid:0)
beingmeasuredaftertheithoperation. Thethinlineshowsnumi,thedashedlineshowssizei,and
thethicklineshowsˆ i. Noticethatimmediatelybeforeanexpansion,thepotentialhasbuiltupto
thenumberofitemsinthetable,andthereforeitcanpayformovingalltheitemstothenewtable.
Afterwards,thepotentialdropsto0,butitisimmediatelyincreasedby2uponinsertingtheitemthat
causedtheexpansion.
Figure 17.3 plots the values of num , size , and ˆ against i. Notice how the
i i i
potential buildstopayforexpanding thetable.
17.4.2 Tableexpansionandcontraction
ToimplementaTABLE-DELETE operation,itissimpleenoughtoremovethespec-
ified item from the table. In order to limit the amount of wasted space, however,
wemightwishtocontractthetablewhentheloadfactorbecomestoosmall. Table
contraction isanalogoustotableexpansion: whenthenumberofitemsinthetable
drops too low, we allocate a new, smaller table and then copy the items from the
oldtableintothenewone. Wecanthenfreethestoragefortheoldtablebyreturn-
ing it to the memory-management system. Ideally, wewould like to preserve two
properties:
 the load factor of the dynamic table is bounded below by a positive constant,
and
 theamortizedcostofatableoperation isbounded abovebyaconstant.
468 Chapter17 AmortizedAnalysis
We assume that we measure the cost in terms of elementary insertions and dele-
tions.
Youmightthinkthatweshoulddoublethetablesizeuponinsertinganiteminto
a full table and halve the size when a deleting an item would cause the table to
become less than half full. This strategy would guarantee that the load factor of
thetableneverdropsbelow1=2,butunfortunately, itcancausetheamortized cost
of an operation to be quite large. Consider the following scenario. We perform n
operationsonatableT,wherenisanexactpowerof2. Thefirstn=2operationsare
insertions, which by our previous analysis cost a total of ‚.n/. At the end of this
sequence of insertions, T:num T:size n=2. For the second n=2 operations,
D D
weperform thefollowingsequence:
insert,delete, delete,insert, insert,delete, delete,insert, insert,....
Thefirstinsertioncausesthetabletoexpandtosizen. Thetwofollowingdeletions
cause the table to contract back to size n=2. Twofurther insertions cause another
expansion, and so forth. The cost of each expansion and contraction is ‚.n/, and
there are ‚.n/ of them. Thus, the total cost of the n operations is ‚.n2/, making
theamortizedcostofanoperation ‚.n/.
The downside of this strategy is obvious: after expanding the table, we do not
delete enough itemstopayforacontraction. Likewise, aftercontracting thetable,
wedonotinsertenough itemstopayforanexpansion.
We can improve upon this strategy by allowing the load factor of the table to
drop below 1=2. Specifically, we continue to double the table size upon inserting
an item into a full table, but we halve the table size when deleting an item causes
the table to become less than 1=4 full, rather than 1=2 full as before. The load
factorofthetableistherefore bounded belowbytheconstant 1=4.
Intuitively, we would consider a load factor of 1=2 to be ideal, and the table’s
potential would then be 0. As the load factor deviates from 1=2, the potential
increasessothatbythetimeweexpandorcontractthetable,thetablehasgarnered
sufficient potential to pay for copying all the items into the newly allocated table.
Thus, we will need a potential function that has grown to T:num by the time that
theloadfactorhaseitherincreasedto1ordecreasedto1=4. Aftereitherexpanding
or contracting the table, the load factor goes back to 1=2 and the table’s potential
reduces backto0.
Weomitthecodefor TABLE-DELETE, sinceitisanalogous to TABLE-INSERT.
For our analysis, we shall assume that whenever the number of items in the table
dropsto0,wefreethestorageforthetable. Thatis,ifT:num 0,thenT:size 0.
D D
We can now use the potential method to analyze the cost of a sequence of n
TABLE-INSERT and TABLE-DELETE operations. We start by defining a poten-
tial function ˆ that is 0 immediately after an expansion or contraction and builds
as the load factor increases to 1 or decreases to 1=4. Let us denote the load fac-
17.4 Dynamictables 469
32
24
size
i
16
num
i
8
F
i
0 i
0 8 16 24 32 40 48
Figure17.4 TheeffectofasequenceofnTABLE-INSERTandTABLE-DELETEoperationsonthe
numbernumi ofitemsinthetable,thenumbersizei ofslotsinthetable,andthepotential
2 numi sizei if˛ i 1=2;
ˆ i  (cid:0) 
D sizei=2 numi if˛ i <1=2;
 (cid:0)
eachmeasuredaftertheithoperation. Thethinlineshowsnumi,thedashedlineshowssizei,and
thethicklineshowsˆ i. Noticethatimmediatelybeforeanexpansion,thepotentialhasbuiltupto
thenumberofitemsinthetable,andthereforeitcanpayformovingalltheitemstothenewtable.
Likewise,immediatelybeforeacontraction,thepotentialhasbuiltuptothenumberofitemsinthe
table.
tor of a nonempty table T by ˛.T/ T:num=T:size. Since for an empty table,
D
T:num T:size 0 and ˛.T/ 1, we always have T:num ˛.T/ T:size,
D D D D 
whetherthetableisemptyornot. Weshalluseasourpotential function
2 T:num T:size if˛.T/ 1=2;
ˆ.T/  (cid:0)  (17.6)
D
(
T:size=2 T:num if˛.T/ < 1=2:
(cid:0)
Observe that the potential of an empty table is 0 and that the potential is never
negative. Thus, the total amortized cost of a sequence of operations with respect
toˆprovides anupperboundontheactualcostofthesequence.
Beforeproceeding withapreciseanalysis, wepausetoobservesomeproperties
of the potential function, as illustrated in Figure 17.4. Notice that when the load
factoris1=2,thepotentialis0. Whentheloadfactoris1,wehaveT:size T:num,
D
which implies ˆ.T/ T:num, and thus the potential can pay for an expansion if
D
anitemisinserted. Whentheloadfactoris1=4,wehaveT:size 4 T:num,which
D 
470 Chapter17 AmortizedAnalysis
impliesˆ.T/ T:num,andthusthepotential canpayforacontraction ifanitem
D
isdeleted.
To analyze a sequence of n TABLE-INSERT and TABLE-DELETE operations,
we let c denote the actual cost of the ith operation, c denote its amortized cost
i i
y
withrespecttoˆ,num denotethenumberofitemsstoredinthetableaftertheith
i
operation, size denote the total size of the table after the ith operation, ˛ denote
i i
theloadfactorofthetableaftertheithoperation, andˆ denotethepotentialafter
i
theithoperation. Initially, num 0,size 0,˛ 1,andˆ 0.
0 0 0 0
D D D D
Westartwiththecaseinwhichtheithoperation is TABLE-INSERT. Theanaly-
sisisidenticaltothatfortableexpansioninSection17.4.1if˛ 1=2. Whether
i 1
the table expands or not, the amortized cost c of the opera(cid:0) tio n is at most 3.
i
y
If ˛ < 1=2, the table cannot expand as a result of the operation, since the ta-
i 1
ble e(cid:0) xpands only when ˛ 1. If ˛ < 1=2 as well, then the amortized cost of
i 1 i
theithoperation is (cid:0) D
c c ˆ ˆ
i i i i 1
y D C (cid:0) (cid:0)
1 .size =2 num / .size =2 num /
i i i 1 i 1
D C (cid:0) (cid:0) (cid:0) (cid:0) (cid:0)
1 .size =2 num / .size =2 .num 1//
i i i i
D C (cid:0) (cid:0) (cid:0) (cid:0)
0:
D
If˛ <1=2but˛ 1=2,then
i 1 i
(cid:0) 
c c ˆ ˆ
i i i i 1
y D C (cid:0) (cid:0)
1 .2 num size / .size =2 num /
i i i 1 i 1
D C  (cid:0) (cid:0) (cid:0) (cid:0) (cid:0)
1 .2.num 1/ size / .size =2 num /
i 1 i 1 i 1 i 1
D C (cid:0) C (cid:0) (cid:0) (cid:0) (cid:0) (cid:0) (cid:0)
3
3 num size 3
i 1 i 1
D  (cid:0) (cid:0) 2 (cid:0) C
3
3˛ size size 3
i 1 i 1 i 1
D (cid:0) (cid:0) (cid:0) 2 (cid:0) C
3 3
< size size 3
i 1 i 1
2 (cid:0) (cid:0) 2 (cid:0) C
3:
D
Thus,theamortized costofaTABLE-INSERT operation isatmost3.
We now turn to the case in which the ith operation is TABLE-DELETE. In this
case, num num 1. If ˛ < 1=2, then we must consider whether the
i i 1 i 1
D (cid:0) (cid:0) (cid:0)
operation causes the table to contract. If it does not, then size size and the
i i 1
D (cid:0)
amortized costoftheoperation is
c c ˆ ˆ
i i i i 1
y D C (cid:0) (cid:0)
1 .size =2 num / .size =2 num /
i i i 1 i 1
D C (cid:0) (cid:0) (cid:0) (cid:0) (cid:0)
1 .size =2 num / .size =2 .num 1//
i i i i
D C (cid:0) (cid:0) (cid:0) C
2:
D
17.4 Dynamictables 471
If˛ < 1=2andtheithoperation doestriggeracontraction, thentheactualcost
i 1
ofth(cid:0) eoperationisc num 1,sincewedeleteoneitemandmovenum items.
i i i
D C
We have size =2 size =4 num num 1, and the amortized cost of
i i 1 i 1 i
D (cid:0) D (cid:0) D C
theoperation is
c c ˆ ˆ
i i i i 1
y D C (cid:0) (cid:0)
.num 1/ .size =2 num / .size =2 num /
i i i i 1 i 1
D C C (cid:0) (cid:0) (cid:0) (cid:0) (cid:0)
.num 1/ ..num 1/ num / ..2 num 2/ .num 1//
i i i i i
D C C C (cid:0) (cid:0)  C (cid:0) C
1:
D
When the ith operation is a TABLE-DELETE and ˛
i 1
1=2, the amortized cost
(cid:0) 
isalsobounded abovebyaconstant. WeleavetheanalysisasExercise17.4-2.
In summary, since the amortized cost of each operation is bounded above by
a constant, the actual time for any sequence of n operations on a dynamic table
isO.n/.
Exercises
17.4-1
Suppose that we wish to implement a dynamic, open-address hash table. Why
might we consider the table to be full when its load factor reaches some value ˛
thatisstrictly lessthan 1? Describe brieflyhowtomakeinsertion intoadynamic,
open-address hashtableruninsuchawaythattheexpectedvalueoftheamortized
costperinsertionisO.1/. Whyistheexpectedvalueoftheactualcostperinsertion
notnecessarily O.1/forallinsertions?
17.4-2
Show that if ˛
i 1
1=2 and the ith operation on a dynamic table is TABLE-
(cid:0) 
DELETE, then the amortized cost of the operation with respect to the potential
function(17.6)isbounded abovebyaconstant.
17.4-3
Suppose that instead of contracting atable by halving its sizewhen itsload factor
dropsbelow1=4,wecontractitbymultiplying itssizeby2=3whenitsloadfactor
dropsbelow1=3. Usingthepotential function
ˆ.T/ 2 T:num T:size ;
D j  (cid:0) j
showthattheamortizedcostofaTABLE-DELETE thatusesthisstrategyisbounded
abovebyaconstant.
472 Chapter17 AmortizedAnalysis
Problems
17-1 Bit-reversed binarycounter
Chapter 30 examines an important algorithm called the fast Fourier transform,
orFFT. ThefirststepoftheFFTalgorithmperformsabit-reversalpermutationon
aninputarrayAŒ0::n 1whoselengthisn 2k forsomenonnegativeintegerk.
(cid:0) D
This permutation swaps elements whose indices have binary representations that
arethereverseofeachother.
We can express each index a as a k-bit sequence a ;a ;:::;a , where
k 1 k 2 0
a
D
k i(cid:0)01a
i
2i. Wedefine h (cid:0) (cid:0) i
D
rev k.Pa
k
1;a
k
2;:::;a
0
/ a 0;a 1;:::;a
k 1
h (cid:0) (cid:0) i D h (cid:0) iI
thus,
k 1
(cid:0)
rev .a/ a 2i :
k k i 1
D (cid:0) (cid:0)
i 0
XD
For example, if n 16 (or, equivalently, k 4), then rev .3/ 12, since
k
D D D
the 4-bit representation of 3 is 0011, which when reversed gives 1100, the 4-bit
representation of12.
a. Givenafunctionrev thatrunsin‚.k/time,writeanalgorithmtoperformthe
k
bit-reversal permutation onanarrayoflengthn 2k inO.nk/time.
D
We can use an algorithm based on an amortized analysis to improve the running
time of the bit-reversal permutation. We maintain a “bit-reversed counter” and a
procedure BIT-REVERSED-INCREMENT that, when given a bit-reversed-counter
value a, produces rev .rev .a/ 1/. If k 4, for example, and the bit-reversed
k k
C D
counter starts at0, then successive calls to BIT-REVERSED-INCREMENT produce
thesequence
0000;1000;0100;1100;0010;1010;::: 0;8;4;12;2;10;::: :
D
b. Assumethatthewordsinyourcomputerstorek-bitvaluesandthatinunittime,
yourcomputercanmanipulatethebinaryvalueswithoperationssuchasshifting
left or right by arbitrary amounts, bitwise-AND, bitwise-OR, etc. Describe
an implementation of the BIT-REVERSED-INCREMENT procedure that allows
the bit-reversal permutation on an n-element array to be performed in a total
ofO.n/time.
c. Supposethatyoucanshiftawordleftorrightbyonlyonebitinunittime. Isit
stillpossible toimplementanO.n/-timebit-reversal permutation?
ProblemsforChapter17 473
17-2 Makingbinarysearchdynamic
Binarysearchofasortedarraytakeslogarithmicsearchtime,butthetimetoinsert
a new element is linear in the size of the array. We can improve the time for
insertion bykeeping severalsortedarrays.
Specifically, suppose that we wish to support SEARCH and INSERT on a set
of n elements. Let k lg.n 1/ , and let the binary representation of n
D d C e
be n ; n ; :::; n . We have k sorted arrays A ;A ;:::;A , where for
k 1 k 2 0 0 1 k 1
i h 0;(cid:0)1;:::;(cid:0)k 1,thei lengthofarrayA is2i. Eacharrayiseither(cid:0) fullorempty,
i
D (cid:0)
depending on whether n 1 or n 0, respectively. The total number of ele-
i i
D D
ments held in all k arrays is therefore k i(cid:0)01n
i
2i
D
n. Although each individual
array is sorted, elements in different arrayDs bear no particular relationship to each
P
other.
a. DescribehowtoperformtheSEARCHoperationforthisdatastructure. Analyze
itsworst-caserunning time.
b. Describe how to perform the INSERT operation. Analyze its worst-case and
amortized runningtimes.
c. Discusshowtoimplement DELETE.
17-3 Amortizedweight-balanced trees
Consider an ordinary binary search tree augmented by adding to each node x the
attribute x:size giving the number of keys stored inthe subtree rooted atx. Let˛
beaconstantintherange1=2 ˛ <1. Wesaythatagivennodex is˛-balanced

if x:left:size ˛ x:size and x:right:size ˛ x:size. The tree as a whole
   
is ˛-balanced if every node in the tree is ˛-balanced. The following amortized
approach tomaintaining weight-balanced treeswassuggested byG.Varghese.
a. A 1=2-balanced tree is, in a sense, as balanced as it can be. Given a node x
in an arbitrary binary search tree, show how to rebuild the subtree rooted at x
sothatitbecomes1=2-balanced. Youralgorithmshouldrunintime‚.x:size/,
anditcanuseO.x:size/auxiliary storage.
b. Show that performing a search in an n-node ˛-balanced binary search tree
takesO.lgn/worst-casetime.
For the remainder of this problem, assume that the constant ˛ is strictly greater
than1=2. Supposethatweimplement INSERT andDELETE asusualforann-node
binary search tree, except that after every such operation, if any node in the tree
is no longer ˛-balanced, then we “rebuild” the subtree rooted at the highest such
nodeinthetreesothatitbecomes1=2-balanced.
474 Chapter17 AmortizedAnalysis
Weshallanalyzethisrebuildingschemeusingthepotentialmethod. Foranodex
inabinarysearchtreeT,wedefine
.x/ x:left:size x:right:size ;
D j (cid:0) j
andwedefinethepotential ofT as
ˆ.T/ c .x/;
D
x T .x/ 2
2 XW 
wherec isasufficientlylargeconstant thatdepends on˛.
c. Argue that any binary search tree has nonnegative potential and that a 1=2-
balanced treehaspotential0.
d. Suppose that m units of potential can pay for rebuilding an m-node subtree.
How large must c be in terms of ˛ in order for it to take O.1/ amortized time
torebuild asubtreethatisnot˛-balanced?
e. Show that inserting anode into or deleting a node from an n-node ˛-balanced
treecostsO.lgn/amortized time.
17-4 Thecostofrestructuringred-black trees
There are four basic operations on red-black trees that perform structural modi-
fications: node insertions, node deletions, rotations, and color changes. We have
seen that RB-INSERT and RB-DELETE use only O.1/ rotations, node insertions,
and node deletions to maintain the red-black properties, but they maymake many
morecolorchanges.
a. Describe a legal red-black tree with n nodes such that calling RB-INSERT to
add the .n 1/st node causes .lgn/ color changes. Then describe a legal
C
red-blacktreewithnnodesforwhichcallingRB-DELETE onaparticularnode
causes.lgn/colorchanges.
Althoughtheworst-casenumberofcolorchangesperoperationcanbelogarithmic,
weshallprovethatanysequenceofmRB-INSERT andRB-DELETE operationson
aninitiallyemptyred-blacktreecausesO.m/structuralmodificationsintheworst
case. Notethatwecounteachcolorchange asastructural modification.
b. Some of the cases handled by the main loop of the code of both RB-INSERT-
FIXUPandRB-DELETE-FIXUP areterminating: onceencountered,theycause
thelooptoterminateafteraconstantnumberofadditionaloperations. Foreach
ofthecasesofRB-INSERT-FIXUP andRB-DELETE-FIXUP, specifywhichare
terminating andwhicharenot. (Hint:LookatFigures13.5,13.6,and13.7.)
ProblemsforChapter17 475
We shall first analyze the structural modifications when only insertions are per-
formed. LetT beared-black tree,anddefineˆ.T/tobethenumberofrednodes
inT. Assume that 1unit ofpotential can pay for the structural modifications per-
formedbyanyofthethreecasesofRB-INSERT-FIXUP.
c. LetT
0
betheresultofapplyingCase1ofRB-INSERT-FIXUP toT. Arguethat
ˆ.T / ˆ.T/ 1.
0
D (cid:0)
d. When we insert a node into a red-black tree using RB-INSERT, we can break
the operation into three parts. List the structural modifications and potential
changes resulting from lines 1–16 of RB-INSERT, from nonterminating cases
of RB-INSERT-FIXUP, andfromterminating casesof RB-INSERT-FIXUP.
e. Usingpart(d),arguethattheamortizednumberofstructuralmodificationsper-
formedbyanycallofRB-INSERT isO.1/.
WenowwishtoprovethatthereareO.m/structural modifications whenthereare
bothinsertions anddeletions. Letusdefine,foreachnodex,
0 ifx isred;
1 ifx isblackandhasnoredchildren ;
w.x/
D 0 ifx isblackandhasoneredchild;
„
2 ifx isblackandhastworedchildren:
Nowweredefinethepotential ofared-black treeT as
ˆ.T/ w.x/;
D
x T
X2
and let T be the tree that results from applying any nonterminating case of RB-
0
INSERT-FIXUP orRB-DELETE-FIXUP toT.
f. Show that ˆ.T 0/ ˆ.T/ 1 for all nonterminating cases of RB-INSERT-
 (cid:0)
FIXUP. Arguethattheamortizednumberofstructuralmodificationsperformed
byanycallofRB-INSERT-FIXUP isO.1/.
g. Show that ˆ.T 0/ ˆ.T/ 1 for all nonterminating cases of RB-DELETE-
 (cid:0)
FIXUP. Arguethattheamortizednumberofstructuralmodificationsperformed
byanycallofRB-DELETE-FIXUP isO.1/.
h. Complete theproof thatintheworstcase, anysequence ofm RB-INSERT and
RB-DELETE operations performsO.m/structural modifications.
476 Chapter17 AmortizedAnalysis
17-5 Competitiveanalysisofself-organizing listswithmove-to-front
A self-organizing list is a linked list of n elements, in which each element has a
unique key. Whenwesearch foranelementinthelist,wearegivenakey,andwe
wanttofindanelementwiththatkey.
Aself-organizing listhastwoimportantproperties:
1. To find an element in the list, given its key, we must traverse the list from the
beginning untilweencounter theelementwiththegivenkey. Ifthatelementis
thekthelementfromthestartofthelist,thenthecosttofindtheelementisk.
2. Wemayreorder thelistelements afteranyoperation, according toagivenrule
withagivencost. Wemaychooseanyheuristicweliketodecidehowtoreorder
thelist.
Assume that we start with a given list of n elements, and we are given an access
sequence   ; ;:::; of keys tofind, inorder. Thecost ofthe sequence
1 2 m
D h i
isthesumofthecostsoftheindividual accesses inthesequence.
Outofthevariouspossible waystoreorder thelistafteranoperation, thisprob-
lemfocusesontransposingadjacentlistelements—switching theirpositionsinthe
list—with a unit cost for each transpose operation. You will show, by means of a
potential function, that aparticular heuristic forreordering the list, move-to-front,
entailsatotalcostnoworsethan4timesthatofanyotherheuristicformaintaining
the list order—even if the other heuristic knows the access sequence in advance!
Wecallthistypeofanalysisacompetitive analysis.
ForaheuristicHandagiveninitialorderingofthelist,denotetheaccesscostof
sequence  byC ./. Letmbethenumberofaccesses in.
H
a. Argue that if heuristic H does not know the access sequence in advance, then
theworst-casecostforHonanaccesssequence isC ./ .mn/.
H
D
Withthemove-to-front heuristic, immediately aftersearching foranelementx,
wemovex tothefirstposition onthelist(i.e.,thefrontofthelist).
Letrank .x/denote the rank ofelement x inlist L,that is, the position of x in
L
list L. For example, if x is the fourth element in L, then rank .x/ 4. Let c
L i
D
denote the cost of access  using the move-to-front heuristic, which includes the
i
cost of finding the element in the list and the cost of moving it to the front of the
listbyaseriesoftranspositions ofadjacent listelements.
b. Show that if  accesses element x in list L using the move-to-front heuristic,
i
thenc 2 rank .x/ 1.
i L
D  (cid:0)
Now we compare move-to-front with any other heuristic H that processes an
accesssequenceaccordingtothetwopropertiesabove. HeuristicHmaytranspose
ProblemsforChapter17 477
elements in the list in any way it wants, and it might even know the entire access
sequence inadvance.
LetL bethelistafteraccess usingmove-to-front, andletL bethelistafter
i i i
access  using heuristic H. We denote the cost of access  by c for move-to-
i i i
frontandbyc forheuristicH. SupposethatheuristicHperformst transpositions
i i
duringaccess .
i
c. In part (b), you showed that c 2 rank .x/ 1. Now show that c
i
D 
Li(cid:0)1
(cid:0)
i
D
rank L i(cid:0)1.x/ Ct i.
We define an inversion in list L as a pair of elements y and ´ such that y
i
precedes´inL and´precedesy inlistL . SupposethatlistL hasq inversions
i i i i
after processing the access sequence  ; ;:::; . Then, we define a potential
1 2 i
h i
functionˆthatmapsL toarealnumberbyˆ.L / 2q . Forexample,ifL has
i i i i
D
the elements e;c;a;d;b and L has the elements c;a;b;d;e , then L has 5
h i
i
h i
i
inversions (.e;c/;.e;a/;.e;d/;.e;b/;.d;b/), and so ˆ.L / 10. Observe that
i
D
ˆ.L / 0 for all i and that, if move-to-front and heuristic H start with the same
i

listL ,thenˆ.L / 0.
0 0
D
d. Argue that a transposition either increases the potential by 2 or decreases the
potential by2.
Suppose that access  finds the element x. To understand how the potential
i
changesdueto ,letuspartitiontheelementsotherthanx intofoursets,depend-
i
ingonwheretheyareinthelistsjustbeforetheithaccess:
 SetAconsistsofelementsthatprecedex inbothL andL .
i 1 i 1
(cid:0) (cid:0)
 SetB consists ofelementsthatprecedex inL andfollowx inL .
i 1 i 1
(cid:0) (cid:0)
 SetC consists ofelementsthatfollowx inL andprecedex inL .
i 1 i 1
(cid:0) (cid:0)
 SetD consists ofelementsthatfollowx inbothL andL .
i 1 i 1
(cid:0) (cid:0)
e. Arguethatrank Li(cid:0)1.x/
D
jA jCjB jC1andrank L i(cid:0)1.x/
D
jA jCjC jC1.
f. Showthataccess causesachangeinpotential of
i
ˆ.L / ˆ.L / 2. A B t /;
i
(cid:0)
i (cid:0)1
 j j(cid:0)j jC
i
where,asbefore, heuristic Hperformst transpositions duringaccess .
i i
Definetheamortizedcostc ofaccess byc c ˆ.L / ˆ.L /.
i i i i i i 1
y y D C (cid:0) (cid:0)
g. Showthattheamortizedcostc ofaccess isboundedfromaboveby4c .
yi i i
h. Conclude that the cost C ./ of access sequence  with move-to-front is at
MTF
most 4 times the cost C ./ of  with any other heuristic H, assuming that
H
bothheuristics startwiththesamelist.
478 Chapter17 AmortizedAnalysis
Chapter notes
Aho, Hopcroft, and Ullman [5] used aggregate analysis to determine the running
time of operations on a disjoint-set forest; weshall analyze this data structure us-
ing the potential method in Chapter 21. Tarjan [331] surveys the accounting and
potential methods of amortized analysis and presents several applications. He at-
tributes the accounting method to several authors, including M. R. Brown, R. E.
Tarjan, S. Huddleston, and K. Mehlhorn. He attributes the potential method to
D.D.Sleator. Theterm“amortized” isduetoD.D.SleatorandR.E.Tarjan.
Potential functions arealsousefulforproving lowerbounds forcertain typesof
problems. For each configuration of the problem, we define a potential function
thatmapstheconfigurationtoarealnumber. Thenwedeterminethepotentialˆ
init
of the initial configuration, the potential ˆ of the final configuration, and the
final
maximum change in potential ˆ due to any step. The number of steps must
max
therefore be at least ˆ ˆ = ˆ . Examples of potential functions to
j final(cid:0) initj j maxj
provelowerboundsinI/OcomplexityappearinworksbyCormen,Sundquist,and
Wisniewski [79]; Floyd [107]; and Aggarwal and Vitter [3]. Krumme, Cybenko,
andVenkataraman[221]appliedpotentialfunctionstoprovelowerboundsongos-
siping: communicating a unique item from each vertex in a graph to every other
vertex.
The move-to-front heuristic from Problem 17-5 works quite well in practice.
Moreover,ifwerecognizethatwhenwefindanelement,wecanspliceitoutofits
position in the list and relocate it to the front of the list in constant time, we can
show thatthecost ofmove-to-front isatmosttwicethecost ofanyother heuristic
including, again,onethatknowstheentireaccesssequence inadvance.
V Advanced Data Structures
Introduction
This part returns to studying data structures that support operations on dynamic
sets, but at amore advanced level than PartIII. Twoof the chapters, for example,
makeextensive useoftheamortizedanalysis techniques wesawinChapter17.
Chapter 18 presents B-trees, which are balanced search trees specifically de-
signed to be stored on disks. Because disks operate much more slowly than
random-access memory, we measure the performance of B-trees not only by how
muchcomputing time thedynamic-set operations consume but also by how many
diskaccessestheyperform. ForeachB-treeoperation,thenumberofdiskaccesses
increases withtheheightoftheB-tree,butB-treeoperations keeptheheightlow.
Chapter 19 gives an implementation of a mergeable heap, which supports the
operations INSERT, MINIMUM, EXTRACT-MIN, and UNION.1 The UNION oper-
ation unites, or merges, two heaps. Fibonacci heaps—the data structure in Chap-
ter19—also support theoperations DELETE and DECREASE-KEY. Weuseamor-
tized time bounds to measure the performance of Fibonacci heaps. The opera-
tions INSERT, MINIMUM, and UNION take only O.1/ actual and amortized time
onFibonacci heaps, andtheoperations EXTRACT-MIN and DELETE takeO.lgn/
amortized time. The most significant advantage of Fibonacci heaps, however, is
that DECREASE-KEY takes only O.1/ amortized time. Because the DECREASE-
1AsinProblem10-2,wehavedefinedamergeableheaptosupportMINIMUMandEXTRACT-MIN,
andsowecanalsorefertoitasamergeablemin-heap. Alternatively, ifitsupported MAXIMUM
andEXTRACT-MAX,itwouldbeamergeablemax-heap. Unlesswespecifyotherwise,mergeable
heapswillbebydefaultmergeablemin-heaps.
482 PartV AdvancedDataStructures
KEYoperationtakesconstantamortizedtime,Fibonacciheapsarekeycomponents
ofsomeoftheasymptotically fastestalgorithmstodateforgraphproblems.
Noting that we can beat the .nlgn/ lower bound for sorting when the keys
are integers in a restricted range, Chapter 20 asks whether we can design a data
structure that supports the dynamic-set operations SEARCH, INSERT, DELETE,
MINIMUM, MAXIMUM, SUCCESSOR, and PREDECESSOR in o.lgn/ time when
the keysare integers inarestricted range. Theanswer turns outtobethat wecan,
byusingarecursivedatastructure knownasavanEmdeBoastree. Ifthekeysare
unique integers drawnfromtheset 0;1;2;:::;u 1 ,whereuisanexactpower
f (cid:0) g
of2,then vanEmdeBoastrees support each oftheabove operations inO.lglgu/
time.
Finally,Chapter21presentsdatastructuresfordisjointsets. Wehaveauniverse
ofnelementsthatarepartitionedintodynamicsets. Initially,eachelementbelongs
toitsownsingletonset. TheoperationUNIONunitestwosets,andthequeryFIND-
SET identifies the unique set that contains a given element at the moment. By
representingeachsetasasimplerootedtree,weobtainsurprisinglyfastoperations:
a sequence of m operations runs in O.m˛.n// time, where ˛.n/ is an incredibly
slowly growing function—˛.n/ is at most 4 in any conceivable application. The
amortized analysis that proves this time bound is as complex as the data structure
issimple.
Thetopicscoveredinthispartarebynomeanstheonlyexamplesof“advanced”
datastructures. Otheradvanceddatastructures includethefollowing:
 Dynamictrees,introducedbySleatorandTarjan[319]anddiscussedbyTarjan
[330], maintain a forest of disjoint rooted trees. Each edge in each tree has
a real-valued cost. Dynamic trees support queries to find parents, roots, edge
costs, and the minimum edge cost on a simple path from a node up to a root.
Treesmaybemanipulatedbycuttingedges,updatingalledgecostsonasimple
path from a node up to a root, linking a root into another tree, and making a
node the root of the tree it appears in. One implementation of dynamic trees
givesanO.lgn/amortizedtimeboundforeachoperation; amorecomplicated
implementationyieldsO.lgn/worst-casetimebounds. Dynamictreesareused
insomeoftheasymptotically fastestnetwork-flow algorithms.
 Splay trees, developed by Sleator and Tarjan [320] and, again, discussed by
Tarjan [330], are a form of binary search tree on which the standard search-
tree operations run in O.lgn/ amortized time. One application of splay trees
simplifiesdynamictrees.
 Persistentdatastructuresallowqueries,andsometimesupdatesaswell,onpast
versions of a data structure. Driscoll, Sarnak, Sleator, and Tarjan [97] present
techniques for making linked data structures persistent with only a small time
PartV AdvancedDataStructures 483
and space cost. Problem 13-1 gives a simple example of a persistent dynamic
set.
 Asin Chapter 20, several data structures allow a faster implementation of dic-
tionary operations (INSERT, DELETE, and SEARCH) for a restricted universe
ofkeys. Bytaking advantage oftheserestrictions, theyareabletoachievebet-
terworst-caseasymptoticrunningtimesthancomparison-baseddatastructures.
Fredman and Willard introduced fusion trees [115], which were the first data
structure toallowfasterdictionary operations whentheuniverseisrestricted to
integers. They showed how to implement these operations in O.lgn=lglgn/
time. Several subsequent data structures, including exponential search trees
[16], have also given improved bounds on some or all of the dictionary opera-
tionsandarementionedinthechapter notesthroughout thisbook.
 Dynamic graph data structures support various queries while allowing the
structure of a graph to change through operations that insert or delete vertices
oredges. Examplesofthequeriesthattheysupportincludevertexconnectivity
[166], edge connectivity, minimum spanning trees [165], biconnectivity, and
transitive closure[164].
Chapternotesthroughout thisbookmentionadditional datastructures.
18 B-Trees
B-trees are balanced search trees designed to work well on disks or other direct-
access secondary storage devices. B-trees are similar to red-black trees (Chap-
ter 13), but they are better atminimizing disk I/O operations. Many database sys-
temsuseB-trees,orvariantsofB-trees,tostoreinformation.
B-treesdifferfromred-blacktreesinthatB-treenodesmayhavemanychildren,
from a few to thousands. That is, the “branching factor” of a B-tree can be quite
large, although it usually depends on characteristics of the disk unit used. B-trees
are similar to red-black trees in that every n-node B-tree has height O.lgn/. The
exact height of a B-tree can be considerably less than that of a red-black tree,
however, because its branching factor, and hence the base of the logarithm that
expresses its height, can be much larger. Therefore, we can also use B-trees to
implementmanydynamic-set operations intimeO.lgn/.
B-trees generalize binary search trees in anatural manner. Figure 18.1 shows a
simple B-tree. Ifan internal B-tree node x contains x:nkeys, then x has x:n 1
C
children. Thekeys innodex serveasdividing points separating therange ofkeys
handled by x into x:n 1 subranges, each handled by one child of x. When
C
searching for a key in a B-tree, we make an .x:n 1/-way decision based on
C
comparisonswiththex:nkeysstoredatnodex. Thestructureofleafnodesdiffers
fromthatofinternal nodes;wewillexaminethesedifferences inSection18.1.
Section 18.1 gives a precise definition of B-trees and proves that the height of
a B-tree grows only logarithmically with the number of nodes it contains. Sec-
tion 18.2 describes how to search for a key and insert a key into a B-tree, and
Section 18.3 discusses deletion. Before proceeding, however, weneed toask why
weevaluate datastructures designed toworkonadiskdifferently fromdatastruc-
turesdesigned toworkinmainrandom-access memory.
Datastructuresonsecondarystorage
Computer systems take advantage of various technologies that provide memory
capacity. Theprimarymemory(ormainmemory)ofacomputersystemnormally
Chapter18 B-Trees 485
T:root
M
D H Q T X
B C F G J K L N P R S V W Y Z
Figure18.1 AB-treewhose keysaretheconsonants of English. Aninternalnode x containing
x:nkeyshasx:n 1children.Allleavesareatthesamedepthinthetree.Thelightlyshadednodes
C
areexaminedinasearchfortheletterR.
spindle
platter track read/write
head
arms
Figure18.2 Atypicaldiskdrive. Itcomprisesoneormoreplatters(twoplattersareshownhere)
thatrotatearoundaspindle. Eachplatterisreadandwrittenwithaheadattheendofanarm. Arms
rotatearoundacommon pivotaxis. Atrackisthesurfacethatpassesbeneaththeread/writehead
whentheheadisstationary.
consists ofsilicon memorychips. Thistechnology istypically morethan anorder
ofmagnitudemoreexpensiveperbitstoredthanmagneticstoragetechnology,such
as tapes or disks. Most computer systems also have secondary storage based on
magneticdisks;theamountofsuchsecondarystorageoftenexceedstheamountof
primarymemorybyatleasttwoordersofmagnitude.
Figure 18.2 shows a typical disk drive. The drive consists of one or more plat-
ters, which rotate at a constant speed around a common spindle. A magnetizable
material covers thesurface ofeach platter. Thedrive reads andwrites eachplatter
by a head at the end of an arm. The arms can move their heads toward or away
486 Chapter18 B-Trees
from the spindle. When a given head is stationary, the surface that passes under-
neath it is called a track. Multiple platters increase only the disk drive’s capacity
andnotitsperformance.
Althoughdisksarecheaperandhavehighercapacitythanmainmemory,theyare
much,muchslowerbecausetheyhavemovingmechanicalparts.1 Themechanical
motionhastwocomponents: platterrotationandarmmovement. Asofthiswriting,
commodity disks rotate at speeds of 5400–15,000 revolutions per minute (RPM).
We typically see 15,000 RPM speeds in server-grade drives, 7200 RPM speeds
in drives for desktops, and 5400 RPM speeds in drives for laptops. Although
7200 RPM may seem fast, one rotation takes 8.33 milliseconds, which is over 5
orders of magnitude longer than the 50 nanosecond access times (more or less)
commonlyfoundforsiliconmemory. Inotherwords,ifwehavetowaitafullrota-
tionforaparticular itemtocomeundertheread/writehead,wecouldaccessmain
memory more than 100,000 times during that span. On average we have to wait
foronlyhalfarotation, butstill, thedifference inaccess timesforsilicon memory
compared with disks is enormous. Moving the arms also takes some time. As of
this writing, average access times for commodity disks are in the range of 8to 11
milliseconds.
In order to amortize the time spent waiting for mechanical movements, disks
accessnotjustoneitembutseveralatatime. Informationisdividedintoanumber
ofequal-sized pages ofbitsthatappear consecutively withintracks, andeach disk
readorwriteisofoneormoreentirepages. Foratypicaldisk,apagemightbe211
to214bytesinlength. Oncetheread/writeheadispositionedcorrectlyandthedisk
hasrotatedtothebeginningofthedesiredpage,readingorwritingamagneticdisk
isentirelyelectronic (asidefromtherotation ofthedisk), andthediskcanquickly
readorwritelargeamountsofdata.
Often, accessing a page of information and reading it from a disk takes longer
than examining all the information read. For this reason, in this chapter we shall
lookseparately atthetwoprincipal components oftherunningtime:
 thenumberofdiskaccesses, and
 theCPU(computing) time.
Wemeasurethenumberofdiskaccesses intermsofthenumberofpagesofinfor-
mation that need to be read from or written to the disk. We note that disk-access
time is not constant—it depends on the distance between the current track and
the desired track and also on the initial rotational position of the disk. We shall
1Asofthiswriting,solid-statedriveshaverecentlycomeontotheconsumermarket. Althoughthey
arefasterthanmechanicaldiskdrives, theycostmorepergigabyteandhavelowercapacitiesthan
mechanicaldiskdrives.
Chapter18 B-Trees 487
nonetheless usethenumberofpagesreadorwrittenasafirst-orderapproximation
ofthetotaltimespentaccessing thedisk.
In a typical B-tree application, the amount of data handled is so large that all
thedatadonotfitintomainmemoryatonce. TheB-treealgorithms copyselected
pages from disk into main memory as needed and write back onto disk the pages
that have changed. B-tree algorithms keep only a constant number of pages in
mainmemoryatanytime;thus,thesizeofmainmemorydoesnotlimitthesizeof
B-treesthatcanbehandled.
Wemodeldiskoperationsinourpseudocodeasfollows. Letx beapointertoan
object. Iftheobjectiscurrentlyinthecomputer’smainmemory,thenwecanrefer
totheattributes oftheobjectasusual: x:key,forexample. Iftheobjectreferred to
byxresidesondisk,however,thenwemustperformtheoperationDISK-READ.x/
to read object x into main memory before we can refer to its attributes. (We as-
sume that if x is already in main memory, then DISK-READ.x/ requires no disk
accesses;itisa“no-op.”) Similarly,theoperation DISK-WRITE.x/isusedtosave
any changes that have been made to the attributes of object x. That is, the typical
patternforworkingwithanobjectisasfollows:
x apointertosomeobject
D
DISK-READ.x/
operations thataccessand/ormodifytheattributes ofx
DISK-WRITE.x/ //omittedifnoattributes ofx werechanged
otheroperations thataccessbutdonotmodifyattributes ofx
The system can keep only a limited number of pages in main memory at any one
time. Weshallassumethatthesystemflushesfrommainmemorypagesnolonger
inuse;ourB-treealgorithms willignorethisissue.
Since in most systems the running time of a B-tree algorithm depends primar-
ily on the number of DISK-READ and DISK-WRITE operations it performs, we
typically want each of these operations to read or write as much information as
possible. Thus,aB-treenodeisusuallyaslargeasawholediskpage,andthissize
limitsthenumberofchildren aB-treenodecanhave.
For a large B-tree stored on a disk, we often see branching factors between 50
and 2000, depending on the size of a key relative to the size of a page. A large
branchingfactordramaticallyreducesboththeheightofthetreeandthenumberof
diskaccessesrequiredtofindanykey. Figure18.3showsaB-treewithabranching
factorof1001andheight2thatcanstoreoveronebillionkeys;nevertheless, since
we can keep the root node permanently in main memory, we can find any key in
thistreebymakingatmostonlytwodiskaccesses.
488 Chapter18 B-Trees
T:root
1000 1 node,
1000 keys
1001
1000 1000 … 1000 1001 nodes,
1,001,000 keys
1001 1001 1001
1000 1000 … 1000 1,002,001 nodes,
1,002,001,000 keys
Figure 18.3 A B-tree of height 2 containing over one billion keys. Shown inside each node x
isx:n, thenumber ofkeysinx. Eachinternalnodeandleafcontains1000keys. ThisB-treehas
1001nodesatdepth1andoveronemillionleavesatdepth2.
18.1 Definition ofB-trees
Tokeepthingssimple,weassume,aswehaveforbinarysearchtreesandred-black
trees, that any “satellite information” associated with a key resides in the same
nodeasthekey. Inpractice,onemightactuallystorewitheachkeyjustapointerto
anotherdiskpagecontainingthesatelliteinformationforthatkey. Thepseudocode
in this chapter implicitly assumes that the satellite information associated with a
key, orthe pointer tosuch satellite information, travels withthe key whenever the
key is moved from node to node. A common variant on a B-tree, known as a
BC-tree, stores all the satellite information in the leaves and stores only keys and
child pointers in the internal nodes, thus maximizing the branching factor of the
internal nodes.
A B-tree T is arooted tree (whose root isT:root) having the following proper-
ties:
1. Everynodex hasthefollowingattributes:
a. x:n,thenumberofkeyscurrently storedinnodex,
b. thex:nkeysthemselves,x:key ;x:key ;:::;x:key ,storedinnondecreas-
1 2 x:n
ingorder, sothatx:key x:key x:key ,
1  2   x:n
c. x:leaf,abooleanvaluethatisTRUEifxisaleafandFALSEifxisaninternal
node.
2. Eachinternalnodex alsocontainsx:n 1pointersx:c ;x:c ;:::;x:c to
its children. Leaf nodes have no childC ren, and so
their1
c
att2
ributes
arex:n
uC
n1
de-
i
fined.
18.1 DefinitionofB-trees 489
3. Thekeysx:key separate theranges ofkeysstored ineach subtree: ifk isany
i i
keystoredinthesubtreewithrootx:c ,then
i
k x:key k x:key x:key k :
1  1  2  2   x:n  x:n C1
4. Allleaveshavethesamedepth, whichisthetree’sheighth.
5. Nodes have lower and upper bounds on the number of keys they can contain.
Weexpress thesebounds intermsofafixedinteger t 2called theminimum

degreeoftheB-tree:
a. Everynode other than theroot musthaveatleast t 1keys. Everyinternal
(cid:0)
node other than theroot thus has atleast t children. Ifthe tree isnonempty,
therootmusthaveatleastonekey.
b. Every node may contain at most 2t 1 keys. Therefore, an internal node
(cid:0)
mayhaveatmost2t children. Wesaythatanodeisfullifitcontainsexactly
2t 1keys.2
(cid:0)
The simplest B-tree occurs when t 2. Every internal node then has either 2,
D
3,or4children,andwehavea2-3-4tree. Inpractice,however,muchlargervalues
oft yieldB-treeswithsmallerheight.
TheheightofaB-tree
The number of disk accesses required for most operations on a B-tree is propor-
tionaltotheheightoftheB-tree. Wenowanalyzetheworst-caseheightofaB-tree.
Theorem18.1
Ifn 1,thenforanyn-keyB-treeT ofheighthandminimumdegreet 2,
 
n 1
h log C :
 t 2
Proof TherootofaB-treeT containsatleastonekey,andallothernodescontain
at least t 1 keys. Thus, T, whose height is h, has at least 2 nodes at depth 1, at
(cid:0)
least 2t nodes at depth 2, at least 2t2 nodes at depth 3, and so on, until at depth h
ithas atleast 2th 1 nodes. Figure 18.4illustrates such atreefor h 3. Thus, the
(cid:0)
D
2Another common variant on a B-tree, known as a B -tree, requires each internal node to be at
least2=3full,ratherthanatleasthalffull,asaB-treerequires.
490 Chapter18 B-Trees
T:root number
depth of nodes
1 0 1
t – 1 t – 1 1 2
t t
… …
t – 1 t – 1 t – 1 t – 1 2 2t
t t t t
… … … …
t – 1 t – 1 t – 1 t – 1 t – 1 t – 1 t – 1 t – 1 3 2t2
Figure18.4 AB-treeofheight3containingaminimumpossiblenumberofkeys. Showninside
eachnodexisx:n.
numbernofkeyssatisfiestheinequality
h
n 1 .t 1/ 2ti 1
(cid:0)
 C (cid:0)
i 1
XD
th 1
1 2.t 1/ (cid:0)
D C (cid:0) t 1
 (cid:0) 
2th 1:
D (cid:0)
Bysimplealgebra, wegetth .n 1/=2. Takingbase-t logarithmsofbothsides
 C
provesthetheorem.
Here we see the power of B-trees, as compared with red-black trees. Although
theheightofthetreegrowsasO.lgn/inbothcases(recallthatt isaconstant),for
B-trees the base of the logarithm can be many times larger. Thus, B-trees save a
factorofaboutlgt overred-black treesinthenumberofnodesexaminedformost
treeoperations. Becauseweusuallyhavetoaccessthedisktoexamineanarbitrary
nodeinatree,B-treesavoidasubstantial numberofdiskaccesses.
Exercises
18.1-1
Whydon’tweallowaminimumdegreeoft 1?
D
18.1-2
Forwhatvaluesoft isthetreeofFigure18.1alegalB-tree?
18.2 BasicoperationsonB-trees 491
18.1-3
ShowalllegalB-treesofminimumdegree2thatrepresent 1;2;3;4;5 .
f g
18.1-4
Asafunctionoftheminimumdegreet,whatisthemaximumnumberofkeysthat
canbestoredinaB-treeofheighth?
18.1-5
Describe thedatastructure thatwouldresult ifeachblack nodeinared-black tree
weretoabsorbitsredchildren, incorporating theirchildren withitsown.
18.2 Basicoperations onB-trees
In this section, we present the details of the operations B-TREE-SEARCH, B-
TREE-CREATE, and B-TREE-INSERT. In these procedures, we adopt two con-
ventions:
 The root of the B-tree is always in main memory, so that we never need to
perform a DISK-READ on the root; we do have to perform a DISK-WRITE of
theroot,however,whenevertherootnodeischanged.
 Anynodes thatarepassed asparameters mustalready havehada DISK-READ
operation performed onthem.
The procedures we present are all “one-pass” algorithms that proceed downward
fromtherootofthetree,withouthavingtobackup.
SearchingaB-tree
Searching aB-tree ismuchlike searching abinary search tree, except that instead
of making a binary, or “two-way,” branching decision at each node, we make a
multiwaybranchingdecisionaccordingtothenumberofthenode’schildren. More
precisely, ateachinternal nodex,wemakean.x:n 1/-waybranching decision.
C
B-TREE-SEARCHisastraightforwardgeneralizationoftheTREE-SEARCHpro-
cedure defined for binary search trees. B-TREE-SEARCH takes as input a pointer
tothe root node x ofasubtree and akey k to besearched for in that subtree. The
top-levelcallisthusoftheformB-TREE-SEARCH.T:root;k/. IfkisintheB-tree,
B-TREE-SEARCH returns the ordered pair .y;i/ consisting of a node y and an
indexi suchthaty:key k. Otherwise,theprocedure returns NIL.
i D
492 Chapter18 B-Trees
B-TREE-SEARCH.x;k/
1 i 1
D
2 whilei x:nandk > x:key
 i
3 i i 1
D C
4 ifi x:nandk ==x:key
 i
5 return.x;i/
6 elseifx:leaf
7 return NIL
8 else DISK-READ.x:c i/
9 return B-TREE-SEARCH.x:c i;k/
Using a linear-search procedure, lines 1–3 find the smallest index i such that
k x:key , or else they set i to x:n 1. Lines 4–5 check to see whether we
 i C
havenowdiscoveredthekey,returningifwehave. Otherwise,lines6–9eitherter-
minatethesearchunsuccessfully (ifx isaleaf)orrecursetosearchtheappropriate
subtree ofx,afterperforming thenecessary DISK-READ onthatchild.
Figure18.1illustratestheoperationofB-TREE-SEARCH. Theprocedureexam-
inesthelightlyshaded nodesduring asearchforthekeyR.
As in the TREE-SEARCH procedure for binary search trees, the nodes encoun-
tered during the recursion form a simple path downward from the root of the
tree. The B-TREE-SEARCH procedure therefore accesses O.h/ O.log n/ disk
D t
pages,wherehistheheightoftheB-treeandnisthenumberofkeysintheB-tree.
Sincex:n< 2t,thewhileloopoflines2–3takesO.t/timewithineachnode,and
thetotalCPUtimeisO.th/ O.tlog n/.
D t
CreatinganemptyB-tree
To build a B-tree T, we first use B-TREE-CREATE to create an empty root node
and then call B-TREE-INSERT to add new keys. Both of these procedures use an
auxiliary procedure ALLOCATE-NODE, which allocates one disk page to be used
as a new node in O.1/ time. We can assume that a node created by ALLOCATE-
NODE requires no DISK-READ, since there is as yet no useful information stored
onthediskforthatnode.
B-TREE-CREATE.T/
1 x ALLOCATE-NODE./
D
2 x:leaf TRUE
D
3 x:n 0
D
4 DISK-WRITE.x/
5 T:root x
D
B-TREE-CREATE requiresO.1/diskoperations andO.1/CPUtime.
18.2 BasicoperationsonB-trees 493
InsertingakeyintoaB-tree
Inserting akeyintoaB-treeissignificantly morecomplicated thaninserting akey
intoabinarysearchtree. Aswithbinarysearchtrees,wesearchfortheleafposition
at which to insert the new key. With a B-tree, however, we cannot simply create
a new leaf node and insert it, as the resulting tree would fail to be a valid B-tree.
Instead, weinsertthenewkeyintoanexisting leafnode. Sincewecannot insert a
keyinto aleaf node thatisfull, weintroduce anoperation that splits afullnode y
(having2t 1keys)arounditsmediankeyy:key intotwonodeshavingonlyt 1
(cid:0) t (cid:0)
keyseach. Themediankeymovesupintoy’sparent toidentify thedividing point
betweenthetwonewtrees. Butify’sparentisalsofull,wemustsplititbeforewe
caninsertthenewkey,andthuswecouldendupsplittingfullnodesallthewayup
thetree.
As with a binary search tree, we can insert a key into a B-tree in a single pass
downthetreefrom theroot toaleaf. Todoso,wedonotwaittofindoutwhether
wewillactuallyneedtosplitafullnodeinordertodotheinsertion. Instead,aswe
traveldownthetreesearchingforthepositionwherethenewkeybelongs,wesplit
eachfullnodewecometoalongtheway(includingtheleafitself). Thuswhenever
wewanttosplitafullnodey,weareassured thatitsparentisnotfull.
SplittinganodeinaB-tree
Theprocedure B-TREE-SPLIT-CHILD takesasinputanonfullinternalnodex (as-
sumedtobeinmainmemory)andanindexi suchthatx:c (alsoassumedtobein
i
main memory) is afull child of x. The procedure then splits this child in twoand
adjustsxsothatithasanadditionalchild. Tosplitafullroot,wewillfirstmakethe
rootachildofanewemptyrootnode, sothatwecanuse B-TREE-SPLIT-CHILD.
Thetreethusgrowsinheight byone;splitting istheonlymeansbywhichthetree
grows.
Figure 18.5 illustrates this process. We split the full node y x:c about its
i
D
median key S, which moves up into y’s parent node x. Those keys in y that are
greater than the median key move into a new node ´, which becomes a new child
ofx.
494 Chapter18 B-Trees
1 1 1
x
x:keyi x:k(cid:0)eyi
x
x:keyi x:k(cid:0)ey xi :keyi C
… N W … … N S W …
y x:c y x:c ´ x:c
D i D i D i C1
P Q R S T U V P Q R T U V
T T T T T T T T T T T T T T T T
1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8
Figure18.5 Splittinganodewitht 4. Nodey x:c i splitsintotwonodes,y and´,andthe
D D
mediankeyS ofymovesupintoy’sparent.
B-TREE-SPLIT-CHILD.x;i/
1 ´ ALLOCATE-NODE./
D
2 y x:c
i
D
3 ´:leaf y:leaf
D
4 ´:n t 1
D (cid:0)
5 forj 1tot 1
D (cid:0)
6 ´:key y:key
j D j t
7 ifnoty:leaf C
8 forj 1tot
D
9 ´:c y:c
j j t
10 y:n t 1 D C
D (cid:0)
11 forj x:n 1downtoi 1
D C C
12 x:c x:c
j 1 j
13 x:c
C´D
i 1
14 forjC D x:ndowntoi
D
15 x:key x:key
j 1 D j
16 x:key yC:key
i D t
17 x:n x:n 1
D C
18 DISK-WRITE.y/
19 DISK-WRITE.´/
20 DISK-WRITE.x/
B-TREE-SPLIT-CHILD worksbystraightforward“cuttingandpasting.” Here,x
isthenodebeingsplit,andyisx’sithchild(setinline2). Nodeyoriginallyhas2t
children (2t 1 keys) but is reduced to t children (t 1 keys) by this operation.
(cid:0) (cid:0)
Node´takesthet largestchildren(t 1keys)fromy,and´becomesanewchild
(cid:0)
18.2 BasicoperationsonB-trees 495
of x, positioned just after y in x’s table of children. The median key of y moves
uptobecomethekeyinx thatseparates y and´.
Lines 1–9 create node ´ and give it the largest t 1 keys and corresponding t
(cid:0)
children ofy. Line10adjusts thekeycount fory. Finally, lines 11–17 insert ´as
a child of x, move the median key from y up to x in order to separate y from ´,
and adjust x’s key count. Lines 18–20 write out all modified disk pages. The
CPU time used by B-TREE-SPLIT-CHILD is ‚.t/, due to the loops on lines 5–6
and 8–9. (The other loops run for O.t/ iterations.) The procedure performs O.1/
diskoperations.
InsertingakeyintoaB-treeinasinglepassdownthetree
We insert a key k into a B-tree T of height h in a single pass down the tree, re-
quiring O.h/ disk accesses. TheCPUtimerequired isO.th/ O.tlog n/. The
D t
B-TREE-INSERT procedure uses B-TREE-SPLIT-CHILD to guarantee that the re-
cursionneverdescends toafullnode.
B-TREE-INSERT.T;k/
1 r T:root
D
2 ifr:n==2t 1
(cid:0)
3 s ALLOCATE-NODE./
D
4 T:root s
D
5 s:leaf FALSE
D
6 s:n 0
D
7 s:c r
1
D
8 B-TREE-SPLIT-CHILD.s;1/
9 B-TREE-INSERT-NONFULL.s;k/
10 else B-TREE-INSERT-NONFULL.r;k/
Lines 3–9 handle the case in which the root node r is full: the root splits and a
new node s (having two children) becomes the root. Splitting the root is the only
way to increase the height of a B-tree. Figure 18.6 illustrates this case. Unlike a
binary search tree, a B-tree increases in height at the top instead of at the bottom.
Theprocedure finishesbycalling B-TREE-INSERT-NONFULL toinsertkeyk into
the tree rooted at the nonfull root node. B-TREE-INSERT-NONFULL recurses as
necessarydownthetree,atalltimesguaranteeingthatthenodetowhichitrecurses
isnotfullbycalling B-TREE-SPLIT-CHILD asnecessary.
TheauxiliaryrecursiveprocedureB-TREE-INSERT-NONFULL insertskeykinto
nodex,whichisassumedtobenonfullwhentheprocedureiscalled. Theoperation
of B-TREE-INSERT and the recursive operation of B-TREE-INSERT-NONFULL
guarantee thatthisassumption istrue.
496 Chapter18 B-Trees
T:root
s
H
T:root
r r
A D F H L N P A D F L N P
T T T T T T T T T T T T T T T T
1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8
Figure18.6 Splittingtheroot witht 4. Root node r splitsintwo, and anew root node s is
D
created. Thenew root containsthemediankeyofr andhasthetwohalvesof r aschildren. The
B-treegrowsinheightbyonewhentherootissplit.
B-TREE-INSERT-NONFULL.x;k/
1 i x:n
D
2 ifx:leaf
3 whilei 1andk < x:key
 i
4 x:key x:key
i 1 D i
5 i iC 1
D (cid:0)
6 x:key k
i 1 D
7 x:n Cx:n 1
D C
8 DISK-WRITE.x/
9 elsewhilei 1andk < x:key
 i
10 i i 1
D (cid:0)
11 i i 1
D C
12 DISK-READ.x:c i/
13 ifx:c :n ==2t 1
i
(cid:0)
14 B-TREE-SPLIT-CHILD.x;i/
15 ifk > x:key
i
16 i i 1
D C
17 B-TREE-INSERT-NONFULL.x:c i;k/
TheB-TREE-INSERT-NONFULL procedureworksasfollows. Lines3–8handle
the case in which x is a leaf node by inserting key k into x. If x is not a leaf
node, then we must insert k into the appropriate leaf node in the subtree rooted
at internal node x. In this case, lines 9–11 determine the child of x to which the
recursion descends. Line13detects whethertherecursion woulddescend toafull
child,inwhichcaseline14usesB-TREE-SPLIT-CHILD tosplitthatchildintotwo
nonfull children, and lines 15–16 determine which of the two children is now the
18.2 BasicoperationsonB-trees 497
correctonetodescendto. (NotethatthereisnoneedforaDISK-READ.x:c i/after
line 16 increments i, since the recursion will descend in this case to a child that
wasjust created by B-TREE-SPLIT-CHILD.) The net effect oflines 13–16 is thus
toguaranteethattheprocedureneverrecursestoafullnode. Line17thenrecurses
toinsert k intotheappropriate subtree. Figure18.7illustrates thevarious cases of
inserting intoaB-tree.
For a B-tree of height h, B-TREE-INSERT performs O.h/ disk accesses, since
only O.1/ DISK-READ and DISK-WRITE operations occur between calls to
B-TREE-INSERT-NONFULL. The total CPU time used is O.th/ O.t log n/.
D t
Since B-TREE-INSERT-NONFULL is tail-recursive, we can alternatively imple-
mentitasawhileloop, thereby demonstrating that thenumber ofpages that need
tobeinmainmemoryatanytimeisO.1/.
Exercises
18.2-1
Showtheresultsofinserting thekeys
F;S;Q;K;C;L;H;T;V;W;M;R;N;P;A;B;X;Y;D;Z;E
in order into an empty B-tree with minimum degree 2. Draw only the configura-
tions ofthe treejust before somenode mustsplit, andalso draw thefinal configu-
ration.
18.2-2
Explainunderwhatcircumstances,ifany,redundantDISK-READ orDISK-WRITE
operations occur during the course of executing a call to B-TREE-INSERT. (A
redundant DISK-READ is a DISK-READ for a page that is already in memory.
Aredundant DISK-WRITE writes todisk apage ofinformation thatisidentical to
whatisalreadystoredthere.)
18.2-3
ExplainhowtofindtheminimumkeystoredinaB-treeandhowtofindtheprede-
cessorofagivenkeystoredinaB-tree.
18.2-4 ?
Suppose that weinsert the keys 1;2;:::;n into an empty B-tree with minimum
f g
degree2. HowmanynodesdoesthefinalB-treehave?
18.2-5
Sinceleaf nodes require no pointers to children, they could conceivably use adif-
ferent (larger) t value than internal nodes for the same disk page size. Show how
to modify the procedures for creating and inserting into a B-tree to handle this
variation.
498 Chapter18 B-Trees
(a) initial tree G M P X
A C D E J K N O R S T U V Y Z
(b) B inserted G M P X
A B C D E J K N O R S T U V Y Z
(c) Q inserted G M P T X
A B C D E J K N O Q R S U V Y Z
(d) L inserted P
G M T X
A B C D E J K L N O Q R S U V Y Z
(e) F inserted P
C G M T X
A B D E F J K L N O Q R S U V Y Z
Figure18.7 InsertingkeysintoaB-tree.Theminimumdegreet forthisB-treeis3,soanodecan
holdatmost5keys. Nodesthataremodifiedbytheinsertionprocessarelightlyshaded. (a)The
initialtreeforthisexample.(b)TheresultofinsertingBintotheinitialtree;thisisasimpleinsertion
intoaleafnode. (c)TheresultofinsertingQintotheprevioustree. ThenodeRSTUV splitsinto
twonodescontainingRS andUV,thekeyT movesuptotheroot,andQisinsertedintheleftmost
of the two halves (the RS node). (d) The result of inserting L into the previous tree. The root
splitsrightaway,sinceitisfull,andtheB-treegrowsinheightbyone. ThenLisinsertedintothe
leafcontainingJK. (e)TheresultofinsertingF intotheprevioustree. ThenodeABCDE splits
beforeF isinsertedintotherightmostofthetwohalves(theDEnode).
18.3 DeletingakeyfromaB-tree 499
18.2-6
Suppose thatweweretoimplement B-TREE-SEARCH tousebinary search rather
than linear search within each node. Show that this change makes the CPU time
requiredO.lgn/,independently ofhowt mightbechosenasafunctionofn.
18.2-7
Suppose that disk hardware allows usto choose the sizeof adisk page arbitrarily,
butthatthetimeittakestoreadthediskpageisa bt,whereaandbarespecified
C
constantsandt istheminimumdegreeforaB-treeusingpagesoftheselectedsize.
Describehowtochooset soastominimize(approximately)theB-treesearchtime.
Suggestanoptimalvalueoft forthecaseinwhicha 5millisecondsandb 10
D D
microseconds.
18.3 Deleting a key from a B-tree
Deletion from aB-tree isanalogous toinsertion but alittle morecomplicated, be-
cause we can delete a key from any node—not just a leaf—and when we delete a
key from an internal node, we will have to rearrange the node’s children. As in
insertion,wemustguardagainstdeletionproducingatreewhosestructureviolates
theB-treeproperties. Justaswehadtoensurethatanodedidn’tgettoobigdueto
insertion, wemustensurethatanodedoesn’tgettoosmallduringdeletion(except
that the root is allowed to have fewer than the minimum number t 1 of keys).
(cid:0)
Just as a simple insertion algorithm might have to back up if a node on the path
to where the key was to be inserted was full, a simple approach to deletion might
havetobackupifanode(otherthantheroot)alongthepathtowherethekeyisto
bedeleted hastheminimumnumberofkeys.
Theprocedure B-TREE-DELETE deletesthekeyk fromthesubtreerootedatx.
Wedesign thisprocedure toguarantee thatwheneveritcallsitselfrecursively ona
node x, the number of keys in x is at least the minimum degree t. Note that this
condition requires one more key than the minimum required by the usual B-tree
conditions,sothatsometimesakeymayhavetobemovedintoachildnodebefore
recursion descends tothat child. Thisstrengthened condition allowsustodelete a
keyfromthetreeinonedownwardpasswithouthavingto“backup”(withoneex-
ception, whichwe’llexplain). Youshould interpret thefollowing specification for
deletionfromaB-treewiththeunderstanding thatiftherootnodex everbecomes
an internal node having no keys (this situation can occur in cases 2c and 3b on
pages 501–502), then we delete x, and x’s only child x:c becomes the new root
1
ofthetree,decreasingtheheightofthetreebyoneandpreservingthepropertythat
therootofthetreecontains atleastonekey(unlessthetreeisempty).
500 Chapter18 B-Trees
(a) initial tree P
C G M T X
A B D E F J K L N O Q R S U V Y Z
(b) F deleted: case 1 P
C G M T X
A B D E J K L N O Q R S U V Y Z
(c) M deleted: case 2a P
C G L T X
A B D E J K N O Q R S U V Y Z
(d) G deleted: case 2c P
C L T X
A B D E J K N O Q R S U V Y Z
Figure18.8 DeletingkeysfromaB-tree. TheminimumdegreeforthisB-treeist 3,soanode
D
(other than the root) cannot have fewer than 2 keys. Nodes that are modified are lightly shaded.
(a) TheB-treeof Figure 18.7(e). (b)Deletion of F. Thisiscase 1: simple deletion fromaleaf.
(c)DeletionofM.Thisiscase2a:thepredecessorLofM movesuptotakeM’sposition.(d)Dele-
tionofG.Thisiscase2c:wepushGdowntomakenodeDEGJKandthendeleteGfromthisleaf
(case1).
Wesketchhowdeletionworksinsteadofpresentingthepseudocode. Figure18.8
illustrates thevariouscasesofdeleting keysfromaB-tree.
1. Ifthekeyk isinnodex andx isaleaf,deletethekeyk fromx.
2. Ifthekeyk isinnodex andx isaninternal node,dothefollowing:
18.3 DeletingakeyfromaB-tree 501
(e) D deleted: case 3b
C L P T X
A B E J K N O Q R S U V Y Z
(e¢) tree shrinks C L P T X
in height
A B E J K N O Q R S U V Y Z
(f) B deleted: case 3a E L P T X
A C J K N O Q R S U V Y Z
Figure 18.8, continued (e) Deletion of D. This is case 3b: the recursion cannot descend to
nodeCLbecause ithasonly2keys, sowepushP downandmergeitwithCLandTX toform
CLPTX;thenwedeleteDfromaleaf(case1).(e0
)After(e),wedeletetherootandthetreeshrinks
inheightbyone. (f)DeletionofB. Thisiscase3a: C movestofillB’spositionandE movesto
fillC’sposition.
a. If the child y that precedes k in node x has at least t keys, then find the
predecessor k of k in the subtree rooted at y. Recursively delete k , and
0 0
replace k by k in x. (We can find k and delete it in a single downward
0 0
pass.)
b. If y has fewer than t keys, then, symmetrically, examine the child ´ that
followsk innodex. If´hasatleastt keys,thenfindthesuccessork ofk in
0
thesubtree rootedat´. Recursively delete k ,andreplace k byk inx. (We
0 0
canfindk anddeleteitinasingledownwardpass.)
0
c. Otherwise,ifbothy and´haveonlyt 1keys,mergek andallof´intoy,
(cid:0)
sothatx loses both k andthepointer to´, andy nowcontains 2t 1keys.
(cid:0)
Thenfree´andrecursively deletek fromy.
3. If the key k is not present in internal node x, determine the root x:c of the
i
appropriate subtree that must contain k, if k is in the tree at all. If x:c has
i
onlyt 1keys,executestep3aor3basnecessarytoguaranteethatwedescend
(cid:0)
toanodecontainingatleastt keys. Thenfinishbyrecursingontheappropriate
childofx.
502 Chapter18 B-Trees
a. Ifx:c hasonlyt 1keysbuthasanimmediatesiblingwithatleastt keys,
i
(cid:0)
give x:c an extra key by moving a key from x down into x:c , moving a
i i
key from x:c ’s immediate left or right sibling up into x, and moving the
i
appropriate childpointerfromthesibling intox:c .
i
b. If x:c and both of x:c ’s immediate siblings have t 1 keys, merge x:c
i i i
(cid:0)
with one sibling, which involves moving a key from x down into the new
mergednodetobecomethemediankeyforthatnode.
Since most of the keys in a B-tree are in the leaves, we may expect that in
practice, deletion operations are most often used to delete keys from leaves. The
B-TREE-DELETE procedure then acts in one downward pass through the tree,
without having to back up. When deleting a key in an internal node, however,
the procedure makes a downward pass through the tree but may have to return to
thenodefromwhichthekeywasdeletedtoreplacethekeywithitspredecessor or
successor (cases2aand2b).
Although this procedure seems complicated, it involves only O.h/ disk oper-
ations for a B-tree of height h, since only O.1/ calls to DISK-READ and DISK-
WRITE are made between recursive invocations of the procedure. The CPU time
required isO.th/ O.tlog n/.
D t
Exercises
18.3-1
ShowtheresultsofdeletingC,P,andV,inorder,fromthetreeofFigure18.8(f).
18.3-2
Writepseudocode for B-TREE-DELETE.
Problems
18-1 Stacksonsecondarystorage
Consider implementing a stack in a computer that has a relatively small amount
of fast primary memory and arelatively large amount ofslower disk storage. The
operations PUSH and POP work on single-word values. The stack we wish to
support can grow to be much larger than can fit in memory, and thus most of it
mustbestoredondisk.
A simple, but inefficient, stack implementation keeps the entire stack on disk.
Wemaintaininmemoryastackpointer,whichisthediskaddressofthetopelement
on the stack. If the pointer has value p, the top element is the .p mod m/th word
onpage p=m ofthedisk,wheremisthenumberofwordsperpage.
b c
ProblemsforChapter18 503
Toimplement the PUSH operation, weincrement thestack pointer, read theap-
propriate page into memory from disk, copy the element to be pushed to the ap-
propriate word on the page, and write the page back to disk. A POP operation is
similar. We decrement the stack pointer, read in the appropriate page from disk,
and return the top of the stack. We need not write back the page, since it was not
modified.
Because disk operations are relatively expensive, we count two costs for any
implementation: the total number of disk accesses and the total CPU time. Any
diskaccesstoapageofmwordsincurschargesofonediskaccessand‚.m/CPU
time.
a. Asymptotically, what is the worst-case number of disk accesses for n stack
operationsusingthissimpleimplementation? WhatistheCPUtimefornstack
operations? (Express your answer interms ofmand nfor this and subsequent
parts.)
Now consider a stack implementation in which we keep one page of the stack in
memory. (Wealsomaintainasmallamountofmemorytokeeptrackofwhichpage
iscurrentlyinmemory.) Wecanperformastackoperationonlyiftherelevantdisk
page resides in memory. If necessary, wecan write the page currently in memory
tothedisk and read inthenew page from the disk tomemory. Iftherelevant disk
pageisalready inmemory,thennodiskaccesses arerequired.
b. What is the worst-case number of disk accesses required for n PUSH opera-
tions? WhatistheCPUtime?
c. Whatistheworst-casenumberofdiskaccessesrequiredfornstackoperations?
WhatistheCPUtime?
Suppose that we now implement the stack by keeping two pages in memory (in
additiontoasmallnumberofwordsforbookkeeping).
d. Describe how to manage the stack pages so that the amortized number of disk
accesses for any stack operation is O.1=m/ and the amortized CPU time for
anystackoperation isO.1/.
18-2 Joiningandsplitting2-3-4trees
The join operation takes two dynamic sets S and S and an element x such that
0 00
for any x S and x S , we have x :key < x:key < x :key. It returns a set
0 0 00 00 0 00
2 2
S S x S . Thesplitoperation islikean“inverse” join: givenadynamic
0 00
D [f g[
set S and an element x S, it creates a set S that consists of all elements in
0
2
S x whose keys are less than x:key and a set S that consists of all elements
00
(cid:0)f g
in S x whose keys are greater than x:key. In this problem, we investigate
(cid:0) f g
504 Chapter18 B-Trees
howtoimplementtheseoperationson2-3-4trees. Weassumeforconveniencethat
elementsconsistonlyofkeysandthatallkeyvaluesaredistinct.
a. Showhowtomaintain,foreverynodexofa2-3-4tree,theheightofthesubtree
rooted at x asan attribute x:height. Make sure that your implementation does
notaffecttheasymptotic runningtimesofsearching, insertion, anddeletion.
b. Show how to implement the join operation. Given two 2-3-4 trees T and T
0 00
and a key k, the join operation should run in O.1 h h / time, where h
0 00 0
Cj (cid:0) j
andh aretheheights ofT andT ,respectively.
00 0 00
c. Consider the simple path p from the root of a 2-3-4 tree T to a given key k,
thesetS ofkeys inT thatarelessthank,andthesetS ofkeysinT thatare
0 00
greaterthank. Showthatp breaks S intoasetoftrees T ;T ;:::;T anda
0
f
00 10 m0
g
set of keys k ;k ;:::;k , where, for i 1;2;:::;m, we have y < k < ´
f
10 20 m0
g D
i0
foranykeysy T and´ T . Whatistherelationship betweentheheights
2 i0 1 2 i0
ofT andT ? Des(cid:0)cribehowp breaksS intosetsoftreesandkeys.
i0 1 i0 00
(cid:0)
d. Show how to implement the split operation on T. Use the join operation to
assemble the keys in S into a single 2-3-4 tree T and the keys in S into a
0 0 00
single2-3-4treeT . TherunningtimeofthesplitoperationshouldbeO.lgn/,
00
where n is the number of keys in T. (Hint: The costs for joining should tele-
scope.)
Chapter notes
Knuth [211], Aho, Hopcroft, and Ullman [5], and Sedgewick [306] give further
discussions ofbalanced-tree schemesandB-trees. Comer[74]providesacompre-
hensive survey of B-trees. Guibas and Sedgewick [155] discuss the relationships
amongvariouskindsofbalanced-treeschemes,includingred-blacktreesand2-3-4
trees.
In 1970, J. E. Hopcroft invented 2-3 trees, a precursor to B-trees and 2-3-4
trees, in which every internal node has either two or three children. Bayer and
McCreight [35] introduced B-trees in 1972; they did not explain their choice of
name.
Bender,Demaine,andFarach-Colton[40]studiedhowtomakeB-treesperform
well in the presence of memory-hierarchy effects. Their cache-oblivious algo-
rithms work efficiently without explicitly knowing the data transfer sizes within
thememoryhierarchy.
19 Fibonacci Heaps
TheFibonacci heap data structure serves adual purpose. First, itsupports aset of
operations that constitutes what is known as a “mergeable heap.” Second, several
Fibonacci-heap operations run in constant amortized time, which makes this data
structure wellsuitedforapplications thatinvoketheseoperations frequently.
Mergeableheaps
Amergeableheapisanydatastructurethatsupportsthefollowingfiveoperations,
inwhicheachelementhasakey:
MAKE-HEAP./createsandreturns anewheapcontaining noelements.
INSERT.H;x/insertselementx,whosekeyhasalreadybeenfilledin,intoheapH.
MINIMUM.H/returnsapointertotheelementinheapH whosekeyisminimum.
EXTRACT-MIN.H/deletes the element from heap H whose key isminimum, re-
turningapointertotheelement.
UNION.H 1;H 2/ creates and returns a new heap that contains all the elements of
heapsH andH . HeapsH andH are“destroyed” bythisoperation.
1 2 1 2
In addition to the mergeable-heap operations above, Fibonacci heaps also support
thefollowingtwooperations:
DECREASE-KEY.H;x;k/ assigns to element x within heap H the new key
valuek,whichweassumetobenogreaterthanitscurrentkeyvalue.1
DELETE.H;x/deleteselementx fromheapH.
1As mentioned in the introduction to Part V, our default mergeable heaps are mergeable min-
heaps, and so the operations MINIMUM, EXTRACT-MIN, and DECREASE-KEY apply. Alterna-
tively, we could define a mergeable max-heap with the operations MAXIMUM, EXTRACT-MAX,
andINCREASE-KEY.
506 Chapter19 FibonacciHeaps
Binaryheap Fibonacciheap
Procedure (worst-case) (amortized)
MAKE-HEAP ‚.1/ ‚.1/
INSERT ‚.lgn/ ‚.1/
MINIMUM ‚.1/ ‚.1/
EXTRACT-MIN ‚.lgn/ O.lgn/
UNION ‚.n/ ‚.1/
DECREASE-KEY ‚.lgn/ ‚.1/
DELETE ‚.lgn/ O.lgn/
Figure19.1 Runningtimesforoperationsontwoimplementationsofmergeableheaps. Thenum-
berofitemsintheheap(s)atthetimeofanoperationisdenotedbyn.
Asthe table in Figure 19.1shows, if wedon’t need the UNION operation, ordi-
nary binary heaps, as used in heapsort (Chapter 6), work fairly well. Operations
other than UNION run in worst-case time O.lgn/ on a binary heap. If we need
to support the UNION operation, however, binary heaps perform poorly. By con-
catenatingthetwoarraysthatholdthebinaryheapstobemergedandthenrunning
BUILD-MIN-HEAP (seeSection6.3),theUNION operationtakes‚.n/timeinthe
worstcase.
Fibonacci heaps, on the other hand, have better asymptotic time bounds than
binary heaps for the INSERT, UNION, and DECREASE-KEY operations, and they
have thesameasymptotic running timesfortheremaining operations. Note, how-
ever, thatthe running timesforFibonacci heaps inFigure19.1 areamortized time
bounds, not worst-case per-operation time bounds. The UNION operation takes
only constant amortized time in a Fibonacci heap, which is significantly better
thanthelinearworst-casetimerequiredinabinaryheap(assuming,ofcourse,that
anamortizedtimeboundsuffices).
Fibonacciheapsintheoryandpractice
From a theoretical standpoint, Fibonacci heaps are especially desirable when the
numberofEXTRACT-MIN and DELETE operations issmallrelativetothenumber
of other operations performed. This situation arises in many applications. For
example,somealgorithmsforgraphproblemsmaycallDECREASE-KEY onceper
edge. Fordense graphs, whichhavemanyedges, the‚.1/amortized timeofeach
callofDECREASE-KEY addsuptoabigimprovementoverthe‚.lgn/worst-case
time of binary heaps. Fast algorithms for problems such as computing minimum
spanning trees (Chapter 23) and finding single-source shortest paths (Chapter 24)
makeessential useofFibonacciheaps.
19.1 StructureofFibonacciheaps 507
From a practical point of view, however, the constant factors and program-
mingcomplexityofFibonacciheapsmakethemlessdesirablethanordinarybinary
(ork-ary)heapsformostapplications, exceptforcertainapplications thatmanage
large amounts ofdata. Thus, Fibonacci heaps arepredominantly oftheoretical in-
terest. If a much simpler data structure with the same amortized time bounds as
Fibonacciheapsweredeveloped, itwouldbeofpractical useaswell.
Both binary heaps and Fibonacci heaps are inefficient in how they support the
operation SEARCH;itcantakeawhiletofindanelementwithagivenkey. Forthis
reason,operations suchas DECREASE-KEY and DELETE thatrefertoagivenele-
mentrequireapointertothatelementaspartoftheirinput. Asinourdiscussionof
priorityqueuesinSection6.5,whenweuseamergeableheapinanapplication, we
oftenstoreahandletothecorrespondingapplicationobjectineachmergeable-heap
element, aswellasahandle tothecorresponding mergeable-heap elementineach
application object. The exact nature of these handles depends on the application
anditsimplementation.
Like several other data structures that wehave seen, Fibonacci heaps are based
on rooted trees. We represent each element by a node within a tree, and each
node has a key attribute. For the remainder of this chapter, we shall use the term
“node” instead of“element.” Weshall also ignore issues ofallocating nodes prior
to insertion and freeing nodes following deletion, assuming instead that the code
callingtheheapprocedures dealswiththesedetails.
Section 19.1 defines Fibonacci heaps, discusses how we represent them, and
presents the potential function used for their amortized analysis. Section 19.2
showshowtoimplementthemergeable-heapoperationsandachievetheamortized
time bounds shown in Figure 19.1. The remaining two operations, DECREASE-
KEY and DELETE, formthefocusofSection19.3. Finally, Section19.4finishesa
keypartoftheanalysisandalsoexplainsthecuriousnameofthedatastructure.
19.1 Structure ofFibonacci heaps
A Fibonacci heap is a collection of rooted trees that are min-heap ordered. That
is,eachtreeobeysthemin-heapproperty: thekeyofanodeisgreaterthanorequal
tothekeyofitsparent. Figure19.2(a)showsanexampleofaFibonacciheap.
As Figure 19.2(b) shows, each node x contains a pointer x:p to its parent and
a pointer x:child to any one of its children. The children of x are linked together
in a circular, doubly linked list, which we call the child list of x. Each child y in
a child list has pointers y:left and y:right that point to y’s left and right siblings,
respectively. Ifnode y isan only child, then y:left y:right y. Siblings may
D D
appearinachildlistinanyorder.
508 Chapter19 FibonacciHeaps
H:min
(a) 23 7 3 17 24
18 52 38 30 26 46
39 41 35
H:min
(b) 23 7 3 17 24
18 52 38 30 26 46
39 41 35
Figure19.2 (a) A Fibonacci heap consisting of fivemin-heap-ordered trees and 14 nodes. The
dashedlineindicatestherootlist. Theminimumnodeoftheheapisthenodecontainingthekey3.
Blacknodesaremarked.ThepotentialofthisparticularFibonacciheapis5 2 3 11.(b)Amore
C  D
complete representation showing pointers p (up arrows), child (down arrows), and left and right
(sidewaysarrows).Theremainingfiguresinthischapteromitthesedetails,sincealltheinformation
shownherecanbedeterminedfromwhatappearsinpart(a).
Circular, doubly linked lists (see Section 10.2) have two advantages for use in
Fibonacci heaps. First, we can insert a node into any location or remove a node
from anywhere in a circular, doubly linked list in O.1/ time. Second, given two
such lists, we can concatenate them (or “splice” them together) into one circular,
doubly linked list in O.1/ time. In the descriptions of Fibonacci heap operations,
we shall refer to these operations informally, letting you fill in the details of their
implementations ifyouwish.
Eachnodehastwootherattributes. Westorethenumberofchildreninthechild
listofnodex inx:degree. Theboolean-valued attribute x:markindicates whether
node x has lost a child since the last time x was made the child of another node.
Newlycreated nodes are unmarked, and anode x becomes unmarked whenever it
ismadethechildofanothernode. UntilwelookattheDECREASE-KEY operation
inSection19.3,wewilljustsetallmarkattributes to FALSE.
We access a given Fibonacci heap H by a pointer H:min to the root of a tree
containingtheminimumkey;wecallthisnodetheminimumnodeoftheFibonacci
19.1 StructureofFibonacciheaps 509
heap. Ifmorethanoneroothasakeywiththeminimumvalue, thenanysuchroot
may serve as the minimum node. When a Fibonacci heap H is empty, H:min
isNIL.
The roots of all the trees in a Fibonacci heap are linked together using their
left and right pointers into a circular, doubly linked list called the root list of the
Fibonacci heap. The pointer H:min thus points to the node in the root list whose
keyisminimum. Treesmayappearinanyorderwithinarootlist.
We rely on one other attribute for a Fibonacci heap H: H:n, the number of
nodescurrentlyinH.
Potentialfunction
As mentioned, we shall use the potential method of Section 17.3 to analyze the
performance of Fibonacci heap operations. For a given Fibonacci heap H, we
indicatebyt.H/thenumberoftreesintherootlistofH andbym.H/thenumber
of marked nodes in H. We then define the potential ˆ.H/ of Fibonacci heap H
by
ˆ.H/ t.H/ 2m.H/: (19.1)
D C
(WewillgainsomeintuitionforthispotentialfunctioninSection19.3.) Forexam-
ple,thepotentialoftheFibonacciheapshowninFigure19.2is5 2 3 11. The
C  D
potential of a set of Fibonacci heaps is the sum of the potentials of its constituent
Fibonacci heaps. We shall assume that a unit of potential can pay for a constant
amountofwork,wheretheconstant issufficientlylargetocoverthecostofanyof
thespecificconstant-time piecesofworkthatwemightencounter.
We assume that a Fibonacci heap application begins with no heaps. The initial
potential, therefore, is 0, and by equation (19.1), the potential is nonnegative at
allsubsequent times. Fromequation(17.3),anupperboundonthetotalamortized
costprovidesanupperboundonthetotalactualcostforthesequenceofoperations.
Maximumdegree
Theamortized analyses weshall perform intheremaining sections ofthischapter
assume that we know an upper bound D.n/ on the maximum degree of any node
in an n-node Fibonacci heap. We won’t prove it, but when only the mergeable-
heapoperationsaresupported,D.n/ lgn . (Problem19-2(d)asksyoutoprove
 b c
this property.) In Sections 19.3 and 19.4, we shall show that when we support
DECREASE-KEY and DELETE aswell,D.n/ O.lgn/.
D
510 Chapter19 FibonacciHeaps
19.2 Mergeable-heap operations
Themergeable-heapoperationsonFibonacciheapsdelayworkaslongaspossible.
Thevariousoperationshaveperformancetrade-offs. Forexample,weinsertanode
by adding it to the root list, which takes just constant time. If we were to start
with an empty Fibonacci heap and then insert k nodes, the Fibonacci heap would
consist of just a root list of k nodes. The trade-off is that if we then perform
an EXTRACT-MIN operation on Fibonacci heap H, after removing the node that
H:minpointsto,wewouldhavetolookthrougheachoftheremainingk 1nodes
(cid:0)
in the root list to find the new minimum node. As long as we have to go through
theentirerootlistduringthe EXTRACT-MIN operation, wealsoconsolidate nodes
intomin-heap-ordered treestoreducethesizeoftherootlist. Weshallseethat,no
matter what the root list looks like before an EXTRACT-MIN operation, afterward
eachnodeintherootlisthasadegreethatisuniquewithintherootlist,whichleads
toarootlistofsizeatmostD.n/ 1.
C
CreatinganewFibonacciheap
TomakeanemptyFibonacci heap, the MAKE-FIB-HEAP procedure allocates and
returns the Fibonacci heap object H, where H:n 0 and H:min NIL; there
D D
are no trees in H. Because t.H/ 0 and m.H/ 0, the potential of the empty
D D
Fibonacci heap is ˆ.H/ 0. The amortized cost of MAKE-FIB-HEAP is thus
D
equaltoitsO.1/actualcost.
Insertinganode
Thefollowing procedure inserts node x intoFibonacci heap H,assuming that the
nodehasalreadybeenallocated andthatx:keyhasalreadybeenfilledin.
FIB-HEAP-INSERT.H;x/
1 x:degree 0
D
2 x:p NIL
D
3 x:child NIL
D
4 x:mark FALSE
D
5 ifH:min == NIL
6 createarootlistforH containing justx
7 H:min x
D
8 elseinsertx intoH’srootlist
9 ifx:key < H:min:key
10 H:min x
D
11 H:n H:n 1
D C
19.2 Mergeable-heapoperations 511
H:min H:min
23 7 3 17 24 23 7 21 3 17 24
18 52 38 30 26 46 18 52 38 30 26 46
39 41 35 39 41 35
(a) (b)
Figure19.3 InsertinganodeintoaFibonacciheap.(a)AFibonacciheapH.(b)FibonacciheapH
afterinsertingthenodewithkey21. Thenodebecomesitsownmin-heap-orderedtreeandisthen
addedtotherootlist,becomingtheleftsiblingoftheroot.
Lines 1–4 initialize some ofthe structural attributes of node x. Line 5tests tosee
whether Fibonacci heap H is empty. If it is, then lines 6–7 make x be the only
node in H’s root list and set H:min to point to x. Otherwise, lines 8–10 insert x
into H’sroot list and update H:minifnecessary. Finally, line 11increments H:n
to reflect the addition of the new node. Figure 19.3 shows a node with key 21
insertedintotheFibonacci heapofFigure19.2.
To determine the amortized cost of FIB-HEAP-INSERT, let H be the input Fi-
bonacci heap and H be the resulting Fibonacci heap. Then, t.H / t.H/ 1
0 0
D C
andm.H / m.H/,andtheincreaseinpotential is
0
D
..t.H/ 1/ 2m.H// .t.H/ 2m.H// 1:
C C (cid:0) C D
SincetheactualcostisO.1/,theamortizedcostisO.1/ 1 O.1/.
C D
Findingtheminimumnode
The minimum node of a Fibonacci heap H is given by the pointer H:min, so we
can findtheminimum node inO.1/ actual time. Because the potential ofH does
notchange, theamortized costofthisoperation isequaltoitsO.1/actualcost.
UnitingtwoFibonacciheaps
ThefollowingprocedureunitesFibonacciheapsH andH ,destroyingH andH
1 2 1 2
in the process. It simply concatenates the root lists of H and H and then deter-
1 2
minesthenewminimumnode. Afterward,theobjectsrepresentingH andH will
1 2
neverbeusedagain.
512 Chapter19 FibonacciHeaps
FIB-HEAP-UNION.H 1;H 2/
1 H MAKE-FIB-HEAP./
D
2 H:min H :min
1
D
3 concatenate therootlistofH withtherootlistofH
2
4 if.H 1:min == NIL/or.H 2:min NIL andH 2:min:key < H 1:min:key/
¤
5 H:min H :min
2
D
6 H:n H :n H :n
1 2
D C
7 returnH
Lines 1–3 concatenate the root lists of H and H into a new root list H. Lines
1 2
2, 4, and 5 set the minimum node of H, and line 6 sets H:n to the total number
of nodes. Line 7 returns the resulting Fibonacci heap H. As in the FIB-HEAP-
INSERT procedure, allrootsremainroots.
Thechangeinpotential is
ˆ.H/ .ˆ.H / ˆ.H //
1 2
(cid:0) C
.t.H/ 2m.H// ..t.H / 2m.H // .t.H / 2m.H ///
1 1 2 2
D C (cid:0) C C C
0;
D
because t.H/ t.H / t.H / and m.H/ m.H / m.H /. The amortized
1 2 1 2
D C D C
costof FIB-HEAP-UNION istherefore equaltoitsO.1/actualcost.
Extractingtheminimumnode
The process of extracting the minimum node is the most complicated of the oper-
ations presented inthissection. Itisalsowherethedelayed workofconsolidating
trees in the root list finally occurs. The following pseudocode extracts the mini-
mum node. Thecode assumes for convenience that whenanode isremoved from
alinkedlist,pointersremaininginthelistareupdated,butpointersintheextracted
node are left unchanged. It also calls the auxiliary procedure CONSOLIDATE,
whichweshallseeshortly.
19.2 Mergeable-heapoperations 513
FIB-HEAP-EXTRACT-MIN.H/
1 ´ H:min
D
2 if´ NIL
¤
3 foreachchildx of´
4 addx totherootlistofH
5 x:p NIL
D
6 remove´fromtherootlistofH
7 if´==´:right
8 H:min NIL
D
9 elseH:min ´:right
D
10 CONSOLIDATE.H/
11 H:n H:n 1
D (cid:0)
12 return´
AsFigure19.4illustrates, FIB-HEAP-EXTRACT-MIN worksbyfirstmakingaroot
outofeachoftheminimumnode’schildrenandremovingtheminimumnodefrom
therootlist. Itthenconsolidates therootlistbylinking rootsofequal degreeuntil
atmostonerootremainsofeachdegree.
We start in line 1 by saving a pointer ´ to the minimum node; the procedure
returnsthispointerattheend. If´isNIL,thenFibonacciheapH isalreadyempty
and weare done. Otherwise, wedelete node ´from H by making all of ´’s chil-
drenroots ofH inlines3–5 (putting them into theroot list) andremoving ´from
the root list in line 6. If ´ is its own right sibling after line 6, then ´ was the
only node on the root list and it had no children, so all that remains is to make
the Fibonacci heap empty in line 8 before returning ´. Otherwise, we set the
pointer H:min into the root list to point to a root other than ´ (in this case, ´’s
right sibling), which is not necessarily going to be the new minimum node when
FIB-HEAP-EXTRACT-MIN is done. Figure 19.4(b) shows the Fibonacci heap of
Figure19.4(a)afterexecutingline9.
Thenextstep, inwhichwereduce thenumber oftreesintheFibonacci heap, is
consolidating therootlistofH,whichthecall CONSOLIDATE.H/accomplishes.
Consolidatingtherootlistconsistsofrepeatedlyexecutingthefollowingstepsuntil
everyrootintherootlisthasadistinct degreevalue:
1. Find two roots x and y in the root list with the same degree. Without loss of
generality, letx:key y:key.

2. Linky tox: removey fromtherootlist,andmakey achildofx bycallingthe
FIB-HEAP-LINK procedure. This procedure increments the attribute x:degree
andclearsthemarkony.
514 Chapter19 FibonacciHeaps
H:min H:min
(a) 23 7 21 3 17 24 (b) 23 7 21 18 52 38 17 24
18 52 38 30 26 46 39 41 30 26 46
39 41 35 35
0 1 2 3 0 1 2 3
A A
w,x w,x
(c) 23 7 21 18 52 38 17 24 (d) 23 7 21 18 52 38 17 24
39 41 30 26 46 39 41 30 26 46
35 35
0 1 2 3 0 1 2 3
A A
w,x w,x
(e) 23 7 21 18 52 38 17 24 (f) 7 21 18 52 38 17 24
39 41 30 26 46 23 39 41 30 26 46
35 35
0 1 2 3 0 1 2 3
A A
w,x w,x
(g) 7 21 18 52 38 24 (h) 7 21 18 52 38
17 23 39 41 26 46 24 17 23 39 41
30 35 26 46 30
35
Figure19.4 Theactionof FIB-HEAP-EXTRACT-MIN. (a) AFibonacci heap H. (b)Thesitua-
tionafterremovingtheminimumnode´fromtherootlistandaddingitschildrentotherootlist.
(c)–(e)ThearrayAandthetreesaftereachofthefirstthreeiterationsoftheforloopoflines4–14of
theprocedureCONSOLIDATE. Theprocedureprocessestherootlistbystartingatthenodepointed
tobyH:minandfollowingrightpointers. Eachpartshowsthevaluesofw andx attheendofan
iteration. (f)–(h)Thenextiterationoftheforloop,withthevaluesofwandx shownattheendof
eachiterationofthewhileloopoflines7–13.Part(f)showsthesituationafterthefirsttimethrough
thewhileloop.Thenodewithkey23hasbeenlinkedtothenodewithkey7,whichxnowpointsto.
Inpart(g),thenodewithkey17hasbeenlinkedtothenodewithkey7,whichx stillpointsto. In
part(h),thenodewithkey24hasbeenlinkedtothenodewithkey7.Sincenonodewaspreviously
pointedtobyAŒ3,attheendoftheforloopiteration,AŒ3issettopointtotherootoftheresulting
tree.
19.2 Mergeable-heapoperations 515
0 1 2 3 0 1 2 3
A A
w,x w,x
(i) 7 21 18 52 38 (j) 7 21 18 52 38
24 17 23 39 41 24 17 23 39 41
26 46 30 26 46 30
35 35
0 1 2 3 0 1 2 3
A A
x w,x
(k) 7 18 38 (l) 7 18 38
24 17 23 21 39 41 24 17 23 21 39 41
26 46 30 52 w 26 46 30 52
35 35
H:min
(m) 7 18 38
24 17 23 21 39 41
26 46 30 52
35
Figure19.4,continued (i)–(l)Thesituationaftereachofthenextfouriterationsoftheforloop.
(m)FibonacciheapH afterreconstructingtherootlistfromthearrayAanddeterminingthenew
H:minpointer.
The procedure CONSOLIDATE uses an auxiliary array AŒ0::D.H:n/ to keep
track of roots according to their degrees. If AŒi y, then y is currently a root
D
withy:degree i. Ofcourse, inorder toallocate thearraywehavetoknowhow
D
tocalculatetheupperboundD.H:n/onthemaximumdegree,butwewillseehow
todosoinSection19.4.
516 Chapter19 FibonacciHeaps
CONSOLIDATE.H/
1 letAŒ0::D.H:n/beanewarray
2 fori 0toD.H:n/
D
3 AŒi NIL
D
4 foreachnodew intherootlistofH
5 x w
D
6 d x:degree
D
7 whileAŒd NIL
¤
8 y AŒd //another nodewiththesamedegreeasx
D
9 ifx:key> y:key
10 exchange x withy
11 FIB-HEAP-LINK.H;y;x/
12 AŒd NIL
D
13 d d 1
D C
14 AŒd x
D
15 H:min NIL
D
16 fori 0toD.H:n/
D
17 ifAŒi NIL
¤
18 ifH:min== NIL
19 createarootlistforH containing justAŒi
20 H:min AŒi
D
21 elseinsertAŒiintoH’srootlist
22 ifAŒi:key <H:min:key
23 H:min AŒi
D
FIB-HEAP-LINK.H;y;x/
1 removey fromtherootlistofH
2 makey achildofx,incrementing x:degree
3 y:mark FALSE
D
In detail, the CONSOLIDATE procedure works as follows. Lines 1–3 allocate
and initialize the array A by making each entry NIL. The for loop of lines 4–14
processes each root w inthe root list. Aswelink roots together, w maybe linked
to some other node and no longer be a root. Nevertheless, w is always in a tree
rooted at some node x, which may or may not be w itself. Because we want at
most one root with each degree, welook in the array A to see whether it contains
a root y with the same degree as x. If it does, then we link the roots x and y but
guaranteeing that x remains a root after linking. That is, we link y to x after first
exchanging the pointers to the two roots if y’s key is smaller than x’s key. After
welinky tox,thedegreeofx hasincreasedby1,andsowecontinuethisprocess,
linkingx andanotherrootwhosedegreeequalsx’snewdegree,untilnootherroot
19.2 Mergeable-heapoperations 517
thatwehaveprocessedhasthesamedegreeasx. Wethensettheappropriateentry
ofA topoint to x,so that asweprocess roots later on, wehave recorded that x is
the unique root of its degree that we have already processed. When this for loop
terminates, atmostonerootofeachdegreewillremain, andthearrayAwillpoint
toeachremaining root.
The while loop of lines 7–13 repeatedly links the root x of the tree containing
nodewtoanothertreewhoseroothasthesamedegreeasx,untilnootherroothas
thesamedegree. Thiswhileloopmaintains thefollowinginvariant:
Atthestartofeachiterationofthewhileloop, d x:degree.
D
Weusethisloopinvariant asfollows:
Initialization: Line6 ensures that the loop invariant holds the first time weenter
theloop.
Maintenance: In each iteration of the while loop, AŒd points to some root y.
Because d x:degree y:degree, we want to link x and y. Whichever of
D D
x and y has the smaller key becomes the parent of the other as a result of the
linkoperation, andsolines9–10exchangethepointerstox andy ifnecessary.
Next, we link y to x by the call FIB-HEAP-LINK.H;y;x/ in line 11. This
callincrements x:degreebutleavesy:degreeasd. Nodey isnolonger aroot,
and so line 12 removes the pointer to it in array A. Because the call of FIB-
HEAP-LINK increments the value of x:degree, line 13 restores the invariant
thatd x:degree.
D
Termination: Werepeat the while loop until AŒd NIL, in which case there is
D
nootherrootwiththesamedegreeasx.
After the while loop terminates, we set AŒd to x in line 14 and perform the next
iterationoftheforloop.
Figures 19.4(c)–(e) show the array A and the resulting trees after the first three
iterations of the for loop of lines 4–14. In the next iteration of the for loop, three
linksoccur;theirresultsareshowninFigures19.4(f)–(h). Figures19.4(i)–(l)show
theresultofthenextfouriterations oftheforloop.
All that remains is to clean up. Once the for loop of lines 4–14 completes,
line 15 empties the root list, and lines 16–23 reconstruct it from the array A. The
resulting Fibonacci heap appears in Figure 19.4(m). After consolidating the root
list, FIB-HEAP-EXTRACT-MIN finishes up by decrementing H:n in line 11 and
returning apointertothedeleted node´inline12.
We are now ready to show that the amortized cost of extracting the minimum
node of an n-node Fibonacci heap is O.D.n//. LetH denote the Fibonacci heap
justpriortothe FIB-HEAP-EXTRACT-MIN operation.
We start by accounting for the actual cost of extracting the minimum node.
An O.D.n// contribution comes from FIB-HEAP-EXTRACT-MIN processing at
518 Chapter19 FibonacciHeaps
most D.n/ children of the minimum node and from the work in lines 2–3 and
16–23 of CONSOLIDATE. Itremainstoanalyze thecontribution from theforloop
oflines4–14 in CONSOLIDATE, forwhichweuseanaggregate analysis. Thesize
of the root list upon calling CONSOLIDATE is at most D.n/ t.H/ 1, since it
C (cid:0)
consists of the original t.H/ root-list nodes, minus the extracted root node, plus
the children of the extracted node, which number at most D.n/. Within a given
iterationoftheforloopoflines4–14,thenumberofiterationsofthewhileloopof
lines7–13dependsontherootlist. Butweknowthateverytimethroughthewhile
loop, one of the roots is linked to another, and thus the total number of iterations
of the while loop over all iterations of the for loop is at most the number of roots
in the root list. Hence, the total amount of work performed in the for loop is at
most proportional to D.n/ t.H/. Thus, the total actual work in extracting the
C
minimumnodeisO.D.n/ t.H//.
C
The potential before extracting the minimum node is t.H/ 2m.H/, and the
C
potentialafterwardisatmost.D.n/ 1/ 2m.H/,sinceatmostD.n/ 1roots
C C C
remain and no nodes become marked during the operation. The amortized cost is
thusatmost
O.D.n/ t.H// ..D.n/ 1/ 2m.H// .t.H/ 2m.H//
C C C C (cid:0) C
O.D.n// O.t.H// t.H/
D C (cid:0)
O.D.n//;
D
since we can scale up the units of potential to dominate the constant hidden
in O.t.H//. Intuitively, the cost of performing each link is paid for by the re-
ductioninpotentialduetothelink’sreducingthenumberofrootsbyone. Weshall
see in Section 19.4 that D.n/ O.lgn/, so that the amortized cost of extracting
D
theminimumnodeisO.lgn/.
Exercises
19.2-1
Show the Fibonacci heap that results from calling FIB-HEAP-EXTRACT-MIN on
theFibonacciheapshowninFigure19.4(m).
19.3 Decreasing akey anddeleting a node
In this section, we show how to decrease the key of a node in a Fibonacci heap
in O.1/ amortized time and how to delete any node from an n-node Fibonacci
heap in O.D.n// amortized time. In Section 19.4, we will show that the maxi-
19.3 Decreasingakeyanddeletinganode 519
mum degree D.n/ is O.lgn/, which will imply that FIB-HEAP-EXTRACT-MIN
and FIB-HEAP-DELETE runinO.lgn/amortizedtime.
Decreasingakey
In the following pseudocode for the operation FIB-HEAP-DECREASE-KEY, we
assume as before that removing a node from a linked list does not change any of
thestructural attributes intheremovednode.
FIB-HEAP-DECREASE-KEY.H;x;k/
1 ifk >x:key
2 error“newkeyisgreaterthancurrentkey”
3 x:key k
D
4 y x:p
D
5 ify NIL andx:key < y:key
¤
6 CUT.H;x;y/
7 CASCADING-CUT.H;y/
8 ifx:key < H:min:key
9 H:min x
D
CUT.H;x;y/
1 removex fromthechildlistofy,decrementing y:degree
2 addx totherootlistofH
3 x:p NIL
D
4 x:mark FALSE
D
CASCADING-CUT.H;y/
1 ´ y:p
D
2 if´ NIL
¤
3 ify:mark == FALSE
4 y:mark TRUE
D
5 else CUT.H;y;´/
6 CASCADING-CUT.H;´/
The FIB-HEAP-DECREASE-KEY procedure works as follows. Lines 1–3 ensure
thatthenewkeyisnogreaterthanthecurrentkeyofx andthenassignthenewkey
to x. If x is a root or if x:key y:key, where y is x’s parent, then no structural

changesneedoccur,sincemin-heaporderhasnotbeenviolated. Lines4–5testfor
thiscondition.
If min-heap order has been violated, many changes may occur. We start by
cuttingx inline6. TheCUTprocedure “cuts”thelinkbetweenx anditsparenty,
makingx aroot.
520 Chapter19 FibonacciHeaps
Weusethemarkattributestoobtainthedesiredtimebounds. Theyrecordalittle
pieceofthehistoryofeachnode. Supposethatthefollowingeventshavehappened
tonodex:
1. atsometime,x wasaroot,
2. thenx waslinkedto(madethechildof)anothernode,
3. thentwochildren ofx wereremovedbycuts.
Assoonasthesecondchildhasbeenlost,wecutxfromitsparent,makingitanew
root. The attribute x:mark is TRUE if steps 1 and 2 have occurred and one child
of x has been cut. The CUT procedure, therefore, clears x:mark inline 4, since it
performs step 1. (We can now see why line 3 of FIB-HEAP-LINK clears y:mark:
nodey isbeinglinkedtoanothernode,andsostep2isbeingperformed. Thenext
timeachildofy iscut,y:markwillbesetto TRUE.)
Weare not yet done, because x might bethe second child cutfrom its parent y
since the time that y was linked to another node. Therefore, line 7 of FIB-HEAP-
DECREASE-KEY attempts to perform a cascading-cut operation on y. If y is a
root,thenthetestinline2ofCASCADING-CUTcausestheproceduretojustreturn.
Ify isunmarked, theprocedure marksitinline4,sinceitsfirstchildhasjustbeen
cut, and returns. If y ismarked, however, it has just lost its second child; y is cut
in line 5, and CASCADING-CUT calls itself recursively in line 6 on y’s parent ´.
TheCASCADING-CUT procedurerecursesitswayupthetreeuntilitfindseithera
rootoranunmarked node.
Onceallthecascadingcutshaveoccurred, lines8–9ofFIB-HEAP-DECREASE-
KEY finishupbyupdatingH:minifnecessary. Theonlynodewhosekeychanged
was the node x whose key decreased. Thus, the new minimum node is either the
original minimumnodeornodex.
Figure 19.5 shows the execution of two calls of FIB-HEAP-DECREASE-KEY,
starting with the Fibonacci heap shown in Figure 19.5(a). The first call, shown
in Figure 19.5(b), involves no cascading cuts. The second call, shown in Fig-
ures19.5(c)–(e), invokes twocascading cuts.
We shall now show that the amortized cost of FIB-HEAP-DECREASE-KEY is
only O.1/. We start by determining its actual cost. The FIB-HEAP-DECREASE-
KEY procedure takesO.1/time,plusthetimetoperformthecascadingcuts. Sup-
pose that a given invocation of FIB-HEAP-DECREASE-KEY results in c calls of
CASCADING-CUT (thecallmadefromline7of FIB-HEAP-DECREASE-KEY fol-
lowed by c 1 recursive calls of CASCADING-CUT). Each call of CASCADING-
(cid:0)
CUT takes O.1/ time exclusive of recursive calls. Thus, the actual cost of FIB-
HEAP-DECREASE-KEY, including allrecursivecalls,isO.c/.
Wenextcomputethechangeinpotential. LetH denotetheFibonacci heapjust
prior to the FIB-HEAP-DECREASE-KEY operation. The call to CUT in line 6 of
19.3 Decreasingakeyanddeletinganode 521
H:min H:min
(a) 7 18 38 (b) 15 7 18 38
24 17 23 21 39 41 24 17 23 21 39 41
26 46 30 52 26 30 52
35 35
H:min H:min
(c) 15 5 7 18 38 (d) 15 5 26 7 18 38
24 17 23 21 39 41 24 17 23 21 39 41
26 30 52 30 52
H:min
(e) 15 5 26 24 7 18 38
17 23 21 39 41
30 52
Figure19.5 Twocallsof FIB-HEAP-DECREASE-KEY. (a) Theinitial Fibonacci heap. (b)The
nodewithkey46hasitskeydecreasedto15.Thenodebecomesaroot,anditsparent(withkey24),
whichhadpreviously beenunmarked, becomesmarked. (c)–(e)Thenodewithkey35hasitskey
decreased to 5. In part (c), the node, now with key 5, becomes a root. Its parent, with key 26,
is marked, so a cascading cut occurs. The node with key 26 is cut from its parent and made an
unmarkedrootin(d). Anothercascadingcutoccurs,sincethenodewithkey24ismarkedaswell.
Thisnode iscut from itsparent and made an unmarked root in part (e). The cascading cuts stop
atthispoint,sincethenodewithkey7isaroot. (Evenifthisnodewerenotaroot,thecascading
cutswouldstop,sinceitisunmarked.) Part(e)showstheresultoftheFIB-HEAP-DECREASE-KEY
operation,withH:minpointingtothenewminimumnode.
FIB-HEAP-DECREASE-KEY creates a new tree rooted at node x and clears x’s
mark bit (which may have already been FALSE). Each call of CASCADING-CUT,
exceptforthelastone,cutsamarkednodeandclearsthemarkbit. Afterward,the
Fibonacciheapcontainst.H/ ctrees(theoriginalt.H/trees,c 1treesproduced
C (cid:0)
bycascadingcuts,andthetreerootedatx)andatmostm.H/ c 2markednodes
(cid:0) C
(c 1wereunmarkedbycascadingcutsandthelastcallofCASCADING-CUTmay
(cid:0)
havemarkedanode). Thechange inpotential istherefore atmost
..t.H/ c/ 2.m.H/ c 2// .t.H/ 2m.H// 4 c :
C C (cid:0) C (cid:0) C D (cid:0)
522 Chapter19 FibonacciHeaps
Thus,theamortized costofFIB-HEAP-DECREASE-KEY isatmost
O.c/ 4 c O.1/;
C (cid:0) D
sincewecanscaleuptheunitsofpotentialtodominatetheconstanthiddeninO.c/.
Youcannowseewhywedefinedthepotential function toinclude atermthatis
twice the number of marked nodes. When a marked node y is cut by a cascading
cut, itsmarkbitiscleared, whichreduces thepotential by2. Oneunitofpotential
pays for the cut and the clearing of the mark bit, and the other unit compensates
fortheunitincrease inpotential duetonodey becomingaroot.
Deletinganode
The following pseudocode deletes a node from an n-node Fibonacci heap in
O.D.n// amortized time. We assume that there is no key value of currently
(cid:0)1
intheFibonacciheap.
FIB-HEAP-DELETE.H;x/
1 FIB-HEAP-DECREASE-KEY.H;x; /
(cid:0)1
2 FIB-HEAP-EXTRACT-MIN.H/
FIB-HEAP-DELETE makesx becometheminimumnodeintheFibonacciheapby
giving it auniquely small key of . The FIB-HEAP-EXTRACT-MIN procedure
(cid:0)1
thenremovesnodex fromtheFibonacci heap. Theamortized timeof FIB-HEAP-
DELETE is the sum of the O.1/ amortized time of FIB-HEAP-DECREASE-KEY
andtheO.D.n//amortizedtimeofFIB-HEAP-EXTRACT-MIN. Sinceweshallsee
in Section 19.4 that D.n/ O.lgn/, the amortized time of FIB-HEAP-DELETE
D
isO.lgn/.
Exercises
19.3-1
Suppose that a root x in a Fibonacci heap is marked. Explain how x came to be
a marked root. Argue that it doesn’t matter to the analysis that x is marked, even
though itisnotarootthatwasfirstlinkedtoanother nodeandthenlostonechild.
19.3-2
JustifytheO.1/amortizedtimeofFIB-HEAP-DECREASE-KEY asanaveragecost
peroperation byusingaggregateanalysis.
19.4 Boundingthemaximumdegree 523
19.4 Bounding the maximumdegree
To prove that the amortized time of FIB-HEAP-EXTRACT-MIN and FIB-HEAP-
DELETE is O.lgn/, we must show that the upper bound D.n/ on the degree of
anynodeofann-nodeFibonacci heapisO.lgn/. Inparticular, weshallshowthat
D.n/ log n ,where isthegoldenratio,definedinequation (3.24)as

.1 p5/=˘2 1:61803::: :
D C D
Thekey totheanalysis isasfollows. Foreach node x withinaFibonacci heap,
define size.x/ to be the number of nodes, including x itself, in the subtree rooted
atx. (Notethat x need notbeintheroot list—itcan beanynode atall.) Weshall
showthatsize.x/isexponential inx:degree. Bearinmindthatx:degreeisalways
maintainedasanaccurate countofthedegreeofx.
Lemma19.1
Let x be any node in a Fibonacci heap, and suppose that x:degree k. Let
D
y ;y ;:::;y denotethechildrenofx intheorderinwhichtheywerelinkedtox,
1 2 k
from the earliest to the latest. Then, y :degree 0 and y :degree i 2 for
1 i
  (cid:0)
i 2;3;:::;k.
D
Proof Obviously, y :degree 0.
1

For i 2, we note that when y was linked to x, all of y ;y ;:::;y were
i 1 2 i 1
children of x, and so we must have had x:degree i 1. Because nod(cid:0) e y is
i
 (cid:0)
linked tox (by CONSOLIDATE) only ifx:degree y i:degree, wemust have also
D
had y :degree i 1 at that time. Since then, node y has lost at most one
i i
 (cid:0)
child, since it would have been cut from x (by CASCADING-CUT) if it had lost
twochildren. Weconclude thaty :degree i 2.
i
 (cid:0)
We finally come to the part of the analysis that explains the name “Fibonacci
heaps.” RecallfromSection3.2thatfork 0;1;2;:::,thekthFibonaccinumber
D
isdefinedbytherecurrence
0 ifk 0;
D
F 1 ifk 1;
k
D D
F F ifk 2:
k 1 k 2
 (cid:0) C (cid:0) 
ThefollowinglemmagivesanotherwaytoexpressF .
k
524 Chapter19 FibonacciHeaps
Lemma19.2
Forallintegersk 0,

k
F 1 F :
k 2 i
C D C
i 0
XD
Proof Theproofisbyinduction onk. Whenk 0,
D
0
1 F 1 F
i 0
C D C
i 0
XD
1 0
D C
F :
2
D
havW ee now assume the inductive hypothesis that F
k C1 D
1
C
k
i
D(cid:0)01F i, and we
P
F F F
k 2 k k 1
C D C C
k 1
(cid:0)
F 1 F
k i
D C C
!
i 0
XD
k
1 F :
i
D C
i 0
XD
Lemma19.3
Forallintegersk 0,the.k 2/ndFibonaccinumbersatisfiesF k.
k 2
 C C 
Proof Theproof isby induction on k. Thebase cases arefor k 0and k 1.
D D
When k 0 we have F 1 0, and when k 1 we have F 2 >
2 3
D D D D D
1:619 > 1. The inductive step is for k 2, and we assume that F > i for
i 2
i 0;1;:::;k 1. Recallthat isthepos itiverootofequation(3.23),Cx2 x 1.
D (cid:0) D C
Thus,wehave
F F F
k 2 k 1 k
C D C C
k 1 k 2 (bytheinductive hypothesis)
(cid:0) (cid:0)
 C
k 2. 1/
(cid:0)
D C
k 2 2 (byequation (3.23))
(cid:0)
D 
k :
D
Thefollowinglemmaanditscorollary completetheanalysis.
19.4 Boundingthemaximumdegree 525
Lemma19.4
Let x be any node in a Fibonacci heap, and let k x:degree. Then size.x/
D 
F k,where .1 p5/=2.
k 2
C  D C
Proof Let s denote the minimum possible size of any node of degree k in any
k
Fibonacci heap. Trivially, s 1 and s 2. The number s is at most size.x/
0 1 k
D D
and, because adding children to a node cannot decrease the node’s size, the value
of s increases monotonically with k. Consider some node ´, in any Fibonacci
k
heap, such that ´:degree k and size.´/ s . Because s size.x/, we
k k
D D 
compute a lower bound on size.x/ by computing a lower bound on s . As in
k
Lemma19.1,lety ;y ;:::;y denote thechildren of´intheorderinwhichthey
1 2 k
werelinkedto´. Tobounds ,wecountonefor´itselfandoneforthefirstchildy
k 1
(forwhichsize.y / 1),giving
1

size.x/ s
k

k
2 s
 C
yi:degree
i 2
XD
k
2 s ;
i 2
 C (cid:0)
i 2
XD
where the last line follows from Lemma 19.1 (so that y :degree i 2) and the
i
 (cid:0)
monotonicity ofs (sothats s ).
Wenow show
bk
y
inductioy ni: ode ngre ke t hati (cid:0)s2
F for all nonnegative integers k.
k k 2
Thebases, fork 0andk 1,aretrivial. FortC heinductivestep,weassumethat
D D
k 2andthats F fori 0;1;:::;k 1. Wehave
i i 2
  C D (cid:0)
k
s 2 s
k i 2
 C (cid:0)
i 2
XD
k
2 F
i
 C
i 2
XD
k
1 F
i
D C
i 0
XD
F (byLemma19.2)
k 2
D C
k (byLemma19.3) .

Thus,wehaveshownthatsize.x/ s F k.
k k 2
  C 
526 Chapter19 FibonacciHeaps
Corollary 19.5
ThemaximumdegreeD.n/ofanynodeinann-nodeFibonacciheapisO.lgn/.
Proof Let x be any node in an n-node Fibonacci heap, and let k x:degree.
D
By Lemma 19.4, we have n size.x/ k. Taking base- logarithms gives
 
us k log n. (In fact, because k is an integer, k log n .) The maximum
 
degreeD.n/ofanynodeisthusO.lgn/.
 ˘
Exercises
19.4-1
ProfessorPinocchioclaimsthattheheightofann-nodeFibonacciheapisO.lgn/.
Show that the professor is mistaken by exhibiting, for any positive integer n, a
sequence ofFibonacci-heap operations thatcreates aFibonacci heapconsisting of
justonetreethatisalinearchainofnnodes.
19.4-2
Suppose we generalize the cascading-cut rule to cut a node x from its parent as
soonasitlosesitskthchild,forsomeintegerconstantk. (TheruleinSection19.3
usesk 2.) Forwhatvaluesofk isD.n/ O.lgn/?
D D
Problems
19-1 Alternativeimplementation ofdeletion
Professor Pisano has proposed the following variant of the FIB-HEAP-DELETE
procedure, claimingthatitrunsfasterwhenthenodebeing deleted isnotthenode
pointed tobyH:min.
PISANO-DELETE.H;x/
1 ifx ==H:min
2 FIB-HEAP-EXTRACT-MIN.H/
3 elsey x:p
D
4 ify NIL
¤
5 CUT.H;x;y/
6 CASCADING-CUT.H;y/
7 addx’schildlisttotherootlistofH
8 removex fromtherootlistofH
ProblemsforChapter19 527
a. The professor’s claim that this procedure runs faster is based partly on the as-
sumptionthatline7canbeperformedinO.1/actualtime. Whatiswrongwith
thisassumption?
b. Give a good upper bound on the actual time of PISANO-DELETE when x is
not H:min. Your bound should be in terms of x:degree and the number c of
callstothe CASCADING-CUT procedure.
c. SupposethatwecallPISANO-DELETE.H;x/,andletH 0betheFibonacciheap
that results. Assuming that node x is not a root, bound the potential of H in
0
termsofx:degree,c,t.H/,andm.H/.
d. Conclude that the amortized time for PISANO-DELETE is asymptotically no
betterthanfor FIB-HEAP-DELETE, evenwhenx H:min.
¤
19-2 Binomialtreesandbinomialheaps
The binomial tree B is an ordered tree (see Section B.5.2) defined recursively.
k
As shown in Figure 19.6(a), the binomial tree B consists of a single node. The
0
binomial tree B consists of two binomial trees B that are linked together so
k k 1
(cid:0)
that the root of one is the leftmost child of the root of the other. Figure 19.6(b)
showsthebinomialtreesB through B .
0 4
a. ShowthatforthebinomialtreeB ,
k
1. thereare2k nodes,
2. theheightofthetreeisk,
3. thereareexactly k nodesatdepthi fori 0;1;:::;k,and
i D
4. theroothasdegreek,whichisgreaterthanthatofanyothernode;moreover,
(cid:0) 
as Figure 19.6(c) shows, if we number the children of the root from left to
rightbyk 1;k 2;:::;0,thenchildi istherootofasubtree B .
i
(cid:0) (cid:0)
AbinomialheapH isasetofbinomialtreesthatsatisfiesthefollowingproper-
ties:
1. Eachnodehasakey(likeaFibonacciheap).
2. EachbinomialtreeinH obeysthemin-heapproperty.
3. For any nonnegative integer k, there is at most one binomial tree in H whose
roothasdegreek.
b. SupposethatabinomialheapH hasatotalofnnodes. Discusstherelationship
between thebinomial treesthat H contains and thebinary representation ofn.
ConcludethatH consists ofatmost lgn 1binomialtrees.
b cC
528 Chapter19 FibonacciHeaps
(a)
B
k–1
B
k–1
B B
0 k depth
0
1
(b) 2
3
4
B B B B B
0 1 2 3 4
(c)
B
0
B
B 1
2
B
B k–2
k–1
B
k
Figure19.6 (a)TherecursivedefinitionofthebinomialtreeB k. Trianglesrepresentrootedsub-
trees. (b)Thebinomial treesB0 through B4. NodedepthsinB4 areshown. (c)Another wayof
lookingatthebinomialtreeB k.
Suppose that we represent a binomial heap as follows. The left-child, right-
sibling scheme of Section 10.4 represents each binomial tree within a binomial
heap. Each node contains its key; pointers to its parent, to its leftmost child, and
to the sibling immediately to its right (these pointers are NIL when appropriate);
anditsdegree(asinFibonacci heaps,howmanychildrenithas). Therootsforma
singly linked root list, ordered by the degrees ofthe roots (from low tohigh), and
weaccessthebinomialheapbyapointertothefirstnodeontherootlist.
c. Complete the description of how to represent a binomial heap (i.e., name the
attributes, describewhenattributes havethevalue NIL,anddefinehowtheroot
list is organized), and show how to implement the same seven operations on
binomial heaps as this chapter implemented on Fibonacci heaps. Each opera-
tion should run in O.lgn/ worst-case time, where n is the number of nodes in
ProblemsforChapter19 529
the binomial heap (or in the case ofthe UNION operation, in the twobinomial
heaps thatarebeing united). The MAKE-HEAP operation should take constant
time.
d. Suppose that we were to implement only the mergeable-heap operations on a
Fibonacciheap(i.e.,wedonotimplementtheDECREASE-KEY orDELETEop-
erations). HowwouldthetreesinaFibonacciheapresemblethoseinabinomial
heap? How would they differ? Show that the maximum degree in an n-node
Fibonacci heapwouldbeatmost lgn .
b c
e. Professor McGee has devised a new data structure based on Fibonacci heaps.
A McGee heap has the same structure as a Fibonacci heap and supports just
the mergeable-heap operations. The implementations of the operations are the
same as for Fibonacci heaps, except that insertion and union consolidate the
root list astheir last step. Whatare theworst-case running times ofoperations
onMcGeeheaps?
19-3 MoreFibonacci-heap operations
We wish to augment a Fibonacci heap H to support two new operations without
changing theamortizedrunningtimeofanyotherFibonacci-heap operations.
a. The operation FIB-HEAP-CHANGE-KEY.H;x;k/ changes the key of node x
tothevaluek. Giveanefficientimplementation ofFIB-HEAP-CHANGE-KEY,
and analyze the amortized running time of your implementation for the cases
inwhichk isgreaterthan,lessthan,orequaltox:key.
b. Give an efficient implementation of FIB-HEAP-PRUNE.H;r/, which deletes
q min.r;H:n/ nodes fromH. Youmaychoose anyq nodes todelete. Ana-
D
lyzetheamortized running timeofyourimplementation. (Hint:Youmayneed
tomodifythedatastructure andpotential function.)
19-4 2-3-4heaps
Chapter18introducedthe2-3-4tree,inwhicheveryinternalnode(otherthanpos-
siblytheroot)hastwo,three,orfourchildrenandallleaveshavethesamedepth. In
thisproblem, weshall implement 2-3-4 heaps, whichsupport the mergeable-heap
operations.
The 2-3-4 heaps differ from 2-3-4 trees in the following ways. In 2-3-4 heaps,
onlyleavesstorekeys,andeachleafxstoresexactlyonekeyintheattributex:key.
The keys in the leaves may appear in any order. Each internal node x contains
a value x:small that is equal to the smallest key stored in any leaf in the subtree
rooted at x. The root r contains an attribute r:height that gives the height of the
530 Chapter19 FibonacciHeaps
tree. Finally, 2-3-4 heaps are designed to be kept in main memory, so that disk
readsandwritesarenotneeded.
Implementthefollowing2-3-4heapoperations. Inparts(a)–(e), eachoperation
shouldruninO.lgn/timeona2-3-4heapwithnelements. TheUNIONoperation
inpart(f)shouldruninO.lgn/time,wherenisthenumberofelementsinthetwo
inputheaps.
a. MINIMUM,whichreturns apointertotheleafwiththesmallestkey.
b. DECREASE-KEY, which decreases the key of a given leaf x to a given value
k x:key.

c. INSERT, whichinsertsleafx withkeyk.
d. DELETE, whichdeletesagivenleafx.
e. EXTRACT-MIN, whichextractstheleafwiththesmallestkey.
f. UNION, which unites two 2-3-4 heaps, returning a single 2-3-4 heap and de-
stroying theinputheaps.
Chapter notes
Fredmanand Tarjan[114]introduced Fibonacci heaps. Theirpaperalso describes
theapplication ofFibonacci heapstotheproblems ofsingle-source shortestpaths,
all-pairs shortest paths, weighted bipartite matching, and the minimum-spanning-
treeproblem.
Subsequently, Driscoll, Gabow, Shrairman, and Tarjan [96] developed “relaxed
heaps” as an alternative to Fibonacci heaps. They devised two varieties of re-
laxed heaps. One gives the same amortized time bounds as Fibonacci heaps. The
other allows DECREASE-KEY toruninO.1/worst-case (notamortized) timeand
EXTRACT-MIN and DELETE to run in O.lgn/ worst-case time. Relaxed heaps
alsohavesomeadvantages overFibonacciheapsinparallelalgorithms.
SeealsothechapternotesforChapter6forotherdatastructuresthatsupportfast
DECREASE-KEY operations when the sequence of values returned by EXTRACT-
MIN calls are monotonically increasing over time and the data are integers in a
specificrange.
20 van Emde Boas Trees
Inpreviouschapters,wesawdatastructuresthatsupporttheoperationsofapriority
queue—binary heaps in Chapter 6, red-black trees in Chapter 13,1 and Fibonacci
heaps in Chapter 19. In each of these data structures, at least one important op-
eration took O.lgn/ time, either worst case or amortized. In fact, because each
ofthesedatastructures basesitsdecisions oncomparing keys,the.nlgn/lower
bound for sorting in Section 8.1 tells us that at least one operation will have to
take.lgn/time. Why? IfwecouldperformboththeINSERTandEXTRACT-MIN
operations ino.lgn/time,thenwecould sortnkeysino.nlgn/timebyfirstper-
formingn INSERT operations, followedbyn EXTRACT-MIN operations.
Wesaw in Chapter 8, however, that sometimes we can exploit additional infor-
mation about the keys to sort in o.nlgn/ time. In particular, with counting sort
we can sort n keys, each an integer in the range 0 to k, in time ‚.n k/, which
C
is‚.n/whenk O.n/.
D
Sincewecancircumventthe.nlgn/lowerboundforsortingwhenthekeysare
integersinaboundedrange,youmightwonderwhetherwecanperformeachofthe
priority-queue operations in o.lgn/ time in asimilar scenario. In this chapter, we
shall see that wecan: van EmdeBoas trees support the priority-queue operations,
and a few others, each in O.lglgn/ worst-case time. The hitch is that the keys
mustbeintegersintherange0ton 1,withnoduplicates allowed.
(cid:0)
Specifically, van Emde Boas trees support each of the dynamic set operations
listed on page 230—SEARCH, INSERT, DELETE, MINIMUM, MAXIMUM, SUC-
CESSOR, and PREDECESSOR—in O.lglgn/ time. In this chapter, we will omit
discussionofsatellitedataandfocusonlyonstoringkeys. Becauseweconcentrate
onkeysanddisallowduplicatekeystobestored,insteadofdescribingtheSEARCH
1Chapter13doesnotexplicitlydiscusshowtoimplementEXTRACT-MINandDECREASE-KEY,but
wecaneasilybuildtheseoperationsforanydatastructurethatsupports MINIMUM, DELETE,and
INSERT.
532 Chapter20 vanEmdeBoasTrees
operation,wewillimplementthesimpleroperationMEMBER.S;x/,whichreturns
aboolean indicating whetherthevaluex iscurrently indynamicsetS.
So far, we have used the parameter n for two distinct purposes: the number of
elements in the dynamic set, and the range of the possible values. To avoid any
further confusion, from here on we will use n to denote the number of elements
currently in the set and u as the range of possible values, so that each van Emde
Boas tree operation runs in O.lglgu/ time. We call the set 0;1;2;:::;u 1
f (cid:0) g
the universe of values that can be stored and u the universe size. We assume
throughout thischapterthatuisanexactpowerof2,i.e.,u 2k forsomeinteger
D
k 1.

Section 20.1 starts us out by examining some simple approaches that will get
us going in the right direction. We enhance these approaches in Section 20.2,
introducingprotovanEmdeBoasstructures,whicharerecursivebutdonotachieve
ourgoalofO.lglgu/-timeoperations. Section20.3modifiesprotovanEmdeBoas
structures to develop van Emde Boas trees, and it shows how to implement each
operation inO.lglgu/time.
20.1 Preliminary approaches
In this section, we shall examine various approaches for storing a dynamic set.
AlthoughnonewillachievetheO.lglgu/timeboundsthatwedesire,wewillgain
insights thatwillhelpusunderstand vanEmdeBoastrees whenweseethem later
inthischapter.
Directaddressing
Direct addressing, as we saw in Section 11.1, provides the simplest approach to
storing a dynamic set. Since in this chapter we are concerned only with storing
keys, wecansimplifythedirect-addressing approach tostorethedynamic setasa
bit vector, as discussed in Exercise 11.1-2. To store a dynamic set of values from
theuniverse 0;1;2;:::;u 1 ,wemaintain anarrayAŒ0::u 1ofubits. The
f (cid:0) g (cid:0)
entryAŒxholdsa1ifthevaluex isinthedynamicset,anditholdsa0otherwise.
Although wecanperform eachofthe INSERT, DELETE, and MEMBER operations
inO.1/timewithabitvector,theremainingoperations—MINIMUM, MAXIMUM,
SUCCESSOR,andPREDECESSOR—each take‚.u/timeintheworstcasebecause
20.1 Preliminaryapproaches 533
1
1 1
1 1 0 1
0 1 1 1 0 0 0 1
A 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Figure 20.1 A binary tree of bits superimposed on top of a bit vector representing the set
2;3;4;5;7;14;15 when u 16. Each internal node contains a 1 if and only if some leaf in
f g D
itssubtreecontainsa1.Thearrowsshowthepathfollowedtodeterminethepredecessorof14inthe
set.
wemighthavetoscanthrough‚.u/elements.2 Forexample,ifasetcontainsonly
the values 0 and u 1, then to find the successor of 0, we would have to scan
(cid:0)
entries1through u 2beforefindinga1inAŒu 1.
(cid:0) (cid:0)
Superimposingabinarytreestructure
Wecanshort-cutlongscansinthebitvectorbysuperimposingabinarytreeofbits
ontop of it. Figure 20.1 showsan example. Theentries of thebit vector form the
leavesofthebinarytree,andeachinternalnodecontainsa1ifandonlyifanyleaf
in its subtree contains a1. In other words, the bit stored in an internal node isthe
logical-or ofitstwochildren.
Theoperationsthattook‚.u/worst-casetimewithanunadornedbitvectornow
usethetreestructure:
 To find the minimum value in the set, start at the root and head down toward
theleaves, alwaystakingtheleftmostnodecontaining a1.
 To find the maximum value in the set, start at the root and head down toward
theleaves, alwaystakingtherightmostnodecontaining a1.
2Weassumethroughout thischapterthat MINIMUMand MAXIMUMreturnNILifthedynamicset
isemptyandthat SUCCESSORandPREDECESSORreturnNILiftheelementtheyaregivenhasno
successororpredecessor,respectively.
534 Chapter20 vanEmdeBoasTrees
 Tofindthesuccessorofx,startattheleafindexedbyx,andheaduptowardthe
rootuntilweenteranodefromtheleftandthisnodehasa1initsrightchild´.
Then head down through node ´, always taking the leftmost node containing
a1(i.e.,findtheminimumvalueinthesubtreerootedattherightchild´).
 Tofindthepredecessor ofx,startattheleafindexedbyx,andheaduptoward
the root until we enter a node from the right and this node has a 1 in its left
child ´. Then head down through node ´, always taking the rightmost node
containing a 1 (i.e., find the maximum value in the subtree rooted at the left
child´).
Figure20.1showsthepathtakentofindthepredecessor, 7,ofthevalue14.
We also augment the INSERT and DELETE operations appropriately. When in-
serting avalue, westore a1 ineach node on the simple path from the appropriate
leaf up to the root. When deleting a value, we go from the appropriate leaf up to
the root, recomputing the bit in each internal node onthe path as the logical-or of
itstwochildren.
Since the height of the tree is lgu and each of the above operations makes at
mostonepassupthetreeandatmostonepassdown,eachoperationtakesO.lgu/
timeintheworstcase.
Thisapproach isonlymarginally betterthanjustusingared-black tree. Wecan
still perform the MEMBER operation inO.1/ time, whereas searching ared-black
tree takes O.lgn/ time. Then again, if the number n of elements stored is much
smaller than the size uof the universe, a red-black tree would be faster for all the
otheroperations.
Superimposingatreeofconstantheight
What happens if we superimpose a tree with greater degree? Let us assume that
the size of the universe is u 22k for some integer k, so that pu is an integer.
D
Instead of superimposing a binary tree on top of the bit vector, we superimpose a
tree of degree pu. Figure 20.2(a) shows such a tree for the same bit vector as in
Figure20.1. Theheightoftheresulting treeisalways2.
As before, each internal node stores the logical-or of the bits within its sub-
tree, so that the pu internal nodes at depth 1 summarize each group of pu val-
ues. As Figure 20.2(b) demonstrates, we can think of these nodes as an array
summaryŒ0::pu 1, where summaryŒi contains a 1 if and only if the subar-
(cid:0)
ray AŒipu::.i 1/pu 1 contains a 1. We call this pu-bit subarray of A
C (cid:0)
the ith cluster. For a given value of x, the bit AŒx appears in cluster num-
ber x=pu . Now INSERT becomes an O.1/-time operation: to insert x, set
b c
both AŒxand summaryŒ x=pu to 1. Wecan usethe summaryarray toperform
b c
20.1 Preliminaryapproaches 535
0 1 2 3
1
summary 1 1 0 1 pubits
1 1 0 1
pubits
A 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 A 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
(a) (b)
Figure20.2 (a)AtreeofdegreepusuperimposedontopofthesamebitvectorasinFigure20.1.
Eachinternalnodestoresthelogical-orofthebitsinitssubtree. (b)Aviewofthesamestructure,
butwiththeinternalnodesatdepth1treatedasanarraysummaryŒ0::pu 1,wheresummaryŒiis
(cid:0)
thelogical-orofthesubarrayAŒipu::.i 1/pu 1.
C (cid:0)
eachoftheoperations MINIMUM, MAXIMUM, SUCCESSOR, PREDECESSOR, and
DELETE inO.pu/time:
 To find the minimum (maximum) value, find the leftmost (rightmost) entry in
summary that contains a1, say summaryŒi, and then doa linear search within
theithclusterfortheleftmost(rightmost) 1.
 Tofindthesuccessor(predecessor) ofx,firstsearchtotheright(left)withinits
cluster. Ifwefinda1,thatpositiongivestheresult. Otherwise,leti x=pu
D b c
and search to the right (left) within the summary array from index i. The first
position that holds a 1 gives the index of a cluster. Search within that cluster
fortheleftmost(rightmost)1. Thatposition holdsthesuccessor (predecessor).
 Todelete thevalue x,let i x=pu . SetAŒxto0 and then set summaryŒi
D b c
tothelogical-or ofthebitsintheithcluster.
Ineachoftheaboveoperations, wesearchthroughatmosttwoclustersofpubits
plusthesummaryarray,andsoeachoperation takesO.pu/time.
Atfirstglance,itseemsasthoughwehavemadenegativeprogress. Superimpos-
ingabinary treegave usO.lgu/-time operations, whichare asymptotically faster
thanO.pu/time. Usingatreeofdegree puwillturn outtobeakeyidea ofvan
EmdeBoastrees,however. Wecontinue downthispathinthenextsection.
Exercises
20.1-1
Modifythedatastructures inthissectiontosupportduplicate keys.
536 Chapter20 vanEmdeBoasTrees
20.1-2
Modifythedatastructuresinthissectiontosupportkeysthathaveassociatedsatel-
litedata.
20.1-3
Observethat,usingthestructuresinthissection,thewaywefindthesuccessorand
predecessor of a value x does not depend on whether x is in the set at the time.
Showhowtofindthesuccessor ofx inabinarysearchtreewhenx isnotstoredin
thetree.
20.1-4
Suppose that instead of superimposing a tree of degree pu, we were to superim-
poseatreeofdegreeu1=k,wherek > 1isaconstant. Whatwouldbetheheightof
suchatree,andhowlongwouldeachoftheoperations take?
20.2 A recursive structure
Inthissection,wemodifytheideaofsuperimposing atreeofdegreepuontopof
abitvector. Intheprevioussection,weusedasummarystructureofsizepu,with
each entry pointing to another stucture of size pu. Now, we make the structure
recursive, shrinking theuniverse sizebythesquare rootateachlevelofrecursion.
Starting with a universe of size u, we make structures holding pu u1=2 items,
D
whichthemselvesholdstructuresofu1=4items,whichholdstructuresofu1=8items,
andsoon,downtoabasesizeof2.
For simplicity, in this section, we assume that u 22k for some integer k, so
D
thatu;u1=2;u1=4;:::areintegers. Thisrestrictionwouldbequitesevereinpractice,
allowingonlyvaluesofuinthesequence 2;4;16;256;65536;:::. Weshallseein
the next section how to relax this assumption and assume only that u 2k for
D
some integer k. Since the structure weexamine in this section is only a precursor
to the true van Emde Boas tree structure, we tolerate this restriction in favor of
aidingourunderstanding.
Recalling that our goal is to achieve running times of O.lglgu/ for the oper-
ations, let’s think about how we might obtain such running times. At the end of
Section4.3,wesawthatbychanging variables, wecouldshowthattherecurrence
T.n/ 2T pn lgn (20.1)
D C
has the solu(cid:0)tion T˘.n/ O.lgnlglgn/. Let’s consider a similar, but simpler,
D
recurrence:
T.u/ T.pu/ O.1/: (20.2)
D C
20.2 Arecursivestructure 537
If we use the same technique, changing variables, we can show that recur-
rence (20.2) has the solution T.u/ O.lglgu/. Let m lgu, so that u 2m
D D D
andwehave
T.2m/ T.2m=2/ O.1/:
D C
NowwerenameS.m/ T.2m/,givingthenewrecurrence
D
S.m/ S.m=2/ O.1/:
D C
Bycase2ofthemastermethod,thisrecurrence hasthesolutionS.m/ O.lgm/.
D
WechangebackfromS.m/toT.u/,givingT.u/ T.2m/ S.m/ O.lgm/
D D D D
O.lglgu/.
Recurrence (20.2) will guide our search for a data structure. We will design a
recursivedatastructurethatshrinksbyafactorofpuineachlevelofitsrecursion.
Whenanoperation traverses thisdatastructure, itwillspend aconstant amountof
timeateach level before recursing to thelevel below. Recurrence (20.2) willthen
characterize therunning timeoftheoperation.
Here is another way to think of how the term lglgu ends up in the solution to
recurrence(20.2). Aswelookattheuniversesizeineachleveloftherecursivedata
structure,weseethesequenceu;u1=2;u1=4;u1=8;:::. Ifweconsiderhowmanybits
we need to store the universe size at each level, we need lgu at the top level, and
each level needs half the bits of the previous level. In general, if we start with b
bits and halve the number of bits at each level, then after lgb levels, weget down
to just one bit. Since b lgu, wesee that after lglgulevels, wehave auniverse
D
sizeof2.
Looking back at the data structure in Figure 20.2, a given value x resides in
cluster number x=pu . If we view x as a lgu-bit binary integer, that cluster
b c
number, x=pu , is given by the most significant .lgu/=2 bits of x. Within its
b c
cluster, x appears in position x mod pu, which is given by the least significant
.lgu/=2 bits of x. We will need to index in this way, and so let us define some
functions thatwillhelpusdoso:
high.x/ x=pu ;
D
low.x/ x mod p ˘u;
D
index.x;y/ xpu y :
D C
The function high.x/ gives the most significant .lgu/=2 bits of x, producing the
numberofx’scluster. Thefunction low.x/givestheleastsignificant.lgu/=2bits
ofxandprovidesx’spositionwithinitscluster. Thefunctionindex.x;y/buildsan
elementnumberfromxandy,treatingxasthemostsignificant.lgu/=2bitsofthe
element number and y as the least significant .lgu/=2 bits. We have the identity
x index.high.x/;low.x//. The value of u used by each of these functions will
D
538 Chapter20 vanEmdeBoasTrees
proto-EB.u/ 0 1 2 3 … pu 1
(cid:0)
u summary cluster
proto-EB.pu/structure
puproto-EB.pu/structures
Figure20.3 Theinformationinaproto-EB.u/structurewhenu 4. Thestructurecontainsthe

universesizeu,apointersummarytoaproto-EB.pu/structure,andanarrayclusterŒ0::pu 1
(cid:0)
ofpupointerstoproto-EB.pu/structures.
always be the universe size of the data structure in which we call the function,
whichchanges aswedescendintotherecursive structure.
20.2.1 ProtovanEmdeBoasstructures
Taking our cue from recurrence (20.2), let us design a recursive data structure to
support theoperations. Althoughthisdatastructure willfailtoachieveourgoalof
O.lglgu/timeforsomeoperations, itservesasabasisforthevanEmdeBoastree
structure thatwewillseeinSection20.3.
For the universe 0;1;2;:::;u 1 , we define a proto van Emde Boas struc-
f (cid:0) g
ture, or proto-vEB structure, which we denote as proto-EB.u/, recursively as
follows. Each proto-EB.u/ structure contains an attribute u giving its universe
size. Inaddition, itcontainsthefollowing:
 Ifu 2,thenitisthebasesize,anditcontainsanarrayAŒ0::1oftwobits.
D
 Otherwise, u 22k for some integer k 1, so that u 4. In addition
D  
to the universe size u, the data structure proto-EB.u/ contains the following
attributes, illustrated inFigure20.3:
 apointernamedsummarytoaproto-EB.pu/structure and
 anarrayclusterŒ0::pu 1ofpupointers,eachtoaproto-EB.pu/struc-
(cid:0)
ture.
The element x, where 0 x < u, is recursively stored in the cluster numbered

high.x/aselementlow.x/withinthatcluster.
In the two-level structure of the previous section, each node stores a summary
array of size pu, in which each entry contains a bit. From the index of each
entry, we can compute the starting index of the subarray of size pu that the bit
summarizes. Intheproto-vEB structure, weuseexplicit pointers ratherthanindex
20.2 Arecursivestructure 539
proto-vEB(16) 0 1 2 3
u 16 summary cluster
proto-vEB(4) cluster
u summary 0 1
4
A
0
1
)2(BEv-otorp
proto-vEB(4) cluster proto-vEB(4) cluster
u summary 0 1 u summary 0 1
4 4
u
2
1
1
clusters 0,1 clusters 2,3 elements 0,1 elements 2,3 elements 4,5 elements 6,7
proto-vEB(4) cluster proto-vEB(4) cluster
u summary 0 1 u summary 0 1
4 4
A
0
1
elements 8,9 elements 10,11 elements 12,13elements 14,15
)2(BEv-otorp
u
2
A
0 1
1 1
)2(BEv-otorp
u
2
A
0 0
1 0
)2(BEv-otorp
u
2
A
0 0
1 1
)2(BEv-otorp
u
2
A
0 0
1 0
)2(BEv-otorp
u
2
A
0 0
1 0
)2(BEv-otorp
A
0
1
u
2
0
0
)2(BEv-otorp
u
2
A
0 0
1 1
)2(BEv-otorp
u
2
A
0 1
1 1
)2(BEv-otorp
u
2
A
0 1
1 1
)2(BEv-otorp
u
2
A
0 1
1 1
)2(BEv-otorp
u
2
A
0 0
1 0
)2(BEv-otorp
u
2
A
0 0
1 1
)2(BEv-otorp
u
2
A
0 0
1 1
)2(BEv-otorp
u
2
1
1
Figure20.4 Aproto-EB.16/structurerepresentingtheset 2;3;4;5;7;14;15 . Itpointstofour
f g
proto-EB.4/structuresinclusterŒ0::3,andtoasummarystructure,whichisalsoaproto-EB.4/.
Each proto-EB.4/ structure points to two proto-EB.2/ structures in clusterŒ0::1, and to a
proto-EB.2/ summary. Each proto-EB.2/ structure contains just an array AŒ0::1 of two bits.
Theproto-EB.2/structuresabove“elementsi,j”storebitsi andj oftheactualdynamicset,and
theproto-EB.2/structuresabove“clustersi,j”storethesummarybitsforclustersi andj inthe
top-levelproto-EB.16/structure. Forclarity,heavyshadingindicatesthetoplevelofaproto-vEB
structure that stores summary information for its parent structure; such a proto-vEB structure is
otherwiseidenticaltoanyotherproto-vEBstructurewiththesameuniversesize.
540 Chapter20 vanEmdeBoasTrees
calculations. Thearraysummarycontains thesummarybitsstoredrecursivelyina
proto-vEB structure, andthearraycluster contains pupointers.
Figure 20.4 shows a fully expanded proto-EB.16/ structure representing the
set 2;3;4;5;7;14;15 . If the value i is in the proto-vEB structure pointed to by
f g
summary, then the ith cluster contains some value in the set being represented.
As in the tree of constant height, clusterŒi represents the values ipu through
.i 1/pu 1,whichformtheithcluster.
C (cid:0)
At the base level, the elements of the actual dynamic sets are stored in some
of the proto-EB.2/ structures, and the remaining proto-EB.2/ structures store
summary bits. Beneath each of the non-summary base structures, the figure in-
dicates which bits it stores. For example, the proto-EB.2/ structure labeled
“elements 6,7” stores bit 6 (0, since element 6 is not in the set) in its AŒ0 and
bit7(1,sinceelement7isintheset)initsAŒ1.
Like the clusters, each summary is just a dynamic set with universe size pu ,
and so werepresent each summary as aproto-EB.pu/ structure. Thefour sum-
mary bits for the main proto-EB.16/ structure are in the leftmost proto-EB.4/
structure, and they ultimately appear in two proto-EB.2/ structures. For exam-
ple,theproto-EB.2/structurelabeled“clusters2,3”hasAŒ0 0,indicatingthat
D
cluster 2of the proto-EB.16/ structure (containing elements 8;9;10;11) isall 0,
and AŒ1 1, telling us that cluster 3 (containing elements 12;13;14;15) has at
D
leastone1. Eachproto-EB.4/structurepointstoitsownsummary,whichisitself
stored as a proto-EB.2/ structure. For example, look at the proto-EB.2/ struc-
ture justtotheleftoftheonelabeled “elements 0,1.” Because itsAŒ0is0,ittells
usthatthe“elements0,1”structureisall0,andbecauseitsAŒ1is1,weknowthat
the“elements2,3”structure contains atleastone1.
20.2.2 OperationsonaprotovanEmdeBoasstructure
We shall now describe how to perform operations on a proto-vEB struc-
ture. We first examine the query operations—MEMBER, MINIMUM, and
SUCCESSOR—which do not change the proto-vEB structure. We then discuss
INSERT and DELETE. We leave MAXIMUM and PREDECESSOR, which are sym-
metricto MINIMUM and SUCCESSOR, respectively, asExercise20.2-1.
EachoftheMEMBER, SUCCESSOR, PREDECESSOR, INSERT, andDELETE op-
erations takes a parameter x, along with a proto-vEB structure V. Each of these
operations assumesthat0 x <V:u.

Determiningwhetheravalueisintheset
To perform MEMBER.x/, we need to find the bit corresponding to x within the
appropriate proto-EB.2/ structure. We can do so in O.lglgu/ time, bypassing
20.2 Arecursivestructure 541
the summary structures altogether. The following procedure takes a proto-EB
structureV andavaluex,anditreturnsabitindicatingwhetherxisinthedynamic
setheldbyV.
PROTO-VEB-MEMBER.V;x/
1 ifV:u ==2
2 returnV:AŒx
3 elsereturn PROTO-VEB-MEMBER.V:clusterŒhigh.x/;low.x//
The PROTO-VEB-MEMBER procedure works as follows. Line 1 tests whether
we are in a base case, where V is a proto-EB.2/ structure. Line 2 handles the
base case, simply returning the appropriate bit of array A. Line 3 deals with the
recursive case, “drilling down” into the appropriate smaller proto-vEB structure.
The value high.x/ says which proto-EB.pu/ structure we visit, and low.x/ de-
termineswhichelementwithinthatproto-EB.pu/structure wearequerying.
Let’s see what happens when we call PROTO-VEB-MEMBER.V;6/ on the
proto-EB.16/ structure in Figure 20.4. Since high.6/ 1 when u 16, we
D D
recurse into the proto-EB.4/ structure in the upper right, and we ask about ele-
mentlow.6/ 2ofthatstructure. Inthisrecursive call,u 4,andsowerecurse
D D
again. With u 4, we have high.2/ 1 and low.2/ 0, and so we ask about
D D D
element0oftheproto-EB.2/structureintheupperright. Thisrecursivecallturns
out to be a base case, and so it returns AŒ0 0 back up through the chain of re-
D
cursive calls. Thus, weget the result that PROTO-VEB-MEMBER.V;6/ returns 0,
indicating that6isnotintheset.
To determine the running time of PROTO-VEB-MEMBER, let T.u/ denote
its running time on a proto-EB.u/ structure. Each recursive call takes con-
stant time, not including the time taken by the recursive calls that it makes.
When PROTO-VEB-MEMBER makes a recursive call, it makes a call on a
proto-EB.pu/structure. Thus,wecancharacterizetherunningtimebytherecur-
rence T.u/ T.pu/ O.1/, which we have already seen as recurrence (20.2).
D C
ItssolutionisT.u/ O.lglgu/,andsoweconcludethatPROTO-VEB-MEMBER
D
runsintimeO.lglgu/.
Findingtheminimumelement
Now we examine how to perform the MINIMUM operation. The procedure
PROTO-VEB-MINIMUM.V/returnstheminimumelementintheproto-vEBstruc-
tureV,or NIL ifV represents anemptyset.
542 Chapter20 vanEmdeBoasTrees
PROTO-VEB-MINIMUM.V/
1 ifV:u == 2
2 ifV:AŒ0 == 1
3 return0
4 elseifV:AŒ1==1
5 return1
6 elsereturn NIL
7 elsemin-cluster PROTO-VEB-MINIMUM.V:summary/
D
8 ifmin-cluster == NIL
9 return NIL
10 elseoffset PROTO-VEB-MINIMUM.V:clusterŒmin-cluster/
D
11 returnindex.min-cluster;offset/
Thisprocedure worksasfollows. Line1testsforthebasecase,whichlines2–6
handle bybrute force. Lines 7–11handle therecursive case. First, line 7findsthe
number of thefirst cluster that contains an element ofthe set. It does so by recur-
sivelycalling PROTO-VEB-MINIMUM onV:summary,whichisaproto-EB.pu/
structure. Line7assigns this cluster number tothe variable min-cluster. Ifthe set
is empty, then the recursive call returned NIL, and line 9 returns NIL. Otherwise,
the minimum element ofthe set issomewhere in cluster number min-cluster. The
recursive callinline10findstheoffsetwithinthecluster oftheminimumelement
in this cluster. Finally, line 11 constructs the value of the minimum element from
theclusternumberandoffset,anditreturnsthisvalue.
Although querying thesummaryinformation allowsustoquickly findtheclus-
ter containing the minimum element, because this procedure makes two recursive
calls on proto-EB.pu/ structures, it does not run in O.lglgu/ time in the worst
case. Letting T.u/ denote the worst-case time for PROTO-VEB-MINIMUM on a
proto-EB.u/structure, wehavetherecurrence
T.u/ 2T.pu/ O.1/: (20.3)
D C
Again, we use a change of variables to solve this recurrence, letting m lgu,
D
whichgives
T.2m/ 2T.2m=2/ O.1/:
D C
RenamingS.m/ T.2m/gives
D
S.m/ 2S.m=2/ O.1/;
D C
which,bycase1ofthemastermethod,hasthesolutionS.m/ ‚.m/. Bychang-
D
ing back from S.m/ to T.u/, we have that T.u/ T.2m/ S.m/ ‚.m/
D D D D
‚.lgu/. Thus, we see that because of the second recursive call, PROTO-VEB-
MINIMUM runsin‚.lgu/timeratherthanthedesiredO.lglgu/time.
20.2 Arecursivestructure 543
Findingthesuccessor
TheSUCCESSORoperationisevenworse. Intheworstcase,itmakestworecursive
calls, along with a call to PROTO-VEB-MINIMUM. The procedure PROTO-VEB-
SUCCESSOR.V;x/ returns the smallest element in the proto-vEB structure V that
isgreater thanx,or NIL ifnoelementinV isgreater thanx. Itdoesnotrequire x
tobeamemberoftheset,butitdoesassumethat0 x < V:u.

PROTO-VEB-SUCCESSOR.V;x/
1 ifV:u==2
2 ifx ==0andV:AŒ1 ==1
3 return1
4 elsereturn NIL
5 elseoffset PROTO-VEB-SUCCESSOR.V:clusterŒhigh.x/;low.x//
D
6 ifoffset NIL
¤
7 returnindex.high.x/;offset/
8 elsesucc-cluster PROTO-VEB-SUCCESSOR.V:summary;high.x//
D
9 ifsucc-cluster == NIL
10 return NIL
11 elseoffset PROTO-VEB-MINIMUM.V:clusterŒsucc-cluster/
D
12 returnindex.succ-cluster;offset/
The PROTO-VEB-SUCCESSOR procedure works as follows. As usual, line 1
testsfor thebasecase, whichlines 2–4 handle bybrute force: theonly waythatx
can have a successor within a proto-EB.2/ structure is when x 0 and AŒ1
D
is 1. Lines 5–12 handle the recursive case. Line 5 searches for a successor to x
within x’s cluster, assigning the result to offset. Line 6 determines whether x has
asuccessor withinitscluster; ifitdoes,thenline7computes andreturnsthevalue
ofthissuccessor. Otherwise, wehavetosearchinotherclusters. Line8assigns to
succ-cluster thenumberofthenextnonemptycluster,usingthesummaryinforma-
tion tofindit. Line 9tests whether succ-cluster is NIL, withline 10 returning NIL
if all succeeding clusters are empty. If succ-cluster is non-NIL, line 11 assigns
thefirstelement within that cluster tooffset, and line 12computes and returns the
minimumelementinthatcluster.
In the worst case, PROTO-VEB-SUCCESSOR calls itself recursively twice on
proto-EB.pu/ structures, and it makes one call to PROTO-VEB-MINIMUM on
a proto-EB.pu/ structure. Thus, the recurrence for the worst-case running
timeT.u/of PROTO-VEB-SUCCESSOR is
T.u/ 2T.pu/ ‚.lgpu/
D C
2T.pu/ ‚.lgu/:
D C
544 Chapter20 vanEmdeBoasTrees
We can employ the same technique that we used for recurrence (20.1) to show
that this recurrence has the solution T.u/ ‚.lgulglgu/. Thus, PROTO-VEB-
D
SUCCESSOR isasymptotically slowerthan PROTO-VEB-MINIMUM.
Insertinganelement
To insert an element, we need to insert it into the appropriate cluster and also set
the summary bit for that cluster to 1. The procedure PROTO-VEB-INSERT.V;x/
insertsthevaluex intotheproto-vEBstructure V.
PROTO-VEB-INSERT.V;x/
1 ifV:u==2
2 V:AŒx 1
D
3 else PROTO-VEB-INSERT.V:clusterŒhigh.x/;low.x//
4 PROTO-VEB-INSERT.V:summary;high.x//
Inthebasecase, line2setstheappropriate bitinthearray Ato1. Intherecursive
case, the recursive call in line 3 inserts x into the appropriate cluster, and line 4
setsthesummarybitforthatclusterto1.
Because PROTO-VEB-INSERT makes two recursive calls in the worst case, re-
currence (20.3) characterizes its running time. Hence, PROTO-VEB-INSERT runs
in‚.lgu/time.
Deletinganelement
TheDELETEoperationismorecomplicatedthaninsertion. Whereaswecanalways
set a summary bit to 1 when inserting, wecannot always reset the same summary
bit to 0 when deleting. We need to determine whether any bit in the appropriate
cluster is1. Aswehave defined proto-vEB structures, wewould have toexamine
allpubitswithinacluster todeterminewhetheranyofthemare1. Alternatively,
we could add an attribute n to the proto-vEB structure, counting how many el-
ements it has. We leave implementation of PROTO-VEB-DELETE as Exercises
20.2-2and20.2-3.
Clearly, weneed to modify the proto-vEB structure to get each operation down
tomakingatmostonerecursivecall. Wewillseeinthenextsection howtodoso.
Exercises
20.2-1
Writepseudocodefortheprocedures PROTO-VEB-MAXIMUM andPROTO-VEB-
PREDECESSOR.
20.3 ThevanEmdeBoastree 545
20.2-2
Write pseudocode for PROTO-VEB-DELETE. It should update the appropriate
summary bit by scanning the related bits within the cluster. What is the worst-
caserunningtimeofyourprocedure?
20.2-3
Add the attribute n to each proto-vEB structure, giving the number of elements
currently in the set it represents, and write pseudocode for PROTO-VEB-DELETE
that uses the attribute n to decide when to reset summary bits to 0. What is the
worst-caserunningtimeofyourprocedure? Whatotherproceduresneedtochange
becauseofthenewattribute? Dothesechanges affecttheirrunningtimes?
20.2-4
Modifytheproto-vEB structuretosupportduplicate keys.
20.2-5
Modifytheproto-vEB structuretosupportkeysthathaveassociated satellitedata.
20.2-6
Writepseudocode foraprocedure thatcreates aproto-EB.u/structure.
20.2-7
Argue that if line 9 of PROTO-VEB-MINIMUM is executed, then the proto-vEB
structure isempty.
20.2-8
Suppose that we designed a proto-vEB structure in which each cluster array had
onlyu1=4 elements. Whatwouldtherunningtimesofeachoperation be?
20.3 The vanEmde Boastree
Theproto-vEBstructureoftheprevioussectionisclosetowhatweneedtoachieve
O.lglgu/running times. Itfallsshort because wehavetorecurse toomanytimes
in most of the operations. In this section, we shall design a data structure that
is similar to the proto-vEB structure but stores a little more information, thereby
removingtheneedforsomeoftherecursion.
In Section 20.2, we observed that the assumption that we made about the uni-
verse size—that u 22k for some integer k—is unduly restrictive, confining the
D
possible values ofutoanoverlysparse set. Fromthispoint on, therefore, wewill
allow the universe size utobeany exact powerof 2, andwhen puisnot aninte-
546 Chapter20 vanEmdeBoasTrees
EB.u/ u min max
…
0 1 2 3 p"u 1
(cid:0)
summary cluster
EB.p" u/
p"uEB.p# u/trees
Figure 20.5 The information in a EB.u/ tree when u > 2. The structure contains the uni-
verse size u, elements min and max, a pointer summary to a EB.p"u/ tree, and an array
clusterŒ0:: p"u 1of p"upointerstoEB.p# u/trees.
(cid:0)
ger—that is, ifuis anodd power of2(u 22k 1 for some integer k 0)—then
C
D 
wewilldividethelgubitsofanumberintothemostsignificant .lgu/=2 bitsand
d e
theleastsignificant .lgu/=2 bits. Forconvenience, wedenote2 .lgu/=2 (the“up-
d e
b c
persquare root”ofu)by p" uand2 .lgu/=2 (the“lowersquare root”ofu)by p# u,
b c
so that u p" u p# u and, when u is an even power of 2 (u 22k for some
D  D
integer k), p" u p# u pu. Because wenow allow utobe anodd power of2,
D D
wemustredefineourhelpfulfunctions fromSection20.2:
high.x/ x=p# u ;
D
low.x/ x mod p ˘# u;
D
index.x;y/ xp# u y :
D C
20.3.1 vanEmdeBoastrees
The van Emde Boas tree, or vEB tree, modifies the proto-vEB structure. We
denote a vEB tree with a universe size of u as EB.u/ and, unless u equals the
base size of 2, the attribute summary points to a EB.p" u/ tree and the array
clusterŒ0:: p" u 1 points to p" u EB.p# u/ trees. As Figure 20.5 illustrates, a
(cid:0)
vEBtreecontainstwoattributes notfoundinaproto-vEB structure:
 minstorestheminimumelementinthevEBtree,and
 maxstoresthemaximumelementinthevEBtree.
Furthermore, the element stored in min does not appear in any of the recur-
sive EB.p# u/ trees that the cluster array points to. The elements stored in a
EB.u/ tree V, therefore, are V:min plus all the elements recursively stored in
the EB.p# u/ trees pointed to by V:clusterŒ0:: p" u 1. Note that when a vEB
(cid:0)
treecontains twoormoreelements, wetreatminandmaxdifferently: theelement
20.3 ThevanEmdeBoastree 547
stored in min does not appear in any of the clusters, but unless the vEB tree con-
tainsjustoneelement(sothattheminimumandmaximumelementsarethesame),
theelementstoredinmaxdoes.
Since the base size is 2, a EB.2/ tree does not need the array A that the cor-
responding proto-EB.2/ structure has. Instead, we can determine its elements
from its min and max attributes. In a vEB tree with no elements, regardless of its
universesizeu,bothminandmaxare NIL.
Figure 20.6 shows a EB.16/ tree V holding the set 2;3;4;5;7;14;15 . Be-
f g
causethesmallestelementis2,V:minequals 2,andeventhough high.2/ 0,the
D
element 2 does not appear in the EB.4/ tree pointed to by V:clusterŒ0: notice
that V:clusterŒ0:min equals 3, and so 2 is not in this vEB tree. Similarly, since
V:clusterŒ0:min equals 3, and 2 and 3 are the only elements in V:clusterŒ0, the
EB.2/clusterswithinV:clusterŒ0areempty.
The min and max attributes will turn out to be key to reducing the number of
recursive calls withintheoperations onvEBtrees. Theseattributes willhelp usin
fourways:
1. TheMINIMUMandMAXIMUMoperationsdonotevenneedtorecurse,forthey
canjustreturnthevaluesofminormax.
2. The SUCCESSOR operation can avoid making a recursive call to determine
whether the successor of a value x lies within high.x/. That is because x’s
successor lies within its cluster if and only if x is strictly less than the max
attribute of its cluster. A symmetric argument holds for PREDECESSOR and
min.
3. WecantellwhetheravEBtreehasnoelements,exactlyoneelement,oratleast
two elements in constant time from its min and max values. This ability will
helpinthe INSERT and DELETE operations. Ifminandmaxareboth NIL, then
thevEBtreehasnoelements. Ifminandmaxarenon-NIL butareequaltoeach
other,thenthevEBtreehasexactlyoneelement. Otherwise,bothminandmax
arenon-NIL butareunequal, andthevEBtreehastwoormoreelements.
4. IfweknowthatavEBtreeisempty,wecaninsertanelementintoitbyupdating
onlyitsminandmaxattributes. Hence,wecaninsertintoanemptyvEBtreein
constant time. Similarly, ifweknow that avEBtree has only one element, we
can delete that element inconstant timeby updating only min and max. These
properties willallowustocutshortthechainofrecursive calls.
Even if the universe size u is an odd power of 2, the difference in the sizes
ofthesummaryvEBtreeandtheclusterswillnotturnouttoaffecttheasymptotic
runningtimesofthevEB-treeoperations. Therecursiveproceduresthatimplement
thevEB-treeoperationswillallhaverunningtimescharacterizedbytherecurrence
T.u/ T.p" u/ O.1/: (20.4)
 C
548 Chapter20 vanEmdeBoasTrees
vEB(16) u 16 min 2 max 15
0 1 2 3
summary cluster
vEB(4) u 4 min 0 max 3 vEB(4) u 4 min 3 max 3 vEB(4) u 4 min 0 max 3
0 1 0 1 0 1
summary cluster summary cluster summary cluster
vEB(2) vEB(2) vEB(2) vEB(2) vEB(2) vEB(2) vEB(2) vEB(2) vEB(2)
u 2 u 2 u 2 u 2 u 2 u 2 u 2 u 2 u 2
min 0 min 1 min 1 min min min min 0 min 1 min 1
max 1 max 1 max 1 max max max max 1 max 1 max 1
vEB(4) u 4 min max vEB(4) u 4 min 2 max 3
0 1 0 1
summary cluster summary cluster
vEB(2) vEB(2) vEB(2) vEB(2) vEB(2) vEB(2)
u 2 u 2 u 2 u 2 u 2 u 2
min min min min 1 min min 1
max max max max 1 max max 1
Figure20.6 AEB.16/treecorrespondingtotheproto-vEBtreeinFigure20.4. Itstorestheset
2;3;4;5;7;14;15 .SlashesindicateNILvalues.ThevaluestoredintheminattributeofavEBtree
f g
doesnotappearinanyofitsclusters.HeavyshadingservesthesamepurposehereasinFigure20.4.
20.3 ThevanEmdeBoastree 549
Thisrecurrence lookssimilartorecurrence (20.2),andwewillsolveitinasimilar
fashion. Lettingm lgu,werewriteitas
D
T.2m/ T.2 m=2 / O.1/:
d e
 C
Notingthat m=2 2m=3forallm 2,wehave
d e  
T.2m/ T.22m=3/ O.1/:
 C
LettingS.m/ T.2m/,werewritethislastrecurrence as
D
S.m/ S.2m=3/ O.1/;
 C
which, by case 2 of the master method, has the solution S.m/ O.lgm/. (In
D
terms of the asymptotic solution, the fraction 2=3 does not make any difference
compared with the fraction 1=2, because when we apply the master method, we
find that log 1 log 1 0:) Thus, we have T.u/ T.2m/ S.m/
3=2 D 2 D D D D
O.lgm/ O.lglgu/.
D
Before using a van Emde Boas tree, we must know the universe size u, so that
wecancreateavanEmdeBoastreeoftheappropriate sizethatinitiallyrepresents
an empty set. As Problem 20-1 asks you to show, the total space requirement of
a van Emde Boas tree is O.u/, and it is straightforward to create an empty tree
inO.u/ time. Incontrast, wecancreate anemptyred-black tree inconstant time.
Therefore, wemightnotwanttouseavanEmdeBoastreewhenweperform only
a small number of operations, since the time to create the data structure would
exceed the time saved in the individual operations. This drawback is usually not
significant,sincewetypicallyuseasimpledatastructure,suchasanarrayorlinked
list,torepresent asetwithonlyafewelements.
20.3.2 OperationsonavanEmdeBoastree
Weare now ready tosee how toperform operations on avan EmdeBoastree. As
wedidfortheprotovanEmdeBoasstructure, wewillconsiderthequerying oper-
ations first, and then INSERT and DELETE. Due to the slight asymmetry between
the minimum and maximum elements in a vEB tree—when a vEB tree contains
at least two elements, the minimum element does not appear within a cluster but
themaximumelementdoes—wewillprovidepseudocodeforallfivequeryingop-
erations. As in the operations on proto van Emde Boas structures, the operations
here that take parameters V and x, where V is a van Emde Boas tree and x is an
element,assumethat0 x < V:u.

Findingtheminimumandmaximumelements
Because we store the minimum and maximum in the attributes min and max, two
oftheoperations areone-liners, takingconstant time:
550 Chapter20 vanEmdeBoasTrees
VEB-TREE-MINIMUM.V/
1 returnV:min
VEB-TREE-MAXIMUM.V/
1 returnV:max
Determiningwhetheravalueisintheset
The procedure VEB-TREE-MEMBER.V;x/ has a recursive case like that of
PROTO-VEB-MEMBER, but the base case is a little different. We also check di-
rectly whether x equals the minimum or maximum element. Since a vEB tree
doesn’t store bitsasaproto-vEB structure does, wedesign VEB-TREE-MEMBER
toreturn TRUE or FALSE ratherthan1or0.
VEB-TREE-MEMBER.V;x/
1 ifx ==V:minorx == V:max
2 return TRUE
3 elseifV:u ==2
4 return FALSE
5 elsereturn VEB-TREE-MEMBER.V:clusterŒhigh.x/;low.x//
Line 1 checks to see whether x equals either the minimum or maximum element.
If it does, line 2 returns TRUE. Otherwise, line 3 tests for the base case. Since
a EB.2/ tree has no elements other than those in min and max, if it is the base
case,line4returnsFALSE. Theotherpossibility—itisnotabasecaseandx equals
neitherminnormax—ishandled bytherecursive callinline5.
Recurrence (20.4) characterizes the running time of the VEB-TREE-MEMBER
procedure, andsothisprocedure takesO.lglgu/time.
Findingthesuccessorandpredecessor
Next we see how to implement the SUCCESSOR operation. Recall that the pro-
cedure PROTO-VEB-SUCCESSOR.V;x/ could make two recursive calls: one to
determine whether x’s successor resides in the same cluster as x and, if it does
not, one to find the cluster containing x’s successor. Because we can access the
maximum value in a vEB tree quickly, we can avoid making two recursive calls,
and instead make onerecursive call on either acluster oronthe summary, but not
onboth.
20.3 ThevanEmdeBoastree 551
VEB-TREE-SUCCESSOR.V;x/
1 ifV:u==2
2 ifx ==0andV:max == 1
3 return1
4 elsereturn NIL
5 elseifV:min NIL andx < V:min
¤
6 returnV:min
7 elsemax-low VEB-TREE-MAXIMUM.V:clusterŒhigh.x//
D
8 ifmax-low NIL andlow.x/ <max-low
¤
9 offset VEB-TREE-SUCCESSOR.V:clusterŒhigh.x/;low.x//
D
10 returnindex.high.x/;offset/
11 elsesucc-cluster VEB-TREE-SUCCESSOR.V:summary;high.x//
D
12 ifsucc-cluster == NIL
13 return NIL
14 elseoffset VEB-TREE-MINIMUM.V:clusterŒsucc-cluster/
D
15 returnindex.succ-cluster;offset/
This procedure has six return statements and several cases. We start with the
basecaseinlines2–4,whichreturns1inline3ifwearetryingtofindthesuccessor
of0and1isinthe2-elementset;otherwise, thebasecasereturns NIL inline4.
If weare not in the base case, we next check in line 5 whether x is strictly less
than the minimum element. If so, then wesimply return the minimum element in
line6.
If we get to line 7, then we know that we are not in a base case and that x is
greater than or equal to the minimum value in the vEB tree V. Line 7 assigns to
max-lowthemaximumelementinx’scluster. Ifx’sclustercontainssomeelement
that is greater than x, then we know that x’s successor lies somewhere within x’s
cluster. Line 8 tests for this condition. If x’s successor is within x’s cluster, then
line9determines whereinthecluster itis,andline10returns thesuccessor inthe
samewayasline7of PROTO-VEB-SUCCESSOR.
We get to line 11 if x is greater than or equal to the greatest element in its
cluster. In this case, lines 11–15 find x’ssuccessor in the same way as lines 8–12
ofPROTO-VEB-SUCCESSOR.
It is easy to see how recurrence (20.4) characterizes the running time of VEB-
TREE-SUCCESSOR. Depending on the result of the test in line 8, the procedure
calls itself recursively in either line 9 (on a vEB tree with universe size p# u) or
line 11 (on a vEB tree with universe size p" u). In either case, the one recursive
call is on a vEB tree with universe size at most p" u. The remainder of the proce-
dure, including the calls to VEB-TREE-MINIMUM and VEB-TREE-MAXIMUM,
takes O.1/ time. Hence, VEB-TREE-SUCCESSOR runs in O.lglgu/ worst-case
time.
552 Chapter20 vanEmdeBoasTrees
The VEB-TREE-PREDECESSOR procedure is symmetric to the VEB-TREE-
SUCCESSOR procedure, butwithoneadditional case:
VEB-TREE-PREDECESSOR.V;x/
1 ifV:u == 2
2 ifx == 1andV:min==0
3 return0
4 elsereturn NIL
5 elseifV:max NIL andx >V:max
¤
6 returnV:max
7 elsemin-low VEB-TREE-MINIMUM.V:clusterŒhigh.x//
D
8 ifmin-low NIL andlow.x/ > min-low
¤
9 offset VEB-TREE-PREDECESSOR.V:clusterŒhigh.x/;low.x//
D
10 returnindex.high.x/;offset/
11 elsepred-cluster VEB-TREE-PREDECESSOR.V:summary;high.x//
D
12 ifpred-cluster == NIL
13 ifV:min NIL andx > V:min
¤
14 returnV:min
15 elsereturn NIL
16 elseoffset VEB-TREE-MAXIMUM.V:clusterŒpred-cluster/
D
17 returnindex.pred-cluster;offset/
Lines 13–14 form the additional case. This case occurs when x’s predecessor,
if it exists, does not reside in x’s cluster. In VEB-TREE-SUCCESSOR, we were
assured that if x’s successor resides outside of x’s cluster, then it must reside in
a higher-numbered cluster. But if x’s predecessor is the minimum value in vEB
tree V, then the predecessor resides in no cluster at all. Line 13 checks for this
condition, andline14returnstheminimumvalueasappropriate.
This extra case does not affect the asymptotic running time of VEB-TREE-
PREDECESSOR when compared with VEB-TREE-SUCCESSOR, and so VEB-
TREE-PREDECESSOR runsinO.lglgu/worst-case time.
Insertinganelement
Now we examine how to insert an element into a vEB tree. Recall that PROTO-
VEB-INSERT madetworecursivecalls: onetoinserttheelementandonetoinsert
the element’s cluster number into the summary. The VEB-TREE-INSERT proce-
durewillmakeonlyonerecursivecall. Howcanwegetawaywithjustone? When
weinsertanelement,eithertheclusterthatitgoesintoalreadyhasanotherelement
or it does not. If the cluster already has another element, then the cluster number
is already in the summary, and so we do not need to make that recursive call. If
20.3 ThevanEmdeBoastree 553
thecluster does notalready have another element, then theelement being inserted
becomestheonlyelementinthecluster, andwedonotneedtorecursetoinsertan
elementintoanemptyvEBtree:
VEB-EMPTY-TREE-INSERT.V;x/
1 V:min x
D
2 V:max x
D
Withthisprocedureinhand,hereisthepseudocodeforVEB-TREE-INSERT.V;x/,
which assumes that x is not already an element in the set represented by vEB
treeV:
VEB-TREE-INSERT.V;x/
1 ifV:min== NIL
2 VEB-EMPTY-TREE-INSERT.V;x/
3 elseifx < V:min
4 exchangex withV:min
5 ifV:u> 2
6 ifVEB-TREE-MINIMUM.V:clusterŒhigh.x//== NIL
7 VEB-TREE-INSERT.V:summary;high.x//
8 VEB-EMPTY-TREE-INSERT.V:clusterŒhigh.x/;low.x//
9 else VEB-TREE-INSERT.V:clusterŒhigh.x/;low.x//
10 ifx > V:max
11 V:max x
D
This procedure works as follows. Line 1 tests whether V is an empty vEB tree
and, if it is, then line 2 handles this easy case. Lines 3–11 assume that V is not
empty, and therefore some element will be inserted into one of V’s clusters. But
thatelementmightnotnecessarilybetheelementxpassedtoVEB-TREE-INSERT.
If x < min, as tested in line 3, then x needs to become the new min. We don’t
wantto lose the original min, however, and so weneed toinsert itinto one of V’s
clusters. In this case, line 4 exchanges x with min, so that we insert the original
minintooneofV’sclusters.
We execute lines 6–9 only if V is not a base-case vEB tree. Line 6 determines
whether the cluster that x will go into is currently empty. If so, then line 7 in-
serts x’s cluster number into the summary and line 8 handles the easy case of
inserting x into an empty cluster. If x’s cluster is not currently empty, then line 9
inserts x into its cluster. In this case, we do not need to update the summary,
sincex’sclusternumberisalreadyamemberofthesummary.
Finally, lines 10–11 take care of updating max if x > max. Note that if V is a
base-casevEBtreethatisnotempty,thenlines3–4and10–11updateminandmax
properly.
554 Chapter20 vanEmdeBoasTrees
Once again, we can easily see how recurrence (20.4) characterizes the running
time. Dependingontheresultofthetestinline6,eithertherecursivecallinline7
(run on a vEB tree with universe size p" u) or the recursive call in line 9 (run on
a vEB with universe size p# u) executes. In either case, the one recursive call is
on a vEB tree with universe size at most p" u. Because the remainder of VEB-
TREE-INSERT takesO.1/time,recurrence (20.4)applies, andsotherunningtime
isO.lglgu/.
Deletinganelement
Finally, we look at how to delete an element from a vEB tree. The procedure
VEB-TREE-DELETE.V;x/assumesthatx iscurrentlyanelementinthesetrepre-
sentedbythevEBtreeV.
VEB-TREE-DELETE.V;x/
1 ifV:min ==V:max
2 V:min NIL
D
3 V:max NIL
D
4 elseifV:u==2
5 ifx == 0
6 V:min 1
D
7 elseV:min 0
D
8 V:max V:min
D
9 elseifx == V:min
10 first-cluster VEB-TREE-MINIMUM.V:summary/
D
11 x index.first-cluster;
D
VEB-TREE-MINIMUM.V:clusterŒfirst-cluster//
12 V:min x
D
13 VEB-TREE-DELETE.V:clusterŒhigh.x/;low.x//
14 ifVEB-TREE-MINIMUM.V:clusterŒhigh.x//== NIL
15 VEB-TREE-DELETE.V:summary;high.x//
16 ifx == V:max
17 summary-max VEB-TREE-MAXIMUM.V:summary/
D
18 ifsummary-max == NIL
19 V:max V:min
D
20 elseV:max index.summary-max;
D
VEB-TREE-MAXIMUM.V:clusterŒsummary-max//
21 elseifx ==V:max
22 V:max index.high.x/;
D
VEB-TREE-MAXIMUM.V:clusterŒhigh.x///
20.3 ThevanEmdeBoastree 555
The VEB-TREE-DELETE procedure works as follows. If the vEB tree V con-
tainsonlyoneelement,thenit’sjustaseasytodeleteitasitwastoinsertanelement
into an empty vEB tree: just set min and max to NIL. Lines 1–3 handle this case.
Otherwise,V hasatleasttwoelements. Line4testswhetherV isabase-casevEB
treeand,ifso,lines5–8setminandmaxtotheoneremainingelement.
Lines 9–22 assume that V has two or more elements and that u 4. In this

case,wewillhavetodeleteanelementfromacluster. Theelementwedeletefrom
a cluster might not be x, however, because if x equals min, then once we have
deleted x, some other element within one of V’s clusters becomes the new min,
andwehavetodeletethatotherelementfromitscluster. Ifthetestinline9reveals
that we are in this case, then line 10 sets first-cluster to the number of the cluster
that contains the lowest element other than min, and line 11 sets x to the value of
the lowest element in that cluster. This element becomes the new min in line 12
and, because we set x to its value, it is the element that will be deleted from its
cluster.
When we reach line 13, we know that we need to delete element x from its
cluster, whether x was the value originally passed to VEB-TREE-DELETE or x
is the element becoming the new minimum. Line 13 deletes x from its cluster.
That cluster might now become empty, which line 14 tests, and if it does, then
we need to remove x’s cluster number from the summary, which line 15 handles.
Afterupdating thesummary, wemightneed toupdate max. Line16checks tosee
whetherwearedeletingthemaximumelementinV and,ifweare,thenline17sets
summary-max tothenumberofthehighest-numbered nonempty cluster. (Thecall
VEB-TREE-MAXIMUM.V:summary/ works because we have already recursively
called VEB-TREE-DELETE onV:summary,andthereforeV:summary:maxhasal-
ready been updated as necessary.) If all of V’s clusters are empty, then the only
remaining element in V is min; line 18 checks for this case, and line 19 updates
max appropriately. Otherwise, line 20 sets max to the maximum element in the
highest-numbered nonempty cluster. (Ifthiscluster iswheretheelement hasbeen
deleted,weagainrelyontherecursivecallinline13havingalreadycorrectedthat
cluster’smaxattribute.)
Finally, we have to handle the case in which x’s cluster did not become empty
due to x being deleted. Although we do not have to update the summary in this
case, we might have to update max. Line 21 tests for this case, and if we have to
update max, line 22 does so (again relying on the recursive call to have corrected
maxinthecluster).
Now we show that VEB-TREE-DELETE runs in O.lglgu/ time in the worst
case. Atfirstglance,youmightthinkthatrecurrence(20.4)doesnotalwaysapply,
because a single call of VEB-TREE-DELETE can make two recursive calls: one
on line 13 and one on line 15. Although the procedure can make both recursive
calls,let’sthinkaboutwhathappenswhenitdoes. Inorderfortherecursivecallon
556 Chapter20 vanEmdeBoasTrees
line 15 to occur, the test on line 14 must show that x’s cluster is empty. Theonly
way that x’scluster can be empty is if x was the only element in its cluster when
wemadetherecursive callonline13. Butifx wastheonly elementinitscluster,
then that recursive call took O.1/ time, because it executed only lines 1–3. Thus,
wehavetwomutuallyexclusivepossibilities:
 Therecursivecallonline13tookconstant time.
 Therecursivecallonline15didnotoccur.
In either case, recurrence (20.4) characterizes the running time of VEB-TREE-
DELETE, andhenceitsworst-caserunning timeisO.lglgu/.
Exercises
20.3-1
ModifyvEBtreestosupportduplicate keys.
20.3-2
ModifyvEBtreestosupportkeysthathaveassociated satellite data.
20.3-3
Writepseudocode foraprocedure thatcreatesanemptyvanEmdeBoastree.
20.3-4
What happens if you call VEB-TREE-INSERT with an element that is already in
thevEBtree? WhathappensifyoucallVEB-TREE-DELETE withanelementthat
is not in the vEB tree? Explain why the procedures exhibit the behavior that they
do. Show how to modify vEB trees and their operations so that we can check in
constant timewhetheranelementispresent.
20.3-5
Suppose that instead of p" uclusters, each with universe size p# u, weconstructed
vEB trees to have u1=k clusters, each with universe size u1 1=k, where k > 1 is a
(cid:0)
constant. If we were to modify the operations appropriately, what would be their
runningtimes? Forthepurposeofanalysis,assumethatu1=kandu1 1=k arealways
(cid:0)
integers.
20.3-6
Creating avEBtreewithuniverse size urequires O.u/ time. Suppose wewishto
explicitly account for that time. What is the smallest number of operations n for
whichtheamortizedtimeofeachoperation inavEBtreeisO.lglgu/?
ProblemsforChapter20 557
Problems
20-1 SpacerequirementsforvanEmdeBoastrees
This problem explores the space requirements for van Emde Boas trees and sug-
gests a wayto modify the data structure to make its space requirement depend on
the number n of elements actually stored in the tree, rather than on the universe
sizeu. Forsimplicity, assumethatpuisalwaysaninteger.
a. ExplainwhythefollowingrecurrencecharacterizesthespacerequirementP.u/
ofavanEmdeBoastreewithuniversesizeu:
P.u/ .pu 1/P.pu/ ‚.pu/: (20.5)
D C C
b. Provethatrecurrence (20.5)hasthesolutionP.u/ O.u/.
D
Inordertoreducethespacerequirements, letusdefineareduced-spacevanEmde
Boastree,orRS-vEBtree,asavEBtreeV butwiththefollowingchanges:
 TheattributeV:cluster,ratherthanbeingstoredasasimplearrayofpointersto
vEBtreeswithuniversesizepu,isahashtable(seeChapter11)storedasady-
namictable(seeSection17.4). CorrespondingtothearrayversionofV:cluster,
the hash table stores pointers to RS-vEB trees with universe size pu. To find
the ith cluster, we look up the key i in the hash table, so that we can find the
ithclusterbyasinglesearchinthehashtable.
 Thehash table stores only pointers to nonempty clusters. A search in the hash
tableforanemptyclusterreturns NIL,indicating thattheclusterisempty.
 TheattributeV:summaryisNILifallclustersareempty. Otherwise,V:summary
pointstoanRS-vEBtreewithuniversesizepu.
Because the hash table is implemented with adynamic table, the space it requires
isproportional tothenumberofnonempty clusters.
WhenweneedtoinsertanelementintoanemptyRS-vEBtree,wecreatetheRS-
vEBtreebycallingthefollowingprocedure,wheretheparameteruistheuniverse
sizeoftheRS-vEBtree:
CREATE-NEW-RS-VEB-TREE.u/
1 allocate anewvEBtreeV
2 V:u u
D
3 V:min NIL
D
4 V:max NIL
D
5 V:summary NIL
D
6 createV:cluster asanemptydynamichashtable
7 returnV
558 Chapter20 vanEmdeBoasTrees
c. Modifythe VEB-TREE-INSERT procedure toproduce pseudocode forthepro-
cedureRS-VEB-TREE-INSERT.V;x/,whichinsertsxintotheRS-vEBtreeV,
calling CREATE-NEW-RS-VEB-TREE asappropriate.
d. Modify the VEB-TREE-SUCCESSOR procedure to produce pseudocode for
theprocedure RS-VEB-TREE-SUCCESSOR.V;x/,whichreturnsthesuccessor
ofx inRS-vEBtreeV,or NIL ifx hasnosuccessor inV.
e. Prove that, under the assumption of simple uniform hashing, your RS-VEB-
TREE-INSERT and RS-VEB-TREE-SUCCESSOR procedures run in O.lglgu/
expectedamortized time.
f. AssumingthatelementsareneverdeletedfromavEBtree,provethatthespace
requirement for the RS-vEB tree structure is O.n/, where n is the number of
elementsactuallystoredintheRS-vEBtree.
g. RS-vEBtreeshaveanother advantage overvEBtrees: theyrequirelesstimeto
create. HowlongdoesittaketocreateanemptyRS-vEBtree?
20-2 y-fasttries
This problem investigates D. Willard’s “y-fast tries” which, like van Emde Boas
trees, perform each of the operations MEMBER, MINIMUM, MAXIMUM, PRE-
DECESSOR, and SUCCESSOR on elements drawn from a universe with size u in
O.lglgu/ worst-case time. The INSERT and DELETE operations take O.lglgu/
amortized time. Like reduced-space van Emde Boas trees (see Problem 20-1), y-
fasttriesuseonlyO.n/spacetostorenelements. Thedesign ofy-fasttriesrelies
onperfecthashing (seeSection11.5).
Asapreliminarystructure,supposethatwecreateaperfecthashtablecontaining
notonlyeveryelementinthedynamicset,buteveryprefixofthebinary represen-
tation of every element in the set. For example, if u 16, so that lgu 4, and
D D
x 13 is in the set, then because the binary representation of 13 is 1101, the
D
perfect hash table would contain the strings 1, 11, 110, and 1101. In addition to
thehashtable,wecreateadoublylinkedlistoftheelementscurrentlyintheset,in
increasing order.
a. Howmuchspacedoesthisstructure require?
b. Showhowtoperform the MINIMUM and MAXIMUM operations inO.1/time;
the MEMBER, PREDECESSOR, and SUCCESSOR operations inO.lglgu/time;
andthe INSERT and DELETE operations inO.lgu/time.
To reduce the space requirement to O.n/, we make the following changes to the
datastructure:
NotesforChapter20 559
 We cluster the n elements into n=lgu groups of size lgu. (Assume for now
thatlgudividesn.) Thefirstgroupconsistsofthelgusmallestelementsinthe
set,thesecondgroupconsistsofthenextlgusmallestelements,andsoon.
 We designate a “representative” value for each group. The representative of
theithgroupisatleastaslargeasthelargestelementintheithgroup,anditis
smallerthaneveryelementofthe.i 1/stgroup. (Therepresentativeofthelast
C
groupcanbethemaximumpossible elementu 1.) Notethatarepresentative
(cid:0)
mightbeavaluenotcurrently intheset.
 Westorethelguelementsofeachgroupinabalanced binarysearchtree,such
as a red-black tree. Each representative points to the balanced binary search
tree for its group, and each balanced binary search tree points to its group’s
representative.
 The perfect hash table stores only the representatives, which are also stored in
adoubly linkedlistinincreasing order.
Wecallthisstructure ay-fasttrie.
c. Showthatay-fasttrierequires onlyO.n/spacetostorenelements.
d. Show how to perform the MINIMUM and MAXIMUM operations in O.lglgu/
timewithay-fasttrie.
e. Showhowtoperform the MEMBER operation inO.lglgu/time.
f. Show how to perform the PREDECESSOR and SUCCESSOR operations in
O.lglgu/time.
g. ExplainwhytheINSERT and DELETE operations take.lglgu/time.
h. Show how to relax the requirement that each group in a y-fast trie has exactly
lguelementstoallowINSERTandDELETE toruninO.lglgu/amortizedtime
withoutaffectingtheasymptotic runningtimesoftheotheroperations.
Chapter notes
Thedatastructure inthischapterisnamedafterP.vanEmdeBoas,whodescribed
an early form of the idea in 1975 [339]. Later papers by van Emde Boas [340]
and van Emde Boas, Kaas, and Zijlstra [341] refined the idea and the exposition.
Mehlhorn and Na¨her [252] subsequently extended the ideas to apply to universe
560 Chapter20 vanEmdeBoasTrees
sizesthatareprime. Mehlhorn’s book[249]contains aslightly different treatment
ofvanEmdeBoastreesthantheoneinthischapter.
Using the ideas behind van Emde Boas trees, Dementiev et al. [83] developed
a nonrecursive, three-level search tree that ran faster than van EmdeBoas trees in
theirownexperiments.
Wang and Lin [347] designed a hardware-pipelined version of van Emde Boas
trees, which achieves constant amortized time per operation and uses O.lglgu/
stagesinthepipeline.
A lower bound by Paˇtras¸cu and Thorup [273, 274] for finding the predecessor
showsthatvanEmdeBoastreesareoptimalforthisoperation, evenifrandomiza-
tionisallowed.
21 Data Structures for Disjoint Sets
Someapplicationsinvolvegroupingndistinctelementsintoacollectionofdisjoint
sets. Theseapplicationsoftenneedtoperformtwooperationsinparticular: finding
the unique set that contains a given element and uniting two sets. This chapter
exploresmethodsformaintaining adatastructure thatsupports theseoperations.
Section 21.1 describes the operations supported by a disjoint-set data structure
and presents asimple application. InSection 21.2, we look at asimple linked-list
implementation for disjoint sets. Section 21.3 presents a more efficient represen-
tation using rooted trees. The running time using the tree representation is theo-
retically superlinear, butforallpractical purposes itislinear. Section 21.4defines
anddiscussesaveryquicklygrowingfunctionanditsveryslowlygrowinginverse,
whichappearsintherunningtimeofoperationsonthetree-basedimplementation,
andthen, byacomplex amortized analysis, proves anupper bound ontherunning
timethatisjustbarelysuperlinear.
21.1 Disjoint-setoperations
A disjoint-set data structure maintains a collection S S ;S ;:::;S of dis-
1 2 k
D f g
jointdynamic sets. Weidentify each setbyarepresentative, whichissomemem-
beroftheset. Insomeapplications, itdoesn’t matterwhichmemberisusedasthe
representative; we care only that if we ask for the representative of a dynamic set
twicewithoutmodifyingthesetbetweentherequests,wegetthesameanswerboth
times. Other applications may require a prespecified rule for choosing the repre-
sentative, such as choosing the smallest member in the set (assuming, of course,
thattheelementscanbeordered).
Asintheotherdynamic-setimplementationswehavestudied,werepresenteach
element of a set by an object. Letting x denote an object, we wish to support the
followingoperations:
562 Chapter21 DataStructuresforDisjointSets
MAKE-SET.x/ creates a new set whose only member (and thus representative)
is x. Since the sets are disjoint, we require that x not already be in some other
set.
UNION.x;y/ unites the dynamic sets that contain x and y, say S
x
and S y, into a
new setthat istheunion ofthese twosets. Weassume thatthe twosetsare dis-
jointpriortotheoperation. Therepresentativeoftheresultingsetisanymember
of S
x
S y, although many implementations of UNION specifically choose the
[
representative of either S or S as the new representative. Since we require
x y
thesets inthecollection tobedisjoint, conceptually wedestroy setsS andS ,
x y
S
removingthemfromthecollection . Inpractice, weoftenabsorbtheelements
ofoneofthesetsintotheotherset.
FIND-SET.x/ returns a pointer to the representative of the (unique) set contain-
ingx.
Throughout this chapter, weshall analyze the running times of disjoint-set data
structures in terms of two parameters: n, the number of MAKE-SET operations,
andm,thetotalnumber of MAKE-SET, UNION,and FIND-SET operations. Since
the sets are disjoint, each UNION operation reduces the number of sets by one.
After n 1 UNION operations, therefore, only one set remains. The number of
(cid:0)
UNION operations is thus at most n 1. Note also that since the MAKE-SET
(cid:0)
operations are included in the total number of operations m, we have m n. We

assumethatthenMAKE-SET operations arethefirstnoperations performed.
Anapplicationofdisjoint-setdatastructures
One of the many applications of disjoint-set data structures arises in determin-
ing the connected components of an undirected graph (see Section B.4). Fig-
ure21.1(a), forexample,showsagraphwithfourconnected components.
The procedure CONNECTED-COMPONENTS that follows uses the disjoint-set
operations tocomputetheconnected components ofagraph. Once CONNECTED-
COMPONENTS has preprocessed the graph, the procedure SAME-COMPONENT
answersqueriesaboutwhethertwoverticesareinthesameconnectedcomponent.1
(In pseudocode, we denote the set of vertices of a graph G by G:V and the set of
edgesbyG:E.)
1Whentheedgesofthegrapharestatic—notchangingovertime—wecancomputetheconnected
components faster by usingdepth-first search (Exercise22.3-12). Sometimes, however, theedges
areaddeddynamicallyandweneedtomaintaintheconnectedcomponentsaseachedgeisadded.In
thiscase,theimplementationgivenherecanbemoreefficientthanrunninganewdepth-firstsearch
foreachnewedge.
21.1 Disjoint-setoperations 563
a b e f h j
c d g i
(a)
Edge processed Collection of disjoint sets
initial sets {a} {b} {c} {d} {e} {f} {g} {h} {i} {j}
(b,d) {a} {b,d} {c} {e} {f} {g} {h} {i} {j}
(e,g) {a} {b,d} {c} {e,g} {f} {h} {i} {j}
(a,c) {a,c} {b,d} {e,g} {f} {h} {i} {j}
(h,i) {a,c} {b,d} {e,g} {f} {h,i} {j}
(a,b) {a,b,c,d} {e,g} {f} {h,i} {j}
(e, f ) {a,b,c,d} {e, f,g} {h,i} {j}
(b,c) {a,b,c,d} {e, f,g} {h,i} {j}
(b)
Figure21.1 (a)Agraphwithfourconnectedcomponents: a;b;c;d , e;f;g , h;i ,and j .
f g f g f g f g
(b)Thecollectionofdisjointsetsafterprocessingeachedge.
CONNECTED-COMPONENTS.G/
1 foreachvertex G:V
2
2 MAKE-SET./
3 foreachedge.u;/ G:E
2
4 ifFIND-SET.u/ FIND-SET./
¤
5 UNION.u;/
SAME-COMPONENT.u;/
1 ifFIND-SET.u/ == FIND-SET./
2 return TRUE
3 elsereturn FALSE
The procedure CONNECTED-COMPONENTS initially places each vertex  in its
own set. Then, for each edge .u;/, it unites the sets containing u and . By
Exercise 21.1-2, after processing all the edges, two vertices are in the same con-
nected component if and only if the corresponding objects are in the same set.
Thus, CONNECTED-COMPONENTS computes sets in such a way that the proce-
dureSAME-COMPONENT candeterminewhethertwoverticesareinthesamecon-
564 Chapter21 DataStructuresforDisjointSets
nected component. Figure 21.1(b) illustrates how CONNECTED-COMPONENTS
computesthedisjoint sets.
Inanactualimplementation ofthisconnected-components algorithm, therepre-
sentations of the graph and the disjoint-set data structure would need to reference
each other. That is, an object representing a vertex would contain a pointer to
the corresponding disjoint-set object, and vice versa. These programming details
depend ontheimplementation language, andwedonotaddress themfurtherhere.
Exercises
21.1-1
Suppose that CONNECTED-COMPONENTS is run on the undirected graph G
D
.V;E/, where V a;b;c;d;e;f;g;h;i;j;k and the edges of E are pro-
D f g
cessed in the order .d;i/;.f;k/;.g;i/;.b;g/;.a;h/;.i;j/;.d;k/;.b;j/;.d;f /;
.g;j/;.a;e/. Listtheverticesineachconnected componentaftereachiterationof
lines3–5.
21.1-2
Showthat after alledges areprocessed by CONNECTED-COMPONENTS, twover-
ticesareinthesameconnected component ifandonlyiftheyareinthesameset.
21.1-3
DuringtheexecutionofCONNECTED-COMPONENTS onanundirectedgraphG
D
.V;E/withk connected components, howmanytimesis FIND-SET called? How
manytimesis UNION called? Expressyouranswersintermsof V , E ,andk.
j j j j
21.2 Linked-list representation ofdisjointsets
Figure21.2(a)showsasimplewaytoimplementadisjoint-set datastructure: each
setisrepresentedbyitsownlinkedlist. Theobjectforeachsethasattributeshead,
pointing to the first object in the list, and tail, pointing to the last object. Each
object inthelistcontains asetmember, apointer tothe nextobject inthelist, and
apointer back to theset object. Within each linked list, theobjects may appear in
anyorder. Therepresentative isthesetmemberinthefirstobjectinthelist.
With this linked-list representation, both MAKE-SET and FIND-SET are easy,
requiring O.1/ time. To carry out MAKE-SET.x/, we create a new linked list
whoseonlyobject isx. For FIND-SET.x/,wejustfollow thepointer from x back
to its set object and then return the member in the object that head points to. For
example,inFigure21.2(a),thecall FIND-SET.g/wouldreturnf.
21.2 Linked-listrepresentationofdisjointsets 565
(a) f g d c h e b
head head
S1 S2
tail tail
(b) f g d c h e b
head
S1
tail
Figure21.2 (a)Linked-listrepresentationsoftwosets.SetS1containsmembersd,f,andg,with
representativef,andsetS2containsmembersb,c,e,andh,withrepresentativec. Eachobjectin
thelistcontainsasetmember, apointertothenextobjectinthelist,andapointerbacktotheset
object. Eachsetobjecthaspointersheadandtailtothefirstandlastobjects,respectively. (b)The
resultofUNION.g;e/,whichappendsthelinkedlistcontainingetothelinkedlistcontainingg.The
representativeoftheresultingsetisf.Thesetobjectfore’slist,S2,isdestroyed.
Asimpleimplementationofunion
Thesimplestimplementation oftheUNION operation usingthelinked-list setrep-
resentation takes significantly more time than MAKE-SET or FIND-SET. As Fig-
ure 21.2(b) shows, we perform UNION.x;y/ by appending y’s list onto the end
ofx’slist. Therepresentativeofx’slistbecomestherepresentativeoftheresulting
set. Weusethetailpointerforx’slisttoquicklyfindwheretoappendy’slist. Be-
causeallmembersofy’slistjoinx’slist,wecandestroythesetobjectfory’slist.
Unfortunately, we must update the pointer to the set object for each object origi-
nallyony’slist,whichtakestimelinearinthelengthofy’slist. InFigure21.2,for
example, the operation UNION.g;e/ causes pointers to be updated in the objects
forb,c,e,andh.
In fact, we can easily construct a sequence of m operations on n objects that
requires ‚.n2/ time. Suppose that we have objects x ;x ;:::;x . We execute
1 2 n
the sequence of n MAKE-SET operations followed by n 1 UNION operations
(cid:0)
showninFigure21.3,sothatm 2n 1. Wespend ‚.n/timeperforming then
D (cid:0)
MAKE-SET operations. Because the ith UNION operation updates i objects, the
totalnumberofobjectsupdated byalln 1UNION operations is
(cid:0)
566 Chapter21 DataStructuresforDisjointSets
Operation Numberofobjectsupdated
MAKE-SET.x1/ 1
MAKE-SET.x2/ 1
: :
: :
: :
MAKE-SET.xn/ 1
UNION.x2;x1/ 1
UNION.x3;x2/ 2
UNION.x4;x3/ 3
: :
: :
: :
UNION.xn;xn 1/ n 1
(cid:0) (cid:0)
Figure21.3 Asequence of 2n 1operationsonnobjectsthattakes‚.n2/time, or ‚.n/time
(cid:0)
peroperationonaverage,usingthelinked-listsetrepresentationandthesimpleimplementationof
UNION.
n 1
(cid:0)
i ‚.n2/:
D
i 1
XD
Thetotalnumberofoperationsis2n 1,andsoeachoperationonaveragerequires
(cid:0)
‚.n/time. Thatis,theamortized timeofanoperation is‚.n/.
Aweighted-unionheuristic
In the worst case, the above implementation of the UNION procedure requires an
average of ‚.n/ time per call because we may be appending a longer list onto
a shorter list; we must update the pointer to the set object for each member of
the longer list. Suppose instead that each list also includes the length of the list
(whichwecaneasilymaintain)andthatwealwaysappendtheshorterlistontothe
longer, breaking tiesarbitrarily. Withthis simpleweighted-union heuristic, asin-
gle UNION operation canstilltake.n/timeifbothsetshave.n/members. As
thefollowingtheorem shows,however,asequence ofm MAKE-SET, UNION,and
FIND-SET operations,nofwhichareMAKE-SET operations, takesO.m nlgn/
C
time.
Theorem21.1
Usingthelinked-list representation ofdisjointsetsandtheweighted-union heuris-
tic, a sequence of m MAKE-SET, UNION, and FIND-SET operations, n of which
are MAKE-SET operations, takesO.m nlgn/time.
C
21.2 Linked-listrepresentationofdisjointsets 567
Proof Because each UNION operation unites two disjoint sets, we perform at
mostn 1UNIONoperationsoverall. Wenowboundthetotaltimetakenbythese
(cid:0)
UNIONoperations. Westartbydetermining,foreachobject,anupperboundonthe
number of times the object’s pointer back to its set object is updated. Consider a
particular object x. Weknowthateachtimex’spointer wasupdated, x musthave
started in the smaller set. The first time x’s pointer was updated, therefore, the
resultingsetmusthavehadatleast2members. Similarly,thenexttimex’spointer
was updated, the resulting set must have had at least 4 members. Continuing on,
we observe that for any k n, after x’s pointer has been updated lgk times,
 d e
theresulting set musthave atleast k members. Since thelargest sethas atmost n
members,eachobject’spointerisupdatedatmost lgn timesoverallthe UNION
d e
operations. Thus the total time spent updating object pointers over all UNION
operations is O.nlgn/. We must also account for updating the tail pointers and
the list lengths, which take only ‚.1/ time per UNION operation. The total time
spentinallUNION operations isthusO.nlgn/.
The time for the entire sequence of m operations follows easily. Each MAKE-
SET and FIND-SET operation takes O.1/ time, and there are O.m/ of them. The
totaltimefortheentiresequence isthusO.m nlgn/.
C
Exercises
21.2-1
Write pseudocode for MAKE-SET, FIND-SET, and UNION using the linked-list
representationandtheweighted-unionheuristic. Makesuretospecifytheattributes
thatyouassumeforsetobjectsandlistobjects.
21.2-2
Show the data structure that results and the answers returned by the FIND-SET
operations in the following program. Use the linked-list representation with the
weighted-union heuristic.
1 fori 1to16
D
2 MAKE-SET.x i/
3 fori 1to15by2
D
4 UNION.x i;x
i
1/
5 fori 1to13byC4
D
6 UNION.x i;x
i
2/
7 UNION.x 1;x 5/ C
8 UNION.x 11;x 13/
9 UNION.x 1;x 10/
10 FIND-SET.x 2/
11 FIND-SET.x 9/
568 Chapter21 DataStructuresforDisjointSets
Assumethatifthesetscontaining x andx havethesamesize,thentheoperation
i j
UNION.x i;x j/appends x j’slistontox i’slist.
21.2-3
Adapt the aggregate proof of Theorem 21.1 to obtain amortized time bounds
of O.1/ for MAKE-SET and FIND-SET and O.lgn/ for UNION using the linked-
listrepresentation andtheweighted-union heuristic.
21.2-4
Giveatightasymptoticboundontherunningtimeofthesequenceofoperationsin
Figure21.3assumingthelinked-listrepresentation andtheweighted-union heuris-
tic.
21.2-5
Professor Gompers suspects that it might be possible to keep just one pointer in
eachsetobject,ratherthantwo(headandtail),whilekeepingthenumberofpoint-
ersineachlistelementattwo. Showthattheprofessor’s suspicioniswellfounded
by describing how to represent each set by a linked list such that each operation
has the same running time as the operations described in this section. Describe
also how the operations work. Your scheme should allow for the weighted-union
heuristic, withthesame effect asdescribed inthis section. (Hint: Usethe tail ofa
linkedlistasitsset’srepresentative.)
21.2-6
SuggestasimplechangetotheUNION procedure forthelinked-list representation
thatremovestheneedtokeepthetailpointertothelastobjectineachlist. Whether
or not the weighted-union heuristic is used, your change should not change the
asymptotic running time of the UNION procedure. (Hint: Rather than appending
onelisttoanother, splicethemtogether.)
21.3 Disjoint-setforests
In a faster implementation of disjoint sets, we represent sets by rooted trees, with
eachnodecontainingonememberandeachtreerepresentingoneset. Inadisjoint-
setforest, illustrated inFigure21.4(a), eachmemberpointsonlytoitsparent. The
root of each tree contains the representative and is its own parent. As we shall
see, although the straightforward algorithms that use this representation are no
faster than ones that use the linked-list representation, by introducing two heuris-
tics—“unionbyrank”and“pathcompression”—wecanachieveanasymptotically
optimaldisjoint-set datastructure.
21.3 Disjoint-setforests 569
c f f
h e d c d
b g h e g
b
(a) (b)
Figure 21.4 A disjoint-set forest. (a) Two trees representing the two sets of Figure 21.2. The
treeontheleftrepresentstheset b;c;e;h ,withc astherepresentative, andthetreeontheright
f g
representstheset d;f;g ,withf astherepresentative.(b)TheresultofUNION.e;g/.
f g
Weperformthethreedisjoint-setoperationsasfollows. AMAKE-SEToperation
simply creates a tree with just one node. We perform a FIND-SET operation by
following parent pointers until we find the root of the tree. The nodes visited on
this simple path toward the root constitute the find path. A UNION operation,
showninFigure21.4(b),causestherootofonetreetopointtotherootoftheother.
Heuristicstoimprovetherunningtime
So far, we have not improved on the linked-list implementation. A sequence of
n 1UNION operationsmaycreateatreethatisjustalinearchainofnnodes. By
(cid:0)
using twoheuristics, however, wecan achieve arunning timethat isalmost linear
inthetotalnumberofoperations m.
Thefirstheuristic, unionbyrank, issimilar totheweighted-union heuristic we
used with the linked-list representation. The obvious approach would be to make
therootofthetreewithfewernodes point tothe rootofthetreewithmorenodes.
Ratherthan explicitly keeping track ofthesizeofthe subtree rooted ateach node,
we shall use an approach that eases the analysis. For each node, we maintain a
rank, which is an upper bound on the height of the node. In union by rank, we
maketherootwithsmallerrankpointtotherootwithlarger rankduring a UNION
operation.
The second heuristic, path compression, is also quite simple and highly effec-
tive. AsshowninFigure21.5,weuseitduringFIND-SET operationstomakeeach
nodeonthefindpathpointdirectly totheroot. Pathcompression doesnotchange
anyranks.
570 Chapter21 DataStructuresforDisjointSets
f
e
f
d
c a b c d e
b
a
(a) (b)
Figure21.5 PathcompressionduringtheoperationFIND-SET.Arrowsandself-loopsatrootsare
omitted. (a)AtreerepresentingasetpriortoexecutingFIND-SET.a/. Trianglesrepresentsubtrees
whose roots are the nodes shown. Each node has a pointer to its parent. (b) The same set after
executingFIND-SET.a/.Eachnodeonthefindpathnowpointsdirectlytotheroot.
Pseudocodefordisjoint-setforests
Toimplement adisjoint-set forest withthe union-by-rank heuristic, wemustkeep
track of ranks. With each node x, we maintain the integer value x:rank, which is
anupperboundontheheightofx (thenumberofedgesinthelongestsimplepath
from adescendant leaftox). When MAKE-SET creates asingleton set, thesingle
node in the corresponding tree has an initial rank of 0. Each FIND-SET operation
leaves all ranks unchanged. The UNION operation has two cases, depending on
whether the roots of the trees have equal rank. If the roots have unequal rank,
we make the root with higher rank the parent of the root with lower rank, but the
ranks themselves remain unchanged. If, instead, the roots have equal ranks, we
arbitrarily chooseoneoftherootsastheparentandincrementitsrank.
Let us put this method into pseudocode. We designate the parent of node x
byx:p. TheLINK procedure, asubroutine calledby UNION,takespointers totwo
rootsasinputs.
21.3 Disjoint-setforests 571
MAKE-SET.x/
1 x:p x
D
2 x:rank 0
D
UNION.x;y/
1 LINK.FIND-SET.x/;FIND-SET.y//
LINK.x;y/
1 ifx:rank >y:rank
2 y:p x
D
3 elsex:p y
D
4 ifx:rank ==y:rank
5 y:rank y:rank 1
D C
TheFIND-SET procedure withpathcompression isquitesimple:
FIND-SET.x/
1 ifx x:p
¤
2 x:p FIND-SET.x:p/
D
3 returnx:p
The FIND-SET procedure is a two-pass method: as it recurses, it makes one pass
up the find path to find the root, and as the recursion unwinds, it makes a second
passbackdownthefindpathtoupdateeachnodetopointdirectlytotheroot. Each
call of FIND-SET.x/ returns x:p in line 3. If x is the root, then FIND-SET skips
line 2 and instead returns x:p, which is x; this is the case in which the recursion
bottomsout. Otherwise, line2executes, andtherecursive callwithparameter x:p
returns a pointer to the root. Line 2 updates node x to point directly to the root,
andline3returnsthispointer.
Effectoftheheuristicsontherunningtime
Separately,eitherunionbyrankorpathcompressionimprovestherunningtimeof
the operations on disjoint-set forests, and the improvement is even greater when
we use the two heuristics together. Alone, union by rank yields a running time
of O.mlgn/ (see Exercise 21.4-4), and this bound is tight (see Exercise21.3-3).
Although we shall not prove it here, for a sequence of n MAKE-SET opera-
tions (and hence at most n 1 UNION operations) and f FIND-SET opera-
(cid:0)
tions, the path-compression heuristic alone gives a worst-case running time of
‚.n f .1 log n//.
C  C 2 f=n
C
572 Chapter21 DataStructuresforDisjointSets
Whenweusebothunionbyrankandpathcompression, theworst-case running
time is O.m˛.n//, where ˛.n/ is a very slowly growing function, which we de-
fineinSection21.4. Inanyconceivable application ofadisjoint-set datastructure,
˛.n/ 4; thus, wecan view the running time as linear in min all practical situa-

tions. Strictly speaking, however, it is superlinear. In Section 21.4, we prove this
upperbound.
Exercises
21.3-1
RedoExercise21.2-2using adisjoint-set forest withunion byrankandpathcom-
pression.
21.3-2
Writeanonrecursive versionofFIND-SET withpathcompression.
21.3-3
Giveasequence ofm MAKE-SET, UNION,and FIND-SET operations, nofwhich
are MAKE-SET operations, that takes .mlgn/ time when we use union by rank
only.
21.3-4
Supposethatwewishtoaddtheoperation PRINT-SET.x/,whichisgivenanodex
and prints all the members of x’s set, in any order. Show how we can add just
a single attribute to each node in a disjoint-set forest so that PRINT-SET.x/ takes
timelinear inthe numberofmembers ofx’ssetand theasymptotic running times
of the other operations are unchanged. Assume that wecan print each member of
thesetinO.1/time.
21.3-5 ?
ShowthatanysequenceofmMAKE-SET, FIND-SET,andLINKoperations,where
allthe LINK operations appear beforeanyofthe FIND-SET operations, takesonly
O.m/ time if weuse both path compression and union by rank. What happens in
thesamesituation ifweuseonlythepath-compression heuristic?
21.4 Analysisofunionbyrankwithpathcompression 573
? 21.4 Analysisofunion by rank withpathcompression
AsnotedinSection21.3,thecombined union-by-rank andpath-compression heu-
ristic runs in time O.m˛.n// for m disjoint-set operations on n elements. In this
section, weshallexaminethefunction ˛ toseejusthowslowlyitgrows. Thenwe
provethisrunningtimeusingthepotential methodofamortizedanalysis.
Averyquicklygrowingfunctionanditsveryslowlygrowinginverse
Forintegers k 0andj 1,wedefinethefunction A .j/as
k
 
j 1 ifk 0;
A .j/ C D
k D
(
A. kj C11/.j/ ifk 1;
(cid:0)
wheretheexpressionA. kj C11/.j/usesthefunctional-iteration
notationgiveninSec-
t Wio en w3 i. l2 l. reS fp ee rc ti ofi tc ha ell py, arA am(cid:0). k0 (cid:0)/
e1
t. ej r/
kD
asj thean led vA el. ki o(cid:0)/ f1. thj e/
fD
uncA
tik o(cid:0)
n1. AA .. ki (cid:0)(cid:0)11/.j// for i

1.
ThefunctionA .j/strictlyincreaseswithbothj andk. Toseejusthowquickly
k
thisfunction grows,wefirstobtainclosed-form expressions forA .j/andA .j/.
1 2
Lemma21.2
Foranyintegerj 1,wehaveA .j/ 2j 1.
1
 D C
Proof Wefirstuseinductiononi toshowthatA.i/.j/ j i. Forthebasecase,
0 D C
we have A. 00/.j/
D
j
D
j C0. For the inductive step, assume that A. 0i (cid:0)1/.j/
D
j C.i (cid:0)1/. ThenA. 0i/.j/
D
A 0.A. 0i (cid:0)1/.j//
D
.j C.i (cid:0)1// C1
D
j Ci. Finally,
wenotethatA 1.j/
D
A. 0j C1/.j/
D
j C.j C1/
D
2j C1.
Lemma21.3
Foranyintegerj 1,wehaveA .j/ 2j 1.j 1/ 1.
2 C
 D C (cid:0)
Proof We first use induction on i to show that A.i/.j/ 2i.j 1/ 1. For
1 D C (cid:0)
the base case, we have A.0/.j/ j 20.j 1/ 1. For the inductive step,
1 D D C (cid:0)
assume that A. 1i (cid:0)1/.j/
D
2i (cid:0)1.j
C
1/
(cid:0)
1. Then A. 1i/.j/
D
A 1.A. 1i (cid:0)1/.j//
D
A .2i 1.j 1/ 1/ 2 .2i 1.j 1/ 1/ 1 2i.j 1/ 2 1 2i.j 1/ 1.
1 (cid:0) (cid:0)
C (cid:0) D  C (cid:0) C D C (cid:0) C D C (cid:0)
Finally,wenotethatA 2.j/
D
A. 1j C1/.j/
D
2j C1.j C1/ (cid:0)1.
NowwecanseehowquicklyA .j/growsbysimplyexaminingA .1/forlevels
k k
k 0;1;2;3;4. From the definition of A .k/ and the above lemmas, we have
0
D
A .1/ 1 1 2,A .1/ 2 1 1 3,andA .1/ 21 1 .1 1/ 1 7.
0 1 2 C
D C D D  C D D  C (cid:0) D
574 Chapter21 DataStructuresforDisjointSets
Wealsohave
A .1/ A.2/.1/
3 D 2
A .A .1//
2 2
D
A .7/
2
D
28 8 1
D  (cid:0)
211 1
D (cid:0)
2047
D
and
A .1/ A.2/.1/
4 D 3
A .A .1//
3 3
D
A .2047/
3
D
A.2048/.2047/
D 2
A .2047/
2
22048 2048 1
D  (cid:0)
> 22048
.24/512
D
16512
D
1080 ;
which is the estimated number of atoms in the observable universe. (The symbol
“ ”denotes the“much-greater-than” relation.)
Wedefinetheinverseofthefunction A .n/,forintegern 0,by
k

˛.n/ min k A .1/ n :
k
D f W  g
Inwords, ˛.n/isthelowest levelk for whichA .1/ isatleast n. Fromthe above
k
valuesofA .1/,weseethat
k
0 for0 n 2;
 
1 forn 3;
D
˛.n/ 2 for4 n 7;
D  
3 for8 n 2047;
 
˚
4 for2048 n A .1/:
4
 
It is only for values of n so large that the term “astronomical” understates them
(greater than A .1/, a huge number) that ˛.n/ > 4, and so ˛.n/ 4 for all
4

practical purposes.
21.4 Analysisofunionbyrankwithpathcompression 575
Propertiesofranks
Intheremainderofthissection,weproveanO.m˛.n//boundontherunningtime
ofthedisjoint-set operations withunionbyrankandpathcompression. Inorderto
provethisbound, wefirstprovesomesimpleproperties ofranks.
Lemma21.4
For all nodes x, we have x:rank x:p:rank, with strict inequality if x x:p.
 ¤
The value of x:rank is initially 0 and increases through time until x x:p; from
¤
then on, x:rank does not change. The value of x:p:rank monotonically increases
overtime.
Proof Theproof is astraightforward induction on the number ofoperations, us-
ing the implementations of MAKE-SET, UNION, and FIND-SET that appear in
Section21.3. WeleaveitasExercise21.4-1.
Corollary21.5
Aswefollow thesimplepath fromanynodetowardaroot, thenoderanks strictly
increase.
Lemma21.6
Everynodehasrankatmostn 1.
(cid:0)
Proof Each node’s rank starts at 0, and it increases only upon LINK operations.
Because there are at most n 1 UNION operations, there are also at most n 1
(cid:0) (cid:0)
LINK operations. Because each LINK operation either leaves all ranks alone or
increases somenode’srankby1,allranksareatmostn 1.
(cid:0)
Lemma 21.6 provides a weak bound on ranks. In fact, every node has rank at
most lgn (see Exercise 21.4-2). The looser bound of Lemma 21.6 will suffice
b c
forourpurposes, however.
Provingthetimebound
Weshallusethepotentialmethodofamortizedanalysis(seeSection17.3)toprove
the O.m˛.n// time bound. In performing the amortized analysis, we will find it
convenient to assume that we invoke the LINK operation rather than the UNION
operation. Thatis, sincetheparameters ofthe LINK procedure arepointers totwo
roots, we act as though we perform the appropriate FIND-SET operations sepa-
rately. The following lemmashows that even if wecount the extra FIND-SET op-
erationsinducedbyUNIONcalls,theasymptoticrunningtimeremainsunchanged.
576 Chapter21 DataStructuresforDisjointSets
Lemma21.7
Suppose weconvert asequence S
0
ofm
0
MAKE-SET, UNION, and FIND-SET op-
erations into asequence S of m MAKE-SET, LINK, and FIND-SET operations by
turning each UNION into two FIND-SET operations followed bya LINK. Then, if
sequence S runsinO.m˛.n//time,sequenceS runsinO.m ˛.n//time.
0 0
Proof Sinceeach UNION operation insequenceS
0
isconverted intothreeopera-
tionsinS,wehavem m 3m. Sincem O.m/,anO.m˛.n//timebound
0 0 0
  D
for the converted sequence S implies an O.m ˛.n// time bound for the original
0
sequence S .
0
In the remainder ofthis section, weshall assume that the initial sequence of m
0
MAKE-SET, UNION, and FIND-SET operations has been converted to asequence
ofm MAKE-SET, LINK,and FIND-SET operations. WenowproveanO.m˛.n//
time bound for the converted sequence and appeal to Lemma 21.7 to prove the
O.m ˛.n//running timeoftheoriginal sequence ofm operations.
0 0
Potentialfunction
The potential function we use assigns a potential .x/ to each node x in the
q
disjoint-set forest after q operations. We sum the node potentials for the poten-
tial of the entire forest: ˆ .x/, where ˆ denotes the potential of the
q D x q q
forest after q operations. The forest is empty prior to the first operation, and we
P
arbitrarily setˆ 0. Nopotentialˆ willeverbenegative.
0 q
D
The value of .x/ depends on whether x is a tree root after the qth operation.
q
Ifitis,orifx:rank 0,then .x/ ˛.n/ x:rank.
q
D D 
Now suppose that after the qth operation, x is not a root and that x:rank 1.

We need to define two auxiliary functions on x before we can define .x/. First
q
wedefine
level.x/ max k x:p:rank A .x:rank/ :
k
D f W  g
That is, level.x/ is the greatest level k for which A , applied to x’s rank, is no
k
greaterthanx’sparent’s rank.
Weclaimthat
0 level.x/< ˛.n/; (21.1)

whichweseeasfollows. Wehave
x:p:rank x:rank 1 (byLemma21.4)
 C
A .x:rank/ (bydefinition ofA .j/) ,
0 0
D
whichimpliesthatlevel.x/ 0,andwehave

21.4 Analysisofunionbyrankwithpathcompression 577
A .x:rank/ A .1/ (because A .j/isstrictlyincreasing)
˛.n/ ˛.n/ k

n (bythedefinitionof˛.n/)

> x:p:rank (byLemma21.6) ,
which implies that level.x/ < ˛.n/. Note that because x:p:rank monotonically
increases overtime,sodoeslevel.x/.
Thesecondauxiliary function applieswhenx:rank 1:

iter.x/ max i x:p:rank A.i/ .x:rank/ :
D W  level.x/
That is, iter.x/˚is the largest number of times(cid:9)we can iteratively apply A ,
level.x/
appliedinitially tox’srank,beforewegetavaluegreaterthanx’sparent’srank.
Weclaimthatwhenx:rank 1,wehave

1 iter.x/ x:rank; (21.2)
 
whichweseeasfollows. Wehave
x:p:rank A .x:rank/ (bydefinitionoflevel.x/)

level.x/
A.1/ .x:rank/ (bydefinitionoffunctional iteration) ,
D level.x/
whichimpliesthatiter.x/ 1,andwehave

A. lex v: er l.a xnk /C1/.x:rank/
D
A
level.x/
C1.x:rank/ (bydefinitionofA k.j/)
> x:p:rank (bydefinitionoflevel.x/) ,
which implies that iter.x/ x:rank. Note that because x:p:rank monotonically

increasesovertime,inorderforiter.x/todecrease,level.x/mustincrease. Aslong
aslevel.x/remainsunchanged, iter.x/musteitherincrease orremainunchanged.
With these auxiliary functions in place, we are ready to define the potential of
nodex afterq operations:
˛.n/ x:rank ifx isarootorx:rank 0;
.x/  D
q
D
(
.˛.n/ level.x// x:rank iter.x/ ifx isnotarootandx:rank 1:
(cid:0)  (cid:0) 
Wenextinvestigate someusefulproperties ofnodepotentials.
Lemma21.8
Foreverynodex,andforalloperation countsq,wehave
0 .x/ ˛.n/ x:rank:
q
  
578 Chapter21 DataStructuresforDisjointSets
Proof Ifxisarootorx:rank 0,then .x/ ˛.n/ x:rankbydefinition. Now
q
D D 
supposethatxisnotarootandthatx:rank 1. Weobtainalowerboundon .x/
q

bymaximizinglevel.x/anditer.x/. Bythebound(21.1),level.x/ ˛.n/ 1,and
 (cid:0)
bythebound(21.2),iter.x/ x:rank. Thus,

.x/ .˛.n/ level.x// x:rank iter.x/
q
D (cid:0)  (cid:0)
.˛.n/ .˛.n/ 1// x:rank x:rank
 (cid:0) (cid:0)  (cid:0)
x:rank x:rank
D (cid:0)
0:
D
Similarly,weobtainanupperboundon .x/byminimizinglevel.x/anditer.x/.
q
Bythebound(21.1),level.x/ 0,andbythebound(21.2),iter.x/ 1. Thus,
 
.x/ .˛.n/ 0/ x:rank 1
q
 (cid:0)  (cid:0)
˛.n/ x:rank 1
D  (cid:0)
< ˛.n/ x:rank:

Corollary 21.9
Ifnodex isnotarootandx:rank > 0,then .x/< ˛.n/ x:rank.
q

Potentialchangesandamortizedcostsofoperations
Wearenowreadytoexaminehowthedisjoint-setoperationsaffectnodepotentials.
With an understanding of the change in potential due to each operation, we can
determine eachoperation’s amortizedcost.
Lemma21.10
Let x be a node that is not a root, and suppose that the qth operation is either a
LINK or FIND-SET. Thenafter the qthoperation, q.x/
q
1.x/. Moreover, if
x:rank 1 and either level.x/ or iter.x/ changes due to the(cid:0)qth operation, then

.x/ .x/ 1. That is, x’spotential cannot increase, and ifit has positive
q q 1
rankan deith(cid:0) erlev(cid:0) el.x/oriter.x/changes, thenx’spotential dropsbyatleast1.
Proof Because x is not a root, the qth operation does not change x:rank, and
becausendoesnotchangeaftertheinitialnMAKE-SET operations, ˛.n/remains
unchanged as well. Hence, these components of the formula for x’s potential re-
mainthesameaftertheqthoperation. Ifx:rank 0,then .x/ .x/ 0.
q q 1
Nowassumethatx:rank 1. D D (cid:0) D

Recall that level.x/ monotonically increases over time. If the qth operation
leaves level.x/ unchanged, then iter.x/ either increases or remains unchanged.
If both level.x/ and iter.x/ are unchanged, then .x/ .x/. If level.x/
q q 1
D (cid:0)
21.4 Analysisofunionbyrankwithpathcompression 579
is unchanged and iter.x/ increases, then it increases by at least 1, and so
.x/ .x/ 1.
q q 1
Fina lly, if(cid:0) the q(cid:0) th operation increases level.x/,itincreases by at least 1, sothat
the value of the term .˛.n/ level.x// x:rank drops by at least x:rank. Be-
(cid:0) 
cause level.x/ increased, the value of iter.x/ might drop, but according to the
bound (21.2), the drop is by at most x:rank 1. Thus, the increase in poten-
(cid:0)
tial due to the change in iter.x/ is less than the decrease in potential due to the
changeinlevel.x/,andweconclude that .x/ .x/ 1.
q q 1
 (cid:0) (cid:0)
OurfinalthreelemmasshowthattheamortizedcostofeachMAKE-SET, LINK,
and FIND-SET operation is O.˛.n//. Recall from equation (17.2) that the amor-
tized cost of each operation is its actual cost plus the increase in potential due to
theoperation.
Lemma21.11
Theamortizedcostofeach MAKE-SET operation isO.1/.
Proof Suppose that the qth operation is MAKE-SET.x/. This operation creates
node x with rank 0, so that .x/ 0. No other ranks or potentials change, and
q
D
so ˆ
q
ˆ
q
1. Noting that the actual cost of the MAKE-SET operation is O.1/
D (cid:0)
completestheproof.
Lemma21.12
Theamortizedcostofeach LINK operation isO.˛.n//.
Proof SupposethattheqthoperationisLINK.x;y/. TheactualcostoftheLINK
operation is O.1/. Without loss ofgenerality, suppose that the LINK makes y the
parentofx.
To determine the change in potential due to the LINK, we note that the only
nodeswhosepotentialsmaychangearex,y,andthechildrenofy justpriortothe
operation. We shall show that the only node whose potential can increase due to
the LINK isy,andthatitsincreaseisatmost˛.n/:
 By Lemma 21.10, any node that is y’s child just before the LINK cannot have
itspotential increaseduetothe LINK.
 Fromthedefinitionof .x/,weseethat,sincex wasarootjustbeforetheqth
q
operation, .x/ ˛.n/ x:rank. Ifx:rank 0,then .x/ .x/ 0.
q 1 q q 1
(cid:0) D  D D (cid:0) D
Otherwise,
.x/ < ˛.n/ x:rank (byCorollary21.9)
q

.x/;
q 1
D (cid:0)
andsox’spotential decreases.
580 Chapter21 DataStructuresforDisjointSets
 Because y is a root prior to the LINK,
q
1.y/ ˛.n/ y:rank. The LINK
operationleavesyasaroot,anditeitherlea(cid:0) vesy’sD rankalo neoritincreasesy’s
rankby1. Therefore, either .y/ .y/or .y/ .y/ ˛.n/.
q q 1 q q 1
D (cid:0) D (cid:0) C
The increase in potential due to the LINK operation, therefore, is at most ˛.n/.
Theamortized costofthe LINK operation isO.1/ ˛.n/ O.˛.n//.
C D
Lemma21.13
Theamortized costofeach FIND-SET operation isO.˛.n//.
Proof Suppose that the qth operation is a FIND-SET and that the find path con-
tains s nodes. The actual cost of the FIND-SET operation is O.s/. We shall
show that no node’s potential increases due to the FIND-SET and that at least
max.0;s .˛.n/ 2// nodes on the find path have their potential decrease by
(cid:0) C
atleast1.
Toseethatnonode’spotential increases, wefirstappealtoLemma21.10forall
nodesotherthantheroot. Ifx istheroot,thenitspotentialis˛.n/ x:rank,which

doesnotchange.
Now we show that at least max.0;s .˛.n/ 2// nodes have their potential
(cid:0) C
decrease by at least 1. Let x be a node on the find path such that x:rank > 0
andx isfollowedsomewhereonthefindpathbyanother nodey thatisnotaroot,
wherelevel.y/ level.x/justbefore the FIND-SET operation. (Nodey need not
D
immediatelyfollowx onthefindpath.) Allbutatmost˛.n/ 2nodesonthefind
C
pathsatisfytheseconstraintsonx. Thosethatdonotsatisfythemarethefirstnode
on the findpath (if it has rank 0), the last node on the path (i.e., the root), and the
lastnodewonthepathforwhichlevel.w/ k,foreachk 0;1;2;:::;˛.n/ 1.
D D (cid:0)
Let us fix such a node x, and we shall show that x’s potential decreases by at
least1. Letk level.x/ level.y/. Justpriortothepathcompression causedby
D D
the FIND-SET, wehave
x:p:rank A.iter.x//.x:rank/ (bydefinitionofiter.x/) ,
 k
y:p:rank A .y:rank/ (bydefinitionoflevel.y/) ,
k

y:rank x:p:rank (byCorollary21.5andbecause

y followsx onthefindpath) .
Putting these inequalities together and letting i be the value of iter.x/before path
compression, wehave
y:p:rank A .y:rank/
k

A .x:p:rank/ (because A .j/isstrictly increasing)
k k

A .A.iter.x//.x:rank//
 k k
D
A. ki C1/.x:rank/:
21.4 Analysisofunionbyrankwithpathcompression 581
Because path compression will make x and y have the same parent, we know
that after path compression, x:p:rank y:p:rank and that the path compression
D
doesnotdecreasey:p:rank. Sincex:rankdoesnotchange,afterpathcompression
we have that x:p:rank

A. ki C1/.x:rank/. Thus, path compression will cause ei-
ther iter.x/ to increase (to at least i 1) or level.x/ to increase (which occurs if
C
iter.x/increases toatleastx:rank 1). Ineithercase,byLemma21.10, wehave
C
.x/ .x/ 1. Hence,x’spotential decreases byatleast1.
q q 1
 (cid:0) (cid:0)
TheamortizedcostoftheFIND-SET operationistheactualcostplusthechange
in potential. The actual cost is O.s/, and we have shown that the total potential
decreases by at least max.0;s .˛.n/ 2//. The amortized cost, therefore, is at
(cid:0) C
mostO.s/ .s .˛.n/ 2// O.s/ s O.˛.n// O.˛.n//,since wecan
(cid:0) (cid:0) C D (cid:0) C D
scaleuptheunitsofpotential todominatetheconstant hiddeninO.s/.
Puttingthepreceding lemmastogetheryieldsthefollowingtheorem.
Theorem21.14
A sequence of m MAKE-SET, UNION, and FIND-SET operations, n of which are
MAKE-SET operations, can be performed on a disjoint-set forest with union by
rankandpathcompression inworst-case timeO.m˛.n//.
Proof ImmediatefromLemmas21.7,21.11,21.12,and21.13.
Exercises
21.4-1
ProveLemma21.4.
21.4-2
Provethateverynodehasrankatmost lgn .
b c
21.4-3
In light of Exercise 21.4-2, how many bits are necessary to store x:rank for each
nodex?
21.4-4
Using Exercise 21.4-2, give a simple proof that operations on a disjoint-set forest
withunionbyrankbutwithoutpathcompression runinO.mlgn/time.
21.4-5
Professor Dante reasons that because node ranks increase strictly along a simple
path to the root, node levels must monotonically increase along the path. In other
582 Chapter21 DataStructuresforDisjointSets
words, if x:rank > 0 and x:p is not a root, then level.x/ level.x:p/. Is the

professor correct?
21.4-6 ?
Consider thefunction ˛ .n/ min k A .1/ lg.n 1/ . Showthat˛ .n/ 3
0 k 0
D f W  C g 
for all practical values of n and, using Exercise 21.4-2, show how to modify the
potential-function argumenttoprovethatwecanperformasequenceofmMAKE-
SET,UNION,andFIND-SEToperations,nofwhichareMAKE-SEToperations,on
a disjoint-set forest with union by rank and path compression in worst-case time
O.m˛ .n//.
0
Problems
21-1 Off-lineminimum
The off-line minimum problem asks us to maintain a dynamic set T of elements
from the domain 1;2;:::;n under the operations INSERT and EXTRACT-MIN.
f g
We are given a sequence S of n INSERT and m EXTRACT-MIN calls, where each
key in 1;2;:::;n is inserted exactly once. We wish to determine which key
f g
is returned by each EXTRACT-MIN call. Specifically, we wish to fill in an array
extractedŒ1::m, where for i 1;2;:::;m, extractedŒi is the key returned by
D
the ith EXTRACT-MIN call. The problem is “off-line” in the sense that we are
allowed to process the entire sequence S before determining any of the returned
keys.
a. In the following instance of the off-line minimum problem, each operation
INSERT.i/ is represented by the value of i and each EXTRACT-MIN is rep-
resentedbytheletterE:
4;8;E;3;E;9;2;6;E;E;E;1;7;E;5:
Fillinthecorrectvaluesintheextracted array.
Todevelop an algorithm for this problem, webreak the sequence S into homoge-
neoussubsequences. Thatis,werepresent S by
I ;E;I ;E;I ;:::;I ;E;I ;
1 2 3 m m 1
C
whereeachErepresentsasingleEXTRACT-MIN callandeachI
j
representsa(pos-
siblyempty)sequenceofINSERT calls. ForeachsubsequenceI j,weinitiallyplace
the keys inserted by these operations into a set K , which is empty if I is empty.
j j
Wethendothefollowing:
ProblemsforChapter21 583
OFF-LINE-MINIMUM.m;n/
1 fori 1ton
D
2 determinej suchthati K
j
2
3 ifj m 1
¤ C
4 extractedŒj i
D
5 letl bethesmallestvaluegreaterthanj
forwhichsetK exists
l
6 K K K ,destroying K
l j l j
D [
7 returnextracted
b. Arguethatthearrayextracted returned byOFF-LINE-MINIMUM iscorrect.
c. Describe how to implement OFF-LINE-MINIMUM efficiently with a disjoint-
set data structure. Give a tight bound on the worst-case running time of your
implementation.
21-2 Depthdetermination
In the depth-determination problem, we maintain a forest F T of rooted
i
D f g
treesunderthreeoperations:
MAKE-TREE./createsatreewhoseonlynodeis.
FIND-DEPTH./returnsthedepthofnode withinitstree.
GRAFT.r;/makes node r, whichisassumed tobethe root ofatree, become the
childofnode,whichisassumedtobeinadifferenttreethanr butmayormay
notitselfbearoot.
a. Suppose that we use a tree representation similar to a disjoint-set forest: :p
is the parent of node , except that :p  if  is a root. Suppose further
D
that we implement GRAFT.r;/ by setting r:p  and FIND-DEPTH./ by
D
followingthefindpathuptotheroot,returningacountofallnodesotherthan
encountered. Showthattheworst-caserunningtimeofasequenceofmMAKE-
TREE, FIND-DEPTH, and GRAFT operations is‚.m2/.
By using the union-by-rank and path-compression heuristics, we can reduce the
worst-case running time. We use the disjoint-set forest S S , where each
i
set S (which is itself a tree) corresponds to a tree T in
theD fof restgF
. The tree
i i
structurewithinasetS ,however,doesnotnecessarily correspondtothatofT . In
i i
fact, theimplementation ofS does notrecord theexact parent-child relationships
i
butnevertheless allowsustodetermineanynode’sdepthinT .
i
The key idea is to maintain in each node  a “pseudodistance” :d, which is
definedsothatthesumofthepseudodistances alongthesimplepathfrom tothe
584 Chapter21 DataStructuresforDisjointSets
rootofitssetS equalsthedepthof inT . Thatis,ifthesimplepathfrom toits
i i
root in S is  ; ;:::; , where   and  is S ’s root, then the depth of 
i 0 1 k 0 k i
D
inT is k  :d.
i j 0 j
D
b. GivePanimplementation ofMAKE-TREE.
c. Showhow tomodify FIND-SET toimplement FIND-DEPTH. Yourimplemen-
tation should perform path compression, and its running time should be linear
in the length of the find path. Make sure that your implementation updates
pseudodistances correctly.
d. Show how to implement GRAFT.r;/, which combines the sets containing r
and , by modifying the UNION and LINK procedures. Make sure that your
implementationupdatespseudodistancescorrectly. NotethattherootofasetS
i
isnotnecessarily therootofthecorresponding treeT .
i
e. Giveatight bound onthe worst-case running timeofasequence ofm MAKE-
TREE, FIND-DEPTH, andGRAFT operations, nofwhichare MAKE-TREE op-
erations.
21-3 Tarjan’soff-lineleast-common-ancestors algorithm
Theleastcommonancestoroftwonodesuand inarootedtreeT isthenodew
that is an ancestor of both u and  and that has the greatest depth in T. In the
off-line least-common-ancestors problem, we are given a rooted tree T and an
arbitrary setP u; ofunordered pairsofnodesinT,andwewishtodeter-
D ff gg
minetheleastcommonancestor ofeachpairinP.
Tosolve theoff-line least-common-ancestors problem, the following procedure
performs atreewalkofT withtheinitial call LCA.T:root/. Weassume thateach
nodeiscolored WHITE priortothewalk.
LCA.u/
1 MAKE-SET.u/
2 FIND-SET.u/:ancestor u
D
3 foreachchild ofuinT
4 LCA./
5 UNION.u;/
6 FIND-SET.u/:ancestor u
D
7 u:color BLACK
D
8 foreachnode suchthat u; P
f g 2
9 if:color == BLACK
10 print“Theleastcommonancestor of”
u“and” “is” FIND-SET./:ancestor
NotesforChapter21 585
a. Arguethatline10executesexactlyonceforeachpair u; P.
f g 2
b. ArguethatatthetimeofthecallLCA.u/,thenumberofsetsinthedisjoint-set
datastructure equalsthedepthofuinT.
c. Provethat LCAcorrectly printstheleastcommonancestorofuand foreach
pair u; P.
f g 2
d. AnalyzetherunningtimeofLCA,assumingthatweusetheimplementationof
thedisjoint-set datastructure inSection21.3.
Chapter notes
Manyoftheimportantresultsfordisjoint-set datastructuresaredueatleastinpart
to R. E. Tarjan. Using aggregate analysis, Tarjan [328, 330] gave the first tight
upper bound intermsoftheveryslowlygrowing inverse ˛.m;n/ofAckermann’s
y
function. (The function A .j/ given in Section 21.4 is similar to Ackermann’s
k
function, and the function ˛.n/ is similar to the inverse. Both ˛.n/ and ˛.m;n/
y
are at most 4 for all conceivable values of m and n.) An O.mlgn/ upper bound
wasprovenearlierbyHopcroftandUllman[5,179]. ThetreatmentinSection21.4
isadapted fromalateranalysis byTarjan[332],whichisinturnbasedonananal-
ysis by Kozen [220]. Harfst and Reingold [161] give a potential-based version of
Tarjan’searlierbound.
TarjanandvanLeeuwen[333]discuss variantsonthepath-compression heuris-
tic, including “one-pass methods,” which sometimes offer better constant factors
in their performance than do two-pass methods. As with Tarjan’s earlier analyses
of the basic path-compression heuristic, the analyses by Tarjan and van Leeuwen
areaggregate. HarfstandReingold[161]latershowedhowtomakeasmallchange
tothepotentialfunctiontoadapttheirpath-compression analysistotheseone-pass
variants. GabowandTarjan[121]showthatincertainapplications, thedisjoint-set
operations canbemadetoruninO.m/time.
Tarjan [329] showed that a lower bound of .m ˛.m;n// time is required for
y
operationsonanydisjoint-setdatastructuresatisfyingcertaintechnicalconditions.
This lower bound waslater generalized by Fredman and Saks [113], who showed
thatintheworstcase,.m˛.m;n//.lgn/-bitwordsofmemorymustbeaccessed.
y
VI Graph Algorithms
Introduction
Graphproblems pervadecomputer science, andalgorithms forworking withthem
are fundamental to the field. Hundreds of interesting computational problems are
couched intermsofgraphs. Inthispart,wetouchonafewofthemoresignificant
ones.
Chapter22showshowwecanrepresentagraphinacomputerandthendiscusses
algorithms based on searching a graph using either breadth-first search or depth-
firstsearch. Thechaptergivestwoapplications ofdepth-firstsearch: topologically
sortingadirectedacyclicgraphanddecomposingadirectedgraphintoitsstrongly
connected components.
Chapter 23 describes how to compute a minimum-weight spanning tree of a
graph: the least-weight way of connecting all of the vertices together when each
edge has an associated weight. Thealgorithms for computing minimum spanning
treesserveasgoodexamplesofgreedy algorithms(seeChapter16).
Chapters 24 and 25 consider how to compute shortest paths between vertices
when each edge has an associated length or “weight.” Chapter 24 shows how to
findshortest paths from agivensource vertextoallother vertices, andChapter 25
examinesmethodstocomputeshortestpathsbetweeneverypairofvertices.
Finally,Chapter26showshowtocomputeamaximumflowofmaterialinaflow
network, which is a directed graph having a specified source vertex of material, a
specified sink vertex, and specified capacities for the amount of material that can
traverse each directed edge. This general problem arises in many forms, and a
good algorithm for computing maximum flows can help solve a variety of related
problemsefficiently.
588 PartVI GraphAlgorithms
When we characterize the running time of a graph algorithm on a given graph
G .V;E/, we usually measure the size of the input in terms of the number of
D
vertices V and the number of edges E of the graph. That is, we describe the
j j j j
sizeoftheinputwithtwoparameters,notjustone. Weadoptacommonnotational
convention for these parameters. Inside asymptotic notation (such as O-notation
or ‚-notation), and only inside such notation, the symbol V denotes V and
j j
the symbol E denotes E . For example, we might say, “the algorithm runs in
j j
time O.VE/,” meaning that the algorithm runs in time O. V E /. This conven-
j jj j
tionmakestherunning-time formulaseasiertoread,withoutriskofambiguity.
Another convention weadopt appears in pseudocode. We denote the vertex set
ofagraphG byG:V anditsedgesetbyG:E. Thatis,thepseudocodeviewsvertex
andedgesetsasattributes ofagraph.
22 Elementary Graph Algorithms
Thischapter presents methods for representing agraph and for searching agraph.
Searching a graph means systematically following the edges of the graph so as to
visit the vertices of the graph. A graph-searching algorithm can discover much
about the structure of a graph. Many algorithms begin by searching their input
graph to obtain this structural information. Several other graph algorithms elabo-
rate on basic graph searching. Techniques for searching agraph lie atthe heart of
thefieldofgraphalgorithms.
Section 22.1 discusses the two most common computational representations of
graphs: asadjacency listsand asadjacency matrices. Section 22.2presents asim-
ple graph-searching algorithm called breadth-first search and shows how to cre-
ate a breadth-first tree. Section 22.3 presents depth-first search and proves some
standard results about the order in which depth-first search visits vertices. Sec-
tion22.4providesourfirstrealapplicationofdepth-firstsearch: topologicallysort-
ingadirectedacyclicgraph. Asecondapplicationofdepth-firstsearch,findingthe
stronglyconnected components ofadirected graph,isthetopicofSection22.5.
22.1 Representations ofgraphs
We can choose between two standard ways to represent a graph G .V;E/:
D
as a collection of adjacency lists or as an adjacency matrix. Either way applies
to both directed and undirected graphs. Because the adjacency-list representation
provides acompact waytorepresent sparse graphs—those for which E is much
less than V 2 —it is usually the method of choice. Most of the graphj alj gorithms
j j
presented in this book assume that an input graph is represented in adjacency-
list form. We may prefer an adjacency-matrix representation, however, when the
graph is dense— E is close to V 2 —or when we need to be able to tell quickly
j j j j
ifthereisanedgeconnecting twogivenvertices. Forexample,twooftheall-pairs
590 Chapter22 ElementaryGraphAlgorithms
1 2 3 4 5
1 2 5 1 0 1 0 0 1
1 2 2 1 5 3 4 2 1 0 1 1 1
3 2 4 3 0 1 0 1 0
3
4 2 5 3 4 0 1 1 0 1
5 4 5 4 1 2 5 1 1 0 1 0
(a) (b) (c)
Figure22.1 Tworepresentationsofanundirectedgraph.(a)AnundirectedgraphGwith5vertices
and 7 edges. (b) An adjacency-list representation of G. (c) The adjacency-matrix representation
ofG.
1 2 3 4 5 6
1 2 4 1 0 1 0 1 0 0
2 5 2 0 0 0 0 1 0
1 2 3 3 6 5 3 0 0 0 0 1 1
4 2 4 0 1 0 0 0 0
5 4 5 0 0 0 1 0 0
4 5 6 6 6 6 0 0 0 0 0 1
(a) (b) (c)
Figure22.2 Tworepresentationsofadirectedgraph.(a)AdirectedgraphGwith6verticesand8
edges.(b)Anadjacency-listrepresentationofG.(c)Theadjacency-matrixrepresentationofG.
shortest-paths algorithms presented in Chapter 25 assume that their input graphs
arerepresented byadjacency matrices.
The adjacency-list representation of a graph G .V;E/ consists of an ar-
D
ray Adj of V lists, one for each vertex in V. For each u V, the adjacency list
j j 2
AdjŒu contains all the vertices  such that there is an edge .u;/ E. That is,
2
AdjŒuconsists ofalltheverticesadjacenttouinG. (Alternatively, itmaycontain
pointerstothesevertices.) Sincetheadjacencylistsrepresenttheedgesofagraph,
in pseudocode we treat the array Adj as an attribute of the graph, just as we treat
the edge set E. In pseudocode, therefore, we will see notation such as G:AdjŒu.
Figure 22.1(b) is an adjacency-list representation of the undirected graph in Fig-
ure 22.1(a). Similarly, Figure 22.2(b) is an adjacency-list representation of the
directed graphinFigure22.2(a).
If G is a directed graph, the sum of the lengths of all the adjacency lists is E ,
j j
sinceanedgeoftheform.u;/isrepresentedbyhavingappearinAdjŒu. IfGis
22.1 Representationsofgraphs 591
anundirected graph,thesumofthelengthsofalltheadjacencylistsis2 E ,since
j j
if.u;/isanundirected edge, then uappears in’sadjacency list andviceversa.
For both directed and undirected graphs, the adjacency-list representation has the
desirable propertythattheamountofmemoryitrequires is‚.V E/.
C
Wecanreadilyadaptadjacencyliststorepresentweightedgraphs,thatis,graphs
forwhicheachedgehasanassociatedweight,typicallygivenbyaweightfunction
w E R . For example, let G .V;E/ be a weighted graph with weight
W ! D
function w. We simply store the weight w.u;/ of the edge .u;/ E with
2
vertex  in u’s adjacency list. The adjacency-list representation is quite robust in
thatwecanmodifyittosupportmanyothergraphvariants.
A potential disadvantage of the adjacency-list representation is that it provides
no quicker way to determine whether a given edge .u;/ is present in the graph
than to search for  in the adjacency list AdjŒu. An adjacency-matrix representa-
tionofthegraphremediesthisdisadvantage,butatthecostofusingasymptotically
morememory. (SeeExercise22.1-8forsuggestionsofvariationsonadjacencylists
thatpermitfasteredgelookup.)
For the adjacency-matrix representation of a graph G .V;E/, we assume
D
that the vertices are numbered 1;2;:::; V in some arbitrary manner. Then the
j j
adjacency-matrix representation of a graph G consists of a V V matrix
j j  j j
A .a /suchthat
ij
D
1 if.i;j/ E ;
a 2
ij
D
(
0 otherwise:
Figures 22.1(c) and 22.2(c) are the adjacency matrices of the undirected and di-
rectedgraphsinFigures22.1(a)and22.2(a),respectively. Theadjacencymatrixof
agraphrequires‚.V2/memory,independent ofthenumberofedgesinthegraph.
Observe the symmetry along the main diagonal of the adjacency matrix in Fig-
ure 22.1(c). Since in an undirected graph, .u;/ and .;u/ represent the same
edge,theadjacencymatrixAofanundirectedgraphisitsowntranspose: A AT.
D
Insomeapplications, itpaystostoreonlytheentriesonandabovethediagonal of
theadjacencymatrix,therebycuttingthememoryneededtostorethegraphalmost
inhalf.
Liketheadjacency-listrepresentation ofagraph,anadjacencymatrixcanrepre-
sentaweightedgraph. Forexample,ifG .V;E/isaweightedgraphwithedge-
D
weightfunction w,wecansimplystoretheweightw.u;/oftheedge.u;/ E
2
as the entry in row u and column  of the adjacency matrix. If an edge does not
exist, wecanstore a NIL value asitscorresponding matrix entry, though formany
problemsitisconvenient touseavaluesuchas0or .
1
Although the adjacency-list representation is asymptotically at least as space-
efficient as the adjacency-matrix representation, adjacency matrices are simpler,
and so we may prefer them when graphs are reasonably small. Moreover, adja-
592 Chapter22 ElementaryGraphAlgorithms
cencymatricescarryafurtheradvantage forunweighted graphs: theyrequireonly
onebitperentry.
Representingattributes
Most algorithms that operate on graphs need to maintain attributes for vertices
and/or edges. We indicate these attributes using our usual notation, such as :d
for an attribute d of a vertex . When we indicate edges as pairs of vertices, we
use thesamestyle ofnotation. Forexample, ifedges haveanattribute f,then we
denote thisattribute foredge .u;/by.u;/:f. Forthepurpose ofpresenting and
understanding algorithms, ourattribute notation suffices.
Implementing vertex and edge attributes in real programs can be another story
entirely. There is no one best way to store and access vertex and edge attributes.
For a given situation, your decision will likely depend on the programming lan-
guageyouareusing,thealgorithmyouareimplementing,andhowtherestofyour
program usesthegraph. Ifyourepresent agraphusingadjacency lists,onedesign
represents vertex attributes in additional arrays, such as an array dŒ1:: V  that
j j
parallelstheAdjarray. IftheverticesadjacenttouareinAdjŒu,thenwhatwecall
theattributeu:dwouldactuallybestoredinthearrayentrydŒu. Manyotherways
of implementing attributes are possible. For example, in an object-oriented pro-
gramming language, vertex attributes might be represented as instance variables
withinasubclass ofaVertexclass.
Exercises
22.1-1
Given an adjacency-list representation of a directed graph, how long does it take
to compute the out-degree of every vertex? How long does it take to compute the
in-degrees?
22.1-2
Giveanadjacency-listrepresentation foracompletebinarytreeon7vertices. Give
anequivalentadjacency-matrix representation. Assumethatverticesarenumbered
from1to7asinabinaryheap.
22.1-3
Thetranspose ofadirected graph G .V;E/ isthe graph GT .V;ET/, where
D D
ET .;u/ V V .u;/ E . Thus, GT is G with all its edges reversed.
D f 2  W 2 g
Describe efficient algorithms for computing GT from G, for both the adjacency-
listandadjacency-matrix representations ofG. Analyzetherunning timesofyour
algorithms.
22.1 Representationsofgraphs 593
22.1-4
Given an adjacency-list representation of a multigraph G .V;E/, describe an
D
O.V E/-time algorithm to compute the adjacency-list representation of the
C
“equivalent” undirected graph G .V;E /, where E consists of the edges in E
0 0 0
D
withallmultipleedgesbetweentwoverticesreplacedbyasingleedgeandwithall
self-loops removed.
22.1-5
The square of a directed graph G .V;E/ is the graph G2 .V;E2/ such that
D D
.u;/ E2 if and only if G contains a path with at most two edges between
2
u and . Describe efficient algorithms for computing G2 from G for both the
adjacency-list and adjacency-matrix representations of G. Analyze the running
timesofyouralgorithms.
22.1-6
Most graph algorithms that take an adjacency-matrix representation as input re-
quiretime.V2/,buttherearesomeexceptions. Showhowtodeterminewhether
adirectedgraphG containsauniversalsink—avertexwithin-degree V 1and
j j(cid:0)
out-degree 0—intimeO.V/,givenanadjacencymatrixforG.
22.1-7
The incidence matrix of a directed graph G .V;E/ with no self-loops is a
D
V E matrixB .b /suchthat
ij
j jj j D
1 ifedgej leavesvertexi ;
(cid:0)
b 1 ifedgej entersvertexi ;
ij
D
0 otherwise:

Describe what the entries of the matrix product BBT represent, where BT is the
transpose ofB.
22.1-8
Supposethatinsteadofalinkedlist,eacharrayentryAdjŒuisahashtablecontain-
ingthevertices forwhich.u;/ E. Ifalledgelookupsareequallylikely,what
2
istheexpectedtimetodeterminewhetheranedgeisinthegraph? Whatdisadvan-
tagesdoesthisschemehave? Suggestanalternate datastructure foreachedgelist
thatsolves theseproblems. Doesyouralternative havedisadvantages compared to
thehashtable?
594 Chapter22 ElementaryGraphAlgorithms
22.2 Breadth-first search
Breadth-first search is one of the simplest algorithms for searching a graph and
the archetype for many important graph algorithms. Prim’s minimum-spanning-
treealgorithm (Section23.2)andDijkstra’ssingle-source shortest-paths algorithm
(Section24.3)useideassimilartothoseinbreadth-first search.
Given a graph G .V;E/ and a distinguished source vertex s, breadth-first
D
search systematically explores the edges of G to “discover” every vertex that is
reachable from s. It computes the distance (smallest number of edges) from s
to each reachable vertex. It also produces a “breadth-first tree” with root s that
contains allreachable vertices. Foranyvertex reachable froms, thesimple path
in the breadth-first tree from s to  corresponds to a “shortest path” from s to 
inG,thatis,apathcontaining thesmallestnumberofedges. Thealgorithm works
onbothdirected andundirected graphs.
Breadth-firstsearchissonamedbecauseitexpandsthefrontierbetweendiscov-
ered and undiscovered vertices uniformly across the breadth of the frontier. That
is, thealgorithm discovers allvertices atdistance k froms before discovering any
verticesatdistancek 1.
C
Tokeeptrackofprogress, breadth-first searchcolorseachvertexwhite,gray,or
black. All vertices start out white and may later become gray and then black. A
vertexisdiscoveredthefirsttimeitisencounteredduringthesearch,atwhichtime
itbecomesnonwhite. Grayandblackvertices,therefore,havebeendiscovered,but
breadth-first search distinguishes between them toensure thatthe search proceeds
in a breadth-first manner.1 If .u;/ E and vertex u is black, then vertex 
2
is either gray or black; that is, all vertices adjacent to black vertices have been
discovered. Gray vertices may have some adjacent white vertices; they represent
thefrontierbetweendiscovered andundiscovered vertices.
Breadth-first search constructs a breadth-first tree, initially containing only its
root, whichisthesourcevertexs. Wheneverthesearchdiscovers awhitevertex
in the course of scanning the adjacency list of an already discovered vertex u, the
vertex andtheedge.u;/areaddedtothetree. Wesaythatuisthepredecessor
orparentof inthebreadth-firsttree. Sinceavertexisdiscoveredatmostonce,it
has at most one parent. Ancestor and descendant relationships in the breadth-first
treearedefinedrelative totheroots asusual: ifuisonthesimplepathinthetree
fromtheroots tovertex,thenuisanancestor of and isadescendant ofu.
1Wedistinguishbetweengrayandblackverticestohelpusunderstandhowbreadth-firstsearchop-
erates.Infact,asExercise22.2-3shows,wewouldgetthesameresultevenifwedidnotdistinguish
betweengrayandblackvertices.
22.2 Breadth-firstsearch 595
The breadth-first-search procedure BFS below assumes that the input graph
G .V;E/ is represented using adjacency lists. It attaches several additional
D
attributes to each vertex in the graph. We store the color of each vertex u V
2
in the attribute u:color and the predecessor of u in the attribute u:. If u has no
predecessor(forexample,ifu soruhasnotbeendiscovered),thenu: NIL.
D D
Theattributeu:d holdsthedistancefromthesources tovertexucomputedbythe
algorithm. Thealgorithm also uses afirst-in, first-out queue Q (see Section 10.1)
tomanagethesetofgrayvertices.
BFS.G;s/
1 foreachvertexu G:V s
2 (cid:0)f g
2 u:color WHITE
D
3 u:d
D 1
4 u: NIL
D
5 s:color GRAY
D
6 s:d 0
D
7 s: NIL
D
8 Q
D ;
9 ENQUEUE.Q;s/
10 whileQ
¤;
11 u DEQUEUE.Q/
D
12 foreach G:AdjŒu
2
13 if:color == WHITE
14 :color GRAY
D
15 :d u:d 1
D C
16 : u
D
17 ENQUEUE.Q;/
18 u:color BLACK
D
Figure22.3illustrates theprogressof BFS onasamplegraph.
Theprocedure BFSworksasfollows. Withtheexceptionofthesourcevertexs,
lines 1–4 paint every vertex white, set u:d tobe infinity foreach vertex u, and set
theparentofeveryvertextobeNIL. Line5paintss gray,sinceweconsiderittobe
discovered as the procedure begins. Line 6 initializes s:d to 0, and line 7 sets the
predecessorofthesourcetobeNIL. Lines8–9initializeQtothequeuecontaining
justthevertexs.
The while loop of lines 10–18 iterates as long as there remain gray vertices,
which are discovered vertices that have not yet had their adjacency lists fully ex-
amined. Thiswhileloopmaintainsthefollowinginvariant:
Atthetestinline10,thequeueQconsists ofthesetofgrayvertices.
596 Chapter22 ElementaryGraphAlgorithms
r s t u r s t u
¥ 0 ¥ ¥ 1 0 ¥ ¥
(a) Q s (b) Q w r
¥ ¥ ¥ ¥ 0 ¥ 1 ¥ ¥ 1 1
v w x y v w x y
r s t u r s t u
1 0 2 ¥ 1 0 2 ¥
(c) Q r t x (d) Q t x v
¥ 1 2 ¥ 1 2 2 2 1 2 ¥ 2 2 2
v w x y v w x y
r s t u r s t u
1 0 2 3 1 0 2 3
(e) Q x v u (f) Q v u y
2 1 2 ¥ 2 2 3 2 1 2 3 2 3 3
v w x y v w x y
r s t u r s t u
1 0 2 3 1 0 2 3
(g) Q u y (h) Q y
2 1 2 3 3 3 2 1 2 3 3
v w x y v w x y
r s t u
1 0 2 3
(i) Q
;
2 1 2 3
v w x y
Figure22.3 TheoperationofBFSonanundirectedgraph. Treeedgesareshownshadedasthey
areproducedbyBFS.Thevalueofu:dappearswithineachvertexu.ThequeueQisshownatthe
beginningofeachiterationofthewhileloopoflines10–18. Vertexdistancesappearbelowvertices
inthequeue.
Although we won’t use this loop invariant to prove correctness, it is easy to see
that itholds prior tothefirstiteration andthat eachiteration ofthe loop maintains
the invariant. Prior to the first iteration, the only gray vertex, and the only vertex
in Q, is the source vertex s. Line 11 determines the gray vertex u at the head of
the queue Q and removes it from Q. The for loop of lines 12–17 considers each
vertex intheadjacencylistofu. If iswhite,thenithasnotyetbeendiscovered,
and the procedure discovers it by executing lines 14–17. The procedure paints
vertexgray,setsitsdistance:dtou:d 1,recordsuasitsparent:,andplaces
C
itatthetailofthequeueQ. Oncetheprocedurehasexaminedalltheverticesonu’s
22.2 Breadth-firstsearch 597
adjacency list, it blackens u in line 18. The loop invariant is maintained because
whenever a vertex is painted gray (in line 14) it is also enqueued (in line 17), and
wheneveravertexisdequeued (inline11)itisalsopainted black(inline18).
Theresultsofbreadth-firstsearchmaydependupontheorderinwhichtheneigh-
borsofagivenvertexarevisitedinline12: thebreadth-first treemayvary,butthe
distances d computed bythealgorithm willnot. (SeeExercise22.2-5.)
Analysis
Beforeprovingthevariousproperties ofbreadth-first search,wetakeonthesome-
what easier job of analyzing its running time on an input graph G .V;E/. We
D
useaggregateanalysis,aswesawinSection17.1. Afterinitialization, breadth-first
search neverwhitens avertex, and thus the test inline 13 ensures that each vertex
is enqueued at most once, and hence dequeued at most once. The operations of
enqueuing and dequeuing take O.1/ time, and so the total time devoted to queue
operations isO.V/. Becausetheprocedure scanstheadjacency listofeachvertex
onlywhenthevertexisdequeued, itscans eachadjacency listatmostonce. Since
the sum of the lengths of all the adjacency lists is ‚.E/, the total time spent in
scanning adjacency lists is O.E/. The overhead for initialization is O.V/, and
thusthetotalrunningtimeoftheBFSprocedureisO.V E/. Thus,breadth-first
C
searchrunsintimelinearinthesizeoftheadjacency-list representation ofG.
Shortestpaths
Atthebeginning ofthissection, weclaimed thatbreadth-first search findsthedis-
tance to each reachable vertex in a graph G .V;E/ from a given source vertex
D
s V. Definetheshortest-pathdistanceı.s;/froms to astheminimumnum-
2
ber of edges in any path from vertex s to vertex ; if there is no path from s to ,
then ı.s;/ . We call a path of length ı.s;/ from s to  a shortest path2
D 1
from s to. Before showing that breadth-first search correctly computes shortest-
pathdistances, weinvestigate animportantproperty ofshortest-path distances.
2InChapters24and25,weshallgeneralizeourstudyofshortestpathstoweightedgraphs,inwhich
everyedgehasareal-valuedweightandtheweightofapathisthesumoftheweightsofitscon-
stituent edges. The graphs considered in the present chapter are unweighted or, equivalently, all
edgeshaveunitweight.
598 Chapter22 ElementaryGraphAlgorithms
Lemma22.1
Let G .V;E/ be a directed or undirected graph, and let s V be an arbitrary
D 2
vertex. Then,foranyedge.u;/ E,
2
ı.s;/ ı.s;u/ 1:
 C
Proof Ifuisreachable froms,thensois. Inthiscase,theshortest pathfroms
to cannotbelongerthantheshortestpathfromstoufollowedbytheedge.u;/,
and thus the inequality holds. Ifuisnot reachable from s, then ı.s;u/ , and
D 1
theinequality holds.
We want to show that BFS properly computes :d ı.s;/ for each ver-
D
tex V. Wefirstshowthat:dbounds ı.s;/fromabove.
2
Lemma22.2
Let G .V;E/ be a directed or undirected graph, and suppose that BFS is run
D
on G from a given source vertex s V. Then upon termination, for each ver-
2
tex V,thevalue:dcomputedby BFS satisfies:d ı.s;/.
2 
Proof We use induction on the number of ENQUEUE operations. Our inductive
hypothesis isthat:d ı.s;/forall V.
 2
Thebasisoftheinductionisthesituationimmediatelyafterenqueuingsinline9
of BFS. The inductive hypothesis holds here, because s:d 0 ı.s;s/ and
D D
:d ı.s;/forall V s .
D1  2 (cid:0)f g
For the inductive step, consider a white vertex  that is discovered during the
searchfromavertexu. Theinductivehypothesisimpliesthatu:d ı.s;u/. From

theassignment performedbyline15andfromLemma22.1,weobtain
:d u:d 1
D C
ı.s;u/ 1
 C
ı.s;/:

Vertex isthenenqueued, anditisneverenqueued againbecauseitisalsograyed
and the then clause of lines 14–17 is executed only for white vertices. Thus, the
valueof:dneverchangesagain,andtheinductive hypothesis ismaintained.
Toprovethat:d ı.s;/,wemustfirstshowmorepreciselyhowthequeueQ
D
operates during the course of BFS. The next lemma shows that at all times, the
queueholdsatmosttwodistinctd values.
22.2 Breadth-firstsearch 599
Lemma22.3
Suppose that during the execution of BFS on a graph G .V;E/, the queue Q
D
contains the vertices  ; ;:::; , where  is the head of Q and  isthe tail.
1 2 r 1 r
h i
Then, :d  :d 1and :d  :dfori 1;2;:::;r 1.
r 1 i i 1
 C  C D (cid:0)
Proof The proof is by induction on the number of queue operations. Initially,
whenthequeuecontains onlys,thelemmacertainly holds.
Fortheinductivestep,wemustprovethatthelemmaholdsafterbothdequeuing
and enqueuing a vertex. If the head  of the queue is dequeued,  becomes the
1 2
newhead. (Ifthequeuebecomes empty, thenthelemmaholdsvacuously.) Bythe
inductivehypothesis,  :d  :d. Butthenwehave :d  :d 1  :d 1,
1 2 r 1 2
  C  C
andtheremaining inequalities areunaffected. Thus, thelemmafollowswith as
2
thehead.
In order to understand what happens upon enqueuing a vertex, we need to ex-
amine the code more closely. When we enqueue a vertex  in line 17 of BFS, it
becomes  . Atthat time, we have already removed vertex u, whose adjacency
r 1
listiscurrenC tlybeingscanned,fromthequeueQ,andbytheinductivehypothesis,
thenewhead has :d u:d. Thus, :d :d u:d 1  :d 1. From
1 1 r 1 1
theinductive hypothesis, wealsohave :C d D u:d D 1,andC so :d uC :d 1
r r
 C  C D
:d  :d, and the remaining inequalities are unaffected. Thus, the lemma
r 1
followD swhC en isenqueued.
The following corollary shows that the d values at the time that vertices are
enqueued aremonotonically increasing overtime.
Corollary22.4
Suppose that vertices  and  are enqueued during the execution of BFS, and
i j
that isenqueued before . Then :d  :datthetimethat isenqueued.
i j i j j

Proof Immediate from Lemma22.3 and theproperty that each vertex receives a
finited valueatmostonceduringthecourseofBFS.
We can now prove that breadth-first search correctly finds shortest-path dis-
tances.
Theorem22.5(Correctnessofbreadth-firstsearch)
Let G .V;E/ be a directed or undirected graph, and suppose that BFS is run
D
onG fromagivensourcevertexs V. Then,duringitsexecution, BFSdiscovers
2
every vertex  V that is reachable from the source s, and upon termination,
2
:d ı.s;/ for all  V. Moreover, for any vertex  s that is reachable
D 2 ¤
600 Chapter22 ElementaryGraphAlgorithms
from s, one of the shortest paths from s to  is a shortest path from s to :
followedbytheedge.:;/.
Proof Assume, for the purpose of contradiction, that some vertex receives a d
value not equal to its shortest-path distance. Let  be the vertex with min-
imum ı.s;/ that receives such an incorrect d value; clearly  s. By
¤
Lemma22.2,:d ı.s;/,andthuswehavethat:d > ı.s;/. Vertex mustbe

reachable from s, forifitisnot, thenı.s;/ :d. Letubethevertex im-
D 1 
mediatelypreceding  onashortestpathfroms to,sothatı.s;/ ı.s;u/ 1.
D C
Becauseı.s;u/ < ı.s;/,andbecauseofhowwechose,wehaveu:d ı.s;u/.
D
Puttingtheseproperties together, wehave
:d >ı.s;/ ı.s;u/ 1 u:d 1: (22.1)
D C D C
Now consider the time when BFS chooses to dequeue vertex u from Q in
line 11. At this time, vertex  is either white, gray, or black. We shall show
that in each of these cases, we derive a contradiction to inequality (22.1). If  is
white, then line 15 sets :d u:d 1, contradicting inequality (22.1). If  is
D C
black,thenitwasalreadyremovedfromthequeueand,byCorollary22.4,wehave
:d u:d, again contradicting inequality (22.1). If  is gray, then it was painted

gray upon dequeuing some vertex w, which was removed from Q earlier than u
andforwhich:d w:d 1. ByCorollary22.4,however,w:d u:d,andsowe
D C 
have:d w:d 1 u:d 1,onceagaincontradicting inequality (22.1).
D C  C
Thus we conclude that :d ı.s;/ for all  V. All vertices  reachable
D 2
froms mustbediscovered, forotherwise theywouldhave :d >ı.s;/. To
1 D
conclude the proof of the theorem, observe that if: u, then :d u:d 1.
D D C
Thus, we can obtain a shortest path from s to  by taking a shortest path from s
to: andthentraversing theedge.:;/.
Breadth-firsttrees
The procedure BFS builds a breadth-first tree as it searches the graph, as Fig-
ure 22.3 illustrates. The tree corresponds to the  attributes. More formally, for
a graph G .V;E/ with source s, we define the predecessor subgraph of G as
D
G .V ;E /,where
  
D
V   V : NIL s
D f 2 W ¤ g[f g
and
E .:;/  V s :
 
D f W 2 (cid:0)f gg
The predecessor subgraph G is a breadth-first tree if V consists of the vertices
 
reachable from s and, for all  V , the subgraph G contains a unique simple
 
2
22.2 Breadth-firstsearch 601
path from s to  that is also a shortest path from s to  in G. A breadth-first tree
isinfactatree, since itisconnected and E V 1(seeTheorem B.2). We
 
j j D j j(cid:0)
calltheedgesinE treeedges.

ThefollowinglemmashowsthatthepredecessorsubgraphproducedbytheBFS
procedure isabreadth-first tree.
Lemma22.6
Whenapplied toadirected orundirected graph G .V;E/,procedure BFS con-
D
structs sothatthepredecessor subgraph G .V ;E /isabreadth-first tree.
  
D
Proof Line16ofBFSsets: uifandonlyif.u;/ E andı.s;/ < —
D 2 1
thatis,ifisreachablefroms—andthusV consistsoftheverticesinV reachable

from s. Since G forms a tree, by Theorem B.2, it contains a unique simple path

from s to each vertex in V . By applying Theorem 22.5 inductively, weconclude

thateverysuchpathisashortestpathinG.
The following procedure prints out the vertices on a shortest path from s to ,
assumingthat BFS hasalreadycomputed abreadth-first tree:
PRINT-PATH.G;s;/
1 if ==s
2 prints
3 elseif: == NIL
4 print“nopathfrom”s “to” “exists”
5 else PRINT-PATH.G;s;:/
6 print
This procedure runs in time linear in the number of vertices in the path printed,
sinceeachrecursivecallisforapathonevertexshorter.
Exercises
22.2-1
Show the d and  values that result from running breadth-first search on the di-
rectedgraphofFigure22.2(a),usingvertex3asthesource.
22.2-2
Showthed and valuesthatresultfromrunningbreadth-first searchontheundi-
rectedgraphofFigure22.3,usingvertexuasthesource.
602 Chapter22 ElementaryGraphAlgorithms
22.2-3
Show that using a single bit to store each vertex color suffices by arguing that the
BFS procedure wouldproducethesameresultifline18wereremoved.
22.2-4
What is the running time of BFS if we represent its input graph by an adjacency
matrixandmodifythealgorithm tohandlethisformofinput?
22.2-5
Argue that in a breadth-first search, the value u:d assigned to a vertex u is inde-
pendent of the order in which the vertices appear in each adjacency list. Using
Figure 22.3 as an example, show that the breadth-first tree computed by BFS can
depend ontheordering withinadjacency lists.
22.2-6
Give an example of a directed graph G .V;E/, a source vertex s V, and a
D 2
setoftreeedges E E such thatforeach vertex V, theunique simple path

 2
in the graph .V;E / from s to  is a shortest path in G, yet the set of edges E
 
cannot beproduced byrunning BFS onG,nomatterhowthevertices areordered
ineachadjacency list.
22.2-7
There are two types of professional wrestlers: “babyfaces” (“good guys”) and
“heels” (“bad guys”). Between any pair of professional wrestlers, there may or
maynotbe arivalry. Suppose wehave nprofessional wrestlers andwehavealist
of r pairs of wrestlers for which there are rivalries. Give an O.n r/-time algo-
C
rithm that determines whether it is possible to designate some of the wrestlers as
babyfaces andthe remainder asheels such that eachrivalry isbetween ababyface
and a heel. If it is possible to perform such a designation, your algorithm should
produce it.
22.2-8 ?
The diameter of a tree T .V;E/ is defined as max ı.u;/, that is, the
u; V
D 2
largest of all shortest-path distances in the tree. Give an efficient algorithm to
computethediameterofatree,andanalyzetherunning timeofyouralgorithm.
22.2-9
LetG .V;E/beaconnected, undirected graph. GiveanO.V E/-timealgo-
D C
rithm to compute a path in G that traverses each edge in E exactly once in each
direction. Describe how you can find your way out of a maze if you are given a
largesupplyofpennies.
22.3 Depth-firstsearch 603
22.3 Depth-first search
The strategy followed by depth-first search is, as its name implies, to search
“deeper” in the graph whenever possible. Depth-first search explores edges out
ofthe mostrecently discovered vertex  that still has unexplored edges leaving it.
Onceallof’sedgeshavebeenexplored,thesearch“backtracks” toexploreedges
leaving the vertex from which  was discovered. This process continues until we
have discovered all the vertices that arereachable from the original source vertex.
Ifany undiscovered vertices remain, then depth-first search selects one ofthem as
anewsource,anditrepeatsthesearchfromthatsource. Thealgorithmrepeatsthis
entireprocess untilithasdiscovered everyvertex.3
Asinbreadth-first search, whenever depth-first search discovers avertex  dur-
ing a scan of the adjacency list of an already discovered vertex u, it records this
event by setting ’s predecessor attribute : to u. Unlike breadth-first search,
whose predecessor subgraph forms a tree, the predecessor subgraph produced by
a depth-first search may be composed of several trees, because the search may
repeat from multiple sources. Therefore, we define the predecessor subgraph of
a depth-first search slightly differently from that of a breadth-first search: we let
G .V;E /,where
 
D
E  .:;/  V and: NIL :
D f W 2 ¤ g
The predecessor subgraph of a depth-first search forms a depth-first forest com-
prisingseveraldepth-firsttrees. TheedgesinE aretreeedges.

Asinbreadth-first search, depth-first search colors vertices during thesearch to
indicate their state. Each vertex is initially white, is grayed when it is discovered
in the search, and is blackened when it is finished, that is, when its adjacency list
hasbeenexaminedcompletely. Thistechniqueguaranteesthateachvertexendsup
inexactlyonedepth-first tree,sothatthesetreesaredisjoint.
Besidescreatingadepth-firstforest,depth-firstsearchalsotimestampseachver-
tex. Each vertex  has two timestamps: the first timestamp :d records when 
is first discovered (and grayed), and the second timestamp :f records when the
search finishes examining ’s adjacency list (and blackens ). These timestamps
3It may seem arbitrary that breadth-first search is limited to only one source whereas depth-first
searchmaysearchfrommultiplesources.Althoughconceptually,breadth-firstsearchcouldproceed
frommultiplesourcesanddepth-firstsearchcouldbelimitedtoonesource,ourapproachreflectshow
theresultsofthesesearchesaretypicallyused. Breadth-firstsearchusuallyservestofindshortest-
pathdistances(andtheassociatedpredecessorsubgraph)fromagivensource. Depth-firstsearchis
oftenasubroutineinanotheralgorithm,asweshallseelaterinthischapter.
604 Chapter22 ElementaryGraphAlgorithms
provide important information about the structure of the graph and are generally
helpful inreasoning aboutthebehavior ofdepth-first search.
TheprocedureDFSbelowrecordswhenitdiscoversvertexuintheattributeu:d
and when it finishes vertex u in the attribute u:f. These timestamps are integers
between1and2 V ,sincethereisonediscovery eventandonefinishingeventfor
j j
eachofthe V vertices. Foreveryvertexu,
j j
u:d < u:f : (22.2)
Vertex u is WHITE before time u:d, GRAY between time u:d and time u:f, and
BLACK thereafter.
The following pseudocode is the basic depth-first-search algorithm. The input
graph G maybeundirected ordirected. Thevariable timeisaglobal variable that
weusefortimestamping.
DFS.G/
1 foreachvertexu G:V
2
2 u:color WHITE
D
3 u: NIL
D
4 time 0
D
5 foreachvertexu G:V
2
6 ifu:color == WHITE
7 DFS-VISIT.G;u/
DFS-VISIT.G;u/
1 time time 1 //whitevertexuhasjustbeendiscovered
D C
2 u:d time
D
3 u:color GRAY
D
4 foreach G:AdjŒu //exploreedge.u;/
2
5 if:color == WHITE
6 : u
D
7 DFS-VISIT.G;/
8 u:color BLACK //blacken u;itisfinished
D
9 time time 1
D C
10 u:f time
D
Figure22.4illustrates theprogress ofDFS onthegraphshowninFigure22.2.
Procedure DFS works as follows. Lines 1–3 paint all vertices white and ini-
tialize their  attributes to NIL. Line 4 resets the global time counter. Lines 5–7
check each vertex in V in turn and, when a white vertex is found, visit it using
DFS-VISIT. Every time DFS-VISIT.G;u/ is called in line 7, vertex u becomes
22.3 Depth-firstsearch 605
u v w u v w u v w u v w
1/ 1/ 2/ 1/ 2/ 1/ 2/
3/ 4/ 3/
x y z x y z x y z x y z
(a) (b) (c) (d)
u v w u v w u v w u v w
1/ 2/ 1/ 2/ 1/ 2/ 1/ 2/7
B B B B
4/ 3/ 4/5 3/ 4/5 3/6 4/5 3/6
x y z x y z x y z x y z
(e) (f) (g) (h)
u v w u v w u v w u v w
1/ 2/7 1/8 2/7 1/8 2/7 9/ 1/8 2/7 9/
F B F B F B F B C
4/5 3/6 4/5 3/6 4/5 3/6 4/5 3/6
x y z x y z x y z x y z
(i) (j) (k) (l)
u v w u v w u v w u v w
1/8 2/7 9/ 1/8 2/7 9/ 1/8 2/7 9/ 1/8 2/7 9/12
F B C F B C F B C F B C
B B B
4/5 3/6 10/ 4/5 3/6 10/ 4/5 3/6 10/11 4/5 3/6 10/11
x y z x y z x y z x y z
(m) (n) (o) (p)
Figure22.4 Theprogressofthedepth-first-searchalgorithmDFSonadirectedgraph. Asedges
are explored by the algorithm, they are shown as either shaded (if they are tree edges) or dashed
(otherwise). Nontree edges are labeled B, C, or F according to whether they are back, cross, or
forwardedges.Timestampswithinverticesindicatediscoverytime/finishingtimes.
the root of a new tree in the depth-first forest. When DFS returns, every vertex u
hasbeenassigned adiscoverytimeu:dandafinishingtimeu:f.
In each call DFS-VISIT.G;u/, vertex u is initially white. Line 1 increments
the global variable time, line 2 records the new value of time as the discovery
timeu:d, andline 3paints ugray. Lines 4–7examine each vertex adjacent tou
and recursively visit  if it is white. As each vertex  AdjŒu is considered in
2
line 4, we say that edge .u;/ is explored by the depth-first search. Finally, after
everyedge leaving uhasbeen explored, lines 8–10 paint ublack, increment time,
andrecordthefinishingtimeinu:f.
Note that the results of depth-first search may depend upon the order in which
line 5 of DFS examines the vertices and upon the order in which line 4 of DFS-
VISIT visits the neighbors of a vertex. These different visitation orders tend not
606 Chapter22 ElementaryGraphAlgorithms
to cause problems in practice, as we can usually use any depth-first search result
effectively, withessentially equivalentresults.
WhatistherunningtimeofDFS? Theloopsonlines1–3andlines5–7ofDFS
taketime‚.V/,exclusiveofthetimetoexecutethecallstoDFS-VISIT. Aswedid
for breadth-first search, we use aggregate analysis. The procedure DFS-VISIT is
calledexactlyonceforeachvertex V,sincethevertexuonwhichDFS-VISIT
2
isinvokedmustbewhiteandthefirstthingDFS-VISIT doesispaintvertexugray.
During an execution of DFS-VISIT.G;/,the loop onlines 4–7 executes AdjŒ
j j
times. Since
AdjŒ ‚.E/;
j j D
 V
X2
the total cost of executing lines 4–7 of DFS-VISIT is ‚.E/. The running timeof
DFS istherefore ‚.V E/.
C
Propertiesofdepth-firstsearch
Depth-first search yields valuable information about the structure of agraph. Per-
haps the most basic property of depth-first search is that the predecessor sub-
graph G does indeed form a forest of trees, since the structure of the depth-

first trees exactly mirrors the structure of recursive calls of DFS-VISIT. That is,
u : if and only if DFS-VISIT.G;/ was called during a search of u’s ad-
D
jacency list. Additionally, vertex  is a descendant of vertex u in the depth-first
forestifandonlyif isdiscovered duringthetimeinwhichuisgray.
Another important property of depth-first search is that discovery and finishing
times have parenthesis structure. If we represent the discovery of vertex u with
aleft parenthesis “.u” and represent its finishing byaright parenthesis “u/”, then
the history of discoveries and finishings makes a well-formed expression in the
sense that theparentheses areproperly nested. Forexample, thedepth-first search
ofFigure22.5(a)correspondstotheparenthesization showninFigure22.5(b). The
followingtheorem providesanother waytocharacterize theparenthesis structure.
Theorem22.7(Parenthesistheorem)
In any depth-first search of a (directed or undirected) graph G .V;E/, for any
D
twovertices uand,exactlyoneofthefollowingthreeconditions holds:
 the intervals Œu:d;u:f and Œ:d;:f are entirely disjoint, and neither u nor 
isadescendant oftheotherinthedepth-firstforest,
 theinterval Œu:d;u:f iscontained entirely within the interval Œ:d;:f,and u
isadescendant of inadepth-first tree,or
 the interval Œ:d;:f iscontained entirely within the interval Œu:d;u:f, and 
isadescendant ofuinadepth-firsttree.
22.3 Depth-firstsearch 607
y z s t
3/6 2/9 1/10 11/16
B F
(a) C B
4/5 7/8 12/13 14/15
C C C
x w v u
s t
z v u
(b)
y w
x
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
(s (z (y (x x) y) (w w) z) s) (t (v v) (u u) t)
s t
B
C
F C
z v u
B
C
(c)
y w
C
x
Figure 22.5 Properties of depth-first search. (a) The result of a depth-first search of a directed
graph. Verticesaretimestamped and edge types are indicated asinFigure22.4. (b) Intervalsfor
thediscoverytimeandfinishingtimeofeachvertexcorrespondtotheparenthesizationshown.Each
rectanglespanstheintervalgivenbythediscoveryandfinishingtimesofthecorrespondingvertex.
Only tree edges are shown. If two intervals overlap, then one is nested within the other, and the
vertexcorrespondingtothesmallerintervalisadescendantofthevertexcorrespondingtothelarger.
(c)Thegraphofpart(a)redrawnwithalltreeandforwardedgesgoingdownwithinadepth-firsttree
andallbackedgesgoingupfromadescendanttoanancestor.
608 Chapter22 ElementaryGraphAlgorithms
Proof We begin with the case in which u:d < :d. We consider two subcases,
according towhether :d < u:f ornot. Thefirstsubcase occurs when:d < u:f,
so  was discovered while u was still gray, which implies that  is a descendant
of u. Moreover, since  was discovered more recently than u, all of its outgo-
ing edges are explored, and  is finished, before the search returns to and fin-
ishes u. In this case, therefore, the interval Œ:d;:f is entirely contained within
the interval Œu:d;u:f. In the other subcase, u:f < :d, and by inequality (22.2),
u:d < u:f < :d < :f; thus the intervals Œu:d;u:f and Œ:d;:f are disjoint.
Because the intervals are disjoint, neither vertex was discovered while the other
wasgray, andsoneithervertexisadescendant oftheother.
Thecaseinwhich:d < u:dissimilar,withtherolesofuand reversedinthe
aboveargument.
Corollary 22.8(Nestingofdescendants’intervals)
Vertex  is aproper descendant of vertex uin the depth-first forest for a(directed
orundirected) graphG ifandonlyifu:d <:d < :f <u:f.
Proof ImmediatefromTheorem22.7.
The next theorem gives another important characterization of when one vertex
isadescendant ofanotherinthedepth-first forest.
Theorem22.9(White-paththeorem)
In a depth-first forest of a (directed or undirected) graph G .V;E/, vertex  is
D
adescendant ofvertex uifand only if atthe timeu:d that the search discovers u,
thereisapathfromuto consisting entirely ofwhitevertices.
Proof : If u,thenthepathfromutocontainsjustvertexu,whichisstill
) D
white when we set the value of u:d. Now, suppose that  is a proper descendant
of u in the depth-first forest. By Corollary 22.8, u:d < :d, and so  is white at
time u:d. Since  can be any descendant of u, all vertices on the unique simple
pathfromuto inthedepth-firstforestarewhiteattimeu:d.
: Suppose thatthereisapathofwhitevertices fromuto attimeu:d,but
(
doesnotbecomeadescendant ofuinthedepth-firsttree. Withoutlossofgeneral-
ity,assumethateveryvertexotherthanalongthepathbecomesadescendantofu.
(Otherwise,let betheclosestvertextoualongthepaththatdoesn’tbecomeade-
scendantofu.) Letw bethepredecessor of inthepath,sothatw isadescendant
ofu(w andumayinfactbethesamevertex). ByCorollary 22.8,w:f u:f. Be-

cause mustbediscoveredafteruisdiscovered, butbeforew isfinished,wehave
u:d < :d < w:f u:f. Theorem 22.7 then implies that the interval Œ:d;:f

22.3 Depth-firstsearch 609
iscontained entirelywithintheintervalŒu:d;u:f. ByCorollary22.8, mustafter
allbeadescendant ofu.
Classification ofedges
Another interesting property of depth-first search is that the search can be used
to classify the edges of the input graph G .V;E/. The type of each edge can
D
provideimportantinformation aboutagraph. Forexample,inthenextsection, we
shallseethatadirectedgraphisacyclicifandonlyifadepth-firstsearchyieldsno
“back”edges(Lemma22.11).
Wecandefinefouredgetypesintermsofthedepth-first forestG produced by

adepth-firstsearchonG:
1. Treeedgesareedgesinthedepth-firstforestG . Edge.u;/isatreeedgeif

wasfirstdiscoveredbyexploring edge.u;/.
2. Back edges are those edges .u;/ connecting a vertex u to an ancestor  in a
depth-firsttree. Weconsiderself-loops, whichmayoccurindirectedgraphs,to
bebackedges.
3. Forward edges are those nontree edges .u;/ connecting a vertex u to a de-
scendant  inadepth-first tree.
4. Cross edges are all other edges. They can go between vertices in the same
depth-firsttree,aslongasonevertexisnotanancestoroftheother,ortheycan
gobetweenverticesindifferent depth-firsttrees.
InFigures22.4and22.5,edgelabelsindicateedgetypes. Figure22.5(c)alsoshows
how to redraw the graph of Figure 22.5(a) so that all tree and forward edges head
downwardinadepth-first treeandallbackedgesgoup. Wecanredrawanygraph
inthisfashion.
TheDFSalgorithmhasenoughinformationtoclassifysomeedgesasitencoun-
ters them. The key idea is that when we first explore an edge .u;/, the color of
vertex tellsussomething abouttheedge:
1. WHITE indicates atreeedge,
2. GRAY indicates abackedge,and
3. BLACK indicates aforwardorcrossedge.
The first case is immediate from the specification of the algorithm. For the sec-
ondcase, observe thatthegray vertices always formalinear chain ofdescendants
corresponding to the stack of active DFS-VISIT invocations; the number of gray
vertices is one more than the depth in the depth-first forest of the vertex most re-
cently discovered. Exploration always proceeds from the deepest gray vertex, so
610 Chapter22 ElementaryGraphAlgorithms
an edge that reaches another gray vertex has reached an ancestor. The third case
handles the remaining possibility; Exercise 22.3-5 asks you to show that such an
edge.u;/isaforwardedgeifu:d < :dandacrossedgeifu:d >:d.
An undirected graph may entail some ambiguity in how we classify edges,
since .u;/ and .;u/ are really the same edge. In such a case, we classify the
edge as the first type in the classification list that applies. Equivalently (see Ex-
ercise 22.3-6), we classify the edge according to whichever of .u;/ or .;u/ the
searchencounters first.
Wenowshowthatforwardandcrossedgesneveroccurinadepth-firstsearchof
anundirected graph.
Theorem22.10
In a depth-first search of an undirected graph G, every edge of G is either a tree
edgeorabackedge.
Proof Let.u;/beanarbitraryedgeofG,andsupposewithoutlossofgenerality
that u:d < :d. Then the search must discover and finish  before it finishes u
(while u is gray), since  is on u’s adjacency list. If the first time that the search
explores edge .u;/, it is in the direction from u to , then  is undiscovered
(white) until that time, for otherwise the search would have explored this edge
already in the direction from  to u. Thus, .u;/ becomes a tree edge. If the
search explores .u;/firstinthedirection from  tou,then .u;/isaback edge,
sinceuisstillgrayatthetimetheedgeisfirstexplored.
Weshallseeseveralapplications ofthesetheoremsinthefollowingsections.
Exercises
22.3-1
Make a 3-by-3 chart with row and column labels WHITE, GRAY, and BLACK. In
each cell .i;j/, indicate whether, at any point during a depth-first search of a di-
rected graph, there can be an edge from a vertex of color i to a vertex of color j.
For each possible edge, indicate what edge types it can be. Make a second such
chartfordepth-first searchofanundirected graph.
22.3-2
Show how depth-first search works on the graph of Figure 22.6. Assume that the
for loop of lines 5–7 of the DFS procedure considers the vertices in alphabetical
order, and assume that each adjacency list is ordered alphabetically. Show the
discovery and finishing times for each vertex, and show the classification of each
edge.
22.3 Depth-firstsearch 611
q r
s t u
v w x y
z
Figure22.6 AdirectedgraphforuseinExercises22.3-2and22.5-2.
22.3-3
Showtheparenthesis structure ofthedepth-firstsearchofFigure22.4.
22.3-4
Show that using a single bit to store each vertex color suffices by arguing that
the DFS procedure would produce the same result if line 8 of DFS-VISIT was
removed.
22.3-5
Showthatedge.u;/is
a. atreeedgeorforwardedgeifandonlyifu:d < :d < :f < u:f,
b. abackedgeifandonlyif:d u:d < u:f :f,and
 
c. acrossedgeifandonlyif:d <:f < u:d < u:f.
22.3-6
Showthatinanundirectedgraph,classifyinganedge.u;/asatreeedgeoraback
edgeaccordingtowhether.u;/or.;u/isencounteredfirstduringthedepth-first
search is equivalent to classifying it according to the ordering of the four types in
theclassification scheme.
22.3-7
Rewritetheprocedure DFS,usingastacktoeliminaterecursion.
22.3-8
Giveacounterexample totheconjecture thatifadirected graphG contains apath
from u to , and if u:d < :d in a depth-first search of G, then  is a descendant
ofuinthedepth-first forestproduced.
612 Chapter22 ElementaryGraphAlgorithms
22.3-9
Giveacounterexample totheconjecture thatifadirected graphG contains apath
fromuto,thenanydepth-first searchmustresultin:d u:f.

22.3-10
Modifythepseudocode fordepth-first searchsothatitprintsouteveryedgeinthe
directedgraphG,togetherwithitstype. Showwhatmodifications,ifany,youneed
tomakeifG isundirected.
22.3-11
Explainhowavertexuofadirectedgraphcanendupinadepth-firsttreecontain-
ingonlyu,eventhoughuhasbothincomingandoutgoing edgesinG.
22.3-12
Showthatwecanuseadepth-first search ofanundirected graph G toidentify the
connected components ofG, and that the depth-first forest contains asmanytrees
as G has connected components. More precisely, show how to modify depth-first
search so that it assigns to each vertex  an integer label :cc between 1 and k,
where k is the number of connected components of G, such that u:cc :cc if
D
andonlyifuand areinthesameconnected component.
22.3-13 ?
AdirectedgraphG .V;E/issinglyconnectedifu ; impliesthatGcontains
D
at most one simple path from u to  for all vertices u; V. Give an efficient
2
algorithm todeterminewhetherornotadirected graphissinglyconnected.
22.4 Topologicalsort
Thissectionshowshowwecanusedepth-firstsearchtoperformatopological sort
ofadirectedacyclicgraph,ora“dag”asitissometimescalled. Atopological sort
ofadagG .V;E/isalinearorderingofallitsverticessuchthatifGcontainsan
D
edge.u;/,thenuappearsbefore intheordering. (Ifthegraphcontainsacycle,
then no linear ordering is possible.) We can view a topological sort of a graph as
anorderingofitsverticesalongahorizontal linesothatalldirectededgesgofrom
left to right. Topological sorting is thus different from the usual kind of “sorting”
studied inPartII.
Many applications use directed acyclic graphs to indicate precedences among
events. Figure 22.7 gives an example that arises when Professor Bumstead gets
dressed in the morning. The professor must don certain garments before others
(e.g.,socksbeforeshoes). Otheritemsmaybeputoninanyorder(e.g.,socksand
22.4 Topologicalsort 613
11/16 undershorts socks 17/18
watch 9/10
12/15 pants shoes 13/14
shirt 1/8
(a) 6/7 belt
tie 2/5
jacket 3/4
(b) socks undershorts pants shoes watch shirt belt tie jacket
17/18 11/16 12/15 13/14 9/10 1/8 6/7 2/5 3/4
Figure22.7 (a)ProfessorBumsteadtopologicallysortshisclothingwhengettingdressed. Each
directed edge .u;/ means that garment u must be put on before garment . The discovery and
finishingtimesfromadepth-firstsearchareshownnexttoeachvertex. (b)Thesamegraphshown
topologicallysorted,withitsverticesarrangedfromlefttorightinorderofdecreasingfinishingtime.
Alldirectededgesgofromlefttoright.
pants). Adirectededge.u;/inthedagofFigure22.7(a)indicatesthatgarmentu
mustbedonnedbeforegarment. Atopological sortofthisdagthereforegivesan
order forgetting dressed. Figure 22.7(b) shows the topologically sorted dag as an
orderingofverticesalongahorizontallinesuchthatalldirectededgesgofromleft
toright.
Thefollowingsimplealgorithm topologically sortsadag:
TOPOLOGICAL-SORT.G/
1 call DFS.G/tocomputefinishingtimes:f foreachvertex
2 aseachvertexisfinished, insertitontothefrontofalinkedlist
3 returnthelinkedlistofvertices
Figure22.7(b)showshowthetopologically sortedverticesappearinreverseorder
oftheirfinishingtimes.
We can perform a topological sort in time ‚.V E/, since depth-first search
C
takes‚.V E/timeandittakesO.1/timetoinserteachofthe V verticesonto
C j j
thefrontofthelinkedlist.
Weprovethecorrectness ofthisalgorithm usingthefollowingkeylemmachar-
acterizing directedacyclicgraphs.
614 Chapter22 ElementaryGraphAlgorithms
Lemma22.11
AdirectedgraphG isacyclicifandonlyifadepth-firstsearchofG yieldsnoback
edges.
Proof : Suppose that a depth-first search produces a back edge .u;/. Then
)
vertex isanancestorofvertexuinthedepth-firstforest. Thus,G containsapath
from tou,andthebackedge.u;/completesacycle.
: Suppose that G contains a cycle c. We show that a depth-first search of G
(
yields abackedge. Let bethefirstvertextobediscovered inc,andlet.u;/be
theprecedingedgeinc. Attime:d,theverticesofc formapathofwhitevertices
from tou. Bythewhite-paththeorem,vertexubecomesadescendantof inthe
depth-first forest. Therefore,.u;/isabackedge.
Theorem22.12
TOPOLOGICAL-SORT produces a topological sort of the directed acyclic graph
provided asitsinput.
Proof Suppose that DFS is run on a given dag G .V;E/ to determine fin-
D
ishing times for its vertices. It suffices to show that for any pair of distinct ver-
tices u; V, if G contains an edge from u to , then :f < u:f. Consider any
2
edge .u;/ explored by DFS.G/. When this edge is explored,  cannot be gray,
since then  would be an ancestor of u and .u;/ would be a back edge, contra-
dicting Lemma 22.11. Therefore,  must be either white or black. If  is white,
it becomes a descendant of u, and so :f < u:f. If  is black, it has already been
finished,sothat:f hasalreadybeenset. Becausewearestillexploringfromu,we
have yettoassign atimestamp tou:f, andsoonce wedo, wewillhave:f < u:f
as well. Thus, for any edge .u;/ in the dag, we have :f < u:f, proving the
theorem.
Exercises
22.4-1
ShowtheorderingofverticesproducedbyTOPOLOGICAL-SORT whenitisrunon
thedagofFigure22.8,undertheassumption ofExercise22.3-2.
22.4-2
Give a linear-time algorithm that takes as input a directed acyclic graph G
D
.V;E/ and two vertices s and t, and returns the number of simple paths from s
to t in G. Forexample, thedirected acyclic graph ofFigure 22.8 contains exactly
four simple paths from vertex p to vertex : po, pory, posry, and psry.
(Youralgorithm needsonlytocountthesimplepaths,notlistthem.)
22.5 Stronglyconnectedcomponents 615
m n o p
q r s
t u v w
x y z
Figure22.8 Adagfortopologicalsorting.
22.4-3
Give an algorithm that determines whether or not a given undirected graph G
D
.V;E/containsasimplecycle. YouralgorithmshouldruninO.V/time,indepen-
dentof E .
j j
22.4-4
Prove or disprove: If a directed graph G contains cycles, then TOPOLOGICAL-
SORT.G/ produces a vertex ordering that minimizes the number of “bad” edges
thatareinconsistent withtheordering produced.
22.4-5
Another way to perform topological sorting on a directed acyclic graph G
D
.V;E/ is to repeatedly find a vertex of in-degree 0, output it, and remove it and
all of its outgoing edges from the graph. Explain how to implement this idea so
thatitrunsintimeO.V E/. Whathappenstothisalgorithm ifG hascycles?
C
22.5 Strongly connected components
We now consider a classic application of depth-first search: decomposing a di-
rectedgraphintoitsstronglyconnectedcomponents. Thissectionshowshowtodo
sousingtwodepth-firstsearches. Manyalgorithms thatworkwithdirectedgraphs
beginwithsuchadecomposition. Afterdecomposing thegraphintostrongly con-
nectedcomponents, suchalgorithms runseparately oneachoneandthencombine
thesolutions according tothestructure ofconnections amongcomponents.
Recall from Appendix B that a strongly connected component of a directed
graph G .V;E/ is a maximal set of vertices C V such that for every pair
ofverticesD uand inC,wehavebothu ;  and; u;thatis,vertices uand
arereachable fromeachother. Figure22.9showsanexample.
616 Chapter22 ElementaryGraphAlgorithms
a b c d
13/14 11/16 1/10 8/9
(a)
12/15 3/4 2/7 5/6
e f g h
a b c d
(b)
e f g h
cd
(c) abe
fg h
Figure22.9 (a)AdirectedgraphG.EachshadedregionisastronglyconnectedcomponentofG.
Eachvertexislabeledwithitsdiscoveryandfinishingtimesinadepth-firstsearch,andtreeedges
are shaded. (b) Thegraph GT, thetranspose of G, withthe depth-first forest computed inline 3
ofSTRONGLY-CONNECTED-COMPONENTSshownandtreeedgesshaded.Eachstronglyconnected
componentcorrespondstoonedepth-firsttree.Verticesb,c,g,andh,whichareheavilyshaded,are
therootsofthedepth-firsttreesproducedbythedepth-firstsearchofGT.(c)Theacycliccomponent
graphGSCC obtainedbycontractingalledgeswithineachstronglyconnected component ofG so
thatonlyasinglevertexremainsineachcomponent.
Our algorithm for finding strongly connected components of a graph G
D
.V;E/ uses the transpose of G, which we defined in Exercise 22.1-3 to be the
graph GT .V;ET/, where ET .u;/ .;u/ E . That is, ET consists of
D D f W 2 g
the edges of G with their directions reversed. Given an adjacency-list representa-
tion of G, the time to create GT is O.V E/. It is interesting to observe that G
C
andGT haveexactly thesamestrongly connected components: uand arereach-
able fromeach otherinG ifandonlyiftheyarereachable fromeach otherinGT.
Figure22.9(b)showsthetransposeofthegraphinFigure22.9(a),withthestrongly
connected components shaded.
22.5 Stronglyconnectedcomponents 617
Thefollowinglinear-time(i.e.,‚.V E/-time)algorithmcomputesthestrongly
C
connected components of a directed graph G .V;E/ using two depth-first
D
searches, oneonG andoneonGT.
STRONGLY-CONNECTED-COMPONENTS.G/
1 call DFS.G/tocomputefinishingtimesu:f foreachvertexu
2 computeGT
3 call DFS.GT/,butinthemainloopof DFS,considerthevertices
inorderofdecreasing u:f (ascomputedinline1)
4 outputtheverticesofeachtreeinthedepth-first forestformedinline3asa
separate stronglyconnected component
The idea behind this algorithm comes from a key property of the component
graph GSCC .VSCC;ESCC/, which we define as follows. Suppose that G
D
has strongly connected components C ;C ;:::;C . The vertex set VSCC is
1 2 k
 ; ;:::; , and it contains a vertex  for each strongly connected compo-
1 2 k i
f g
nentC ofG. Thereisanedge. ; / ESCC ifG containsadirectededge.x;y/
i i j
2
for some x C and some y C . Looked at another way, by contracting all
i j
2 2
edges whose incident vertices are within the same strongly connected component
of G, the resulting graph is GSCC. Figure 22.9(c) shows the component graph of
thegraphinFigure22.9(a).
The key property is that the component graph is a dag, which the following
lemmaimplies.
Lemma22.13
Let C and C be distinct strongly connected components in directed graph G
0
.V;E/, let u; C, let u; C , and suppose that G contains a path u ; uD .
0 0 0 0
ThenG cannota2 lsocontainap2 ath ; .
0
Proof If G contains a path  ; , then it contains paths u ; u ;  and
0 0 0
 ;  ; u. Thus, uand  are reachable from each other, thereby contradicting
0 0
theassumptionthatC andC aredistinct strongly connected components.
0
Weshall see that by considering vertices in the second depth-first search in de-
creasing order of the finishing times that were computed in the first depth-first
search, we are, in essence, visiting the vertices of the component graph (each of
whichcorrespondstoastronglyconnectedcomponentofG)intopologicallysorted
order.
Because the STRONGLY-CONNECTED-COMPONENTS procedure performs two
depth-first searches, there is the potential for ambiguity when we discuss u:d
or u:f. In this section, these values always refer to the discovery and finishing
timesascomputedbythefirstcallofDFS,inline1.
618 Chapter22 ElementaryGraphAlgorithms
We extend the notation for discovery and finishing times to sets of vertices.
If U V, then we define d.U/ min u:d and f.U/ max u:f .
u U u U
That i s, d.U/ and f.U/ are the eaD rliest di2 scof veryg time and lateD st finish2 ingf timeg ,
respectively, ofanyvertexinU.
Thefollowinglemmaanditscorollarygiveakeypropertyrelatingstronglycon-
nectedcomponents andfinishingtimesinthefirstdepth-firstsearch.
Lemma22.14
Let C and C be distinct strongly connected components in directed graph G
0
D
.V;E/. Suppose thatthere isanedge.u;/ E,whereu C and C . Then
0
2 2 2
f.C/ >f.C /.
0
Proof We consider two cases, depending on which strongly connected compo-
nent,C orC ,hadthefirstdiscovered vertexduringthedepth-firstsearch.
0
Ifd.C/ < d.C /,letx bethefirstvertexdiscovered inC. Attimex:d,allver-
0
tices inC andC arewhite. Atthattime,G contains apath fromx toeach vertex
0
inC consistingonlyofwhitevertices. Because.u;/ E,foranyvertexw C ,
0
2 2
thereisalsoapathinG attimex:dfromx tow consisting onlyofwhitevertices:
x ; u  ; w. By the white-path theorem, all vertices in C and C become
0
!
descendantsofxinthedepth-firsttree. ByCorollary22.8,xhasthelatestfinishing
timeofanyofitsdescendants, andsox:f f.C/ >f.C /.
0
D
If instead we have d.C/ > d.C /, let y be the first vertex discovered in C .
0 0
At time y:d, all vertices in C are white and G contains a path from y to each
0
vertex inC consisting only ofwhitevertices. Bythewhite-path theorem, allver-
0
tices inC becomedescendants ofy inthedepth-first tree, andbyCorollary 22.8,
0
y:f f.C /. Attimey:d,allverticesinC arewhite. Sincethereisanedge.u;/
0
D
from C to C , Lemma 22.13 implies that there cannot be a path from C to C.
0 0
Hence,novertexinC isreachablefromy. Attimey:f,therefore,allverticesinC
are still white. Thus, for any vertex w C, we have w:f > y:f, which implies
2
thatf.C/ > f.C /.
0
ThefollowingcorollarytellsusthateachedgeinGTthatgoesbetweendifferent
strongly connected components goes from a component with an earlier finishing
time(inthefirstdepth-first search)toacomponentwithalaterfinishingtime.
Corollary 22.15
Let C and C be distinct strongly connected components in directed graph G
0
D
.V;E/. Supposethatthereisanedge.u;/ ET,whereu C and C . Then
0
2 2 2
f.C/ <f.C /.
0
22.5 Stronglyconnectedcomponents 619
Proof Since .u;/ ET, we have .;u/ E. Because the strongly con-
2 2
nected components of G and GT are the same, Lemma 22.14 implies that
f.C/< f.C /.
0
Corollary 22.15 provides the key to understanding why the strongly connected
components algorithm works. Letusexaminewhathappens whenweperform the
second depth-first search, which is on GT. We start with the strongly connected
component C whose finishing time f.C/ is maximum. The search starts from
somevertexx C,anditvisitsallverticesinC. ByCorollary22.15,GT contains
2
no edges from C to any other strongly connected component, and so the search
from x will not visit vertices in any other component. Thus, the tree rooted at x
contains exactly the vertices of C. Having completed visiting all vertices in C,
the search in line 3 selects as a root a vertex from some other strongly connected
componentC whosefinishingtimef.C /ismaximumoverallcomponents other
0 0
than C. Again, the search will visit all vertices in C , but by Corollary 22.15,
0
the only edges in GT from C to any other component must be to C, which we
0
have already visited. In general, when the depth-first search of GT in line 3 visits
any strongly connected component, any edges out of that component must be to
componentsthatthesearchalreadyvisited. Eachdepth-firsttree,therefore, willbe
exactlyonestronglyconnectedcomponent. Thefollowingtheoremformalizesthis
argument.
Theorem22.16
The STRONGLY-CONNECTED-COMPONENTS procedure correctly computes the
stronglyconnected components ofthedirected graphG providedasitsinput.
Proof We argue by induction on the number of depth-first trees found in the
depth-first search of GT in line 3 that the vertices of each tree form a strongly
connected component. The inductive hypothesis is that the first k trees produced
in line 3 are strongly connected components. The basis for the induction, when
k 0,istrivial.
D
Intheinductivestep,weassumethateachofthefirstkdepth-firsttreesproduced
in line 3 is a strongly connected component, and we consider the .k 1/st tree
C
produced. Let the root of this tree be vertex u, and let u be in strongly connected
component C. Because ofhowwechoose roots inthe depth-first search in line3,
u:f f.C/ > f.C / for any strongly connected component C other than C
0 0
D
that has yet to be visited. By the inductive hypothesis, at the time that the search
visitsu,allotherverticesofC arewhite. Bythewhite-paththeorem,therefore, all
other vertices of C are descendants of u in its depth-first tree. Moreover, by the
inductivehypothesisandbyCorollary22.15,anyedgesinGTthatleaveC mustbe
tostrongly connected components that have already been visited. Thus, no vertex
620 Chapter22 ElementaryGraphAlgorithms
inanystronglyconnectedcomponentotherthanC willbeadescendantofuduring
the depth-first search ofGT. Thus, the vertices of the depth-first tree in GT that is
rooted at uform exactly one strongly connected component, which completes the
inductive stepandtheproof.
Hereisanother waytolookathowthesecond depth-first searchoperates. Con-
sider the component graph .GT/SCC of GT. If we map each strongly connected
component visited inthesecond depth-first search toavertexof.GT/SCC,the sec-
ond depth-first search visits vertices of .GT/SCC in the reverse of a topologically
sorted order. If we reverse the edges of .GT/SCC, we get the graph ..GT/SCC/T.
Because ..GT/SCC/T GSCC (see Exercise 22.5-4), the second depth-first search
D
visitstheverticesofGSCC intopologically sortedorder.
Exercises
22.5-1
Howcanthenumberofstronglyconnectedcomponentsofagraphchangeifanew
edgeisadded?
22.5-2
Show how the procedure STRONGLY-CONNECTED-COMPONENTS works on the
graphofFigure22.6. Specifically,showthefinishingtimescomputedinline1and
the forest produced inline 3. Assumethatthe loop oflines 5–7of DFS considers
verticesinalphabetical orderandthattheadjacencylistsareinalphabetical order.
22.5-3
Professor Bacon claims that the algorithm for strongly connected components
would be simpler if it used the original (instead of the transpose) graph in the
second depth-first search and scanned the vertices in order of increasing finishing
times. Doesthissimpleralgorithm alwaysproducecorrect results?
22.5-4
Prove that for any directed graph G, we have ..GT/SCC/T GSCC. That is, the
D
transpose ofthecomponentgraphofGT isthesameasthecomponentgraphofG.
22.5-5
GiveanO.V E/-timealgorithm tocompute thecomponent graph ofadirected
C
graph G .V;E/. Makesurethatthereisatmostoneedgebetween twovertices
D
inthecomponentgraphyouralgorithm produces.
ProblemsforChapter22 621
22.5-6
Given a directed graph G .V;E/, explain how to create another graph G
0
D D
.V;E /suchthat(a)G hasthesamestrongly connected components asG,(b)G
0 0 0
hasthesamecomponent graphasG,and(c)E isassmallaspossible. Describea
0
fastalgorithm tocomputeG .
0
22.5-7
AdirectedgraphG .V;E/issemiconnectedif,forallpairsofverticesu; V,
we have u ;  orD  ; u. Give an efficient algorithm to determine whe2 ther
or not G is semiconnected. Prove that your algorithm is correct, and analyze its
runningtime.
Problems
22-1 Classifyingedgesbybreadth-firstsearch
A depth-first forest classifies the edges of a graph into tree, back, forward, and
cross edges. A breadth-first tree can also be used to classify the edges reachable
fromthesourceofthesearchintothesamefourcategories.
a. Provethat in abreadth-first search ofan undirected graph, the following prop-
ertieshold:
1. Therearenobackedgesandnoforwardedges.
2. Foreachtreeedge.u;/,wehave:d u:d 1.
D C
3. Foreachcrossedge.u;/,wehave:d u:d or:d u:d 1.
D D C
b. Provethatinabreadth-firstsearchofadirectedgraph,thefollowingproperties
hold:
1. Therearenoforwardedges.
2. Foreachtreeedge.u;/,wehave:d u:d 1.
D C
3. Foreachcrossedge.u;/,wehave:d u:d 1.
 C
4. Foreachbackedge.u;/,wehave0 :d u:d.
 
22-2 Articulation points,bridges, andbiconnectedcomponents
Let G .V;E/ be a connected, undirected graph. An articulation point of G is
D
a vertex whose removal disconnects G. A bridge of G is an edge whose removal
disconnects G. A biconnected component of G is a maximal set of edges such
thatanytwoedgesinthesetlieonacommonsimplecycle. Figure22.10illustrates
622 Chapter22 ElementaryGraphAlgorithms
2
1 6
4
3
5
Figure22.10 Thearticulationpoints,bridges,andbiconnectedcomponentsofaconnected,undi-
rected graph for use inProblem22-2. Thearticulationpoints aretheheavily shaded vertices, the
bridgesaretheheavilyshadededges,andthebiconnectedcomponentsaretheedgesintheshaded
regions,withabccnumberingshown.
these definitions. We can determine articulation points, bridges, and biconnected
components usingdepth-first search. LetG .V;E /beadepth-first treeofG.
 
D
a. Prove that the root of G is an articulation point of G if and only if it has at

leasttwochildren inG .

b. LetbeanonrootvertexofG . ProvethatisanarticulationpointofG ifand

onlyif hasachilds suchthatthereisnobackedgefroms oranydescendant
ofs toaproperancestor of.
c. Let
:d;
:low min
D
(
w:d .u;w/isabackedgeforsomedescendant uof :
W
Showhowtocompute:lowforallvertices V inO.E/time.
2
d. Showhowtocomputeallarticulation pointsinO.E/time.
e. Prove that an edge of G is a bridge if and only if it does not lie on any simple
cycleofG.
f. ShowhowtocomputeallthebridgesofG inO.E/time.
g. ProvethatthebiconnectedcomponentsofGpartitionthenonbridgeedgesofG.
h. Give an O.E/-time algorithm to label each edge e of G with a positive in-
teger e:bcc such that e:bcc e:bcc if and only if e and e are in the same
0 0
D
biconnected component.
NotesforChapter22 623
22-3 Eulertour
AnEulertour ofa strongly connected, directed graph G .V;E/ isa cycle that
D
traverses each edge of G exactly once, although it may visit a vertex more than
once.
a. Showthat G hasanEuler tourifand only ifin-degree./ out-degree./for
D
eachvertex V.
2
b. DescribeanO.E/-timealgorithmtofindanEulertourofGifoneexists. (Hint:
Mergeedge-disjoint cycles.)
22-4 Reachability
Let G .V;E/ be a directed graph in which each vertex u V is labeled with
D 2
a unique integer L.u/ from the set 1;2;:::; V . For each vertex u V, let
R.u/  V u;  be the setf of verticesj thj ag t are reachable from u2 . Define
D f 2 W g
min.u/tobethevertexinR.u/whoselabelisminimum,i.e.,min.u/isthevertex
suchthatL./ min L.w/ w R.u/ . GiveanO.V E/-timealgorithmthat
D f W 2 g C
computesmin.u/forallverticesu V.
2
Chapter notes
Even[103]andTarjan[330]areexcellentreferences forgraphalgorithms.
Breadth-first search was discovered by Moore [260] in the context of finding
paths through mazes. Lee [226] independently discovered the same algorithm in
thecontextofroutingwiresoncircuitboards.
HopcroftandTarjan[178]advocatedtheuseoftheadjacency-list representation
over the adjacency-matrix representation for sparse graphs and were the first to
recognize the algorithmic importance of depth-first search. Depth-first search has
beenwidelyusedsincethelate1950s,especiallyinartificialintelligenceprograms.
Tarjan[327]gavealinear-timealgorithmforfindingstronglyconnectedcompo-
nents. ThealgorithmforstronglyconnectedcomponentsinSection22.5isadapted
fromAho,Hopcroft,andUllman[6],whocreditittoS.R.Kosaraju(unpublished)
and M. Sharir [314]. Gabow [119] also developed an algorithm for strongly con-
nectedcomponentsthatisbasedoncontractingcyclesandusestwostackstomake
it run in linear time. Knuth [209] was the first to give a linear-time algorithm for
topological sorting.
23 Minimum Spanning Trees
Electronic circuitdesigns oftenneedtomakethepinsofseveralcomponents elec-
tricallyequivalentbywiringthemtogether. Tointerconnect asetofnpins,wecan
useanarrangement ofn 1wires,eachconnecting twopins. Ofallsucharrange-
(cid:0)
ments,theonethatusestheleastamountofwireisusuallythemostdesirable.
We can model this wiring problem with a connected, undirected graph G
D
.V;E/,whereV isthesetofpins,Eisthesetofpossibleinterconnectionsbetween
pairs of pins, and for each edge .u;/ E, we have a weight w.u;/ specifying
2
the cost (amount of wire needed) to connect u and . We then wish to find an
acyclicsubsetT E thatconnects alloftheverticesandwhosetotalweight

w.T/ w.u;/
D
.u;/ T
X2
isminimized. SinceT isacyclicandconnectsallofthevertices,itmustformatree,
whichwecallaspanningtreesinceit“spans”thegraphG. Wecalltheproblemof
determining thetreeT theminimum-spanning-treeproblem.1 Figure23.1shows
anexampleofaconnected graphandaminimumspanning tree.
In this chapter, we shall examine two algorithms for solving the minimum-
spanning-tree problem: Kruskal’s algorithm and Prim’s algorithm. We can easily
make each of them run in time O.ElgV/ using ordinary binary heaps. By using
Fibonacci heaps, Prim’s algorithm runs in time O.E V lgV/, which improves
C
overthebinary-heap implementation if V ismuchsmallerthan E .
j j j j
The two algorithms are greedy algorithms, as described in Chapter 16. Each
stepofagreedyalgorithm mustmakeoneofseveralpossible choices. Thegreedy
strategy advocates making the choice that is the best at the moment. Such astrat-
egydoesnotgenerallyguaranteethatitwillalwaysfindgloballyoptimalsolutions
1Thephrase“minimumspanningtree”isashortenedformofthephrase“minimum-weightspanning
tree.” Wearenot,forexample,minimizingthenumberofedgesinT,sinceallspanningtreeshave
exactly V 1edgesbyTheoremB.2.
j j(cid:0)
23.1 Growingaminimumspanningtree 625
8 7
b c d
4 9
2
a 11 i 4 14 e
7 6
8 10
h g f
1 2
Figure23.1 Aminimumspanningtreeforaconnected graph. Theweightsonedgesareshown,
andtheedgesinaminimumspanningtreeareshaded.Thetotalweightofthetreeshownis37.This
minimumspanningtreeisnotunique:removingtheedge.b;c/andreplacingitwiththeedge.a;h/
yieldsanotherspanningtreewithweight37.
toproblems. Fortheminimum-spanning-tree problem,however,wecanprovethat
certaingreedystrategiesdoyieldaspanningtreewithminimumweight. Although
you can read this chapter independently of Chapter 16, the greedy methods pre-
sentedhereareaclassicapplication ofthetheoretical notions introduced there.
Section23.1introduces a“generic” minimum-spanning-tree methodthat grows
a spanning tree by adding one edge at a time. Section 23.2 gives two algorithms
that implement the generic method. Thefirstalgorithm, due toKruskal, issimilar
to the connected-components algorithm from Section 21.1. The second, due to
Prim,resemblesDijkstra’s shortest-paths algorithm (Section24.3).
Becauseatreeisatypeofgraph, inorder tobeprecise wemustdefineatreein
terms of not just its edges, but its vertices as well. Although this chapter focuses
on trees in terms of their edges, we shall operate with the understanding that the
verticesofatreeT arethosethatsomeedgeofT isincident on.
23.1 Growinga minimum spanning tree
Assume that we have a connected, undirected graph G .V;E/ with a weight
function w E R , and we wish to find a minimum sD panning tree for G. The
W !
two algorithms weconsider in this chapter use a greedy approach to the problem,
although theydifferinhowtheyapplythisapproach.
Thisgreedy strategy iscaptured by the following generic method, which grows
theminimumspanning treeoneedgeatatime. Thegeneric methodmanagesaset
ofedgesA,maintaining thefollowingloopinvariant:
Priortoeachiteration, Aisasubsetofsomeminimumspanning tree.
Ateach step, we determine an edge .u;/ that we can add to A without violating
thisinvariant,inthesensethatA .u;/ isalsoasubsetofaminimumspanning
[f g
626 Chapter23 MinimumSpanningTrees
tree. WecallsuchanedgeasafeedgeforA,sincewecanadditsafelytoAwhile
maintaining theinvariant.
GENERIC-MST.G;w/
1 A
D ;
2 whileAdoesnotformaspanning tree
3 findanedge.u;/thatissafeforA
4 A A .u;/
D [f g
5 returnA
Weusetheloopinvariant asfollows:
Initialization: Afterline1,thesetAtriviallysatisfiestheloopinvariant.
Maintenance: The loop in lines 2–4 maintains the invariant by adding only safe
edges.
Termination: All edges added to A are in a minimum spanning tree, and so the
setAreturned inline5mustbeaminimumspanning tree.
Thetricky partis,ofcourse, findingasafeedgeinline3. Onemustexist, since
when line 3 is executed, the invariant dictates that there is a spanning tree T such
that A T. Within the while loop body, A must be a proper subset of T, and

therefore there must bean edge .u;/ T such that .u;/ A and .u;/is safe
2 62
forA.
Intheremainderofthissection, weprovidearule(Theorem23.1)forrecogniz-
ingsafeedges. Thenextsection describes twoalgorithms thatusethisruletofind
safeedgesefficiently.
We first need some definitions. A cut .S;V S/ of an undirected graph G
(cid:0) D
.V;E/ is a partition of V. Figure 23.2 illustrates this notion. Wesay that anedge
.u;/ E crosses the cut.S;V S/ ifoneofitsendpoints isinS andtheother
2 (cid:0)
isinV S. WesaythatacutrespectsasetAofedgesifnoedgeinAcrossesthe
(cid:0)
cut. Anedgeisalightedgecrossingacutifitsweightistheminimumofanyedge
crossing thecut. Notethat there canbemorethan onelight edge crossing acutin
the case of ties. More generally, we say that an edge is a light edge satisfying a
givenproperty ifitsweightistheminimumofanyedgesatisfying theproperty.
Ourruleforrecognizing safeedgesisgivenbythefollowingtheorem.
Theorem23.1
LetG .V;E/beaconnected, undirected graph withareal-valued weightfunc-
D
tion w defined on E. Let A be a subset of E that is included in some minimum
spanning treefor G, let.S;V S/beanycut ofG that respects A,and let.u;/
(cid:0)
bealightedgecrossing.S;V S/. Then,edge.u;/issafeforA.
(cid:0)
23.1 Growingaminimumspanningtree 627
h
8
a 7
4 11 i 1
b 6
2 g
8 7 d 8
b c d
4 9 7
9 c 2
S 2 S
a 11 i 14 e
4 e 14 4
V – S 7 6 V – S
8 10
10
h g f f
1 2
S V – S
(a) (b)
Figure 23.2 Two ways of viewing a cut .S;V S/ of the graph from Figure 23.1. (a) Black
(cid:0)
vertices are in the set S, and white vertices are in V S. The edges crossing the cut are those
(cid:0)
connectingwhiteverticeswithblackvertices. Theedge.d;c/istheuniquelightedgecrossingthe
cut. AsubsetAoftheedgesisshaded;notethatthecut.S;V S/respectsA,sincenoedgeofA
(cid:0)
crossesthecut. (b)ThesamegraphwiththeverticesinthesetS ontheleftandtheverticesinthe
setV S ontheright.Anedgecrossesthecutifitconnectsavertexontheleftwithavertexonthe
(cid:0)
right.
Proof Let T be a minimum spanning tree that includes A, and assume that T
does not contain the light edge .u;/, since if it does, we are done. We shall
construct anotherminimumspanning treeT thatincludesA .u;/ byusinga
0
[f g
cut-and-paste technique, therebyshowingthat.u;/isasafeedgeforA.
The edge .u;/ forms a cycle with the edges on the simple path p from u
to  in T, as Figure 23.3 illustrates. Since u and  are on opposite sides of the
cut .S;V S/, at least one edge in T lies on the simple path p and also crosses
(cid:0)
the cut. Let .x;y/ be any such edge. Theedge .x;y/ is not in A, because the cut
respects A. Since .x;y/ is on the unique simple path from u to  in T, remov-
ing .x;y/breaks T into twocomponents. Adding .u;/ reconnects them to form
anewspanning treeT T .x;y/ .u;/ .
0
D (cid:0)f g[f g
We next show that T is a minimum spanning tree. Since .u;/ is a light edge
0
crossing.S;V S/and.x;y/alsocrossesthiscut,w.u;/ w.x;y/. Therefore,
(cid:0) 
w.T / w.T/ w.x;y/ w.u;/
0
D (cid:0) C
w.T/:

628 Chapter23 MinimumSpanningTrees
x
p
u
y
v
Figure23.3 TheproofofTheorem23.1. BlackverticesareinS,andwhiteverticesareinV S.
(cid:0)
Theedgesintheminimumspanning treeT areshown, buttheedgesinthegraphG arenot. The
edges inAareshaded, and .u;/ isalight edge crossing thecut .S;V S/. Theedge .x;y/is
(cid:0)
anedgeontheuniquesimplepathp fromuto inT. ToformaminimumspanningtreeT 0 that
contains.u;/,removetheedge.x;y/fromT andaddtheedge.u;/.
But T is a minimum spanning tree, so that w.T/ w.T /; thus, T must be a
0 0

minimumspanning treealso.
It remains to show that .u;/ is actually a safe edge for A. We have A T ,
0

sinceA T and.x;y/ A;thus,A .u;/ T . Consequently, sinceT isa
0 0
 62 [f g 
minimumspanning tree,.u;/issafeforA.
Theorem23.1givesusabetterunderstanding oftheworkingsofthe GENERIC-
MST method on a connected graph G .V;E/. As the method proceeds, the
D
set A is always acyclic; otherwise, a minimum spanning tree including A would
contain acycle, which is acontradiction. Atany point in the execution, the graph
G .V;A/ is a forest, and each of the connected components of G is a tree.
A A
D
(Some of the trees may contain just one vertex, as is the case, for example, when
the method begins: A is empty and the forest contains V trees, one for each
j j
vertex.) Moreover,anysafeedge.u;/forAconnectsdistinctcomponentsofG ,
A
sinceA .u;/ mustbeacyclic.
[f g
Thewhileloopinlines 2–4of GENERIC-MST executes V 1timesbecause
j j(cid:0)
it finds one of the V 1 edges of a minimum spanning tree in each iteration.
j j (cid:0)
Initially, when A , there are V trees in G , and each iteration reduces that
A
D ; j j
numberby1. Whentheforestcontains onlyasingletree,themethodterminates.
ThetwoalgorithmsinSection23.2usethefollowingcorollarytoTheorem23.1.
23.1 Growingaminimumspanningtree 629
Corollary23.2
LetG .V;E/beaconnected, undirected graph withareal-valued weight func-
D
tion w defined on E. Let A be a subset of E that is included in some minimum
spanningtreeforG,andletC .V ;E /beaconnectedcomponent(tree)inthe
C C
D
forestG .V;A/. If.u;/isalightedgeconnectingC tosomeothercomponent
A
D
inG ,then.u;/issafeforA.
A
Proof The cut .V ;V V / respects A, and .u;/ is a light edge for this cut.
C C
(cid:0)
Therefore, .u;/issafeforA.
Exercises
23.1-1
Let .u;/ be a minimum-weight edge in a connected graph G. Show that .u;/
belongstosomeminimumspanning treeofG.
23.1-2
Professor Sabatier conjectures the following converse of Theorem 23.1. Let G
D
.V;E/beaconnected, undirected graph withareal-valued weight function w de-
fined on E. Let A be a subset of E that is included in some minimum spanning
tree forG, let.S;V S/ be anycut ofG that respects A, andlet .u;/ beasafe
(cid:0)
edgeforAcrossing .S;V S/. Then,.u;/isalightedgeforthecut. Showthat
(cid:0)
theprofessor’s conjecture isincorrect bygivingacounterexample.
23.1-3
Showthatifanedge.u;/iscontained insomeminimumspanning tree,thenitis
alightedgecrossingsomecutofthegraph.
23.1-4
Give a simple example of a connected graph such that the set of edges .u;/
f W
there exists acut .S;V S/ such that .u;/ is alight edge crossing .S;V S/
(cid:0) (cid:0) g
doesnotformaminimumspanning tree.
23.1-5
Lete beamaximum-weight edge onsomecycleofconnected graph G .V;E/.
D
Prove that there is a minimum spanning tree of G .V;E e / that is also a
0
D (cid:0)f g
minimumspanning treeofG. Thatis,thereisaminimumspanning treeofG that
doesnotincludee.
630 Chapter23 MinimumSpanningTrees
23.1-6
Show that a graph has a unique minimum spanning tree if, for every cut of the
graph, there is a unique light edge crossing the cut. Show that the converse is not
truebygivingacounterexample.
23.1-7
Arguethatifalledgeweightsofagrapharepositive,thenanysubsetofedgesthat
connectsallverticesandhasminimumtotalweightmustbeatree. Giveanexample
to show that the same conclusion does not follow if we allow some weights to be
nonpositive.
23.1-8
LetT beaminimumspanning treeofagraphG,andletLbethesortedlistofthe
edge weights of T. Show that for any other minimum spanning tree T of G, the
0
listLisalsothesortedlistofedgeweightsofT .
0
23.1-9
LetT beaminimum spanning treeofagraph G .V;E/,and letV beasubset
0
D
ofV. LetT bethesubgraph ofT inducedbyV ,andletG bethesubgraph ofG
0 0 0
induced by V . Show that if T is connected, then T is a minimum spanning tree
0 0 0
ofG .
0
23.1-10
Given a graph G and a minimum spanning tree T, suppose that we decrease the
weight of one of the edges in T. Show that T is still a minimum spanning tree
forG. Moreformally, letT beaminimumspanning treeforG withedgeweights
givenbyweightfunctionw. Chooseoneedge.x;y/ T andapositivenumberk,
2
anddefinetheweightfunction w by
0
w.u;/ if.u;/ .x;y/;
w .u;/ ¤
0
D
(
w.x;y/ k if.u;/ .x;y/:
(cid:0) D
ShowthatT isaminimumspanning treeforG withedgeweightsgivenbyw.
0
23.1-11 ?
Given a graph G and a minimum spanning tree T, suppose that we decrease the
weight of one of the edges not in T. Give an algorithm for finding the minimum
spanning treeinthemodifiedgraph.
23.2 ThealgorithmsofKruskalandPrim 631
23.2 The algorithmsofKruskal andPrim
Thetwominimum-spanning-tree algorithms described inthissection elaborate on
thegenericmethod. Theyeachuseaspecificruletodetermineasafeedgeinline3
ofGENERIC-MST. InKruskal’salgorithm,thesetAisaforestwhoseverticesare
all those of the given graph. The safe edge added to A is always a least-weight
edge in the graph that connects twodistinct components. In Prim’salgorithm, the
set A forms a single tree. The safe edge added to A is always aleast-weight edge
connecting thetreetoavertexnotinthetree.
Kruskal’salgorithm
Kruskal’salgorithmfindsasafeedgetoaddtothegrowingforestbyfinding,ofall
the edges that connect any two trees in the forest, an edge .u;/ of least weight.
LetC andC denotethetwotreesthatareconnected by.u;/. Since.u;/must
1 2
bealightedgeconnectingC tosomeothertree,Corollary23.2impliesthat.u;/
1
isasafeedge for C . Kruskal’s algorithm qualifies asagreedy algorithm because
1
ateachstepitaddstotheforestanedgeofleastpossible weight.
Our implementation of Kruskal’s algorithm is like the algorithm to compute
connected components from Section 21.1. It uses a disjoint-set data structure to
maintainseveraldisjointsetsofelements. Eachsetcontainstheverticesinonetree
ofthecurrentforest. Theoperation FIND-SET.u/returns arepresentative element
fromthesetthatcontains u. Thus,wecandeterminewhethertwoverticesuand
belongtothesametreebytestingwhether FIND-SET.u/equals FIND-SET./. To
combinetrees,Kruskal’salgorithm callsthe UNION procedure.
MST-KRUSKAL.G;w/
1 A
D ;
2 foreachvertex G:V
2
3 MAKE-SET./
4 sorttheedgesofG:E intonondecreasing orderbyweightw
5 foreachedge.u;/ G:E,takeninnondecreasing orderbyweight
2
6 ifFIND-SET.u/ FIND-SET./
¤
7 A A .u;/
D [f g
8 UNION.u;/
9 returnA
Figure23.4showshowKruskal’salgorithmworks. Lines1–3initializethesetA
to the empty set and create V trees, one containing each vertex. The for loop in
j j
lines 5–8 examines edges in order of weight, from lowest to highest. The loop
632 Chapter23 MinimumSpanningTrees
8 7 8 7
b c d b c d
4 9 4 9
2 2
(a) a 11 i 4 14 e (b) a 11 i 4 14 e
7 6 7 6
10 10
8 8
h g f h g f
1 2 1 2
8 7 8 7
b c d b c d
4 9 4 9
2 2
(c) a 11 i 4 14 e (d) a 11 i 4 14 e
7 6 7 6
10 10
8 8
h g f h g f
1 2 1 2
8 7 8 7
b c d b c d
4 9 4 9
2 2
(e) a 11 i 4 14 e (f) a 11 i 4 14 e
7 6 7 6
10 10
8 8
h g f h g f
1 2 1 2
8 7 8 7
b c d b c d
4 9 4 9
2 2
(g) a 11 i 4 14 e (h) a 11 i 4 14 e
7 6 7 6
10 10
8 8
h g f h g f
1 2 1 2
Figure23.4 TheexecutionofKruskal’salgorithmonthegraphfromFigure23.1. Shadededges
belongtotheforestAbeinggrown. Thealgorithmconsiderseachedgeinsortedorderbyweight.
Anarrowpointstotheedgeunderconsiderationateachstepofthealgorithm. Iftheedgejoinstwo
distincttreesintheforest,itisaddedtotheforest,therebymergingthetwotrees.
checks, for each edge .u;/, whether the endpoints u and  belong to the same
tree. Iftheydo,thentheedge.u;/cannotbeaddedtotheforestwithoutcreating
a cycle, and the edge is discarded. Otherwise, the two vertices belong todifferent
trees. In this case, line 7 adds the edge .u;/ to A, and line 8merges the vertices
inthetwotrees.
23.2 ThealgorithmsofKruskalandPrim 633
8 7 8 7
b c d b c d
4 9 4 9
2 2
(i) a 11 i 4 14 e (j) a 11 i 4 14 e
7 6 7 6
10 10
8 8
h g f h g f
1 2 1 2
8 7 8 7
b c d b c d
4 9 4 9
2 2
(k) a 11 i 4 14 e (l) a 11 i 4 14 e
7 6 7 6
10 10
8 8
h g f h g f
1 2 1 2
8 7 8 7
b c d b c d
4 9 4 9
2 2
(m) a 11 i 4 14 e (n) a 11 i 4 14 e
7 6 7 6
10 10
8 8
h g f h g f
1 2 1 2
Figure23.4,continued FurtherstepsintheexecutionofKruskal’salgorithm.
The running time of Kruskal’s algorithm for a graph G .V;E/ depends
D
on how we implement the disjoint-set data structure. We assume that we use
the disjoint-set-forest implementation of Section 21.3 with the union-by-rank and
path-compression heuristics, since it is the asymptotically fastest implementation
known. Initializing the set A in line 1 takes O.1/ time, and the time to sort the
edges inline 4isO.ElgE/. (Wewillaccount forthecost ofthe V MAKE-SET
j j
operations in the for loop of lines 2–3 in a moment.) The for loop of lines 5–8
performsO.E/FIND-SET andUNIONoperationsonthedisjoint-setforest. Along
withthe V MAKE-SET operations, these takeatotalofO..V E/˛.V//time,
j j C
where ˛ is the very slowly growing function defined in Section 21.4. Because we
assumethatG isconnected, wehave E V 1,andsothedisjoint-set opera-
j j  j j(cid:0)
tionstakeO.E˛.V//time. Moreover,since˛. V / O.lgV/ O.lgE/,theto-
talrunning timeofKruskal’s algorithm isO.Ej lgj ED /. ObservinD g that E < V 2 ,
j j j j
we have lg E O.lgV/, and so we can restate the running time of Kruskal’s
j j D
algorithm asO.ElgV/.
634 Chapter23 MinimumSpanningTrees
Prim’salgorithm
Like Kruskal’s algorithm, Prim’s algorithm is a special case of the generic min-
imum-spanning-tree method from Section 23.1. Prim’s algorithm operates much
likeDijkstra’salgorithmforfindingshortestpathsinagraph,whichweshallseein
Section 24.3. Prim’salgorithm hasthe property that theedges inthesetAalways
form a single tree. As Figure 23.5 shows, the tree starts from an arbitrary root
vertex r and growsuntil the tree spans allthe vertices inV. Each step adds tothe
tree A a light edge that connects A to an isolated vertex—one on which no edge
of A is incident. By Corollary 23.2, this rule adds only edges that are safe for A;
therefore,whenthealgorithmterminates,theedgesinAformaminimumspanning
tree. Thisstrategy qualifies asgreedy since ateach stepitaddstothetree anedge
thatcontributes theminimumamountpossible tothetree’sweight.
In order toimplement Prim’salgorithm efficiently, weneed afast waytoselect
anew edge toadd tothetree formed bythe edges inA. Inthe pseudocode below,
the connected graph G and the root r of the minimum spanning tree to be grown
are inputs to the algorithm. During execution of the algorithm, all vertices that
are not in the tree reside in a min-priority queue Q based on a key attribute. For
eachvertex,theattribute :keyistheminimumweightofanyedgeconnecting 
to a vertex in the tree; by convention, :key if there is no such edge. The
D 1
attribute : namestheparentof inthetree. Thealgorithm implicitly maintains
thesetAfrom GENERIC-MST as
A .;:/  V r Q :
D f W 2 (cid:0)f g(cid:0) g
When the algorithm terminates, the min-priority queue Q isempty; theminimum
spanning treeAforG isthus
A .;:/  V r :
D f W 2 (cid:0)f gg
MST-PRIM.G;w;r/
1 foreachu G:V
2
2 u:key
D 1
3 u: NIL
D
4 r:key 0
D
5 Q G:V
D
6 whileQ
¤ ;
7 u EXTRACT-MIN.Q/
D
8 foreach G:AdjŒu
2
9 if Qandw.u;/ < :key
2
10 : u
D
11 :key w.u;/
D
23.2 ThealgorithmsofKruskalandPrim 635
8 7 8 7
b c d b c d
4 9 4 9
2 2
(a) a 11 i 4 14 e (b) a 11 i 4 14 e
7 6 7 6
10 10
8 8
h g f h g f
1 2 1 2
8 7 8 7
b c d b c d
4 9 4 9
2 2
(c) a 11 i 4 14 e (d) a 11 i 4 14 e
7 6 7 6
10 10
8 8
h g f h g f
1 2 1 2
8 7 8 7
b c d b c d
4 9 4 9
2 2
(e) a 11 i 4 14 e (f) a 11 i 4 14 e
7 6 7 6
10 10
8 8
h g f h g f
1 2 1 2
8 7 8 7
b c d b c d
4 9 4 9
2 2
(g) a 11 i 4 14 e (h) a 11 i 4 14 e
7 6 7 6
10 10
8 8
h g f h g f
1 2 1 2
8 7
b c d
4 9
2
(i) a 11 i 4 14 e
7 6
10
8
h g f
1 2
Figure23.5 The execution of Prim’salgorithm on the graph fromFigure 23.1. Theroot vertex
isa. Shadededgesareinthetreebeinggrown, andblackverticesareinthetree. Ateachstepof
thealgorithm,theverticesinthetreedetermineacutofthegraph,andalightedgecrossingthecut
isadded tothe tree. Inthe second step, for example, thealgorithm has a choice of adding either
edge.b;c/oredge.a;h/tothetreesincebotharelightedgescrossingthecut.
636 Chapter23 MinimumSpanningTrees
Figure 23.5 shows how Prim’s algorithm works. Lines 1–5 set the key of each
vertex to (except for the root r, whose key is set to 0 so that it will be the
1
firstvertex processed), setthe parent ofeach vertex to NIL, and initialize themin-
priorityqueueQtocontainallthevertices. Thealgorithmmaintainsthefollowing
three-part loopinvariant:
Priortoeachiteration ofthewhileloopoflines6–11,
1. A .;:/  V r Q .
D f W 2 (cid:0)f g(cid:0) g
2. Thevertices already placed intothe minimum spanning tree are those in
V Q.
(cid:0)
3. For all vertices  Q, if : NIL, then :key < and :key is
2 ¤ 1
the weight of a light edge .;:/ connecting  to some vertex already
placedintotheminimumspanning tree.
Line 7 identifies a vertex u Q incident on a light edge that crosses the cut
2
.V Q;Q/(withtheexceptionofthefirstiteration,inwhichu r duetoline4).
(cid:0) D
Removing u from the set Q adds it to the set V Q of vertices in the tree, thus
(cid:0)
adding .u;u:/ toA. Thefor loop of lines 8–11 updates the key and  attributes
of every vertex  adjacent to u but not in the tree, thereby maintaining the third
partoftheloopinvariant.
The running time of Prim’s algorithm depends on how we implement the min-
priority queue Q. If we implement Q as a binary min-heap (see Chapter 6), we
canusetheBUILD-MIN-HEAP proceduretoperformlines1–5inO.V/time. The
body of the while loop executes V times, and since each EXTRACT-MIN opera-
j j
tiontakesO.lgV/time,thetotaltimeforallcallstoEXTRACT-MIN isO.V lgV/.
The for loop in lines 8–11 executes O.E/ times altogether, since the sum of the
lengths of all adjacency lists is 2 E . Within the for loop, we can implement the
j j
testformembershipinQinline9inconstanttimebykeepingabitforeachvertex
thattellswhetherornotitisinQ,andupdatingthebitwhenthevertexisremoved
from Q. The assignment in line 11 involves an implicit DECREASE-KEY opera-
tion on the min-heap, which a binary min-heap supports in O.lgV/ time. Thus,
thetotaltimeforPrim’salgorithm isO.V lgV ElgV/ O.ElgV/,whichis
C D
asymptotically thesameasforourimplementation ofKruskal’salgorithm.
We can improve the asymptotic running time of Prim’s algorithm by using Fi-
bonacci heaps. Chapter 19 shows that if a Fibonacci heap holds V elements, an
j j
EXTRACT-MIN operation takes O.lgV/ amortized time and a DECREASE-KEY
operation(toimplementline11)takesO.1/amortizedtime. Therefore,ifweusea
Fibonacciheaptoimplementthemin-priorityqueueQ,therunningtimeofPrim’s
algorithm improvestoO.E V lgV/.
C
23.2 ThealgorithmsofKruskalandPrim 637
Exercises
23.2-1
Kruskal’salgorithmcanreturndifferentspanningtreesforthesameinputgraphG,
depending on how it breaks ties when the edges are sorted into order. Show that
for each minimum spanning tree T of G, there is a way to sort the edges of G in
Kruskal’salgorithm sothatthealgorithm returnsT.
23.2-2
Suppose that we represent the graph G .V;E/ as an adjacency matrix. Give a
D
simpleimplementation ofPrim’salgorithm forthiscasethatrunsinO.V2/time.
23.2-3
For a sparse graph G .V;E/, where E ‚.V/, is the implementation of
D j j D
Prim’salgorithm withaFibonacciheapasymptotically fasterthanthebinary-heap
implementation? What about for a dense graph, where E ‚.V2/? How
j j D
must the sizes E and V be related for the Fibonacci-heap implementation to
j j j j
beasymptotically fasterthanthebinary-heap implementation?
23.2-4
Suppose that all edge weights in a graph are integers in the range from 1 to V .
j j
How fast can you make Kruskal’s algorithm run? What if the edge weights are
integersintherangefrom1toW forsomeconstant W?
23.2-5
Suppose that all edge weights in a graph are integers in the range from 1 to V .
j j
HowfastcanyoumakePrim’salgorithmrun? Whatiftheedgeweightsareintegers
intherangefrom1toW forsomeconstant W?
23.2-6 ?
Suppose that the edge weights in a graph are uniformly distributed over the half-
open interval Œ0;1/. Which algorithm, Kruskal’s or Prim’s, can you make run
faster?
23.2-7 ?
Suppose that a graph G has a minimum spanning tree already computed. How
quickly can we update the minimum spanning tree if we add a new vertex and
incidentedgestoG?
23.2-8
Professor Borden proposes a new divide-and-conquer algorithm for computing
minimum spanning trees, which goes as follows. Given a graph G .V;E/,
D
partition thesetV ofverticesintotwosetsV andV suchthat V and V differ
1 2 1 2
j j j j
638 Chapter23 MinimumSpanningTrees
byatmost1. LetE bethesetofedgesthatareincidentonlyonverticesinV ,and
1 1
letE bethesetofedgesthatareincidentonlyonverticesinV . Recursivelysolve
2 2
a minimum-spanning-tree problem on each of the two subgraphs G .V ;E /
1 1 1
D
andG .V ;E /. Finally,selecttheminimum-weightedgeinE thatcrossesthe
2 2 2
D
cut .V ;V /, and use this edge to unite the resulting two minimum spanning trees
1 2
intoasinglespanning tree.
Either argue that the algorithm correctly computes a minimum spanning tree
ofG,orprovideanexampleforwhichthealgorithm fails.
Problems
23-1 Second-bestminimumspanningtree
Let G .V;E/ be an undirected, connected graph whose weight function is
w E
DR
,andsuppose that E V andalledgeweightsaredistinct.
W ! j j  j j T
We define a second-best minimum spanning tree as follows. Let be the set
of all spanning trees of G, and let T be a minimum spanning tree of G. Then
0
a second-best minimum spanning tree is a spanning tree T such that w.T/
D
min T00 T T0 w.T 00/ .
2 (cid:0)f gf g
a. Showthattheminimumspanning treeisunique, butthatthesecond-best mini-
mumspanning treeneednotbeunique.
b. Let T be the minimum spanning tree of G. Prove that G contains edges
.u;/ T and .x;y/ T such that T .u;/ .x;y/ is a second-best
2 62 (cid:0)f g[f g
minimumspanning treeofG.
c. LetT beaspanning treeofG and,foranytwoverticesu; V,letmaxŒu;
2
denoteanedgeofmaximumweightontheuniquesimplepathbetweenuand
inT. DescribeanO.V2/-timealgorithmthat,givenT,computesmaxŒu;for
allu; V.
2
d. Giveanefficientalgorithmtocomputethesecond-bestminimumspanningtree
ofG.
23-2 Minimumspanningtreeinsparse graphs
For a very sparse connected graph G .V;E/, we can further improve upon the
D
O.E V lgV/runningtimeofPrim’salgorithm withFibonacciheapsbyprepro-
C
cessing G to decrease the number of vertices before running Prim’s algorithm. In
particular, wechoose, foreachvertexu,theminimum-weightedge.u;/incident
on u, and we put .u;/ into the minimum spanning tree under construction. We
ProblemsforChapter23 639
then contract all chosen edges (see Section B.4). Rather than contracting these
edges one at a time, we first identify sets of vertices that are united into the same
new vertex. Then we create the graph that would have resulted from contracting
theseedges oneatatime, butwedosoby“renaming” edges according tothesets
intowhichtheirendpointswereplaced. Severaledgesfromtheoriginalgraphmay
be renamed the same as each other. In such a case, only one edge results, and its
weightistheminimumoftheweightsofthecorresponding original edges.
Initially, we set the minimum spanning tree T being constructed to be empty,
and for each edge .u;/ E, we initialize the attributes .u;/:orig .u;/
2 D
and .u;/:c w.u;/. We use the orig attribute to reference the edge from the
D
initialgraphthatisassociatedwithanedgeinthecontractedgraph. Thec attribute
holdstheweightofanedge,andasedgesarecontracted, weupdateitaccordingto
theaboveschemeforchoosing edgeweights. Theprocedure MST-REDUCE takes
inputs G and T, and itreturns a contracted graph G with updated attributes orig
0 0
and c . The procedure also accumulates edges of G into the minimum spanning
0
treeT.
MST-REDUCE.G;T/
1 foreach G:V
2
2 :mark FALSE
D
3 MAKE-SET./
4 foreachu G:V
2
5 ifu:mark == FALSE
6 choose G:AdjŒusuchthat.u;/:cisminimized
2
7 UNION.u;/
8 T T .u;/:orig
D [f g
9 u:mark :mark TRUE
D D
10 G 0:V FIND-SET./  G:V
D f W 2 g
11 G :E
0
D ;
12 foreach.x;y/ G:E
2
13 u FIND-SET.x/
D
14  FIND-SET.y/
D
15 if.u;/ G :E
0
62
16 G :E G :E .u;/
0 0
D [f g
17 .u;/:orig .x;y/:orig
0
D
18 .u;/:c .x;y/:c
0
D
19 elseif.x;y/:c< .u;/:c
0
20 .u;/:orig .x;y/:orig
0
D
21 .u;/:c .x;y/:c
0
D
22 construct adjacencylistsG :AdjforG
0 0
23 returnG andT
0
640 Chapter23 MinimumSpanningTrees
a. LetT bethesetofedgesreturnedbyMST-REDUCE,andletAbetheminimum
spanningtreeofthegraphG 0formedbythecallMST-PRIM.G 0;c 0;r/,wherec
0
istheweightattribute ontheedges ofG :E andr isanyvertexinG :V. Prove
0 0
thatT .x;y/:orig .x;y/ A isaminimumspanning treeofG.
0
[f W 2 g
b. Arguethat G :V V =2.
0
j j  j j
c. Show how to implement MST-REDUCE so that it runs in O.E/ time. (Hint:
Usesimpledatastructures.)
d. Supposethatwerunk phasesofMST-REDUCE, usingtheoutputG
0
produced
by one phase as the input G to the next phase and accumulating edges in T.
Arguethattheoverallrunning timeofthek phasesisO.kE/.
e. Suppose that after running k phases of MST-REDUCE, as in part (d), we run
Prim’s algorithm by calling MST-PRIM.G 0;c 0;r/, where G 0, with weight at-
tribute c , isreturned bythe last phase and r isanyvertex inG :V. Show how
0 0
to pick k so that the overall running time is O.ElglgV/. Argue that your
choiceofk minimizestheoverallasymptoticrunning time.
f. Forwhatvaluesof E (intermsof V )doesPrim’salgorithmwithpreprocess-
j j j j
ingasymptotically beatPrim’salgorithm withoutpreprocessing?
23-3 Bottleneckspanningtree
A bottleneck spanning tree T of an undirected graph G is a spanning tree of G
whose largest edge weight is minimum over all spanning trees of G. We say that
the value of the bottleneck spanning tree is the weight of the maximum-weight
edgeinT.
a. Arguethataminimumspanning treeisabottleneck spanning tree.
Part (a) shows that finding a bottleneck spanning tree is no harder than finding
a minimum spanning tree. In the remaining parts, we will show how to find a
bottleneck spanning treeinlineartime.
b. Givealinear-time algorithm thatgivenagraphG andanintegerb,determines
whetherthevalueofthebottleneck spanning treeisatmostb.
c. Use your algorithm for part (b) as a subroutine in a linear-time algorithm for
thebottleneck-spanning-tree problem. (Hint:Youmaywanttouseasubroutine
that contracts sets of edges, as in the MST-REDUCE procedure described in
Problem23-2.)
NotesforChapter23 641
23-4 Alternativeminimum-spanning-treealgorithms
Inthisproblem,wegivepseudocodeforthreedifferentalgorithms. Eachonetakes
aconnected graphandaweightfunctionasinputandreturnsasetofedgesT. For
eachalgorithm, either provethat T isaminimumspanning treeorprove that T is
not necessarily a minimum spanning tree. Also describe the most efficient imple-
mentationofeachalgorithm,whetherornotitcomputesaminimumspanningtree.
a. MAYBE-MST-A.G;w/
1 sorttheedgesintononincreasing orderofedgeweightsw
2 T E
D
3 foreachedgee,takeninnonincreasing orderbyweight
4 ifT e isaconnected graph
(cid:0)f g
5 T T e
D (cid:0)f g
6 returnT
b. MAYBE-MST-B.G;w/
1 T
D ;
2 foreachedgee,takeninarbitrary order
3 ifT e hasnocycles
[f g
4 T T e
D [f g
5 returnT
c. MAYBE-MST-C.G;w/
1 T
D ;
2 foreachedgee,takeninarbitrary order
3 T T e
D [f g
4 ifT hasacyclec
5 lete beamaximum-weightedgeonc
0
6 T T e
0
D (cid:0)f g
7 returnT
Chapter notes
Tarjan [330] surveys the minimum-spanning-tree problem and provides excellent
advanced material. Graham and Hell [151] compiled a history of the minimum-
spanning-tree problem.
Tarjan attributes the first minimum-spanning-tree algorithm to a 1926 paper by
O. Boru°vka. Boru°vka’s algorithm consists of running O.lgV/ iterations of the
642 Chapter23 MinimumSpanningTrees
procedure MST-REDUCE described in Problem 23-2. Kruskal’s algorithm was
reported by Kruskal [222] in 1956. The algorithm commonly known as Prim’s
algorithm was indeed invented by Prim [285], but it was also invented earlier by
V.Jarn´ıkin1930.
Thereasonunderlying whygreedy algorithms areeffective atfindingminimum
spanning trees is that the set of forests of a graph forms a graphic matroid. (See
Section16.4.)
When E .V lgV/,Prim’salgorithm, implemented withFibonacci heaps,
j j D
runs in O.E/ time. For sparser graphs, using a combination of the ideas from
Prim’salgorithm,Kruskal’salgorithm,andBoru°vka’salgorithm,togetherwithad-
vanced data structures, Fredman and Tarjan [114] give an algorithm that runs in
O.Elg V/ time. Gabow, Galil, Spencer, and Tarjan [120] improved this algo-

rithm to run in O.Elglg V/ time. Chazelle [60] gives an algorithm that runs

in O.E ˛.E;V// time, where ˛.E;V/ is the functional inverse of Ackermann’s
y y
function. (See the chapter notes for Chapter 21 for a brief discussion of Acker-
mann’s function and its inverse.) Unlike previous minimum-spanning-tree algo-
rithms,Chazelle’s algorithm doesnotfollowthegreedymethod.
A related problem is spanning-tree verification, in which wearegiven agraph
G .V;E/andatreeT E,andwewishtodeterminewhetherT isaminimum
D 
spanning tree of G. King [203]gives alinear-time algorithm to verify a spanning
tree,buildingonearlierworkofKomlo´s[215]andDixon,Rauch,andTarjan[90].
The above algorithms are all deterministic and fall into the comparison-based
model described in Chapter 8. Karger, Klein, and Tarjan [195] give arandomized
minimum-spanning-tree algorithm that runs in O.V E/ expected time. This
C
algorithm usesrecursioninamannersimilartothelinear-time selection algorithm
in Section 9.3: a recursive call on an auxiliary problem identifies a subset of the
edges E that cannot be in any minimum spanning tree. Another recursive call
0
on E E then finds the minimum spanning tree. The algorithm also uses ideas
0
(cid:0)
fromBoru°vka’salgorithm andKing’salgorithm forspanning-tree verification.
Fredman and Willard [116] showed how to find a minimum spanning tree in
O.V E/timeusingadeterministicalgorithmthatisnotcomparisonbased. Their
C
algorithm assumes that the data are b-bit integers and that the computer memory
consists ofaddressable b-bitwords.
24 Single-Source Shortest Paths
Professor Patrick wishes to find the shortest possible route from Phoenix to Indi-
anapolis. Given a road map of the United States on which the distance between
each pair of adjacent intersections is marked, how can she determine this shortest
route?
One possible way would be to enumerate all the routes from Phoenix to Indi-
anapolis, add up the distances on each route, and select the shortest. It is easy to
see, however, that even disallowing routes that contain cycles, Professor Patrick
would have to examine an enormous number of possibilities, most of which are
simply not worth considering. Forexample, aroute from Phoenix to Indianapolis
that passes through Seattle is obviously a poor choice, because Seattle is several
hundredmilesoutoftheway.
In this chapter and in Chapter 25, we show how to solve such problems ef-
ficiently. In a shortest-paths problem, we are given a weighted, directed graph
G .V;E/, with weight function w E R mapping edges to real-valued
D W !
weights. Theweightw.p/ofpath p  ; ;:::; isthe sum of the weights
0 1 k
D h i
ofitsconstituent edges:
k
w.p/ w. ; /:
i 1 i
D (cid:0)
i 1
XD
Wedefinetheshortest-path weightı.u;/fromuto by
min w.p/
u;p
 ifthereisapathfromuto ;
ı.u;/ f W g
D
(
otherwise:
1
Ashortestpathfromvertexutovertex isthendefinedasanypathpwithweight
w.p/ ı.u;/.
D
InthePhoenix-to-Indianapolis example,wecanmodeltheroadmapasagraph:
vertices represent intersections, edges represent road segments between intersec-
tions,andedgeweightsrepresentroaddistances. Ourgoalistofindashortestpath
fromagivenintersection inPhoenixtoagivenintersection inIndianapolis.
644 Chapter24 Single-SourceShortestPaths
Edge weights can represent metrics other than distances, such as time, cost,
penalties, loss, or any other quantity that accumulates linearly along a path and
thatwewouldwanttominimize.
The breadth-first-search algorithm from Section 22.2 is a shortest-paths algo-
rithmthatworksonunweightedgraphs,thatis,graphsinwhicheachedgehasunit
weight. Because many ofthe concepts from breadth-first search arise inthe study
ofshortestpathsinweightedgraphs,youmightwanttoreviewSection22.2before
proceeding.
Variants
Inthischapter, weshallfocusonthesingle-source shortest-paths problem: given
a graph G .V;E/, we want to find a shortest path from a given source vertex
D
s V to each vertex  V. The algorithm for the single-source problem can
2 2
solvemanyotherproblems, including thefollowingvariants.
Single-destination shortest-pathsproblem: Find a shortest path to a given des-
tinationvertext fromeachvertex. Byreversingthedirectionofeachedgein
thegraph,wecanreducethisproblem toasingle-source problem.
Single-pairshortest-path problem: Find a shortest path from u to  for given
vertices u and . If we solve the single-source problem with source vertex u,
we solve this problem also. Moreover, all known algorithms for this problem
have the same worst-case asymptotic running time as the best single-source
algorithms.
All-pairsshortest-paths problem: Findashortestpathfromuto foreverypair
of vertices u and . Although we can solve this problem by running a single-
source algorithm once from each vertex, we usually can solve it faster. Addi-
tionally, its structure is interesting in its own right. Chapter 25 addresses the
all-pairsproblem indetail.
Optimalsubstructureofashortestpath
Shortest-paths algorithms typically rely on the property that a shortest path be-
tween two vertices contains other shortest paths within it. (The Edmonds-Karp
maximum-flow algorithm in Chapter 26 also relies on this property.) Recall
that optimal substructure is one of the key indicators that dynamic programming
(Chapter 15) and the greedy method (Chapter 16) might apply. Dijkstra’s algo-
rithm, which we shall see in Section 24.3, is a greedy algorithm, and the Floyd-
Warshall algorithm, which finds shortest paths between all pairs of vertices (see
Section 25.2), is a dynamic-programming algorithm. The following lemma states
theoptimal-substructure property ofshortest pathsmoreprecisely.
Chapter24 Single-SourceShortestPaths 645
Lemma24.1(Subpathsofshortestpathsareshortestpaths)
Given a weighted, directed graph G .V;E/ with weight function w E R ,
D W !
letp  ; ;:::; beashortest pathfromvertex tovertex and, forany
0 1 k 0 k
D h i
i andj such that 0 i j k,letp  ; ;:::; bethesubpath ofp
ij i i 1 j
fromvertex tover tex  . T hen,p isaD shoh rtestpC athfromi to .
i j ij i j
Proof If we decompose path p into 
p;0i

p;ij

p;jk
 , then we have that
0 i j k
w.p/ w.p / w.p / w.p /. Now,assumethatthereisapathp from
D
0i
C
ij
C
jk i0j i
to withweightw.p / < w.p /. Then, p;0i  p ;i0 j  p;jk  isapathfrom 
j i0j ij 0 i j k 0
to whoseweightw.p / w.p / w.p /islessthanw.p/,whichcontradicts
k 0i
C
i0j
C
jk
theassumptionthatp isashortest pathfrom to .
0 k
Negative-weight edges
Some instances of the single-source shortest-paths problem may include edges
whose weights are negative. If the graph G .V;E/ contains no negative-
D
weight cycles reachable from the source s, then for all  V, the shortest-path
2
weight ı.s;/ remains well defined, even if it has a negative value. If the graph
contains anegative-weight cyclereachable froms,however,shortest-path weights
are not well defined. No path from s to a vertex on the cycle can be a short-
estpath—we can always findapath withlowerweight by following theproposed
“shortest”pathandthentraversingthenegative-weightcycle. Ifthereisanegative-
weightcycleonsomepathfroms to,wedefineı.s;/ .
D(cid:0)1
Figure 24.1 illustrates the effect of negative weights and negative-weight cy-
cles on shortest-path weights. Because there is only one path from s to a (the
path s;a ), we have ı.s;a/ w.s;a/ 3. Similarly, there is only one path
h i D D
from s to b, and so ı.s;b/ w.s;a/ w.a;b/ 3 . 4/ 1. There are
D C D C (cid:0) D (cid:0)
infinitely many paths from s to c: s;c , s;c;d;c , s;c;d;c;d;c , and so on.
h i h i h i
Becausethecycle c;d;c hasweight6 . 3/ 3> 0,theshortestpathfroms
h i C (cid:0) D
toc is s;c ,withweightı.s;c/ w.s;c/ 5. Similarly,theshortestpathfroms
h i D D
tod is s;c;d ,withweightı.s;d/ w.s;c/ w.c;d/ 11. Analogously,there
h i D C D
are infinitely many paths from s to e: s;e , s;e;f;e , s;e;f;e;f;e , and so
h i h i h i
on. Because the cycle e;f;e has weight 3 . 6/ 3 < 0, however, there
h i C (cid:0) D (cid:0)
is no shortest path from s to e. By traversing the negative-weight cycle e;f;e
h i
arbitrarilymanytimes,wecanfindpathsfroms toewitharbitrarilylargenegative
weights, and soı.s;e/ . Similarly, ı.s;f/ . Because g isreachable
D (cid:0)1 D (cid:0)1
fromf,wecanalsofindpaths witharbitrarily large negativeweightsfroms tog,
andsoı.s;g/ . Verticesh,i,andj alsoformanegative-weightcycle. They
D (cid:0)1
arenotreachable froms,however,andsoı.s;h/ ı.s;i/ ı.s;j/ .
D D D1
646 Chapter24 Single-SourceShortestPaths
a b
–4
3 –1
h i
3 4 ¥ 2 ¥
s c 6 d g
0 5 5 11 8 –¥ –8 3
–3
¥
2 e 3 f 7
j
–¥ –¥
–6
Figure24.1 Negativeedge weightsinadirectedgraph. Theshortest-path weightfromsource s
appearswithineachvertex.Becauseverticeseandf formanegative-weightcyclereachablefroms,
theyhaveshortest-pathweightsof .Becausevertexgisreachablefromavertexwhoseshortest-
(cid:0)1
pathweightis ,it,too,hasashortest-pathweightof . Verticessuchash,i,andj arenot
(cid:0)1 (cid:0)1
reachablefroms,andsotheirshortest-pathweightsare ,eventhoughtheylieonanegative-weight
1
cycle.
Some shortest-paths algorithms, such as Dijkstra’s algorithm, assume that all
edgeweightsintheinputgrapharenonnegative,asintheroad-mapexample. Oth-
ers, such as the Bellman-Ford algorithm, allow negative-weight edges in the in-
put graph and produce a correct answer as long as no negative-weight cycles are
reachable from the source. Typically, if there is such a negative-weight cycle, the
algorithm candetectandreportitsexistence.
Cycles
Can a shortest path contain a cycle? As we have just seen, it cannot contain a
negative-weight cycle. Nor can it contain a positive-weight cycle, since remov-
ing the cycle from the path produces a path with the same source and destination
vertices and a lower path weight. That is, if p  ; ;:::; is a path and
0 1 k
D h i
c  ; ;:::; is apositive-weight cycle on this path (so that   and
i i 1 j i j
w.D c/h > 0)C , then thei path p  ;  ; :::;  ;  ;  ; :::;  haD s weight
0 0 1 i j 1 j 2 k
w.p / w.p/ w.c/< w.pD /,ah ndsop cannotbeC ashorC test pathfroi m to .
0 0 k
D (cid:0)
That leaves only 0-weight cycles. We can remove a 0-weight cycle from any
pathtoproduce anotherpathwhoseweightisthesame. Thus,ifthereisashortest
pathfromasourcevertexstoadestinationvertex thatcontainsa0-weightcycle,
then there is another shortest path from s to  without this cycle. As long as a
shortest pathhas0-weightcycles,wecanrepeatedly removethesecyclesfromthe
path until we have a shortest path that is cycle-free. Therefore, without loss of
generality we can assume that when we are finding shortest paths, they have no
cycles, i.e., they are simple paths. Since any acyclic path in a graph G .V;E/
D
Chapter24 Single-SourceShortestPaths 647
containsatmost V distinctvertices,italsocontainsatmost V 1edges. Thus,
j j j j(cid:0)
wecanrestrict ourattention toshortestpathsofatmost V 1edges.
j j(cid:0)
Representingshortestpaths
Weoftenwishtocomputenotonlyshortest-pathweights,buttheverticesonshort-
est paths as well. We represent shortest paths similarly to how we represented
breadth-first trees in Section 22.2. Given a graph G .V;E/, we maintain for
D
each vertex  V a predecessor : that is either another vertex or NIL. The
2
shortest-pathsalgorithmsinthischaptersetthe attributessothatthechainofpre-
decessorsoriginatingatavertexrunsbackwardsalongashortestpathfromsto.
Thus, given a vertex  for which : NIL, the procedure PRINT-PATH.G;s;/
¤
fromSection22.2willprintashortest pathfroms to.
Inthemidstofexecutingashortest-pathsalgorithm,however,the valuesmight
notindicate shortest paths. Asinbreadth-first search, weshall beinterested inthe
predecessor subgraph G .V ;E / induced by the  values. Here again, we
  
D
define the vertex set V

to be the set of vertices of G with non-NIL predecessors,
plusthesources:
V   V : NIL s :
D f 2 W ¤ g[f g
The directed edge set E is the set of edges induced by the  values for vertices

inV :

E .:;/ E  V s :
 
D f 2 W 2 (cid:0)f gg
Weshallprovethatthe valuesproducedbythealgorithmsinthischapterhave
thepropertythatatterminationG isa“shortest-paths tree”—informally, arooted

tree containing a shortest path from the source s to every vertex that is reachable
from s. A shortest-paths tree islike the breadth-first tree from Section 22.2, but it
containsshortestpathsfromthesourcedefinedintermsofedgeweightsinsteadof
numbers of edges. To be precise, let G .V;E/ be a weighted, directed graph
withweightfunction w E R ,andassD umethatG contains nonegative-weight
W !
cycles reachable from the source vertex s V, so that shortest paths are well
2
defined. A shortest-paths tree rooted at s is a directed subgraph G .V ;E /,
0 0 0
D
whereV V andE E,suchthat
0 0
 
1. V isthesetofverticesreachable froms inG,
0
2. G formsarootedtreewithroots,and
0
3. forall V ,theuniquesimplepathfromsto inG isashortestpathfroms
0 0
2
to inG.
648 Chapter24 Single-SourceShortestPaths
t x t x t x
6 6 6
3 9 3 9 3 9
3 3 3
4 4 4
2 1 2 7 2 1 2 7 2 1 2 7
s 0 s 0 s 0
3 3 3
5 5 5
5 11 5 11 5 11
6 6 6
y z y z y z
(a) (b) (c)
Figure 24.2 (a) A weighted, directed graph with shortest-path weights from source s. (b) The
shadededgesformashortest-pathstreerootedatthesources. (c)Anothershortest-pathstreewith
thesameroot.
Shortest paths are notnecessarily unique, and neither are shortest-paths trees. For
example,Figure24.2showsaweighted,directedgraphandtwoshortest-pathstrees
withthesameroot.
Relaxation
The algorithms in this chapter use the technique of relaxation. For each vertex
 V, we maintain an attribute :d, which is an upper bound on the weight of
2
a shortest path from source s to . We call :d a shortest-path estimate. We
initializetheshortest-path estimatesandpredecessors bythefollowing‚.V/-time
procedure:
INITIALIZE-SINGLE-SOURCE.G;s/
1 foreachvertex G:V
2
2 :d
D 1
3 : NIL
D
4 s:d 0
D
Afterinitialization, wehave: NIL forall V, s:d 0,and :d for
D 2 D D 1
 V s .
2 (cid:0)f g
The process of relaxing an edge .u;/ consists of testing whether we can im-
prove the shortest path to  found so far by going through u and, if so, updat-
ing :d and :. A relaxation step1 may decrease the value of the shortest-path
1Itmayseemstrangethattheterm“relaxation”isusedforanoperationthattightensanupperbound.
Theuse of thetermishistorical. Theoutcome of arelaxation stepcan beviewed asarelaxation
oftheconstraint:d u:d w.u;/,which,bythetriangleinequality(Lemma24.10), mustbe
 C
satisfiedifu:d ı.s;u/and:d ı.s;/.Thatis,if:d u:d w.u;/,thereisno“pressure”
D D  C
tosatisfythisconstraint,sotheconstraintis“relaxed.”
Chapter24 Single-SourceShortestPaths 649
u v u v
2 2
5 9 5 6
RELAX(u,v,w) RELAX(u,v,w)
u v u v
2 2
5 7 5 6
(a) (b)
Figure24.3 Relaxinganedge.u;/withweightw.u;/ 2. Theshortest-pathestimateofeach
D
vertexappearswithinthevertex. (a) Because:d > u:d w.u;/prior torelaxation, thevalue
C
of:ddecreases.(b)Here,:d u:d w.u;/beforerelaxingtheedge,andsotherelaxationstep
 C
leaves:dunchanged.
estimate :d and update ’s predecessor attribute :. The following code per-
formsarelaxation steponedge.u;/inO.1/time:
RELAX.u;;w/
1 if:d > u:d w.u;/
C
2 :d u:d w.u;/
D C
3 : u
D
Figure24.3showstwoexamples ofrelaxing anedge,oneinwhichashortest-path
estimatedecreases andoneinwhichnoestimatechanges.
Eachalgorithm inthis chapter calls INITIALIZE-SINGLE-SOURCE and then re-
peatedly relaxes edges. Moreover, relaxation istheonlymeansbywhichshortest-
path estimates and predecessors change. The algorithms in this chapter differ in
howmanytimestheyrelaxeachedgeandtheorderinwhichtheyrelaxedges. Dijk-
stra’s algorithm and the shortest-paths algorithm for directed acyclic graphs relax
each edge exactly once. The Bellman-Ford algorithm relaxes each edge V 1
j j (cid:0)
times.
Propertiesofshortestpathsandrelaxation
To prove the algorithms in this chapter correct, we shall appeal to several prop-
erties of shortest paths and relaxation. We state these properties here, and Sec-
tion 24.5 proves them formally. For your reference, each property stated here in-
cludes the appropriate lemma or corollary number from Section 24.5. The latter
five of these properties, which refer to shortest-path estimates or the predecessor
subgraph, implicitlyassumethatthegraphisinitializedwithacalltoINITIALIZE-
SINGLE-SOURCE.G;s/ andthattheonly waythatshortest-path estimates andthe
predecessor subgraph changearebysomesequence ofrelaxation steps.
650 Chapter24 Single-SourceShortestPaths
Triangleinequality (Lemma24.10)
Foranyedge.u;/ E,wehaveı.s;/ ı.s;u/ w.u;/.
2  C
Upper-boundproperty (Lemma24.11)
Wealwayshave:d ı.s;/forallvertices V,andonce:dachievesthe
 2
valueı.s;/,itneverchanges.
No-pathproperty (Corollary 24.12)
Ifthereisnopathfroms to,thenwealwayshave:d ı.s;/ .
D D1
Convergenceproperty (Lemma24.14)
Ifs ; u  isashortestpathinG forsomeu; V,andifu:d ı.s;u/at
! 2 D
anytimepriortorelaxingedge.u;/,then:d ı.s;/atalltimesafterward.
D
Path-relaxation property (Lemma24.15)
If p  ; ;:::; is a shortest path from s  to  , and we relax the
0 1 k 0 k
D h i D
edges of p in the order . ; /;. ; /;:::;. ; /, then  :d ı.s; /.
0 1 1 2 k 1 k k k
(cid:0) D
This property holds regardless of any other relaxation steps that occur, even if
theyareintermixed withrelaxations oftheedgesofp.
Predecessor-subgraph property (Lemma24.17)
Once:d ı.s;/ forall V,the predecessor subgraph isashortest-paths
D 2
treerootedats.
Chapteroutline
Section 24.1 presents theBellman-Ford algorithm, which solves thesingle-source
shortest-pathsprobleminthegeneralcaseinwhichedgescanhavenegativeweight.
The Bellman-Ford algorithm is remarkably simple, and it has the further benefit
of detecting whether a negative-weight cycle is reachable from the source. Sec-
tion 24.2 gives a linear-time algorithm for computing shortest paths from a single
sourceinadirectedacyclicgraph. Section24.3coversDijkstra’salgorithm,which
has a lower running time than the Bellman-Ford algorithm but requires the edge
weights tobenonnegative. Section 24.4shows how wecan use theBellman-Ford
algorithm to solve a special case of linear programming. Finally, Section 24.5
provestheproperties ofshortest pathsandrelaxation statedabove.
We require some conventions for doing arithmetic with infinities. We shall as-
sumethatforanyrealnumbera ,wehavea a . Also,to
¤(cid:0)1 C1 D1C D1
make our proofs hold in the presence of negative-weight cycles, we shall assume
thatforanyrealnumbera ,wehavea . / . / a .
¤1 C (cid:0)1 D (cid:0)1 C D(cid:0)1
All algorithms in this chapter assume that the directed graph G is stored in the
adjacency-list representation. Additionally, stored witheach edgeisitsweight, so
thataswetraverseeachadjacencylist,wecandeterminetheedgeweightsinO.1/
timeperedge.
24.1 TheBellman-Fordalgorithm 651
24.1 The Bellman-Fordalgorithm
The Bellman-Ford algorithm solves the single-source shortest-paths problem in
the general case in which edge weights may be negative. Given a weighted, di-
rected graph G .V;E/ with source s and weight function w E R , the
D W !
Bellman-Fordalgorithm returns aboolean valueindicating whetherornotthere is
a negative-weight cycle that is reachable from the source. If there is such a cy-
cle, the algorithm indicates that no solution exists. If there is no such cycle, the
algorithm produces theshortestpathsandtheirweights.
The algorithm relaxes edges, progressively decreasing an estimate :d on the
weight of a shortest path from the source s to each vertex  V until it achieves
2
the actual shortest-path weight ı.s;/. The algorithm returns TRUE if and only if
thegraphcontains nonegative-weight cyclesthatarereachable fromthesource.
BELLMAN-FORD.G;w;s/
1 INITIALIZE-SINGLE-SOURCE.G;s/
2 fori 1to G:V 1
D j j(cid:0)
3 foreachedge.u;/ G:E
2
4 RELAX.u;;w/
5 foreachedge.u;/ G:E
2
6 if:d > u:d w.u;/
C
7 return FALSE
8 return TRUE
Figure 24.4 shows the execution of the Bellman-Ford algorithm on a graph
with 5 vertices. After initializing the d and  values of all vertices in line 1,
the algorithm makes V 1 passes over the edges of the graph. Each pass is
j j (cid:0)
oneiteration ofthefor loop oflines 2–4 andconsists ofrelaxing each edge ofthe
graph once. Figures 24.4(b)–(e) show the state of the algorithm after each of the
four passes over the edges. After making V 1 passes, lines 5–8 check for a
j j (cid:0)
negative-weight cycle and return the appropriate boolean value. (We’ll see a little
laterwhythischeckworks.)
The Bellman-Ford algorithm runs in time O.VE/, since the initialization in
line 1 takes ‚.V/ time, each of the V 1 passes over the edges in lines 2–4
j j (cid:0)
takes‚.E/time,andtheforloopoflines5–7takesO.E/time.
ToprovethecorrectnessoftheBellman-Fordalgorithm,westartbyshowingthat
iftherearenonegative-weightcycles,thealgorithmcomputescorrectshortest-path
weightsforallverticesreachable fromthesource.
652 Chapter24 Single-SourceShortestPaths
t 5 x t 5 x t 5 x
¥ ¥ ¥
–2 6 –2 6 –2 4
6 6 6
–3 –3 –3
s 0 8 7 s 0 8 7 s 0 8 7
–4 –4 –4
2 2 2
7 7 7
¥ ¥ ¥
7 7 2
9 9 9
y z y z y z
(a) (b) (c)
t 5 x t 5 x
2 –2 4 2 –2 4
6 6
–3 –3
s 0 8 7 s 0 8 7
–4 –4
2 2
7 7
7 2 7 –2
9 9
y z y z
(d) (e)
Figure 24.4 The execution of the Bellman-Ford algorithm. The source is vertex s. The d val-
ues appear within the vertices, and shaded edges indicate predecessor values: if edge .u;/ is
shaded, then : u. In this particular example, each pass relaxes the edges in the order
D
.t;x/;.t;y/;.t;´/;.x;t/;.y;x/;.y;´/;.´;x/;.´;s/;.s;t/;.s;y/. (a) The situationjust before the
first pass over the edges. (b)–(e) The situation after each successive pass over the edges. The d
and  values in part (e) are the final values. The Bellman-Ford algorithm returns TRUE in this
example.
Lemma24.2
Let G .V;E/ be a weighted, directed graph with source s and weight func-
tion w D E R , and assume that G contains no negative-weight cycles that are
W !
reachable from s. Then, after the V 1 iterations of the for loop of lines 2–4
j j (cid:0)
of BELLMAN-FORD, we have :d ı.s;/ for all vertices  that are reachable
D
froms.
Proof We prove the lemma by appealing to the path-relaxation property. Con-
sider any vertex  that is reachable from s, and let p  ; ;:::; , where
0 1 k
D h i
 s and  , be any shortest path from s to . Because shortest paths are
0 k
D D
simple,p hasatmost V 1edges,andsok V 1. Eachofthe V 1itera-
j j(cid:0)  j j(cid:0) j j(cid:0)
tionsoftheforloopoflines2–4relaxesall E edges. Amongtheedgesrelaxedin
j j
theithiteration, fori 1;2;:::;k,is. ; /. Bythepath-relaxation property,
i 1 i
therefore, :d  :dD ı.s; / ı.s;/(cid:0) .
k k
D D D
24.1 TheBellman-Fordalgorithm 653
Corollary24.3
Let G .V;E/ be a weighted, directed graph with source vertex s and weight
functionD w E R , and assume that G contains no negative-weight cycles that
W !
are reachable from s. Then, for each vertex  V, there is a path from s to  if
2
andonlyifBELLMAN-FORD terminates with:d < whenitisrunonG.
1
Proof TheproofisleftasExercise24.1-2.
Theorem24.4(CorrectnessoftheBellman-Fordalgorithm)
Let BELLMAN-FORD be run on a weighted, directed graph G .V;E/ with
sources andweightfunctionw E R . IfG containsnonegativeD -weightcycles
W !
thatarereachable froms,thenthealgorithm returns TRUE, wehave:d ı.s;/
D
for all vertices  V, and the predecessor subgraph G is a shortest-paths tree

2
rooted at s. If G does contain a negative-weight cycle reachable from s, then the
algorithm returns FALSE.
Proof Suppose that graph G contains no negative-weight cycles that are reach-
able from thesource s. Wefirstprovethe claim that attermination, :d ı.s;/
D
forallvertices V. Ifvertex isreachablefroms,thenLemma24.2provesthis
2
claim. If isnot reachable from s, then the claim follows from the no-path prop-
erty. Thus,theclaimisproven. Thepredecessor-subgraph property,alongwiththe
claim, impliesthatG isashortest-paths tree. Nowweusetheclaim toshowthat

BELLMAN-FORD returns TRUE. Attermination, wehaveforalledges.u;/ E,
2
:d ı.s;/
D
ı.s;u/ w.u;/ (bythetriangle inequality)
 C
u:d w.u;/;
D C
andsononeofthetestsinline6causes BELLMAN-FORD toreturn FALSE. There-
fore,itreturns TRUE.
Now, suppose that graph G contains a negative-weight cycle that is reachable
fromthesources;letthiscyclebec  ; ;:::; ,where  . Then,
0 1 k 0 k
Dh i D
k
w. ; / < 0: (24.1)
i 1 i
(cid:0)
i 1
XD
Assume for the purpose of contradiction that the Bellman-Ford algorithm returns
TRUE. Thus,  i:d 
i
1:d w.
i
1; i/ for i 1;2;:::;k. Summing the
inequalities
aroundc ycle(cid:0)c giveC
sus
(cid:0) D
654 Chapter24 Single-SourceShortestPaths
k k
 :d . :d w. ; //
i i 1 i 1 i
 (cid:0) C (cid:0)
i 1 i 1
XD XD
k k
 :d w. ; /:
i 1 i 1 i
D (cid:0) C (cid:0)
i 1 i 1
XD XD
Since   , each vertex in c appears exactly once in each of the summations
0 k
D
k  :d and k  :d,andso
i 1 i i 1 i 1
D D (cid:0)
Pk k P
 :d  :d :
i i 1
D (cid:0)
i 1 i 1
XD XD
Moreover, byCorollary 24.3, :d isfinitefori 1;2;:::;k. Thus,
i
D
k
0 w. ; /;
i 1 i
 (cid:0)
i 1
XD
whichcontradicts inequality(24.1). WeconcludethattheBellman-Fordalgorithm
returns TRUE if graph G contains no negative-weight cycles reachable from the
source, and FALSE otherwise.
Exercises
24.1-1
Run the Bellman-Ford algorithm on the directed graph of Figure 24.4, using ver-
tex´asthesource. Ineachpass,relaxedgesinthesameorderasinthefigure,and
show the d and  values after each pass. Now, change the weight of edge .´;x/
to4andrunthealgorithm again,usings asthesource.
24.1-2
ProveCorollary 24.3.
24.1-3
Given a weighted, directed graph G .V;E/ with no negative-weight cycles,
D
let m be the maximum over all vertices  V of the minimum number of edges
2
inashortest path fromthesource s to. (Here, theshortest path isbyweight, not
thenumberofedges.) SuggestasimplechangetotheBellman-Fordalgorithmthat
allowsittoterminateinm 1passes,evenifmisnotknowninadvance.
C
24.1-4
ModifytheBellman-Fordalgorithm sothatitsets:dto forallvertices for
(cid:0)1
whichthereisanegative-weight cycleonsomepathfromthesourceto.
24.2 Single-sourceshortestpathsindirectedacyclicgraphs 655
24.1-5 ?
LetG .V;E/ be aweighted, directed graph with weight function w E R .
D W !
Givean O.VE/-time algorithm to find, for each vertex  V, the value ı ./

2 D
min ı.u;/ .
u V
2 f g
24.1-6 ?
Suppose that aweighted, directed graph G .V;E/has anegative-weight cycle.
D
Give an efficient algorithm to list the vertices of one such cycle. Prove that your
algorithm iscorrect.
24.2 Single-source shortestpaths indirected acyclicgraphs
By relaxing the edges of a weighted dag (directed acyclic graph) G .V;E/
D
according toatopological sortofitsvertices, wecancompute shortest paths from
asinglesourcein‚.V E/time. Shortestpathsarealwayswelldefinedinadag,
C
sinceeveniftherearenegative-weight edges, nonegative-weight cyclescanexist.
The algorithm starts by topologically sorting the dag (see Section 22.4) to im-
pose a linear ordering on the vertices. If the dag contains a path from vertex u to
vertex,thenuprecedes inthetopological sort. Wemakejustonepassoverthe
verticesinthetopologicallysortedorder. Asweprocesseachvertex,werelaxeach
edgethatleavesthevertex.
DAG-SHORTEST-PATHS.G;w;s/
1 topologically sorttheverticesofG
2 INITIALIZE-SINGLE-SOURCE.G;s/
3 foreachvertexu,takenintopologically sortedorder
4 foreachvertex G:AdjŒu
2
5 RELAX.u;;w/
Figure24.5showstheexecution ofthisalgorithm.
Therunningtimeofthisalgorithmiseasytoanalyze. AsshowninSection22.4,
the topological sort of line 1 takes ‚.V E/ time. The call of INITIALIZE-
C
SINGLE-SOURCE inline2takes ‚.V/time. Theforloopoflines3–5makesone
iterationpervertex. Altogether, theforloopoflines4–5relaxeseachedgeexactly
once. (We have used an aggregate analysis here.) Because each iteration of the
innerforlooptakes‚.1/time,thetotalrunningtimeis‚.V E/,whichislinear
C
inthesizeofanadjacency-list representation ofthegraph.
The following theorem shows that the DAG-SHORTEST-PATHS procedure cor-
rectlycomputes theshortest paths.
656 Chapter24 Single-SourceShortestPaths
6 1 6 1
r s t x y z r s t x y z
5 2 7 –1 –2 5 2 7 –1 –2
¥ 0 ¥ ¥ ¥ ¥ ¥ 0 ¥ ¥ ¥ ¥
3 4 2 3 4 2
(a) (b)
6 1 6 1
r s t x y z r s t x y z
5 2 7 –1 –2 5 2 7 –1 –2
¥ 0 2 6 ¥ ¥ ¥ 0 2 6 6 4
3 4 2 3 4 2
(c) (d)
6 1 6 1
r s t x y z r s t x y z
5 2 7 –1 –2 5 2 7 –1 –2
¥ 0 2 6 5 4 ¥ 0 2 6 5 3
3 4 2 3 4 2
(e) (f)
6 1
r s t x y z
5 2 7 –1 –2
¥ 0 2 6 5 3
3 4 2
(g)
Figure 24.5 The execution of the algorithm for shortest paths in a directed acyclic graph. The
vertices are topologically sorted from left to right. The source vertex is s. The d values appear
withinthevertices,andshadededgesindicatethevalues.(a)Thesituationbeforethefirstiteration
oftheforloopoflines3–5. (b)–(g)Thesituationaftereachiterationoftheforloopoflines3–5.
Thenewlyblackenedvertexineachiterationwasusedasuinthatiteration. Thevaluesshownin
part(g)arethefinalvalues.
Theorem24.5
If a weighted, directed graph G .V;E/ has source vertex s and no cycles, then
D
at the termination of the DAG-SHORTEST-PATHS procedure, :d ı.s;/ for all
D
vertices V,andthepredecessor subgraph G isashortest-paths tree.

2
Proof We first show that :d ı.s;/ for all vertices  V at termina-
D 2
tion. If  is not reachable from s, then :d ı.s;/ by the no-path
D D 1
property. Now, suppose that  is reachable from s, so that there is a short-
est path p  ; ;:::; , where  s and  . Because we pro-
0 1 k 0 k
D h i D D
24.2 Single-sourceshortestpathsindirectedacyclicgraphs 657
cess the vertices in topologically sorted order, we relax the edges on p in the
order . ; /;. ; /;:::;. ; /. The path-relaxation property implies that
0 1 1 2 k 1 k
 :d ı.s; / at termination (cid:0) for i 0;1;:::;k. Finally, by the predecessor-
i i
D D
subgraph property, G isashortest-paths tree.

An interesting application of this algorithm arises in determining critical paths
inPERTchart2 analysis. Edgesrepresent jobstobeperformed, andedge weights
represent the times required to perform particular jobs. If edge .u;/ enters ver-
tex andedge.;x/leaves,thenjob.u;/mustbeperformedbeforejob.;x/.
Apaththrough this dagrepresents asequence ofjobs thatmustbeperformed ina
particular order. A critical path is a longest path through the dag, corresponding
tothelongest timetoperform any sequence ofjobs. Thus, theweight ofacritical
path provides alowerbound onthetotal timetoperform allthejobs. Wecanfind
acriticalpathbyeither
 negatingtheedgeweightsandrunning DAG-SHORTEST-PATHS, or
 running DAG-SHORTEST-PATHS, with the modification that we replace “ ”
1
by “ ” in line 2 of INITIALIZE-SINGLE-SOURCE and “>” by “<” in the
(cid:0)1
RELAX procedure.
Exercises
24.2-1
Run DAG-SHORTEST-PATHS on the directed graph of Figure 24.5, using vertex r
asthesource.
24.2-2
Supposewechangeline3ofDAG-SHORTEST-PATHS toread
3 forthefirst V 1vertices, takenintopologically sortedorder
j j(cid:0)
Showthattheprocedure wouldremaincorrect.
24.2-3
The PERTchart formulation given above is somewhat unnatural. In a more natu-
ral structure, vertices would represent jobs and edges would represent sequencing
constraints;thatis,edge.u;/wouldindicatethatjobumustbeperformedbefore
job . We would then assign weights to vertices, not edges. Modify the DAG-
SHORTEST-PATHS procedure so that it finds a longest path in a directed acyclic
graphwithweightedverticesinlineartime.
2“PERT”isanacronymfor“programevaluationandreviewtechnique.”
658 Chapter24 Single-SourceShortestPaths
24.2-4
Giveanefficient algorithm tocountthetotalnumber ofpathsinadirected acyclic
graph. Analyzeyouralgorithm.
24.3 Dijkstra’s algorithm
Dijkstra’salgorithmsolvesthesingle-sourceshortest-pathsproblemonaweighted,
directedgraphG .V;E/forthecaseinwhichalledgeweightsarenonnegative.
D
Inthissection,therefore,weassumethatw.u;/ 0foreachedge.u;/ E. As
 2
weshallsee,withagoodimplementation, therunningtimeofDijkstra’salgorithm
islowerthanthatoftheBellman-Fordalgorithm.
Dijkstra’s algorithm maintains a set S of vertices whose final shortest-path
weights from the source s have already been determined. The algorithm repeat-
edlyselectsthevertexu V S withtheminimumshortest-pathestimate,addsu
2 (cid:0)
to S, and relaxes all edges leaving u. In the following implementation, we use a
min-priority queueQofvertices, keyedbytheird values.
DIJKSTRA.G;w;s/
1 INITIALIZE-SINGLE-SOURCE.G;s/
2 S
D ;
3 Q G:V
D
4 whileQ
¤;
5 u EXTRACT-MIN.Q/
D
6 S S u
D [f g
7 foreachvertex G:AdjŒu
2
8 RELAX.u;;w/
Dijkstra’s algorithm relaxes edges as shown in Figure 24.6. Line 1 initializes
the d and  values in the usual way, and line 2 initializes the set S to the empty
set. The algorithm maintains the invariant that Q V S at the start of each
D (cid:0)
iterationofthewhileloopoflines4–8. Line3initializesthemin-priorityqueueQ
tocontain allthevertices inV;since S atthattime, theinvariant istrue after
D ;
line3. Eachtimethroughthewhileloopoflines4–8,line5extractsavertexufrom
Q V S andline6addsittosetS,therebymaintainingtheinvariant. (Thefirst
D (cid:0)
time through this loop, u s.) Vertex u, therefore, has the smallest shortest-path
D
estimateofanyvertexinV S. Then,lines7–8relaxeachedge.u;/leaving u,
(cid:0)
thus updating the estimate :d and the predecessor : if we can improve the
shortest path to  found so far by going through u. Observe that the algorithm
never inserts vertices into Q after line 3 and that each vertex is extracted from Q
24.3 Dijkstra’salgorithm 659
t x t x t x
1 1 1
¥ ¥ 10 ¥ 8 14
10 10 10
9 9 9
2 3 4 6 2 3 4 6 2 3 4 6
s 0 s 0 s 0
7 7 7
5 5 5
¥ ¥ 5 ¥ 5 7
2 2 2
y z y z y z
(a) (b) (c)
t x t x t x
1 1 1
8 13 8 9 8 9
10 10 10
9 9 9
2 3 4 6 2 3 4 6 2 3 4 6
s 0 s 0 s 0
7 7 7
5 5 5
5 7 5 7 5 7
2 2 2
y z y z y z
(d) (e) (f)
Figure 24.6 The execution of Dijkstra’s algorithm. The source s is the leftmost vertex. The
shortest-path estimates appear within the vertices, and shaded edges indicate predecessor values.
BlackverticesareinthesetS,andwhiteverticesareinthemin-priorityqueueQ V S.(a)The
D (cid:0)
situationjustbeforethefirstiterationofthewhileloopoflines4–8.Theshadedvertexhasthemini-
mumd valueandischosenasvertexuinline5.(b)–(f)Thesituationaftereachsuccessiveiteration
ofthewhileloop.Theshadedvertexineachpartischosenasvertexuinline5ofthenextiteration.
Thed valuesandpredecessorsshowninpart(f)arethefinalvalues.
andaddedtoS exactlyonce,sothatthewhileloopoflines4–8iteratesexactly V
j j
times.
Because Dijkstra’s algorithm always chooses the “lightest” or “closest” vertex
inV S toaddtosetS,wesaythatitusesagreedystrategy. Chapter16explains
(cid:0)
greedy strategies in detail, but you need not have read that chapter to understand
Dijkstra’s algorithm. Greedystrategies donotalwaysyieldoptimalresults ingen-
eral,butasthefollowingtheoremanditscorollaryshow,Dijkstra’salgorithmdoes
indeedcomputeshortestpaths. Thekeyistoshowthateachtimeitaddsavertexu
tosetS,wehaveu:d ı.s;u/.
D
Theorem24.6(CorrectnessofDijkstra’salgorithm)
Dijkstra’s algorithm, run on a weighted, directed graph G .V;E/ with non-
D
negative weight function w and source s, terminates with u:d ı.s;u/ for all
D
verticesu V.
2
660 Chapter24 Single-SourceShortestPaths
u
p
2
S
s
p y
1
x
Figure24.7 TheproofofTheorem24.6.SetS isnonemptyjustbeforevertexuisaddedtoit.We
decomposeashortestpathpfromsources tovertexuintos
;p1
x y
;p2
u,wherey isthefirst
!
vertexonthepaththatisnotinS andx S immediatelyprecedesy. Verticesxandyaredistinct,
2
butwemayhaves xory u.Pathp2mayormaynotreentersetS.
D D
Proof Weusethefollowingloopinvariant:
At the start of each iteration of the while loop of lines 4–8, :d ı.s;/
D
foreachvertex S.
2
Itsufficestoshowforeachvertexu V,wehaveu:d ı.s;u/atthetimewhenu
2 D
is added to set S. Once we show that u:d ı.s;u/, we rely on the upper-bound
D
property toshowthattheequalityholdsatalltimesthereafter.
Initialization: Initially, S ,andsotheinvariant istriviallytrue.
D;
Maintenance: Wewishtoshowthatineachiteration,u:d ı.s;u/forthevertex
D
added to set S. For the purpose of contradiction, let u be the first vertex for
which u:d ı.s;u/ when it is added to set S. We shall focus our attention
¤
on the situation at the beginning of the iteration of the while loop in which u
is added to S and derive the contradiction that u:d ı.s;u/ at that time by
D
examining a shortest path from s to u. We must have u s because s is the
¤
firstvertex added to set S and s:d ı.s;s/ 0at that time. Because u s,
D D ¤
we also have that S just before u is added to S. There must be some
¤ ;
path from s to u, for otherwise u:d ı.s;u/ by the no-path property,
D D 1
which would violate our assumption that u:d ı.s;u/. Because there is at
¤
least one path, there is a shortest path p from s to u. Prior to adding u to S,
pathpconnectsavertexinS,namelys,toavertexinV S,namelyu. Letus
(cid:0)
consider the firstvertex y along p such that y V S, and let x S be y’s
2 (cid:0) 2
predecessoralongp. Thus,asFigure24.7illustrates,wecandecomposepathp
intos
;p1
x y
;p2
u. (Eitherofpathsp orp mayhavenoedges.)
1 2
!
We claim that y:d ı.s;y/ when u is added to S. To prove this claim, ob-
D
serve that x S. Then, because we chose u as the first vertex for which
2
u:d ı.s;u/ when itisadded toS, wehadx:d ı.s;x/ whenx wasadded
¤ D
24.3 Dijkstra’salgorithm 661
to S. Edge .x;y/ was relaxed at that time, and the claim follows from the
convergence property.
We can now obtain a contradiction to prove that u:d ı.s;u/. Because y
D
appears before u on a shortest path from s to u and all edge weights are non-
negative(notablythoseonpathp ),wehaveı.s;y/ ı.s;u/,andthus
2

y:d ı.s;y/
D
ı.s;u/ (24.2)

u:d (bytheupper-bound property) .

Butbecausebothverticesuandy wereinV S whenuwaschoseninline5,
(cid:0)
we have u:d y:d. Thus, the two inequalities in (24.2) are in fact equalities,

giving
y:d ı.s;y/ ı.s;u/ u:d:
D D D
Consequently, u:d ı.s;u/, which contradicts our choice of u. Weconclude
D
thatu:d ı.s;u/whenuisaddedtoS,andthatthisequality ismaintained at
D
alltimesthereafter.
Termination: Attermination, Q which, along withourearlier invariant that
D ;
Q V S,impliesthatS V. Thus,u:d ı.s;u/forallverticesu V.
D (cid:0) D D 2
Corollary24.7
If we run Dijkstra’s algorithm on a weighted, directed graph G .V;E/ with
D
nonnegative weight function w and source s, then at termination, the predecessor
subgraph G isashortest-paths treerootedats.

Proof ImmediatefromTheorem24.6andthepredecessor-subgraph property.
Analysis
How fast is Dijkstra’s algorithm? It maintains the min-priority queue Q by call-
ing three priority-queue operations: INSERT (implicit in line 3), EXTRACT-MIN
(line 5), and DECREASE-KEY (implicit in RELAX, whichiscalled inline 8). The
algorithm calls both INSERT and EXTRACT-MIN once per vertex. Because each
vertexu V isaddedtosetS exactlyonce,eachedgeintheadjacency listAdjŒu
2
is examined in the for loop of lines 7–8 exactly once during the course of the al-
gorithm. Since the total number of edges in all the adjacency lists is E , this for
j j
loop iterates atotal of E times, and thus the algorithm calls DECREASE-KEY at
j j
most E timesoverall. (Observeonceagainthatweareusingaggregateanalysis.)
j j
The running time of Dijkstra’s algorithm depends on how we implement the
min-priority queue. Consider first the case in which wemaintain the min-priority
662 Chapter24 Single-SourceShortestPaths
queue by taking advantage of the vertices being numbered 1 to V . We simply
j j
store:dinthethentryofanarray. EachINSERTandDECREASE-KEY operation
takes O.1/ time, and each EXTRACT-MIN operation takes O.V/ time (since we
havetosearchthroughtheentirearray),foratotaltimeofO.V2 E/ O.V2/.
C D
If the graph is sufficiently sparse—in particular, E o.V2=lgV/—we can
D
improvethealgorithm byimplementingthemin-priority queuewithabinarymin-
heap. (As discussed in Section 6.5, the implementation should make sure that
vertices and corresponding heap elements maintain handles to each other.) Each
EXTRACT-MIN operation then takes time O.lgV/. As before, there are V such
j j
operations. Thetimetobuildthebinarymin-heapisO.V/. EachDECREASE-KEY
operation takes time O.lgV/, and there are still at most E such operations. The
j j
totalrunningtimeisthereforeO..V E/lgV/,whichisO.ElgV/ifallvertices
C
are reachable from the source. This running time improves upon the straightfor-
wardO.V2/-timeimplementation ifE o.V2=lgV/.
D
We can in fact achieve a running time of O.V lgV E/ by implementing the
C
min-priority queue with a Fibonacci heap (see Chapter 19). The amortized cost
of each of the V EXTRACT-MIN operations is O.lgV/, and each DECREASE-
j j
KEY call, of which there are at most E , takes only O.1/ amortized time. His-
j j
torically, the development of Fibonacci heaps was motivated by the observation
that Dijkstra’s algorithm typically makes many more DECREASE-KEY calls than
EXTRACT-MIN calls, so that any method of reducing the amortized time of each
DECREASE-KEY operation to o.lgV/ without increasing the amortized time of
EXTRACT-MIN wouldyieldanasymptotically fasterimplementation thanwithbi-
naryheaps.
Dijkstra’s algorithm resembles both breadth-first search (see Section 22.2) and
Prim’s algorithm for computing minimum spanning trees (see Section 23.2). It is
like breadth-first search in that set S corresponds to the set of black vertices in a
breadth-first search; just as vertices in S have their final shortest-path weights, so
doblackverticesinabreadth-firstsearchhavetheircorrectbreadth-firstdistances.
Dijkstra’s algorithm is like Prim’s algorithm in that both algorithms use a min-
priorityqueuetofindthe“lightest”vertexoutsideagivenset(thesetS inDijkstra’s
algorithm and the tree being grown in Prim’s algorithm), add this vertex into the
set,andadjusttheweightsoftheremaining verticesoutsidethesetaccordingly.
Exercises
24.3-1
Run Dijkstra’s algorithm on the directed graph of Figure 24.2, first using vertex s
as the source and then using vertex ´ as the source. In the style of Figure 24.6,
show the d and values and thevertices insetS aftereach iteration ofthewhile
loop.
24.3 Dijkstra’salgorithm 663
24.3-2
Give a simple example of a directed graph with negative-weight edges for which
Dijkstra’s algorithm produces incorrect answers. Why doesn’t the proof of Theo-
rem24.6gothrough whennegative-weight edgesareallowed?
24.3-3
Supposewechangeline4ofDijkstra’salgorithm tothefollowing.
4 while Q > 1
j j
Thischangecausesthewhilelooptoexecute V 1timesinsteadof V times. Is
j j(cid:0) j j
thisproposed algorithm correct?
24.3-4
Professor Gaedel has written a program that he claims implements Dijkstra’s al-
gorithm. The program produces :d and : for each vertex  V. Give an
2
O.V E/-timealgorithmtochecktheoutputoftheprofessor’sprogram. Itshould
C
determine whetherthed and attributes matchthoseofsomeshortest-paths tree.
Youmayassumethatalledgeweightsarenonnegative.
24.3-5
Professor Newman thinks that he has worked out a simpler proof of correctness
for Dijkstra’s algorithm. He claims that Dijkstra’s algorithm relaxes the edges of
everyshortest path inthe graph intheorder inwhich theyappear onthe path, and
therefore the path-relaxation property applies to every vertex reachable from the
source. Show that the professor is mistaken by constructing a directed graph for
whichDijkstra’salgorithm couldrelaxtheedgesofashortest pathoutoforder.
24.3-6
Weare given adirected graph G .V;E/ onwhich each edge .u;/ E has an
D 2
associated value r.u;/,which isarealnumber intherange 0 r.u;/ 1that
 
represents the reliability of a communication channel from vertex u to vertex .
We interpret r.u;/ as the probability that the channel from u to  will not fail,
andweassumethattheseprobabilitiesareindependent. Giveanefficientalgorithm
tofindthemostreliablepathbetweentwogivenvertices.
24.3-7
Let G .V;E/ be a weighted, directed graph with positive weight function
D
w E 1;2;:::;W forsomepositive integer W,andassume thatnotwover-
W ! f g
ticeshave thesameshortest-path weights from source vertex s. Nowsuppose that
we define an unweighted, directed graph G .V V ;E / by replacing each
0 0 0
D [
edge .u;/ E with w.u;/ unit-weight edges in series. How many vertices
2
does G have? Now suppose that we run a breadth-first search on G . Show that
0 0
664 Chapter24 Single-SourceShortestPaths
the order in which the breadth-first search of G colors vertices in V black is the
0
sameastheorderinwhichDijkstra’salgorithm extractstheverticesofV fromthe
priority queuewhenitrunsonG.
24.3-8
Let G .V;E/ be a weighted, directed graph with nonnegative weight function
D
w E 0;1;:::;W forsomenonnegative integer W. Modify Dijkstra’s algo-
W ! f g
rithm to compute the shortest paths from agiven source vertex s in O.WV E/
C
time.
24.3-9
Modify your algorithm from Exercise 24.3-8 to run in O..V E/lgW/ time.
C
(Hint: How many distinct shortest-path estimates can there be in V S at any
(cid:0)
pointintime?)
24.3-10
Suppose thatwearegivenaweighted, directed graph G .V;E/inwhichedges
D
that leave the source vertex s may have negative weights, all other edge weights
are nonnegative, and there are no negative-weight cycles. Argue that Dijkstra’s
algorithm correctlyfindsshortestpathsfroms inthisgraph.
24.4 Difference constraints and shortestpaths
Chapter 29 studies the general linear-programming problem, in which wewish to
optimize alinear function subject toasetoflinearinequalities. Inthissection, we
investigate aspecial caseoflinear programming thatwereduce tofindingshortest
paths from a single source. We can then solve the single-source shortest-paths
problem that results by running the Bellman-Ford algorithm, thereby also solving
thelinear-programming problem.
Linearprogramming
In the general linear-programming problem, we are given an m n matrix A,

an m-vector b, and an n-vector c. We wish to find a vector x of n elements that
maximizes the objective function n c x subject tothemconstraints givenby
i 1 i i
Ax b. D
 P
Although the simplex algorithm, which is the focus of Chapter 29, does not
always run in time polynomial in the size of its input, there are other linear-
programmingalgorithmsthatdoruninpolynomialtime. Weofferheretworeasons
tounderstandthesetupoflinear-programming problems. First,ifweknowthatwe
24.4 Differenceconstraintsandshortestpaths 665
cancastagivenproblemasapolynomial-sized linear-programming problem,then
we immediately have a polynomial-time algorithm to solve the problem. Second,
faster algorithms exist for many special cases of linear programming. For exam-
ple,thesingle-pairshortest-pathproblem(Exercise24.4-4)andthemaximum-flow
problem(Exercise26.1-5)arespecialcasesoflinearprogramming.
Sometimeswedon’treallycareabouttheobjectivefunction;wejustwishtofind
any feasible solution, that is, any vector x that satisfies Ax b, or to determine

thatnofeasible solution exists. Weshallfocusononesuchfeasibility problem.
Systemsofdifferenceconstraints
Inasystemofdifferenceconstraints,eachrowofthelinear-programmingmatrixA
contains one 1 and one 1, and all other entries of A are 0. Thus, the constraints
(cid:0)
given by Ax b are a set of m difference constraints involving n unknowns, in

whicheachconstraint isasimplelinearinequality oftheform
x x b ;
j i k
(cid:0) 
where1 i;j n,i j,and1 k m.
  ¤  
Forexample,consider theproblem offindinga5-vectorx .x /thatsatisfies
i
D
1 1 0 0 0 0
(cid:0)
1 0 0 0 1 1
(cid:0) x (cid:0)
0 1 0 0 1 1 1
(cid:0) x
1 0 1 0 0 2 5
(cid:0) x :
1 0 0 1 0 3  4
(cid:0) 0 0 1 1 0 ˘ˇx 4  1˘
(cid:0) x (cid:0)
0 0 1 0 1 5 3
(cid:0) (cid:0)
0 0 0 1 1 3
(cid:0) (cid:0)
This problem is equivalent to finding values for the unknowns x ;x ;x ;x ;x ,
1 2 3 4 5
satisfying thefollowing8difference constraints:
x x 0, (24.3)
1 2
(cid:0) 
x x 1, (24.4)
1 5
(cid:0)  (cid:0)
x x 1, (24.5)
2 5
(cid:0) 
x x 5, (24.6)
3 1
(cid:0) 
x x 4, (24.7)
4 1
(cid:0) 
x x 1, (24.8)
4 3
(cid:0)  (cid:0)
x x 3, (24.9)
5 3
(cid:0)  (cid:0)
x x 3. (24.10)
5 4
(cid:0)  (cid:0)
666 Chapter24 Single-SourceShortestPaths
Onesolution tothisproblemisx . 5; 3;0; 1; 4/, whichyoucanverifydi-
D (cid:0) (cid:0) (cid:0) (cid:0)
rectlybycheckingeachinequality. Infact,thisproblemhasmorethanonesolution.
Another is x .0;2;5;4;1/. These two solutions are related: each component
0
D
of x is 5 larger than the corresponding component of x. This fact is not mere
0
coincidence.
Lemma24.8
Let x .x ;x ;:::;x / be a solution to a system Ax b of difference con-
1 2 n
D 
straints, and let d beany constant. Then x d .x d;x d;:::;x d/
1 2 n
C D C C C
isasolutiontoAx b aswell.

Proof For each x and x , we have .x d/ .x d/ x x . Thus, if x
i j j i j i
C (cid:0) C D (cid:0)
satisfiesAx b,sodoesx d.
 C
Systems of difference constraints occur in many different applications. For ex-
ample,theunknownsx maybetimesatwhicheventsaretooccur. Eachconstraint
i
states that at least a certain amount of time, or at most a certain amount of time,
mustelapse between twoevents. Perhaps theevents arejobstobeperformed dur-
ing the assembly of aproduct. Ifweapply anadhesive that takes 2hours to setat
timex andwehavetowaituntilitsetstoinstallapartattimex ,thenwehavethe
1 2
constraint thatx x 2or, equivalently, thatx x 2. Alternatively, we
2 1 1 2
 C (cid:0)  (cid:0)
might require that the part be installed after the adhesive has been applied but no
laterthanthetimethattheadhesivehassethalfway. Inthiscase,wegetthepairof
constraintsx x andx x 1or,equivalently, x x 0andx x 1.
2 1 2 1 1 2 2 1
  C (cid:0)  (cid:0) 
Constraintgraphs
We can interpret systems of difference constraints from a graph-theoretic point
of view. In a system Ax b of difference constraints, we view the m n
 
linear-programming matrix A as the transpose of an incidence matrix (see Exer-
cise 22.1-7) for agraph withn vertices and m edges. Each vertex  inthe graph,
i
for i 1;2;:::;n, corresponds to one of the n unknown variables x . Each di-
i
D
rected edge in the graph corresponds to one of the m inequalities involving two
unknowns.
Moreformally,givenasystemAx bofdifferenceconstraints,thecorrespond-

ingconstraintgraphisaweighted, directed graphG .V;E/,where
D
V  ; ;:::;
0 1 n
D f g
and
E . ; / x x b isaconstraint
i j j i k
D f W (cid:0)  g
. ; /;. ; /;. ; /;:::;. ; / :
0 1 0 2 0 3 0 n
[f g
24.4 Differenceconstraintsandshortestpaths 667
0 v
1
–5
0
–1 0
v
5
1
–4 –3
0 v
2
v 0
0
–3 –3 5
4
0 –1
–1 0 v
3
v
4
0
Figure24.8 Theconstraint graph corresponding to thesystem (24.3)–(24.10) of difference con-
straints. The value of ı.0; i/ appears in each vertex  i. One feasible solution to the system is
x . 5; 3;0; 1; 4/.
D (cid:0) (cid:0) (cid:0) (cid:0)
The constraint graph contains the additional vertex  , as we shall see shortly, to
0
guarantee thatthegraphhassomevertexwhichcanreachallothervertices. Thus,
the vertex set V consists of a vertex  for each unknown x , plus an additional
i i
vertex  . The edge set E contains an edge for each difference constraint, plus
0
an edge . ; / for each unknown x . If x x b is a difference constraint,
0 i i j i k
(cid:0) 
then the weight of edge . ; / is w. ; / b . The weight of each edge leav-
i j i j k
D
ing  is 0. Figure 24.8 shows the constraint graph for the system (24.3)–(24.10)
0
ofdifference constraints.
The following theorem shows that we can find a solution to a system of differ-
ence constraints by finding shortest-path weights in the corresponding constraint
graph.
Theorem24.9
Given a system Ax b of difference constraints, let G .V;E/ be the corre-
 D
sponding constraint graph. IfG containsnonegative-weight cycles,then
x .ı. ; /;ı. ; /;ı. ; /;:::;ı. ; // (24.11)
0 1 0 2 0 3 0 n
D
is a feasible solution for the system. If G contains a negative-weight cycle, then
thereisnofeasible solutionforthesystem.
Proof We first show that if the constraint graph contains no negative-weight
cycles, then equation (24.11) gives a feasible solution. Consider any edge
. ; / E. By the triangle inequality, ı. ; / ı. ; / w. ; / or,
i j 0 j 0 i i j
2  C
equivalently, ı. ; / ı. ; / w. ; /. Thus, letting x ı. ; / and
0 j 0 i i j i 0 i
(cid:0)  D
668 Chapter24 Single-SourceShortestPaths
x ı. ; / satisfies the difference constraint x x w. ; / that corre-
j 0 j j i i j
D (cid:0) 
sponds toedge. ; /.
i j
Nowweshowthatiftheconstraint graphcontainsanegative-weightcycle,then
the system of difference constraints has no feasible solution. Without loss of gen-
erality, let the negative-weight cycle be c  ;  ; :::;  , where   .
1 2 k 1 k
D h i D
(The vertex  cannot be on cycle c, because it has no entering edges.) Cycle c
0
corresponds tothefollowingdifferenceconstraints:
x x w. ; /;
2 1 1 2
(cid:0) 
x x w. ; /;
3 2 2 3
(cid:0) 
:
:
:
x x w. ; /;
k 1 k 2 k 2 k 1
(cid:0) (cid:0) (cid:0)  (cid:0) (cid:0)
x x w. ; /:
k k 1 k 1 k
(cid:0) (cid:0)  (cid:0)
Wewillassumethatxhasasolutionsatisfyingeachofthesekinequalitiesandthen
derive a contradiction. The solution must also satisfy the inequality that results
when we sum the k inequalities together. If we sum the left-hand sides, each
unknown x is added in once and subtracted out once (remember that  
i 1 k
D
implies x x ), so that the left-hand side of the sum is 0. The right-hand side
1 k
D
sumstow.c/,andthusweobtain0 w.c/. Butsincecisanegative-weightcycle,

w.c/< 0,andweobtainthecontradiction that0 w.c/< 0.

Solvingsystemsofdifferenceconstraints
Theorem 24.9 tells us that we can use the Bellman-Ford algorithm to solve a
system of difference constraints. Because the constraint graph contains edges
from the source vertex  to all other vertices, any negative-weight cycle in the
0
constraint graph is reachable from  . If the Bellman-Ford algorithm returns
0
TRUE, then the shortest-path weights give a feasible solution to the system. In
Figure 24.8, for example, the shortest-path weights provide the feasible solution
x . 5; 3;0; 1; 4/, andbyLemma24.8,x .d 5;d 3;d;d 1;d 4/
D (cid:0) (cid:0) (cid:0) (cid:0) D (cid:0) (cid:0) (cid:0) (cid:0)
isalsoafeasiblesolutionforanyconstantd. IftheBellman-Fordalgorithmreturns
FALSE, thereisnofeasible solution tothesystemofdifference constraints.
A system of difference constraints withm constraints on n unknowns produces
a graph with n 1 vertices and n m edges. Thus, using the Bellman-Ford
C C
algorithm, we can solve the system in O..n 1/.n m// O.n2 nm/ time.
C C D C
Exercise24.4-5asksyoutomodifythealgorithm toruninO.nm/time,evenifm
ismuchlessthann.
24.4 Differenceconstraintsandshortestpaths 669
Exercises
24.4-1
Findafeasiblesolutionordeterminethatnofeasiblesolutionexistsforthefollow-
ingsystemofdifference constraints:
x x 1,
1 2
(cid:0) 
x x 4,
1 4
(cid:0)  (cid:0)
x x 2,
2 3
(cid:0) 
x x 7,
2 5
(cid:0) 
x x 5,
2 6
(cid:0) 
x x 10,
3 6
(cid:0) 
x x 2,
4 2
(cid:0) 
x x 1,
5 1
(cid:0)  (cid:0)
x x 3,
5 4
(cid:0) 
x x 8.
6 3
(cid:0)  (cid:0)
24.4-2
Findafeasiblesolutionordeterminethatnofeasiblesolutionexistsforthefollow-
ingsystemofdifference constraints:
x x 4,
1 2
(cid:0) 
x x 5,
1 5
(cid:0) 
x x 6,
2 4
(cid:0)  (cid:0)
x x 1,
3 2
(cid:0) 
x x 3,
4 1
(cid:0) 
x x 5,
4 3
(cid:0) 
x x 10,
4 5
(cid:0) 
x x 4,
5 3
(cid:0)  (cid:0)
x x 8.
5 4
(cid:0)  (cid:0)
24.4-3
Cananyshortest-path weightfromthenewvertex inaconstraintgraphbeposi-
0
tive? Explain.
24.4-4
Expressthesingle-pair shortest-path problem asalinearprogram.
670 Chapter24 Single-SourceShortestPaths
24.4-5
Show how to modify the Bellman-Ford algorithm slightly so that when we use it
tosolveasystemofdifference constraints withminequalities onnunknowns, the
running timeisO.nm/.
24.4-6
Suppose that in addition to a system of difference constraints, we want to handle
equality constraints of the form x x b . Show how to adapt the Bellman-
i j k
D C
Fordalgorithm tosolvethisvarietyofconstraint system.
24.4-7
ShowhowtosolveasystemofdifferenceconstraintsbyaBellman-Ford-likealgo-
rithmthatrunsonaconstraint graphwithouttheextravertex .
0
24.4-8 ?
LetAx b beasystem ofmdifferenceconstraints innunknowns. Showthatthe

Bellman-Ford algorithm, when run on the corresponding constraint graph, maxi-
mizes n x subjecttoAx b andx 0forallx .
i 1 i  i  i
D
24.4-9P?
ShowthattheBellman-Fordalgorithm,whenrunontheconstraintgraphforasys-
temAx bofdifferenceconstraints,minimizesthequantity.max x min x /
i i
 f g(cid:0) f g
subject toAx b. Explain how this fact might come inhandy ifthe algorithm is

usedtoschedule construction jobs.
24.4-10
SupposethateveryrowinthematrixAofalinearprogramAx bcorrespondsto

adifferenceconstraint,asingle-variableconstraintoftheformx b ,orasingle-
i k

variable constraint of the form x b . Show how to adapt the Bellman-Ford
i k
(cid:0) 
algorithm tosolvethisvarietyofconstraint system.
24.4-11
Give an efficient algorithm to solve a system Ax b of difference constraints

when all of the elements of b are real-valued and all of the unknowns x must be
i
integers.
24.4-12 ?
Give an efficient algorithm to solve a system Ax b of difference constraints

when all of the elements of b are real-valued and a specified subset of some, but
notnecessarily all,oftheunknowns x mustbeintegers.
i
24.5 Proofsofshortest-pathsproperties 671
24.5 Proofs ofshortest-paths properties
Throughout this chapter, our correctness arguments have relied on the triangle
inequality, upper-bound property, no-path property, convergence property, path-
relaxationproperty,andpredecessor-subgraph property. Westatedtheseproperties
withoutproofatthebeginning ofthischapter. Inthissection, weprovethem.
Thetriangleinequality
In studying breadth-first search (Section 22.2), we proved as Lemma 22.1 a sim-
ple property of shortest distances in unweighted graphs. The triangle inequality
generalizes theproperty toweightedgraphs.
Lemma24.10(Triangleinequality)
Let G .V;E/ be a weighted, directed graph with weight function w E R
D W !
andsourcevertexs. Then,foralledges.u;/ E,wehave
2
ı.s;/ ı.s;u/ w.u;/:
 C
Proof Suppose that p is a shortest path from source s to vertex . Then p has
no more weight than any other path from s to . Specifically, path p has no more
weightthan the particular path that takes ashortest path from source s tovertexu
andthentakesedge.u;/.
Exercise 24.5-3 asks you to handle the case in which there is no shortest path
froms to.
Effectsofrelaxation onshortest-path estimates
Thenextgroupoflemmasdescribeshowshortest-pathestimatesareaffectedwhen
we execute a sequence of relaxation steps on the edges of a weighted, directed
graphthathasbeeninitialized by INITIALIZE-SINGLE-SOURCE.
Lemma24.11(Upper-boundproperty)
LetG .V;E/ be aweighted, directed graph with weight function w E R .
D W !
Let s V be the source vertex, and let the graph be initialized by INITIALIZE-
2
SINGLE-SOURCE.G;s/. Then, :d ı.s;/ for all  V, and this invariant is
 2
maintained over any sequence of relaxation steps on the edges of G. Moreover,
once:dachievesitslowerboundı.s;/,itneverchanges.
672 Chapter24 Single-SourceShortestPaths
Proof We prove the invariant :d ı.s;/ for all vertices  V by induction
 2
overthenumberofrelaxation steps.
Forthe basis, :d ı.s;/ is certainly true after initialization, since :d
 D 1
implies :d ı.s;/ for all  V s , and since s:d 0 ı.s;s/ (note that
 2 (cid:0)f g D 
ı.s;s/ ifs isonanegative-weight cycleand0otherwise).
D(cid:0)1
Fortheinductivestep,considertherelaxationofanedge.u;/. Bytheinductive
hypothesis, x:d ı.s;x/ for all x V prior to the relaxation. The only d value
 2
thatmaychangeis:d. Ifitchanges, wehave
:d u:d w.u;/
D C
ı.s;u/ w.u;/ (bytheinductivehypothesis)
 C
ı.s;/ (bythetriangleinequality) ,

andsotheinvariant ismaintained.
Tosee that thevalue of:d never changes once :d ı.s;/, note thathaving
D
achieved its lower bound, :d cannot decrease because we have just shown that
:d ı.s;/, and it cannot increase because relaxation steps do not increase d

values.
Corollary 24.12(No-pathproperty)
Suppose that in a weighted, directed graph G .V;E/ with weight function
w E R , no path connects a source vertex sD V to a given vertex  V.
W ! 2 2
Then, after the graph is initialized by INITIALIZE-SINGLE-SOURCE.G;s/, we
have :d ı.s;/ , and this equality is maintained as an invariant over
D D 1
anysequence ofrelaxation stepsontheedgesofG.
Proof By the upper-bound property, we always have ı.s;/ :d, and
1 D 
thus:d ı.s;/.
D1 D
Lemma24.13
LetG .V;E/ bea weighted, directed graph withweight function w E R ,
D W !
and let .u;/ E. Then, immediately after relaxing edge .u;/ by executing
2
RELAX.u;;w/,wehave:d u:d w.u;/.
 C
Proof If, just prior to relaxing edge .u;/, we have :d > u:d w.u;/, then
C
:d u:d w.u;/ afterward. If, instead, :d u:d w.u;/ just before
D C  C
the relaxation, then neither u:d nor :d changes, and so :d u:d w.u;/
 C
afterward.
Lemma24.14(Convergenceproperty)
Let G .V;E/ be a weighted, directed graph with weight function w E R ,
let s D V be a source vertex, and let s ; u  be a shortest pathW in G! for
2 !
24.5 Proofsofshortest-pathsproperties 673
some vertices u; V. Suppose that G is initialized by INITIALIZE-SINGLE-
2
SOURCE.G;s/ and then a sequence of relaxation steps that includes the call
RELAX.u;;w/ is executed on the edges of G. If u:d ı.s;u/ at any time
D
priortothecall,then:d ı.s;/atalltimesafterthecall.
D
Proof By the upper-bound property, if u:d ı.s;u/ at some point prior to re-
D
laxing edge .u;/, then this equality holds thereafter. In particular, after relaxing
edge.u;/,wehave
:d u:d w.u;/ (byLemma24.13)
 C
ı.s;u/ w.u;/
D C
ı.s;/ (byLemma24.1) .
D
By the upper-bound property, :d ı.s;/, from which we conclude that

:d ı.s;/,andthisequalityismaintained thereafter.
D
Lemma24.15(Path-relaxation property)
LetG .V;E/ be aweighted, directed graph with weight function w E R ,
D W !
andlets V beasource vertex. Consider anyshortest path p  ; ;:::;
0 1 k
2 D h i
from s 
0
to k. IfG is initialized by INITIALIZE-SINGLE-SOURCE.G;s/ and
D
thenasequenceofrelaxationstepsoccursthatincludes,inorder,relaxingtheedges
. ; /;. ; /;:::;. ; /, then  :d ı.s; / after these relaxations and
0 1 1 2 k 1 k k k
(cid:0) D
at all times afterward. This property holds no matter what other edge relaxations
occur,including relaxations thatareintermixedwithrelaxations oftheedgesofp.
Proof Weshowbyinductionthataftertheithedgeofpathp isrelaxed,wehave
 :d ı.s; /. Forthebasis,i 0,andbeforeanyedgesofp havebeenrelaxed,
i i
D D
wehavefromtheinitialization that :d s:d 0 ı.s;s/. Bytheupper-bound
0
D D D
property, thevalueofs:d neverchanges afterinitialization.
For the inductive step, we assume that  :d ı.s; /, and we examine
i 1 i 1
what happens when we relax edge . ; /.(cid:0) By tD he conver(cid:0) gence property, after
i 1 i
relaxing this edge, we have  :d ı.(cid:0)s; /, and this equality is maintained at all
i i
D
timesthereafter.
Relaxation andshortest-pathstrees
Wenowshow that once asequence ofrelaxations has caused the shortest-path es-
timatestoconvergetoshortest-pathweights,thepredecessorsubgraphG induced

by the resulting  values is a shortest-paths tree for G. We start with the follow-
inglemma,whichshowsthatthepredecessor subgraph alwaysformsarootedtree
whoserootisthesource.
674 Chapter24 Single-SourceShortestPaths
Lemma24.16
LetG .V;E/ bea weighted, directed graph withweight function w E R ,
D W !
let s V be a source vertex, and assume that G contains no negative-weight
2
cyclesthatarereachablefroms. Then,afterthegraphisinitializedbyINITIALIZE-
SINGLE-SOURCE.G;s/, the predecessor subgraph G

forms a rooted tree with
roots,andanysequence ofrelaxation stepsonedgesofG maintains thisproperty
asaninvariant.
Proof Initially, theonlyvertex inG isthesource vertex, andthelemmaistriv-

ially true. Consider a predecessor subgraph G that arises after a sequence of

relaxation steps. We shall first prove that G is acyclic. Suppose for the sake of

contradiction thatsomerelaxationstepcreatesacycleinthegraphG . Letthecy-

clebec  ; ;:::; ,where  . Then, :  fori 1;2;:::;k
0 1 k k 0 i i 1
and,withD ouh tlossofgenerai lity,wecanD assumethatrelaxiD nged(cid:0) ge. D ; /created
k 1 k
thecycleinG . (cid:0)

We claim that all vertices on cycle c are reachable from the source s. Why?
Eachvertexonc hasanon-NIL predecessor, andsoeachvertexonc wasassigned
a finite shortest-path estimate when it was assigned its non-NIL  value. By the
upper-bound property, each vertex on cycle c has a finite shortest-path weight,
whichimpliesthatitisreachable froms.
We shall examine the shortest-path estimates on c just prior to the call
RELAX.
k
1; k;w/ and show that c is a negative-weight cycle, thereby contra-
dictingthe(cid:0) assumptionthatGcontainsnonegative-weightcyclesthatarereachable
fromthesource. Justbeforethecall,wehave :  fori 1;2;:::;k 1.
i i 1
Thus, for i 1;2;:::;k 1, the last update toD  :(cid:0) d was bD y the assignm(cid:0) ent
i
D (cid:0)
 :d  :d w. ; /. If :dchangedsincethen,itdecreased. Therefore,
i i 1 i 1 i i 1
justbD efore(cid:0) theC call RE(cid:0) LAX.
k
1;(cid:0) k;w/,wehave
(cid:0)
 :d  :d w. ; / foralli 1;2;:::;k 1: (24.12)
i i 1 i 1 i
 (cid:0) C (cid:0) D (cid:0)
Because  : is changed by the call, immediately beforehand we also have the
k
strictinequality
 :d >  :d w. ; /:
k k 1 k 1 k
(cid:0) C (cid:0)
Summing this strict inequality with the k 1 inequalities (24.12), we obtain the
(cid:0)
sumoftheshortest-path estimatesaroundcyclec:
k k
 :d > . :d w. ; //
i i 1 i 1 i
(cid:0) C (cid:0)
i 1 i 1
XD XD
k k
 :d w. ; /:
i 1 i 1 i
D (cid:0) C (cid:0)
i 1 i 1
XD XD
24.5 Proofsofshortest-pathsproperties 675
x
z
s v
u
y
Figure24.9 ShowingthatasimplepathinG fromsourcestovertexisunique.Iftherearetwo
pathsp1(s ; u ; x ´ ; )andp2(s ; u ; y ´ ; ),wherex y,then´: x
! ! ¤ D
and´: y,acontradiction.
D
But
k k
 :d  :d ;
i i 1
D (cid:0)
i 1 i 1
XD XD
since each vertex in the cycle c appears exactly once in each summation. This
equalityimplies
k
0> w. ; /:
i 1 i
(cid:0)
i 1
XD
Thus,thesumofweightsaroundthecyclecisnegative,whichprovidesthedesired
contradiction.
Wehave nowproven thatG isadirected, acyclic graph. Toshow thatitforms

a rooted tree with root s, it suffices (see Exercise B.5-2) to prove that for each
vertex V ,thereisauniquesimplepathfroms to inG .
 
2
We first must show that a path from s exists for each vertex in V . The ver-

tices in V

are those with non-NIL  values, plus s. The idea here is to prove by
induction that a path exists from s to all vertices in V . We leave the details as

Exercise24.5-6.
To complete the proof of the lemma, we must now show that for any vertex
 V ,thegraphG containsatmostonesimplepathfromsto. Supposeother-
 
2
wise. Thatis,supposethat,asFigure24.9illustrates,G containstwosimplepaths

from s to some vertex : p , which we decompose into s ; u ; x ´; ,
1
andp ,whichwedecompose intos ; u; y ´; ,wherex y! (thoughu
2
! ¤
could be s and ´ could be ). But then, ´: x and ´: y, which implies
D D
thecontradiction that x y. Weconclude that G contains aunique simple path

D
froms to,andthusG formsarootedtreewithroots.

Wecannowshowthatif,afterwehaveperformedasequenceofrelaxationsteps,
allvertices havebeen assigned their trueshortest-path weights, thenthepredeces-
sorsubgraph G isashortest-paths tree.

676 Chapter24 Single-SourceShortestPaths
Lemma24.17(Predecessor-subgraph property)
LetG .V;E/ bea weighted, directed graph withweight function w E R ,
D W !
lets V beasourcevertex,andassumethatGcontainsnonegative-weightcycles
2
thatarereachablefroms. LetuscallINITIALIZE-SINGLE-SOURCE.G;s/andthen
executeanysequenceofrelaxationstepsonedgesofGthatproduces:d ı.s;/
D
for all  V. Then, the predecessor subgraph G is a shortest-paths tree rooted

2
ats.
Proof We must prove that the three properties of shortest-paths trees given on
page 647 hold forG . Toshow the firstproperty, wemustshow that V isthe set
 
of vertices reachable from s. By definition, a shortest-path weight ı.s;/ is finite
ifandonlyif isreachable froms,andthustheverticesthatarereachable froms
areexactly thosewithfinited values. Butavertex V s hasbeenassigned
2 (cid:0)f g
afinitevaluefor:difandonlyif: NIL. Thus,theverticesinV

areexactly
¤
thosereachable froms.
Thesecondproperty followsdirectly fromLemma24.16.
It remains, therefore, toprove the last property ofshortest-paths trees: for each
vertex  V , the unique simple path s
;p
 in G is ashortest path from s to 
 
2
in G. Let p  ; ;:::; , where  s and  . For i 1;2;:::;k,
0 1 k 0 k
D h i D D D
we have both  :d ı.s; / and  :d  :d w. ; /, from which we
i i i i 1 i 1 i
conclude w. ;D / ı.s; / ı.s; /. (cid:0) SummC ing the(cid:0) weights along path p
i 1 i i i 1
(cid:0)  (cid:0) (cid:0)
yields
k
w.p/ w. ; /
i 1 i
D (cid:0)
i 1
XD
k
.ı.s; / ı.s; //
i i 1
 (cid:0) (cid:0)
i 1
XD
ı.s; / ı.s; / (because thesumtelescopes)
k 0
D (cid:0)
ı.s; / (because ı.s; / ı.s;s/ 0) .
k 0
D D D
Thus, w.p/ ı.s; /. Since ı.s; / is a lowerbound on the weight ofany path
k k

from s to  , we conclude that w.p/ ı.s; /, and thus p is a shortest path
k k
D
froms to  .
k
D
Exercises
24.5-1
Give two shortest-paths trees for the directed graph of Figure 24.2 (on page 648)
otherthanthetwoshown.
24.5 Proofsofshortest-pathsproperties 677
24.5-2
Give an example of a weighted, directed graph G .V;E/ with weight function
w E R and source vertex s such that G satisfiD es the following property: For
W !
everyedge.u;/ E,thereisashortest-paths treerootedats thatcontains.u;/
2
andanothershortest-paths treerootedats thatdoesnotcontain.u;/.
24.5-3
Embellish the proof of Lemma 24.10 to handle cases in which shortest-path
weightsare or .
1 (cid:0)1
24.5-4
Let G .V;E/ be a weighted, directed graph with source vertex s, and let G
D
be initialized by INITIALIZE-SINGLE-SOURCE.G;s/. Provethat ifa sequence of
relaxation steps sets s: to a non-NIL value, then G contains a negative-weight
cycle.
24.5-5
LetG .V;E/beaweighted, directed graphwithnonegative-weight edges. Let
D
s V be the source vertex, and suppose that we allow : to be the predecessor
2
of  on any shortest path to  from source s if  V s is reachable from s,
2 (cid:0)f g
and NIL otherwise. Give an example of such a graph G and an assignment of 
valuesthatproduces acycleinG . (ByLemma24.16,suchanassignment cannot

beproduced byasequence ofrelaxation steps.)
24.5-6
Let G .V;E/ be a weighted, directed graph with weight function w E R
D W !
andnonegative-weightcycles. Lets V bethesourcevertex,andletG beinitial-
2
ized by INITIALIZE-SINGLE-SOURCE.G;s/. Prove that for every vertex  V ,
2
there exists a path from s to  in G and that this property is maintained as an

invariant overanysequence ofrelaxations.
24.5-7
Let G .V;E/ be a weighted, directed graph that contains no negative-weight
D
cycles. Let s V be the source vertex, and let G be initialized by INITIALIZE-
2
SINGLE-SOURCE.G;s/. Prove that there exists a sequence of V 1 relaxation
j j(cid:0)
stepsthatproduces :d ı.s;/forall V.
D 2
24.5-8
LetG beanarbitraryweighted,directedgraphwithanegative-weightcyclereach-
ablefromthesourcevertexs. Showhowtoconstructaninfinitesequenceofrelax-
ations oftheedges ofG such thatevery relaxation causes ashortest-path estimate
tochange.
678 Chapter24 Single-SourceShortestPaths
Problems
24-1 Yen’simprovementtoBellman-Ford
Suppose that we order the edge relaxations in each pass of the Bellman-Ford al-
gorithm as follows. Before the first pass, we assign an arbitrary linear order
 ; ;:::; to the vertices of the input graph G .V;E/. Then, we parti-
1 2 V
tion the edgej j set E into E E , where E .D ; / E i < j and
f b f i j
[ D f 2 W g
E . ; / E i > j . (AssumethatG containsnoself-loops, sothatevery
b i j
D f 2 W g
edgeisineitherE orE .) DefineG .V;E /andG .V;E /.
f b f f b b
D D
a. Prove that G is acyclic with topological sort  ; ;:::; and that G is
f 1 2 V b
acyclicwithtopological sort  ; ;:::;h . j ji
V V 1 1
h j j j j(cid:0) i
Suppose that we implement each pass of the Bellman-Ford algorithm in the fol-
lowingway. Wevisiteachvertexintheorder ; ;:::; ,relaxingedgesofE
1 2 V f
that leave the vertex. We then visit each vertex in the oj rdj er  ; ;:::; ,
V V 1 1
relaxing edgesofE thatleavethevertex. j j j j(cid:0)
b
b. Prove that with this scheme, if G contains no negative-weight cycles that are
reachable from the source vertex s, then after only V =2 passes over the
dj j e
edges,:d ı.s;/forallvertices V.
D 2
c. Does this scheme improve the asymptotic running time of the Bellman-Ford
algorithm?
24-2 Nestingboxes
A d-dimensional box with dimensions .x ;x ;:::;x / nests within another box
1 2 d
with dimensions .y ;y ;:::;y / if there exists a permutation  on 1;2;:::;d
1 2 d
f g
suchthatx < y ,x < y ,...,x < y .
.1/ 1 .2/ 2 .d/ d
a. Arguethatthenestingrelation istransitive.
b. Describe an efficient method to determine whether or not one d-dimensional
boxnestsinsideanother.
c. Suppose that you are given a set of n d-dimensional boxes B ;B ;:::;B .
1 2 n
f g
Give an efficient algorithm to find the longest sequence B ;B ;:::;B of
h
i1 i2 iki
boxes such that B nests within B for j 1;2;:::;k 1. Express the
ij ijC1
D (cid:0)
runningtimeofyouralgorithm intermsofnandd.
ProblemsforChapter24 679
24-3 Arbitrage
Arbitrage is the use of discrepancies in currency exchange rates to transform one
unit of a currency into more than one unit of the same currency. For example,
suppose that 1 U.S. dollar buys 49 Indian rupees, 1 Indian rupee buys 2 Japanese
yen,and1Japaneseyenbuys0:0107U.S.dollars. Then,byconverting currencies,
atradercanstartwith1U.S.dollarandbuy49 2 0:0107 1:0486U.S.dollars,
  D
thusturningaprofitof4:86percent.
Suppose that we are given n currencies c ;c ;:::;c and an n n table R of
1 2 n

exchangerates,suchthatoneunitofcurrencyc buysRŒi;junitsofcurrencyc .
i j
a. Giveanefficientalgorithm todetermine whetherornotthereexistsasequence
ofcurrencies c ;c ;:::;c suchthat
h
i1 i2 iki
RŒi ;i  RŒi ;i  RŒi ;i  RŒi ;i  > 1:
1 2 2 3 k 1 k k 1
  (cid:0) 
Analyzetherunningtimeofyouralgorithm.
b. Give an efficient algorithm to print out such asequence if one exists. Analyze
therunning timeofyouralgorithm.
24-4 Gabow’sscalingalgorithmforsingle-source shortestpaths
A scaling algorithm solves a problem by initially considering only the highest-
order bit of each relevant input value (such as an edge weight). It then refines the
initial solution by looking at the two highest-order bits. It progressively looks at
more and more high-order bits, refining the solution each time, until it has exam-
inedallbitsandcomputedthecorrectsolution.
Inthisproblem,weexamineanalgorithmforcomputingtheshortestpathsfrom
asinglesourcebyscalingedgeweights. WearegivenadirectedgraphG .V;E/
D
with nonnegative integer edge weights w. Let W max w.u;/ . Our
.u;/ E
goal is to develop an algorithm that runs in O.ElgD W/ time. W2 ef assume tg hat all
verticesarereachable fromthesource.
Thealgorithm uncoversthebitsinthebinaryrepresentation oftheedgeweights
oneatatime,from themostsignificant bittotheleastsignificant bit. Specifically,
let k lg.W 1/ be the number of bits in the binary representation of W,
D d C e
and for i 1;2;:::;k, let w .u;/ w.u;/=2k i . That is, w .u;/ is the
i (cid:0) i
D D
“scaled-down” version of w.u;/ given by the i most significant bits of w.u;/.
 ˘
(Thus, w .u;/ w.u;/ for all .u;/ E.) For example, if k 5 and
k
D 2 D
w.u;/ 25, which has the binary representation 11001 , then w .u;/
3
D h i D
110 6. As another example with k 5, if w.u;/ 00100 4, then
h i D D D h i D
w .u;/ 001 1. Let us define ı .u;/ as the shortest-path weight from
3 i
Dh i D
vertex u to vertex  using weight function w . Thus, ı .u;/ ı.u;/ for all
i k
D
u; V. For a given source vertex s, the scaling algorithm first computes the
2
680 Chapter24 Single-SourceShortestPaths
shortest-path weights ı .s;/ for all V, then computes ı .s;/ forall V,
1 2
2 2
and so on, until it computes ı .s;/ for all  V. We assume throughout that
k
2
E V 1, and weshall see that computing ı from ı takes O.E/ time, so
i i 1
j thaj t thej enj t(cid:0) irealgorithm takesO.kE/ O.ElgW/time.(cid:0)
D
a. Suppose that for all vertices  V, wehave ı.s;/ E . Show that wecan
2  j j
computeı.s;/forall V inO.E/time.
2
b. Showthatwecancomputeı .s;/forall V inO.E/time.
1
2
Letusnowfocusoncomputing ı fromı .
i i 1
(cid:0)
c. Prove that for i 2;3;:::;k, we have either w .u;/ 2w .u;/ or
i i 1
w .u;/ 2w D .u;/ 1. Then,provethat D (cid:0)
i i 1
D (cid:0) C
2ı .s;/ ı .s;/ 2ı .s;/ V 1
i 1 i i 1
(cid:0)   (cid:0) Cj j(cid:0)
forall V.
2
d. Definefori 2;3;:::;k andall.u;/ E,
D 2
w .u;/ w .u;/ 2ı .s;u/ 2ı .s;/:
i i i 1 i 1
y D C (cid:0) (cid:0) (cid:0)
Provethatfori 2;3;:::;kandallu; V,the“reweighted”valuew .u;/
i
D 2 y
ofedge.u;/isanonnegative integer.
e. Now, define ı .s;/ as the shortest-path weight from s to  using the weight
yi
function w . Provethatfori 2;3;:::;k andall V,
i
y D 2
ı .s;/ ı .s;/ 2ı .s;/
i
D
yi
C
i (cid:0)1
andthatı .s;/ E .
yi
 j j
f. Showhowtocomputeı .s;/fromı .s;/forall V inO.E/time,and
i i 1
conclude thatwecancomputeı.s;/f(cid:0) orall V inO2 .ElgW/time.
2
24-5 Karp’sminimummean-weightcyclealgorithm
Let G .V;E/ be a directed graph with weight function w E R , and let
D W !
n V . Wedefinethemeanweightofacyclec e ;e ;:::;e ofedgesinE
1 2 k
D j j Dh i
tobe
k
1
.c/ w.e /:
i
D k
i 1
XD
ProblemsforChapter24 681
Let min .c/,wherecrangesoveralldirectedcyclesinG. Wecallacyclec
 c
D
for which .c/  a minimum mean-weight cycle. This problem investigates

D
anefficientalgorithm forcomputing .

Assume without loss of generality that every vertex  V is reachable from a
2
sourcevertexs V. Letı.s;/betheweightofashortestpathfromsto,andlet
2
ı .s;/ betheweight ofashortest path from s to consisting ofexactly k edges.
k
Ifthereisnopathfroms to withexactlyk edges, thenı .s;/ .
k
D1
a. Showthatif 0,thenG contains nonegative-weight cyclesandı.s;/

D D
min ı .s;/forallvertices V.
0 k n 1 k
  (cid:0) 2
b. Showthatif 0,then

D
ı .s;/ ı .s;/
n k
max (cid:0) 0
0 k n 1 n k 
  (cid:0) (cid:0)
forallvertices V. (Hint:Usebothproperties frompart(a).)
2
c. Let c be a 0-weight cycle, and let u and  be any two vertices on c. Suppose
that 0andthattheweightofthesimplepathfrom uto along thecycle

D
is x. Prove that ı.s;/ ı.s;u/ x. (Hint: The weight of the simple path
D C
from toualongthecycleis x.)
(cid:0)
d. Show that if  0, then on each minimum mean-weight cycle there exists a

D
vertex suchthat
ı .s;/ ı .s;/
max n (cid:0) k 0:
0 k n 1 n k D
  (cid:0) (cid:0)
(Hint: Show how to extend ashortest path to any vertex on aminimum mean-
weight cycle along the cycle to make a shortest path to the next vertex on the
cycle.)
e. Showthatif 0,then

D
ı .s;/ ı .s;/
n k
min max (cid:0) 0:
 V 0 k n 1 n k D
2   (cid:0) (cid:0)
f. Show that if we add a constant t to the weight of each edge of G, then 

increases byt. Usethisfacttoshowthat
ı .s;/ ı .s;/
 min max n (cid:0) k :

D  V 0 k n 1 n k
2   (cid:0) (cid:0)
g. GiveanO.VE/-timealgorithm tocompute .

682 Chapter24 Single-SourceShortestPaths
24-6 Bitonicshortestpaths
A sequence is bitonic if it monotonically increases and then monotonically de-
creases, orifbyacircular shiftitmonotonically increases andthenmonotonically
decreases. Forexample the sequences 1;4;6;8;3; 2 , 9;2; 4; 10; 5 , and
h (cid:0) i h (cid:0) (cid:0) (cid:0) i
1;2;3;4 arebitonic, but 1;3;12;4;2;10 isnotbitonic. (SeeProblem15-3for
h i h i
thebitoniceuclidean traveling-salesman problem.)
Suppose that we are given a directed graph G .V;E/ with weight function
w E R ,wherealledgeweightsareunique, anD dwewishtofindsingle-source
W !
shortest paths from a source vertex s. We are given one additional piece of infor-
mation: for each vertex  V, the weights of the edges along any shortest path
2
froms to formabitonicsequence.
Givethemostefficient algorithm you cantosolve thisproblem, and analyze its
running time.
Chapter notes
Dijkstra’salgorithm[88]appearedin1959,butitcontainednomentionofapriority
queue. The Bellman-Ford algorithm is based on separate algorithms by Bellman
[38]andFord[109]. Bellmandescribes therelation ofshortest pathstodifference
constraints. Lawler [224] describes the linear-time algorithm for shortest paths in
adag,whichheconsiders partofthefolklore.
When edge weights arerelatively small nonnegative integers, wehave moreef-
ficientalgorithms tosolvethesingle-source shortest-paths problem. Thesequence
of values returned by the EXTRACT-MIN calls in Dijkstra’s algorithm monoton-
ically increases over time. As discussed in the chapter notes for Chapter 6, in
this case several data structures can implement the various priority-queue opera-
tions more efficiently than a binary heap or a Fibonacci heap. Ahuja, Mehlhorn,
Orlin, and Tarjan [8] give an algorithm that runs in O.E V lgW/ time on
C
graphs withnonnegative edgeweights, where W isthelargest weight ofanyedge
p
in the graph. The best bounds are by Thorup [337], who gives an algorithm that
runs inO.ElglgV/time, and byRaman [291], who gives analgorithm that runs
in O E V min .lgV/1=3 ;.lgW/1=4  time. These two algorithms use an
C C
C
amount of space that depends on the word size of the underlying machine. Al-
(cid:0) ˚ (cid:9)
though theamount ofspace used can be unbounded in the size ofthe input, itcan
bereducedtobelinearinthesizeoftheinputusingrandomized hashing.
Forundirected graphs withinteger weights, Thorup [336]gives an O.V E/-
C
timealgorithm forsingle-source shortest paths. Incontrast tothealgorithms men-
tioned in theprevious paragraph, this algorithm isnot animplementation ofDijk-
NotesforChapter24 683
stra’s algorithm, since the sequence of values returned by EXTRACT-MIN calls
doesnotmonotonically increase overtime.
For graphs with negative edge weights, an algorithm due to Gabow and Tar-
jan [122] runs in O.pVElg.VW// time, and one by Goldberg [137] runs in
O.pVElgW/time,whereW max w.u;/ .
.u;/ E
D 2 fj jg
Cherkassky, Goldberg, and Radzik [64] conducted extensive experiments com-
paringvariousshortest-path algorithms.
25 All-Pairs Shortest Paths
Inthischapter, weconsidertheproblemoffindingshortestpathsbetweenallpairs
ofvertices inagraph. Thisproblem mightariseinmaking atableofdistances be-
tweenallpairsofcitiesforaroadatlas. AsinChapter24,wearegivenaweighted,
directed graph G .V;E/ with a weight function w E R that maps edges
D W !
to real-valued weights. We wish to find, for every pair of vertices u; V, a
2
shortest (least-weight) path from u to , where the weight of a path is the sum of
the weights ofitsconstituent edges. Wetypically wanttheoutput intabular form:
theentryinu’srowand’scolumnshouldbetheweightofashortestpathfromu
to.
We can solve an all-pairs shortest-paths problem by running a single-source
shortest-paths algorithm V times, once for each vertex as the source. If all
j j
edge weights are nonnegative, we can use Dijkstra’s algorithm. If we use
the linear-array implementation of the min-priority queue, the running time is
O.V3 VE/ O.V3/. Thebinarymin-heapimplementation ofthemin-priority
C D
queueyieldsarunning timeofO.VElgV/,whichisanimprovementifthegraph
issparse. Alternatively,wecanimplementthemin-priorityqueuewithaFibonacci
heap,yielding arunning timeofO.V2lgV VE/.
C
If thegraph has negative-weight edges, wecannot use Dijkstra’s algorithm. In-
stead,wemustruntheslowerBellman-Fordalgorithmoncefromeachvertex. The
resultingrunningtimeisO.V2E/,whichonadensegraphisO.V4/. Inthischap-
ter we shall see how to do better. We also investigate the relation of the all-pairs
shortest-paths problem tomatrixmultiplication andstudyitsalgebraicstructure.
Unlike the single-source algorithms, which assume an adjacency-list represen-
tation of the graph, most of the algorithms in this chapter use an adjacency-
matrix representation. (Johnson’s algorithm for sparse graphs, in Section 25.3,
uses adjacency lists.) Forconvenience, we assume that the vertices are numbered
1;2;:::; V ,sothattheinputisann nmatrixW representing theedgeweights
j j 
ofann-vertexdirectedgraphG .V;E/. Thatis,W .w /,where
ij
D D
Chapter25 All-PairsShortestPaths 685
0 ifi j ;
D
w theweightofdirected edge.i;j/ ifi j and.i;j/ E ; (25.1)
ij
D ¤ 2
ifi j and.i;j/ E :
 1 ¤ 62
We allow negative-weight edges, but we assume for the time being that the input
graphcontains nonegative-weight cycles.
The tabular output of the all-pairs shortest-paths algorithms presented in this
chapter is an n n matrix D .d /, where entry d contains the weight of a
ij ij
 D
shortestpathfromvertexi tovertexj. Thatis,ifweletı.i;j/denotetheshortest-
path weight from vertex i to vertex j (as in Chapter 24), then d ı.i;j/ at
ij
D
termination.
To solve the all-pairs shortest-paths problem on an input adjacency matrix, we
need to compute not only the shortest-path weights but also apredecessor matrix
… . ij/, where  ij is NIL if either i j or there is no path from i to j,
D D
and otherwise  is the predecessor of j on some shortest path from i. Just as
ij
the predecessor subgraph G from Chapter 24 is a shortest-paths tree for a given

source vertex, the subgraph induced by the ith row of the … matrix should be a
shortest-paths tree with root i. For each vertex i V, we define the predecessor
2
subgraphofG fori asG .V ;E /,where
;i ;i ;i
D
V ;i j V  ij NIL i
D f 2 W ¤ g[f g
and
E . ;j/ j V i :
;i ij ;i
D f W 2 (cid:0)f gg
If G is a shortest-paths tree, then the following procedure, which is a modified
;i
versionofthePRINT-PATH procedurefromChapter22,printsashortestpathfrom
vertexi tovertexj.
PRINT-ALL-PAIRS-SHORTEST-PATH.…;i;j/
1 ifi ==j
2 printi
3 elseif ij == NIL
4 print“nopathfrom”i “to”j “exists”
5 else PRINT-ALL-PAIRS-SHORTEST-PATH.…;i; ij/
6 printj
Inordertohighlighttheessentialfeaturesoftheall-pairsalgorithmsinthischapter,
wewon’t cover the creation and properties of predecessor matrices as extensively
aswedealtwithpredecessorsubgraphsinChapter24. Someoftheexercisescover
thebasics.
686 Chapter25 All-PairsShortestPaths
Chapteroutline
Section 25.1 presents a dynamic-programming algorithm based on matrix multi-
plication tosolvetheall-pairs shortest-paths problem. Usingthetechnique of“re-
peatedsquaring,”wecanachievearunningtimeof‚.V3lgV/. Section25.2gives
another dynamic-programming algorithm, the Floyd-Warshall algorithm, which
runs in time ‚.V3/. Section 25.2 also covers the problem of finding the tran-
sitive closure of a directed graph, which is related to the all-pairs shortest-paths
problem. Finally, Section25.3presents Johnson’s algorithm, whichsolvestheall-
pairs shortest-paths problem in O.V2lgV VE/ time and is a good choice for
C
large, sparsegraphs.
Beforeproceeding, weneedtoestablishsomeconventions foradjacency-matrix
representations. First, weshall generally assume that theinput graph G .V;E/
D
has n vertices, so that n V . Second, we shall use the convention of denoting
D j j
matrices by uppercase letters, such as W, L, or D, and their individual elements
by subscripted lowercase letters, such as w , l , or d . Somematrices will have
ij ij ij
parenthesized superscripts, as in L.m/ l.m/ or D.m/ d.m/ , to indicate
D ij D ij
iterates. Finally, foragivenn nmatrixA,weshallassumethatthevalueofnis
 (cid:0)  (cid:0) 
storedintheattribute A:rows.
25.1 Shortest paths andmatrix multiplication
Thissection presents adynamic-programming algorithm fortheall-pairs shortest-
paths problem on a directed graph G .V;E/. Each major loop of the dynamic
D
program will invoke an operation that is very similar to matrix multiplication, so
that the algorithm will look like repeated matrix multiplication. We shall start by
developing a ‚.V4/-time algorithm for the all-pairs shortest-paths problem and
thenimproveitsrunning timeto‚.V3lgV/.
Before proceeding, let us briefly recap the steps given in Chapter 15 for devel-
opingadynamic-programming algorithm.
1. Characterize thestructure ofanoptimalsolution.
2. Recursivelydefinethevalueofanoptimalsolution.
3. Computethevalueofanoptimalsolutioninabottom-upfashion.
We reserve the fourth step—constructing an optimal solution from computed in-
formation—for theexercises.
25.1 Shortestpathsandmatrixmultiplication 687
Thestructureofashortestpath
We start by characterizing the structure of an optimal solution. For the all-pairs
shortest-paths problem on a graph G .V;E/, we have proven (Lemma 24.1)
D
that all subpaths of a shortest path are shortest paths. Suppose that we represent
the graph by an adjacency matrix W .w /. Consider a shortest path p from
ij
D
vertex i to vertex j,and suppose that p contains at mostmedges. Assuming that
there are no negative-weight cycles, m is finite. If i j, then p has weight 0
D
and no edges. If vertices i and j are distinct, then we decompose path p into
i
;p0
k j, where path p now contains at most m 1 edges. By Lemma 24.1,
0
! (cid:0)
p isashortestpathfromi tok,andsoı.i;j/ ı.i;k/ w .
0 kj
D C
Arecursive solutiontotheall-pairsshortest-pathsproblem
Now, let l.m/ be the minimum weight of any path from vertex i to vertex j that
ij
contains at most m edges. When m 0, there is a shortest path from i to j with
D
noedgesifandonlyifi j. Thus,
D
0 ifi j ;
l.0/ D
ij D
(
ifi j :
1 ¤
For m

1, we compute l i. jm/ as the minimum of l i. jm (cid:0)1/ (the weight of a shortest
pathfromi toj consistingofatmostm 1edges)andtheminimumweightofany
(cid:0)
pathfromi toj consisting ofatmostmedges,obtained bylookingatallpossible
predecessors k ofj. Thus,werecursively define
l i. jm/
D
min l i. jm (cid:0)1/; 1m kin
n
l i. km (cid:0)1/ Cw
kj
   
D
1m kin
n
l i. km (cid:0)1/ Cw k˚
j
: (cid:9) (25.2)
 
˚ (cid:9)
Thelatterequality followssincew 0forallj.
jj
D
What are the actual shortest-path weights ı.i;j/? If the graph contains
no negative-weight cycles, then for every pair of vertices i and j for which
ı.i;j/ < ,thereisashortestpathfromi toj thatissimpleandthuscontains at
1
most n 1 edges. A path from vertex i to vertex j with more than n 1 edges
(cid:0) (cid:0)
cannothavelowerweightthanashortestpathfromi toj. Theactualshortest-path
weightsaretherefore givenby
ı.i;j/
D
l i. jn (cid:0)1/
D
l i. jn/
D
l i. jn C1/
D
: (25.3)
688 Chapter25 All-PairsShortestPaths
Computingtheshortest-path weightsbottomup
Taking as our input the matrix W .w /, we now compute a series of matrices
ij
D
L.1/;L.2/;:::;L.n 1/, where for m 1;2;:::;n 1, we have L.m/ l.m/ .
(cid:0) D (cid:0) D ij
The final matrix L.n 1/ contains the actual shortest-path weights. Observe that
(cid:0) (cid:0) 
l.1/ w forallverticesi;j V,andsoL.1/ W.
ij D ij 2 D
The heart of the algorithm is the following procedure, which, given matrices
L.m 1/ andW,returns thematrix L.m/. Thatis,itextends theshortest paths com-
(cid:0)
putedsofarbyonemoreedge.
EXTEND-SHORTEST-PATHS.L;W/
1 n L:rows
D
2 letL l beanewn nmatrix
0
D
i0j

3 fori 1ton
D (cid:0) 
4 forj 1ton
D
5 l
i0j
D 1
6 fork 1ton
D
7 l min.l ;l w /
i0j
D
i0j ik
C
kj
8 returnL
0
TheprocedurecomputesamatrixL .l /,whichitreturnsattheend. Itdoesso
0
D
i0j
by computing equation (25.2) for alli and j,using LforL.m 1/ andL forL.m/.
(cid:0) 0
(It is written without the superscripts to make its input and output matrices inde-
pendent ofm.) Itsrunningtimeis‚.n3/duetothethreenestedforloops.
Nowwecanseetherelation tomatrixmultiplication. Supposewewishtocom-
pute the matrix product C A B of two n n matrices A and B. Then, for
D  
i;j 1;2;:::;n,wecompute
D
n
c a b : (25.4)
ij ik kj
D 
k 1
XD
Observethatifwemakethesubstitutions
l.m 1/ a;
(cid:0)
!
w b ;
!
l.m/ c ;
!
min ;
! C
C ! 
in equation (25.2), we obtain equation (25.4). Thus, if we make these changes to
EXTEND-SHORTEST-PATHS and also replace (the identity for min) by 0 (the
1
25.1 Shortestpathsandmatrixmultiplication 689
identity for ), we obtain the same ‚.n3/-time procedure for multiplying square
C
matricesthatwesawinSection4.2:
SQUARE-MATRIX-MULTIPLY.A;B/
1 n A:rows
D
2 letC beanewn nmatrix

3 fori 1ton
D
4 forj 1ton
D
5 c 0
ij
D
6 fork 1ton
D
7 c c a b
ij ij ik kj
D C 
8 returnC
Returning to the all-pairs shortest-paths problem, we compute the shortest-path
weights by extending shortest paths edge by edge. Letting A B denote the ma-

trix“product”returnedbyEXTEND-SHORTEST-PATHS.A;B/,wecomputethese-
quenceofn 1matrices
(cid:0)
L.1/ L.0/ W W ;
D  D
L.2/ L.1/ W W2 ;
D  D
L.3/ L.2/ W W3 ;
D : D
:
:
L.n 1/ L.n 2/ W Wn 1 :
(cid:0) (cid:0) (cid:0)
D  D
Aswearguedabove,thematrixL.n 1/ Wn 1containstheshortest-pathweights.
(cid:0) (cid:0)
D
Thefollowingprocedure computesthissequence in‚.n4/time.
SLOW-ALL-PAIRS-SHORTEST-PATHS.W/
1 n W:rows
D
2 L.1/ W
D
3 form 2ton 1
D (cid:0)
4 letL.m/ beanewn nmatrix

5 L.m/ EXTEND-SHORTEST-PATHS.L.m (cid:0)1/;W/
D
6 returnL.n 1/
(cid:0)
Figure 25.1 shows a graph and the matrices L.m/ computed by the procedure
SLOW-ALL-PAIRS-SHORTEST-PATHS.
Improvingtherunningtime
Our goal, however, is not to compute all the L.m/ matrices: we are interested
only in matrix L.n 1/. Recall that in the absence of negative-weight cycles, equa-
(cid:0)
690 Chapter25 All-PairsShortestPaths
2
3 4
1 3
8
2
–4 –5
7 1
5 4
6
0 3 8 4 0 3 8 2 4
1 (cid:0) (cid:0)
0 1 7 3 0 4 1 7
L.1/ 1 4 1 0 L.2/ 4 (cid:0) 0 5 11
D 1 1 1 D 1
2 5 0 2 1 5 0 2
1 (cid:0) 1 (cid:0) (cid:0) (cid:0)
 6 0˘  8 1 6 0˘
1 1 1 1
0 3 3 2 4 0 1 3 2 4
(cid:0) (cid:0) (cid:0) (cid:0)
3 0 4 1 1 3 0 4 1 1
L.3/ 7 4 (cid:0) 0 5 (cid:0) 11 L.4/ 7 4 (cid:0) 0 5 (cid:0) 3
D D
2 1 5 0 2 2 1 5 0 2
(cid:0) (cid:0) (cid:0) (cid:0) (cid:0) (cid:0)
8 5 1 6 0 ˘ 8 5 1 6 0 ˘
Figure25.1 AdirectedgraphandthesequenceofmatricesL.m/computedbySLOW-ALL-PAIRS-
SHORTEST-PATHS. YoumightwanttoverifythatL.5/,definedasL.4/ W,equalsL.4/,andthus
L.m/ L.4/forallm 4. 
D 
tion (25.3) implies L.m/ L.n 1/ for all integers m n 1. Just as tradi-
(cid:0)
D  (cid:0)
tional matrix multiplication is associative, so is matrix multiplication defined by
the EXTEND-SHORTEST-PATHS procedure (see Exercise 25.1-4). Therefore, we
can compute L.n 1/ with only lg.n 1/ matrix products by computing the se-
(cid:0)
d (cid:0) e
quence
L.1/ W ;
D
L.2/ W2 W W ;
D D 
L.4/ W4 W2 W2
D D 
L.8/ W8 W4 W4 ;
D : D 
:
:
L.2dlg.n(cid:0)1/e/ W2dlg.n(cid:0)1/e W2dlg.n(cid:0)1/e(cid:0)1 W2dlg.n(cid:0)1/e(cid:0)1 :
D D 
Since2 lg.n 1/ n 1,thefinalproductL.2dlg.n(cid:0)1/e/ isequaltoL.n 1/.
d (cid:0) e (cid:0)
 (cid:0)
Thefollowingprocedurecomputestheabovesequenceofmatricesbyusingthis
technique ofrepeated squaring.
25.1 Shortestpathsandmatrixmultiplication 691
1 2
1 2 3
–4 7 10 –8
2 –1 5
4 5 6
3
Figure25.2 Aweighted,directedgraphforuseinExercises25.1-1,25.2-1,and25.3-1.
FASTER-ALL-PAIRS-SHORTEST-PATHS.W/
1 n W:rows
D
2 L.1/ W
D
3 m 1
D
4 whilem < n 1
(cid:0)
5 letL.2m/ beanewn nmatrix

6 L.2m/ EXTEND-SHORTEST-PATHS.L.m/;L.m//
D
7 m 2m
D
8 returnL.m/
In each iteration of the while loop of lines 4–7, we compute L.2m/ L.m/ 2 ,
D
starting with m 1. At the end of each iteration, we double the value
D (cid:0) 
of m. The final iteration computes L.n 1/ by actually computing L.2m/ for some
(cid:0)
n 1 2m <2n 2. Byequation(25.3),L.2m/ L.n 1/. Thenexttimethetest
(cid:0)
(cid:0)  (cid:0) D
in line 4 is performed, m has been doubled, so now m n 1, the test fails, and
 (cid:0)
theprocedure returnsthelastmatrixitcomputed.
Because each of the lg.n 1/ matrix products takes ‚.n3/ time, FASTER-
d (cid:0) e
ALL-PAIRS-SHORTEST-PATHS runs in ‚.n3lgn/ time. Observe that the code
is tight, containing no elaborate data structures, and the constant hidden in the
‚-notation istherefore small.
Exercises
25.1-1
Run SLOW-ALL-PAIRS-SHORTEST-PATHS on the weighted, directed graph of
Figure 25.2, showing the matrices that result for each iteration of the loop. Then
dothesameforFASTER-ALL-PAIRS-SHORTEST-PATHS.
25.1-2
Whydowerequirethatw 0forall1 i n?
ii
D  
692 Chapter25 All-PairsShortestPaths
25.1-3
Whatdoesthematrix
0
1 1  1
0
1 1  1
L.0/ 0
D 1:
:
:
1:
:
:
:
:
:
 :: : 1:
:
:
(cid:0) 0 
1 1 1 
used in the shortest-paths algorithms correspond to in regular matrix multiplica-
tion?
25.1-4
Show that matrix multiplication defined by EXTEND-SHORTEST-PATHS is asso-
ciative.
25.1-5
Showhowtoexpressthesingle-source shortest-paths problem asaproductofma-
tricesandavector. DescribehowevaluatingthisproductcorrespondstoaBellman-
Ford-likealgorithm (seeSection24.1).
25.1-6
Supposewealsowishtocomputetheverticesonshortestpathsinthealgorithmsof
this section. Show how to compute the predecessor matrix … from the completed
matrixLofshortest-path weightsinO.n3/time.
25.1-7
We can also compute the vertices on shortest paths as we compute the shortest-
pathweights. Define.m/ asthepredecessor ofvertexj onanyminimum-weight
ij
path from i toj thatcontains atmostmedges. Modifythe EXTEND-SHORTEST-
PATHS and SLOW-ALL-PAIRS-SHORTEST-PATHS procedures tocomputethema-
trices….1/;….2/;:::;….n 1/ asthematricesL.1/;L.2/;:::;L.n 1/ arecomputed.
(cid:0) (cid:0)
25.1-8
The FASTER-ALL-PAIRS-SHORTEST-PATHS procedure, aswritten, requires usto
store lg.n 1/ matrices,eachwithn2 elements,foratotalspacerequirement of
d (cid:0) e
‚.n2lgn/. Modify the procedure to require only ‚.n2/ space by using only two
n nmatrices.

25.1-9
Modify FASTER-ALL-PAIRS-SHORTEST-PATHS so that it can determine whether
thegraphcontains anegative-weight cycle.
25.2 TheFloyd-Warshallalgorithm 693
25.1-10
Give an efficient algorithm to find the length (number of edges) of a minimum-
lengthnegative-weight cycleinagraph.
25.2 The Floyd-Warshallalgorithm
Inthissection,weshalluseadifferentdynamic-programming formulationtosolve
the all-pairs shortest-paths problem on a directed graph G .V;E/. The result-
D
ing algorithm, known as the Floyd-Warshall algorithm, runs in ‚.V3/ time. As
before, negative-weight edges may be present, but we assume that there are no
negative-weight cycles. As in Section 25.1, we follow the dynamic-programming
process to develop the algorithm. After studying the resulting algorithm, we
presentasimilarmethodforfindingthetransitiveclosureofadirectedgraph.
Thestructureofashortestpath
In the Floyd-Warshall algorithm, we characterize the structure of a shortest path
differentlyfromhowwecharacterizeditinSection25.1. TheFloyd-Warshallalgo-
rithmconsiders theintermediate verticesofashortest path,whereanintermediate
vertexofasimplepathp  ; ;:::; isanyvertexofp other than or ,
1 2 l 1 l
D h i
thatis,anyvertexintheset  ; ;:::; .
2 3 l 1
f (cid:0) g
The Floyd-Warshall algorithm relies on the following observation. Under our
assumption that the vertices of G are V 1;2;:::;n , let us consider a subset
D f g
1;2;:::;k ofvertices forsomek. Foranypairofvertices i;j V,consider all
f g 2
pathsfromi toj whoseintermediateverticesarealldrawnfrom 1;2;:::;k ,and
f g
letp beaminimum-weightpathfromamongthem. (Pathpissimple.) TheFloyd-
Warshallalgorithmexploitsarelationshipbetweenpathpandshortestpathsfromi
to j with all intermediate vertices in the set 1;2;:::;k 1 . The relationship
f (cid:0) g
dependsonwhetherornotk isanintermediate vertexofpathp.
 If k is not an intermediate vertex of path p, then all intermediate vertices of
path p are in the set 1;2;:::;k 1 . Thus, a shortest path from vertex i
f (cid:0) g
to vertex j with all intermediate vertices in the set 1;2;:::;k 1 is also a
f (cid:0) g
shortest pathfromi toj withallintermediate vertices intheset 1;2;:::;k .
f g
 Ifkisanintermediatevertexofpathp,thenwedecomposepintoi
;p1
k
;p2
j,
as Figure 25.3 illustrates. By Lemma 24.1, p is a shortest path from i to k
1
with all intermediate vertices in the set 1;2;:::;k . In fact, we can make a
f g
slightly stronger statement. Because vertex k is not an intermediate vertex of
path p , all intermediate vertices of p are in the set 1;2;:::;k 1 . There-
1 1
f (cid:0) g
694 Chapter25 All-PairsShortestPaths
allintermediateverticesin 1;2;:::;k 1 allintermediateverticesin 1;2;:::;k 1
f (cid:0) g f (cid:0) g
p k
1 p
2
j
i
p:allintermediateverticesin 1;2;:::;k
f g
Figure 25.3 Path p is a shortest path from vertex i to vertex j, and k is the highest-numbered
intermediatevertexofp.Pathp1,theportionofpathpfromvertexitovertexk,hasallintermediate
verticesintheset 1;2;:::;k 1 .Thesameholdsforpathp2fromvertexktovertexj.
f (cid:0) g
fore, p is a shortest path from i to k with all intermediate vertices in the set
1
1;2;:::;k 1 . Similarly,p isashortestpathfromvertexktovertexj with
2
f (cid:0) g
allintermediate verticesintheset 1;2;:::;k 1 .
f (cid:0) g
Arecursivesolutiontotheall-pairsshortest-pathsproblem
Based on the above observations, we define a recursive formulation of shortest-
path estimates that differs from the one in Section 25.1. Let
d.k/
be the weight
ij
of a shortest path from vertex i to vertex j for which all intermediate vertices
are in the set 1;2;:::;k . When k 0, a path from vertex i to vertex j with
f g D
no intermediate vertex numbered higher than 0has no intermediate vertices atall.
Such a path has at most one edge, and hence d.0/ w . Following the above
ij D ij
discussion,
wedefined.k/
recursively by
ij
w ifk 0;
d.k/ ij
D (25.5)
ij D ( min d i. jk (cid:0)1/;d i. kk (cid:0)1/ Cd k. jk (cid:0)1/ ifk 1:
Because for an(cid:0)ypath, allintermediatevertices areintheset 1;2;:::;n ,the ma-
f g
trixD.n/ d.n/ givesthefinalanswer: d.n/ ı.i;j/foralli;j V.
D ij ij D 2
(cid:0) 
Computingtheshortest-path weightsbottomup
Basedonrecurrence(25.5),wecanusethefollowingbottom-upproceduretocom-
putethevaluesd.k/ inorderofincreasingvaluesofk. Itsinputisann nmatrixW
ij 
defined as in equation (25.1). The procedure returns the matrix D.n/ of shortest-
pathweights.
25.2 TheFloyd-Warshallalgorithm 695
FLOYD-WARSHALL.W/
1 n W:rows
D
2 D.0/ W
D
3 fork 1ton
D
4 letD.k/ d.k/ beanewn nmatrix
D ij 
5 fori 1ton
D (cid:0) 
6 forj 1ton
D
7 d i. jk/
D
min d i. jk (cid:0)1/;d i. kk (cid:0)1/ Cd k. jk (cid:0)1/
8 returnD.n/
(cid:0) 
Figure 25.4 shows the matrices D.k/ computed by the Floyd-Warshall algorithm
forthegraphinFigure25.1.
The running time of the Floyd-Warshall algorithm is determined by the triply
nested for loops of lines 3–7. Because each execution of line 7 takes O.1/ time,
the algorithm runs in time ‚.n3/. As in the final algorithm in Section 25.1, the
code is tight, with no elaborate data structures, and so the constant hidden in the
‚-notationissmall. Thus,theFloyd-Warshallalgorithm isquitepracticalforeven
moderate-sized inputgraphs.
Constructingashortestpath
ThereareavarietyofdifferentmethodsforconstructingshortestpathsintheFloyd-
Warshallalgorithm. Onewayistocompute thematrixD ofshortest-path weights
and then construct the predecessor matrix … from the D matrix. Exercise 25.1-6
asks you to implement this method so that it runs in O.n3/ time. Given the pre-
decessor matrix…,the PRINT-ALL-PAIRS-SHORTEST-PATH procedure willprint
theverticesonagivenshortest path.
Alternatively, we can compute the predecessor matrix … while the algorithm
computes the matrices D.k/. Specifically, we compute a sequence of matrices
….0/;….1/;:::;….n/, where … ….n/ and we define .k/ as the predecessor of
D ij
vertex j on a shortest path from vertex i with all intermediate vertices in the set
1;2;:::;k .
f g
Wecangivearecursiveformulationof.k/ . Whenk 0,ashortestpathfromi
ij D
toj hasnointermediate vertices atall. Thus,
.0/ NIL ifi
D
j orw ij D1;
(25.6)
ij D
(
i ifi j andw
ij
< :
¤ 1
For k 1, if we take the path i ; k ; j, where k j, then the predecessor
 ¤
of j we choose is the same as the predecessor of j we chose on a shortest path
from k with all intermediate vertices in the set 1;2;:::;k 1 . Otherwise, we
f (cid:0) g
696 Chapter25 All-PairsShortestPaths
0 3 8 4 NIL 1 1 NIL 1
1 (cid:0)
0 1 7 NIL NIL NIL 2 2
D.0/ 1 4 1 0 ….0/ NIL 3 NIL NIL NIL
D 1 1 1 D
2 5 0 4 NIL 4 NIL NIL
1 (cid:0) 1
 6 0 ˘  NIL NIL NIL 5 NIL ˘
1 1 1
0 3 8 4 NIL 1 1 NIL 1
1 (cid:0)
0 1 7 NIL NIL NIL 2 2
D.1/ 1 4 1 0 ….1/ NIL 3 NIL NIL NIL
D 1 1 1 D
2 5 5 0 2 4 1 4 NIL 1
(cid:0) (cid:0)
 6 0 ˘  NIL NIL NIL 5 NIL ˘
1 1 1
0 3 8 4 4 NIL 1 1 2 1
(cid:0)
0 1 7 NIL NIL NIL 2 2
D.2/ 1 4 1 0 5 11 ….2/ NIL 3 NIL 2 2
D 1 D
2 5 5 0 2 4 1 4 NIL 1
(cid:0) (cid:0)
 6 0 ˘  NIL NIL NIL 5 NIL ˘
1 1 1
0 3 8 4 4 NIL 1 1 2 1
(cid:0)
0 1 7 NIL NIL NIL 2 2
D.3/ 1 4 1 0 5 11 ….3/ NIL 3 NIL 2 2
D 1 D
2 1 5 0 2 4 3 4 NIL 1
(cid:0) (cid:0) (cid:0)
 6 0˘  NIL NIL NIL 5 NIL ˘
1 1 1
0 3 1 4 4 NIL 1 4 2 1
(cid:0) (cid:0)
3 0 4 1 1 4 NIL 4 2 1
D.4/ 7 4 (cid:0) 0 5 (cid:0) 3 ….4/ 4 3 NIL 2 1
D D
2 1 5 0 2 4 3 4 NIL 1
(cid:0) (cid:0) (cid:0)
 8 5 1 6 0˘  4 3 4 5 NIL ˘
0 1 3 2 4 NIL 3 4 5 1
(cid:0) (cid:0)
3 0 4 1 1 4 NIL 4 2 1
D.5/ 7 4 (cid:0) 0 5 (cid:0) 3 ….5/ 4 3 NIL 2 1
D D
2 1 5 0 2 4 3 4 NIL 1
(cid:0) (cid:0) (cid:0)
 8 5 1 6 0˘  4 3 4 5 NIL ˘
Figure25.4 ThesequenceofmatricesD.k/and….k/computedbytheFloyd-Warshallalgorithm
forthegraphinFigure25.1.
25.2 TheFloyd-Warshallalgorithm 697
choose thesamepredecessor ofj that wechose onashortest path from i withall
intermediate verticesintheset 1;2;:::;k 1 . Formally,fork 1,
f (cid:0) g 
.k/
 i. jk (cid:0)1/ ifd i. jk (cid:0)1/

d i. kk (cid:0)1/ Cd k. jk (cid:0)1/ ;
(25.7)
ij D
(
 k. jk (cid:0)1/ ifd i. jk (cid:0)1/ > d i. kk (cid:0)1/ Cd k. jk (cid:0)1/ :
We leave the incorporation of the ….k/ matrix computations into the FLOYD-
WARSHALLprocedureasExercise25.2-3. Figure25.4showsthesequenceof….k/
matrices that the resulting algorithm computes for the graph of Figure 25.1. The
exercise also asks for the more difficult task of proving that the predecessor sub-
graphG isashortest-paths treewithrooti. Exercise25.2-7asksforyetanother
;i
waytoreconstruct shortest paths.
Transitiveclosureofadirected graph
Given a directed graph G .V;E/ with vertex set V 1;2;:::;n , we might
D D f g
wish to determine whether G contains a path from i to j for all vertex pairs
i;j V. WedefinethetransitiveclosureofG asthegraphG .V;E /,where
 
2 D
E .i;j/ thereisapathfromvertexi tovertexj inG :

D f W g
Onewaytocompute thetransitive closure ofagraph in‚.n3/timeistoassign
aweightof1toeachedgeofE andruntheFloyd-Warshallalgorithm. Ifthereisa
pathfromvertexi tovertexj,wegetd < n. Otherwise,wegetd .
ij ij
D1
There is another, similar way to compute the transitive closure of G in ‚.n3/
time that can save time and space in practice. This method substitutes the logical
operations (logical OR)and (logical AND)for the arithmetic operations min
_ ^
and intheFloyd-Warshall algorithm. Fori;j;k 1;2;:::;n,wedefinet.k/ to
C D ij
be1ifthereexistsapathingraphG fromvertexi tovertexj withallintermediate
verticesintheset 1;2;:::;k ,and0otherwise. Weconstructthetransitiveclosure
f g
G .V;E / by putting edge .i;j/ into E if and only if t.n/ 1. A recursive
 D   ij D
definitionoft.k/
,analogous torecurrence (25.5),is
ij
0 ifi j and.i;j/ E ;
t.0/
¤ 62
ij D
(
1 ifi j or.i;j/ E ;
D 2
andfork 1,

t i. jk/
D
t i. jk (cid:0)1/
_
t i. kk (cid:0)1/ ^t k. jk (cid:0)1/ : (25.8)
As in the Flo(cid:0) yd-Warshall alg orithm, we compute the matrices T.k/ t.k/ in
D ij
orderofincreasing k.
(cid:0) 
698 Chapter25 All-PairsShortestPaths
1 2
4 3
1 0 0 0 1 0 0 0 1 0 0 0
0 1 1 1 0 1 1 1 0 1 1 1
T.0/ T.1/ T.2/
D 0 1 1 0 D 0 1 1 0 D 0 1 1 1
1 0 1 1 1 0 1 1 1 0 1 1
(cid:0)  (cid:0)  (cid:0) 
1 0 0 0 1 0 0 0
0 1 1 1 1 1 1 1
T.3/ T.4/
D 0 1 1 1 D 1 1 1 1
1 1 1 1 1 1 1 1
(cid:0)  (cid:0) 
Figure25.5 AdirectedgraphandthematricesT.k/computedbythetransitive-closurealgorithm.
TRANSITIVE-CLOSURE.G/
1 n G:V
D j j
2 letT.0/ t.0/ beanewn nmatrix
D ij 
3 fori 1ton
D (cid:0) 
4 forj 1ton
D
5 ifi ==j or.i;j/ G:E
2
6 t.0/ 1
ij D
7 elset.0/ 0
ij D
8 fork 1ton
D
9 letT.k/ t.k/ beanewn nmatrix
D ij 
10 fori 1ton
D (cid:0) 
11 forj 1ton
D
12 t i. jk/
D
t i. jk (cid:0)1/
_
t i. kk (cid:0)1/ ^t k. jk (cid:0)1/
13 returnT.n/
(cid:0) 
Figure 25.5 shows the matrices T.k/ computed by the TRANSITIVE-CLOSURE
procedure on a sample graph. The TRANSITIVE-CLOSURE procedure, like the
Floyd-Warshall algorithm, runs in ‚.n3/ time. On some computers, though, log-
ical operations on single-bit values execute faster than arithmetic operations on
integer words of data. Moreover, because the direct transitive-closure algorithm
uses only boolean values rather than integer values, its space requirement is less
25.2 TheFloyd-Warshallalgorithm 699
thantheFloyd-Warshallalgorithm’sbyafactorcorrespondingtothesizeofaword
ofcomputerstorage.
Exercises
25.2-1
RuntheFloyd-Warshall algorithm ontheweighted, directed graph ofFigure25.2.
ShowthematrixD.k/ thatresultsforeachiteration oftheouterloop.
25.2-2
Showhowtocomputethetransitive closureusingthetechnique ofSection25.1.
25.2-3
ModifytheFLOYD-WARSHALLproceduretocomputethe….k/matricesaccording
toequations (25.6)and(25.7). Proverigorously thatforalli V,thepredecessor
2
subgraph G is a shortest-paths tree with root i. (Hint: To show that G is
;i ;i
acyclic, first show that .k/ l implies d.k/ d.k/ w , according to the
ij D ij  il C lj
definitionof.k/
. Then,adapttheproofofLemma24.16.)
ij
25.2-4
Asitappearsabove, theFloyd-Warshall algorithm requires ‚.n3/space,since we
compute d.k/ for i;j;k 1;2;:::;n. Show that the following procedure, which
ij D
simplydropsallthesuperscripts, iscorrect,andthusonly‚.n2/spaceisrequired.
FLOYD-WARSHALL0.W/
1 n W:rows
D
2 D W
D
3 fork 1ton
D
4 fori 1ton
D
5 forj 1ton
D
6 d min.d ;d d /
ij ij ik kj
D C
7 returnD
25.2-5
Supposethatwemodifythewayinwhichequation (25.7)handles equality:
.k/
 i. jk (cid:0)1/ ifd i. jk (cid:0)1/ < d i. kk (cid:0)1/ Cd k. jk (cid:0)1/ ;
ij D
(
 k. jk (cid:0)1/ ifd i. jk (cid:0)1/

d i. kk (cid:0)1/ Cd k. jk (cid:0)1/ :
Isthisalternative definitionofthepredecessor matrix…correct?
700 Chapter25 All-PairsShortestPaths
25.2-6
HowcanweusetheoutputoftheFloyd-Warshall algorithm todetectthepresence
ofanegative-weight cycle?
25.2-7
Another way to reconstruct shortest paths in the Floyd-Warshall algorithm uses
values .k/ for i;j;k 1;2;:::;n, where .k/ is the highest-numbered interme-
ij D ij
diate vertex of a shortest path from i to j in which all intermediate vertices are
intheset 1;2;:::;k . Givearecursive formulation for .k/ ,modifythe FLOYD-
f g ij
WARSHALL procedure to compute the
.k/
values, and rewrite the PRINT-ALL-
ij
PAIRS-SHORTEST-PATH procedure to take the matrix ˆ .n/ as an input.
D ij
Howisthematrix ˆlikethe s table inthematrix-chain multiplication problem of
(cid:0) 
Section15.2?
25.2-8
Give an O.VE/-time algorithm for computing the transitive closure of a directed
graphG .V;E/.
D
25.2-9
Suppose that we can compute the transitive closure of a directed acyclic graph in
f. V ; E /time,wheref isamonotonically increasing function of V and E .
j j j j j j j j
Show that the time to compute the transitive closure G .V;E / of a general
 
D
directed graphG .V;E/isthenf. V ; E / O.V E /.

D j j j j C C
25.3 Johnson’s algorithm forsparsegraphs
Johnson’s algorithm finds shortest paths between all pairs in O.V2lgV VE/
C
time. Forsparsegraphs, itisasymptotically fasterthaneitherrepeatedsquaring of
matricesortheFloyd-Warshallalgorithm. Thealgorithmeitherreturnsamatrixof
shortest-pathweightsforallpairsofverticesorreportsthattheinputgraphcontains
a negative-weight cycle. Johnson’s algorithm uses as subroutines both Dijkstra’s
algorithm andtheBellman-Fordalgorithm, whichChapter24describes.
Johnson’salgorithmusesthetechniqueofreweighting,whichworksasfollows.
If all edge weights w in a graph G .V;E/ are nonnegative, we can find short-
D
est paths between all pairs of vertices by running Dijkstra’s algorithm once from
each vertex; with the Fibonacci-heap min-priority queue, the running time of this
all-pairs algorithm is O.V2lgV VE/. If G has negative-weight edges but no
C
negative-weightcycles,wesimplycomputeanewsetofnonnegativeedgeweights
25.3 Johnson’salgorithmforsparsegraphs 701
thatallowsustousethesamemethod. Thenewsetofedgeweightswmustsatisfy
y
twoimportantproperties:
1. Forallpairs ofvertices u; V,apath p isashortest path from uto using
2
weight function w if and only if p is also a shortest path from u to  using
weightfunction w.
y
2. Foralledges.u;/,thenewweightw.u;/isnonnegative.
y
As we shall see in a moment, we can preprocess G to determine the new weight
functionw inO.VE/time.
y
Preservingshortestpathsbyreweighting
The following lemma shows how easily we can reweight the edges to satisfy the
firstpropertyabove. Weuseıtodenoteshortest-path weightsderivedfromweight
functionw andı todenoteshortest-path weightsderivedfromweightfunction w.
y y
Lemma25.1(Reweightingdoesnotchangeshortestpaths)
Given a weighted, directed graph G .V;E/ with weight function w E R ,
let h V R be any function mappD ing vertices to real numbers. ForW each! edge
W !
.u;/ E,define
2
w.u;/ w.u;/ h.u/ h./: (25.9)
y D C (cid:0)
Let p  ; ;:::; be any path from vertex  to vertex  . Then p is a
0 1 k 0 k
D h i
shortestpathfrom to withweightfunctionwifandonlyifitisashortestpath
0 k
withweightfunctionw. Thatis,w.p/ ı. ; /ifandonlyifw.p/ ı. ; /.
y D 0 k y D y 0 k
Furthermore, G has a negative-weight cycle using weight function w if and only
ifG hasanegative-weight cycleusingweightfunction w.
y
Proof Westartbyshowingthat
w.p/ w.p/ h. / h. /: (25.10)
0 k
y D C (cid:0)
Wehave
k
w.p/ w. ; /
i 1 i
y D y (cid:0)
i 1
XD
k
.w. ; / h. / h. //
i 1 i i 1 i
D (cid:0) C (cid:0) (cid:0)
i 1
XD
k
w. ; / h. / h. / (becausethesumtelescopes)
i 1 i 0 k
D (cid:0) C (cid:0)
i 1
XD
w.p/ h. / h. /:
0 k
D C (cid:0)
702 Chapter25 All-PairsShortestPaths
Therefore, any path p from  to  has w.p/ w.p/ h. / h. /. Be-
0 k 0 k
y D C (cid:0)
cause h. / and h. / do not depend on the path, if one path from  to  is
0 k 0 k
shorterthananotherusingweightfunctionw,thenitisalsoshorterusingw. Thus,
y
w.p/ ı. ; /ifandonlyifw.p/ ı. ; /.
D 0 k y D y 0 k
Finally, weshowthat G hasanegative-weight cycleusing weightfunction w if
and only if G has a negative-weight cycle using weight function w. Consider any
y
cyclec  ; ;:::; ,where  . Byequation (25.10),
0 1 k 0 k
D h i D
w.c/ w.c/ h. / h. /
0 k
y D C (cid:0)
w.c/;
D
and thus c has negative weight using w if and only if it has negative weight us-
ingw.
y
Producingnonnegativeweightsbyreweighting
Our next goal is to ensure that the second property holds: we want w.u;/ to be
y
nonnegative for all edges .u;/ E. Given a weighted, directed graph G
.V;E/ with weight function w E2 R , we make a new graph G .V ;ED /,
0 0 0
W ! D
where V V s for somenew vertex s V and E E .s;/  V .
0 0
D [f g 62 D [f W 2 g
We extend the weight function w so that w.s;/ 0 for all  V. Note that
D 2
because s has no edges that enter it, no shortest paths in G , other than those with
0
source s, contain s. Moreover, G has no negative-weight cycles if and only if G
0
has no negative-weight cycles. Figure 25.6(a) shows the graph G corresponding
0
tothegraphG ofFigure25.1.
Now suppose that G and G have no negative-weight cycles. Let us define
0
h./ ı.s;/ for all  V . By the triangle inequality (Lemma 24.10),
0
D 2
we have h./ h.u/ w.u;/ for all edges .u;/ E . Thus, if we de-
0
 C 2
fine the new weights w by reweighting according to equation (25.9), we have
y
w.u;/ w.u;/ h.u/ h./ 0,andwehavesatisfiedthesecondproperty.
y D C (cid:0) 
Figure25.6(b)showsthegraphG fromFigure25.6(a)withreweightededges.
0
Computingall-pairsshortestpaths
Johnson’s algorithm to compute all-pairs shortest paths uses the Bellman-Ford al-
gorithm (Section 24.1) and Dijkstra’s algorithm (Section 24.3) as subroutines. It
assumes implicitly that the edges are stored in adjacency lists. The algorithm re-
turns the usual V V matrix D d , where d ı.i;j/, or it reports that
ij ij
j jj j D D
the input graph contains a negative-weight cycle. As is typical for an all-pairs
shortest-paths algorithm, weassumethattheverticesarenumberedfrom1to V .
j j
25.3 Johnson’salgorithmforsparsegraphs 703
0 5
2 2
0 –1 1 –1
3 4 4 0
1 1
0 0 –5 3 0 0 –5 3
8 13
0 2 0 2
–4 – 5 0 0
0 7 1 4 10 0
–4 0 4 –4 0 4
6 2
5 5
0 0
(a) (b)
2 2 2
2/1 0/0 0/4
4 0 4 0 4 0
1 3 1 3 1 3
0/0 2/–3 2/3 0/–4 2/7 0/0
13 13 13
2 2 2
0 0 0 0 0 0
10 0 10 0 10 0
0/–4 2/2 2/–1 0/1 2/3 0/5
2 2 2
5 4 5 4 5 4
(c) (d) (e)
2 2
0/–1 2/5
4 0 4 0
1 3 1 3
2/2 0/–5 4/8 2/1
13 13
2 2
0 0 0 0
10 0 10 0
2/–2 0/0 0/0 2/6
2 2
5 4 5 4
(f) (g)
Figure 25.6 Johnson’s all-pairs shortest-paths algorithm run on the graph of Figure 25.1. Ver-
tex numbers appear outside the vertices. (a) The graph G with the original weight function w.
0
The new vertex s is black. Within each vertex  is h./ ı.s;/. (b) After reweighting each
D
edge.u;/withweightfunctionw.u;/ w.u;/ h.u/ h./. (c)–(g)Theresult ofrunning
y D C (cid:0)
Dijkstra’salgorithmoneachvertexofGusingweightfunctionw. Ineachpart,thesourcevertexu
y
isblack, and shaded edges arein theshortest-paths treecomputed bythe algorithm. Withineach
vertexarethevaluesı y.u;/andı.u;/,separatedbyaslash.Thevaluedu Dı.u;/isequalto
ı.u;/ h./ h.u/.
y C (cid:0)
704 Chapter25 All-PairsShortestPaths
JOHNSON.G;w/
1 computeG ,whereG :V G:V s ,
0 0
D [f g
G :E G:E .s;/  G:V ,and
0
D [f W 2 g
w.s;/ 0forall G:V
D 2
2 ifBELLMAN-FORD.G 0;w;s/ == FALSE
3 print“theinputgraphcontains anegative-weight cycle”
4 elseforeachvertex G :V
0
2
5 seth./tothevalueofı.s;/
computed bytheBellman-Fordalgorithm
6 foreachedge.u;/ G :E
0
2
7 w.u;/ w.u;/ h.u/ h./
y D C (cid:0)
8 letD .d /beanewn nmatrix
u
D 
9 foreachvertexu G:V
2
10 run DIJKSTRA.G;w y;u/tocomputeı y.u;/forall 2G:V
11 foreachvertex G:V
2
12 d ı.u;/ h./ h.u/
u D y C (cid:0)
13 returnD
This code simply performs the actions we specified earlier. Line 1 produces G .
0
Line2runstheBellman-Fordalgorithm onG withweightfunction w andsource
0
vertex s. If G , and hence G, contains a negative-weight cycle, line 3 reports the
0
problem. Lines4–12assumethatG containsnonegative-weightcycles. Lines4–5
0
set h./ to the shortest-path weight ı.s;/ computed by the Bellman-Ford algo-
rithm forall V . Lines6–7compute thenewweights w. Foreach pairofver-
0
2 y
ticesu; V,theforloopoflines9–12computestheshortest-pathweightı.u;/
2 y
by calling Dijkstra’s algorithm once from each vertex in V. Line 12 stores in
matrix entry d the correct shortest-path weight ı.u;/, calculated using equa-
u
tion (25.10). Finally, line 13 returns the completed D matrix. Figure 25.6 depicts
theexecution ofJohnson’s algorithm.
If we implement the min-priority queue in Dijkstra’s algorithm by a Fibonacci
heap,Johnson’salgorithmrunsinO.V2lgV VE/time. Thesimplerbinarymin-
C
heap implementation yields a running time of O.VElgV/, which is still asymp-
totically fasterthantheFloyd-Warshall algorithm ifthegraphissparse.
Exercises
25.3-1
Use Johnson’s algorithm to find the shortest paths between all pairs of vertices in
thegraphofFigure25.2. Showthevaluesofhandw computedbythealgorithm.
y
ProblemsforChapter25 705
25.3-2
Whatisthepurposeofaddingthenewvertexs toV,yielding V ?
0
25.3-3
Suppose that w.u;/ 0 for all edges .u;/ E. What is the relationship
 2
betweentheweightfunctions w andw?
y
25.3-4
Professor Greenstreet claims that there is a simpler way to reweight edges than
the method used in Johnson’s algorithm. Letting w min w.u;/ , just
 .u;/ E
define w.u;/ w.u;/ w for all edges .u;/ D E. What2 is wf rong wig ththe

y D (cid:0) 2
professor’s methodofreweighting?
25.3-5
SupposethatwerunJohnson’s algorithm onadirected graphG withweightfunc-
tion w. Show that if G contains a 0-weight cycle c, then w.u;/ 0 for every
y D
edge.u;/inc.
25.3-6
Professor Michener claims that there is no need to create a new source vertex in
line1ofJOHNSON. HeclaimsthatinsteadwecanjustuseG
0
Gandletsbeany
D
vertex. Givean example of aweighted, directed graph G for which incorporating
theprofessor’s ideainto JOHNSON causes incorrect answers. Thenshow thatifG
isstronglyconnected(everyvertexisreachablefromeveryothervertex),theresults
returnedby JOHNSON withtheprofessor’s modification arecorrect.
Problems
25-1 Transitiveclosureofadynamicgraph
Suppose that we wish to maintain the transitive closure of a directed graph G
D
.V;E/ as we insert edges into E. That is, after each edge has been inserted, we
want to update the transitive closure of the edges inserted so far. Assume that the
graph G has no edges initially and that we represent the transitive closure as a
booleanmatrix.
a. ShowhowtoupdatethetransitiveclosureG .V;E /ofagraphG .V;E/
 
D D
inO.V2/timewhenanewedgeisaddedtoG.
b. Giveanexampleofagraph G and anedgee suchthat .V2/timeisrequired
to update the transitive closure after the insertion of e into G, no matter what
algorithm isused.
706 Chapter25 All-PairsShortestPaths
c. Describe an efficient algorithm for updating the transitive closure as edges are
insertedintothegraph. Foranysequenceofninsertions,youralgorithmshould
runintotaltime n t O.V3/,wheret isthetimetoupdatethetransitive
i 1 i D i
closure upon insertinDg theithedge. Provethatyouralgorithm attains thistime
P
bound.
25-2 Shortestpathsin-densegraphs
A graph G .V;E/ is -dense if E ‚.V1 / for some constant  in the
C
D j j D
range 0 <  1. By using d-ary min-heaps (see Problem 6-2) in shortest-paths

algorithms on-densegraphs, wecanmatchtherunning timesofFibonacci-heap-
basedalgorithms withoutusingascomplicated adatastructure.
a. What are the asymptotic running times for INSERT, EXTRACT-MIN, and
DECREASE-KEY, as a function of d and the number n of elements in a d-ary
min-heap? What are these running times if we choose d ‚.n˛/ for some
D
constant 0 < ˛ 1? Compare these running times to the amortized costs of

theseoperations foraFibonacciheap.
b. Showhowtocomputeshortestpathsfromasinglesourceonan-densedirected
graphG .V;E/withnonegative-weight edgesinO.E/time. (Hint:Pickd
D
asafunction of.)
c. Show how to solve the all-pairs shortest-paths problem on an-dense directed
graphG .V;E/withnonegative-weight edgesinO.VE/time.
D
d. Show how to solve the all-pairs shortest-paths problem in O.VE/ time on an
-dense directed graph G .V;E/ that may have negative-weight edges but
D
hasnonegative-weight cycles.
Chapter notes
Lawler [224] has a good discussion of the all-pairs shortest-paths problem, al-
though he does not analyze solutions for sparse graphs. He attributes the matrix-
multiplication algorithm to the folklore. The Floyd-Warshall algorithm is due to
Floyd [105], who based it on a theorem of Warshall [349] that describes how to
compute the transitive closure of boolean matrices. Johnson’s algorithm is taken
from[192].
Several researchers have given improved algorithms for computing shortest
paths via matrix multiplication. Fredman [111] shows how to solve the all-
pairs shortest paths problem using O.V5=2/ comparisons between sums of edge
NotesforChapter25 707
weightsandobtainsanalgorithm thatrunsinO.V3.lglgV=lgV/1=3/time,which
isslightlybetterthantherunningtimeoftheFloyd-Warshallalgorithm. Han[159]
reduced the running time to O.V3.lglgV=lgV/5=4/. Another line of research
demonstrates that we can apply algorithms for fast matrix multiplication (see the
chapter notes for Chapter 4)tothe all-pairs shortest paths problem. LetO.n!/ be
therunning timeofthefastest algorithm formultiplying n nmatrices; currently

! < 2:376 [78]. Galil and Margalit [123, 124] and Seidel [308] designed algo-
rithms that solve the all-pairs shortest paths problem in undirected, unweighted
graphs in .V!p.V// time, where p.n/ denotes a particular function that is poly-
logarithmically bounded in n. In dense graphs, these algorithms are faster than
theO.VE/timeneeded toperform V breadth-first searches. Severalresearchers
j j
have extended these results to give algorithms for solving the all-pairs shortest
paths problem in undirected graphs in which the edge weights are integers in the
range 1;2;:::;W . The asymptotically fastest such algorithm, by Shoshan and
f g
Zwick[316],runsintimeO.WV!p.VW//.
Karger,Koller,andPhillips[196]andindependently McGeoch[247]havegiven
a time bound that depends on E , the set of edges in E that participate in some

shortestpath. Givenagraphwithnonnegativeedgeweights,theiralgorithmsrunin
O.VE V2lgV/timeandimproveuponrunningDijkstra’salgorithm V times

C j j
when E o.E/.

j j D
Baswana, Hariharan, and Sen [33] examined decremental algorithms for main-
taining all-pairs shortest paths and transitive-closure information. Decremen-
tal algorithms allow a sequence of intermixed edge deletions and queries; by
comparison, Problem 25-1, in which edges are inserted, asks for an incremen-
tal algorithm. The algorithms by Baswana, Hariharan, and Sen are randomized
and, when a path exists, their transitive-closure algorithm can fail to report it
with probability 1=nc for an arbitrary c > 0. The query times are O.1/ with
high probability. For transitive closure, the amortized time for each update is
O.V4=3lg1=3V/. For all-pairs shortest paths, the update times depend on the
queries. For queries just giving the shortest-path weights, the amortized time per
update is O.V3=Elg2V/. To report the actual shortest path, the amortized up-
date time is min.O.V3=2 lgV/;O.V3=Elg2V//. Demetrescu and Italiano [84]
showed how to handle update and query operations when edges are both inserted
p
and deleted, as long as each given edge has a bounded range of possible values
drawnfromtherealnumbers.
Aho,Hopcroft,andUllman[5]definedanalgebraicstructureknownasa“closed
semiring,” which serves as a general framework for solving path problems in di-
rected graphs. Both the Floyd-Warshall algorithm and the transitive-closure algo-
rithmfromSection25.2areinstantiationsofanall-pairsalgorithmbasedonclosed
semirings. Maggsand Plotkin [240]showed how tofindminimum spanning trees
usingaclosedsemiring.
26 Maximum Flow
Just as we can model a road map as a directed graph in order to find the shortest
path from one point to another, we can also interpret a directed graph as a “flow
network” and use it to answer questions about material flows. Imagine a mate-
rial coursing through a system from a source, where the material is produced, to
a sink, where it is consumed. The source produces the material at some steady
rate, andthesink consumes thematerial atthesamerate. The“flow”ofthemate-
rial at any point in the system is intuitively the rate at which the material moves.
Flownetworkscanmodelmanyproblems,includingliquidsflowingthroughpipes,
partsthrough assemblylines, currentthrough electrical networks, andinformation
through communication networks.
Wecanthinkofeachdirected edge inaflownetwork asaconduit forthemate-
rial. Eachconduithasastatedcapacity, givenasamaximumrateatwhichthema-
terial canflowthrough theconduit, suchas200gallons ofliquid perhourthrough
a pipe or 20 amperes of electrical current through a wire. Vertices are conduit
junctions, and other than the source and sink, material flows through the vertices
without collecting inthem. Inother words, therateatwhichmaterialenters aver-
tex must equal the rate at which it leaves the vertex. We call this property “flow
conservation,” and it is equivalent to Kirchhoff’s current law when the material is
electrical current.
In the maximum-flow problem, we wish to compute the greatest rate at which
we can ship material from the source to the sink without violating any capacity
constraints. It is one of the simplest problems concerning flow networks and, as
we shall see in this chapter, this problem can be solved by efficient algorithms.
Moreover,wecanadaptthebasictechniquesusedinmaximum-flowalgorithmsto
solveothernetwork-flowproblems.
Thischapterpresentstwogeneralmethodsforsolvingthemaximum-flowprob-
lem. Section 26.1 formalizes the notions of flow networks and flows, formally
defining the maximum-flow problem. Section 26.2 describes theclassical method
ofFordandFulkerson forfindingmaximumflows. Anapplication ofthismethod,
26.1 Flownetworks 709
finding a maximum matching in an undirected bipartite graph, appears in Sec-
tion26.3. Section26.4presentsthepush-relabel method,whichunderliesmanyof
thefastestalgorithmsfornetwork-flowproblems. Section26.5coversthe“relabel-
to-front” algorithm, a particular implementation of the push-relabel method that
runs in time O.V3/. Although this algorithm is not the fastest algorithm known,
it illustrates some of the techniques used in the asymptotically fastest algorithms,
anditisreasonably efficientinpractice.
26.1 Flownetworks
Inthissection, wegiveagraph-theoretic definition offlownetworks, discuss their
properties, and define the maximum-flow problem precisely. We also introduce
somehelpful notation.
Flownetworksandflows
A flow network G .V;E/ is a directed graph in which each edge .u;/ E
D 2
has a nonnegative capacity c.u;/ 0. We further require that if E contains an

edge .u;/, then there is no edge .;u/ in the reverse direction. (We shall see
shortly how to work around this restriction.) If .u;/ E, then for convenience
62
we define c.u;/ 0, and we disallow self-loops. We distinguish two vertices
D
inaflownetwork: asource s and asinkt. Forconvenience, weassume that each
vertexliesonsomepathfromthesourcetothesink. Thatis,foreachvertex V,
the flow network contains a path s ;  ; t. The graph is therefore conne2 cted
and, since each vertex other than s has at least one entering edge, E V 1.
j j  j j(cid:0)
Figure26.1showsanexampleofaflownetwork.
We are now ready to define flows more formally. Let G .V;E/ be a flow
D
networkwithacapacityfunctionc. Lets bethesourceofthenetwork,andlett be
the sink. A flow in G is a real-valued function f V V R that satisfies the
W  !
followingtwoproperties:
Capacityconstraint: Forallu; V,werequire0 f.u;/ c.u;/.
2  
Flowconservation: Forallu V s;t ,werequire
2 (cid:0)f g
f.;u/ f.u;/:
D
 V  V
X2 X2
When.u;/ E,therecanbenoflowfromuto,andf.u;/ 0.
62 D
710 Chapter26 MaximumFlow
12
1 6 20
s t 7 9 4
Edmonton Saskatoon
12/12
Vancouver Winnipeg 1 1/1 6 15/20
s t
13 4
14
Calgary Regina
7/7 9 4/ 4/1
v v v v
1 3 1 3
v v
8/13
v v
4/4
2 4 2 4
11/14
(a) (b)
Figure26.1 (a) Aflow network G .V;E/ for theLuckyPuckCompany’s truckingproblem.
D
TheVancouverfactoryisthesources,andtheWinnipegwarehouseisthesinkt.Thecompanyships
pucksthroughintermediatecities,butonlyc.u;/cratesperdaycangofromcityutocity. Each
edgeislabeledwithitscapacity.(b)Aflowf inGwithvalue f 19.Eachedge.u;/islabeled
j jD
byf.u;/=c.u;/. Theslashnotationmerelyseparatestheflowandcapacity;itdoesnotindicate
division.
Wecallthenonnegativequantityf.u;/theflowfromvertexutovertex. The
value f ofaflowf isdefinedas
j j
f f.s;/ f.;s/; (26.1)
j j D (cid:0)
 V  V
X2 X2
thatis,thetotalflowoutofthesourceminustheflowintothesource. (Here,the
jj
notation denotes flow value, not absolute value or cardinality.) Typically, a flow
networkwillnothaveanyedgesintothesource,andtheflowintothesource,given
bythesummation f.;s/,willbe0. Weinclude it,however, because when
 V
we introduce residual2networks later in this chapter, the flow into the source will
P
becomesignificant. Inthemaximum-flowproblem,wearegivenaflownetworkG
withsources andsinkt,andwewishtofindaflowofmaximumvalue.
Before seeing an example of a network-flow problem, let us briefly explore the
definition of flow and the two flow properties. The capacity constraint simply
says that the flow from one vertex to another must be nonnegative and must not
exceed the given capacity. Theflow-conservation property says that the total flow
into a vertex other than the source or sink must equal the total flow out of that
vertex—informally, “flowinequalsflowout.”
Anexampleofflow
A flow network can model the trucking problem shown in Figure 26.1(a). The
Lucky Puck Company has a factory (source s) in Vancouver that manufactures
hockeypucks,andithasawarehouse(sinkt)inWinnipegthatstocksthem. Lucky
26.1 Flownetworks 711
12
1 6 20
s t 7 9 4
v v
1 3
13 4
v v
2 4
14
(a) (b)
01
12
1 6 20
s t 7 9 4
v v
1 3
10
v¢
10
13 4
v v
2 4
14
Figure26.2 Convertinganetworkwithantiparalleledgestoanequivalentonewithnoantiparallel
edges.(a)Aflownetworkcontainingboththeedges.1;2/and.2;1/.(b)Anequivalentnetwork
withnoantiparalleledges. Weaddthenewvertex 0,andwereplaceedge.1;2/bythepairof
edges.1; 0/and. 0;2/,bothwiththesamecapacityas.1;2/.
Puck leases space on trucks from another firm to ship the pucks from the factory
tothe warehouse. Because thetrucks travel overspecified routes (edges) between
cities (vertices) and have a limited capacity, Lucky Puck can ship at most c.u;/
crates per day between each pair of cities u and  in Figure 26.1(a). Lucky Puck
has no control over these routes and capacities, and so the company cannot alter
the flow network shown in Figure 26.1(a). They need to determine the largest
numberpofcratesperdaythattheycanshipandthentoproducethisamount,since
there is no point in producing more pucks than they can ship to their warehouse.
Lucky Puck is not concerned with how long it takes for a given puck to get from
thefactorytothewarehouse; theycareonlythatp cratesperdayleavethefactory
andp cratesperdayarriveatthewarehouse.
We can model the “flow” of shipments with a flow in this network because the
number of crates shipped perday from one city toanother issubject toa capacity
constraint. Additionally, the model must obey flow conservation, for in a steady
state,therateatwhichpucksenteranintermediatecitymustequaltherateatwhich
theyleave. Otherwise,crateswouldaccumulate atintermediate cities.
Modelingproblemswithantiparallel edges
Suppose that the trucking firm offered Lucky Puck the opportunity to lease space
for 10crates in trucks going from Edmonton toCalgary. It would seem natural to
addthisopportunitytoourexampleandformthenetworkshowninFigure26.2(a).
Thisnetwork suffers from one problem, however: itviolates our original assump-
tionthatifanedge. ; / E,then. ; / E. Wecallthetwoedges. ; /
1 2 2 1 1 2
2 62
and . ; / antiparallel. Thus, if wewish to model a flow problem with antipar-
2 1
allel edges, we must transform the network into an equivalent one containing no
712 Chapter26 MaximumFlow
antiparallel edges. Figure 26.2(b) displays this equivalent network. We choose
oneofthetwoantiparallel edges, inthiscase. ; /,andsplititbyadding anew
1 2
vertex  and replacing edge . ; / with the pair of edges . ; / and . ; /.
0 1 2 1 0 0 2
We also set the capacity of both new edges to the capacity of the original edge.
The resulting network satisfies the property that if an edge is in the network, the
reverseedgeisnot. Exercise26.1-1asksyoutoprovethattheresulting networkis
equivalent totheoriginal one.
Thus, we see that a real-world flow problem might be most naturally modeled
by a network with antiparallel edges. It will be convenient to disallow antipar-
allel edges, however, and so we have a straightforward way to convert a network
containing antiparallel edgesintoanequivalent onewithnoantiparallel edges.
Networkswithmultiplesourcesandsinks
A maximum-flow problem may have several sources and sinks, rather than just
one of each. The Lucky Puck Company, for example, might actually have a set
of m factories s ;s ;:::;s and a set of n warehouses t ;t ;:::;t , as shown
1 2 m 1 2 n
f g f g
in Figure 26.3(a). Fortunately, this problem is no harder than ordinary maximum
flow.
We can reduce the problem of determining a maximum flow in a network with
multiple sources and multiple sinks to an ordinary maximum-flow problem. Fig-
ure26.3(b)showshowtoconvertthenetworkfrom(a)toanordinaryflownetwork
with only a single source and a single sink. We add a supersource s and add a
directed edge.s;s /withcapacity c.s;s / foreachi 1;2;:::;m. Wealso
i i
D 1 D
createanewsupersinkt andaddadirectededge.t ;t/withcapacityc.t ;t/
i i
D1
foreach i 1;2;:::;n. Intuitively, anyflowinthenetwork in(a)corresponds to
D
a flow in the network in (b), and vice versa. The single source s simply provides
as much flow as desired for the multiple sources s , and the single sink t likewise
i
consumes as much flow as desired for the multiple sinks t . Exercise 26.1-2 asks
i
youtoproveformallythatthetwoproblemsareequivalent.
Exercises
26.1-1
Show that splitting anedge inaflownetwork yields anequivalent network. More
formally, suppose that flow network G contains edge .u;/, and we create a new
flow network G by creating a new vertex x and replacing .u;/ by new edges
0
.u;x/ and .x;/ with c.u;x/ c.x;/ c.u;/. Show that a maximum flow
D D
inG hasthesamevalueasamaximumflowinG.
0
26.1 Flownetworks 713
s s
1 1
10 10
1 2 1 2
3 ¥ 3
s t s t
2 1 2 1
1 5 1 5
5 ¥ 5 ¥
8 8
6 ¥ 6 ¥
s t s s t t
3 2 3 2
2 0 2 0
14 14
¥
¥
7 7
13 13
s t s t
4 3 ¥ 4 3
1 8 1 8
11 11
2 2
s s
5 5
(a) (b)
Figure26.3 Convertingamultiple-source, multiple-sinkmaximum-flow problemintoaproblem
withasinglesourceandasinglesink. (a)AflownetworkwithfivesourcesS s1;s2;s3;s4;s5
Df g
andthreesinksT t1;t2;t3 . (b)Anequivalentsingle-source,single-sinkflownetwork. Weadd
Df g
asupersources andanedgewithinfinitecapacityfroms toeachofthemultiplesources. Wealso
addasupersinkt andanedgewithinfinitecapacityfromeachofthemultiplesinkstot.
26.1-2
Extend the flow properties and definitions to the multiple-source, multiple-sink
problem. Show that any flow in a multiple-source, multiple-sink flow network
corresponds to a flow of identical value in the single-source, single-sink network
obtained byaddingasupersource andasupersink, andviceversa.
26.1-3
SupposethataflownetworkG .V;E/violates theassumption thatthenetwork
containsapaths ;  ; t foraD llvertices V. Letubeavertexforwhichthere
is no path s ; u ; t. Show that there mus2 t exist a maximum flow f in G such
thatf.u;/ f.;u/ 0forallvertices V.
D D 2
714 Chapter26 MaximumFlow
26.1-4
Letf beaflowinanetwork,andlet˛ bearealnumber. Thescalarflowproduct,
denoted ˛f,isafunction fromV V toR definedby

.˛f/.u;/ ˛ f.u;/:
D 
Provethattheflowsinanetworkformaconvexset. Thatis,showthatiff andf
1 2
areflows,thensois˛f .1 ˛/f forall˛ intherange0 ˛ 1.
1 2
C (cid:0)  
26.1-5
Statethemaximum-flowproblemasalinear-programming problem.
26.1-6
ProfessorAdamhastwochildrenwho,unfortunately, dislikeeachother. Theprob-
lem issosevere thatnotonly dotheyrefuse towalktoschool together, butinfact
eachonerefuses towalkonanyblock thattheotherchildhasstepped onthatday.
The children have no problem with their paths crossing at a corner. Fortunately
boththeprofessor’s houseandtheschool areoncorners, butbeyondthatheisnot
sure if it is going to be possible to send both of his children to the same school.
The professor has a map of his town. Show how to formulate the problem of de-
terminingwhetherbothhischildrencangotothesameschoolasamaximum-flow
problem.
26.1-7
Suppose that, inaddition to edge capacities, aflownetwork hasvertex capacities.
Thatiseachvertex  hasalimitl./onhowmuchflowcanpass though . Show
howtotransform aflownetworkG .V;E/withvertexcapacities intoanequiv-
D
alent flownetworkG .V ;E /withoutvertexcapacities, suchthatamaximum
0 0 0
D
flow in G has the same value as a maximum flow in G. How many vertices and
0
edgesdoesG have?
0
26.2 The Ford-Fulkerson method
This section presents the Ford-Fulkerson method for solving the maximum-flow
problem. Wecallita“method”ratherthanan“algorithm”becauseitencompasses
severalimplementationswithdifferingrunningtimes. TheFord-Fulkersonmethod
depends on three important ideas that transcend the method and are relevant to
many flow algorithms and problems: residual networks, augmenting paths, and
cuts. These ideas are essential to the important max-flow min-cut theorem (The-
orem 26.6), which characterizes the value of a maximum flow in terms of cuts of
26.2 TheFord-Fulkersonmethod 715
the flow network. We end this section by presenting one specific implementation
oftheFord-Fulkerson methodandanalyzing itsrunning time.
TheFord-Fulkerson method iteratively increases thevalue ofthe flow. Westart
with f.u;/ 0 for all u; V, giving an initial flow of value 0. At each
D 2
iteration, we increase the flow value in G by finding an “augmenting path” in an
associated “residual network” G . Once we know the edges of an augmenting
f
path in G , we can easily identify specific edges in G for which we can change
f
the flow so that we increase the value of the flow. Although each iteration of the
Ford-Fulkerson method increases the value of the flow, we shall see that the flow
onanyparticularedgeofG mayincreaseordecrease;decreasingtheflowonsome
edgesmaybenecessaryinordertoenableanalgorithmtosendmoreflowfromthe
source tothe sink. Werepeatedly augment the flowuntil the residual network has
no more augmenting paths. The max-flow min-cut theorem will show that upon
termination, thisprocess yieldsamaximumflow.
FORD-FULKERSON-METHOD.G;s;t/
1 initialize flowf to0
2 whilethereexistsanaugmenting pathp intheresidual networkG
f
3 augmentflowf alongp
4 returnf
In order to implement and analyze the Ford-Fulkerson method, we need to intro-
duceseveraladditional concepts.
Residualnetworks
Intuitively,givenaflownetworkG andaflowf,theresidualnetworkG consists
f
ofedgeswithcapacities thatrepresenthowwecanchangetheflowonedgesofG.
An edge of the flow network can admit an amount of additional flow equal to the
edge’s capacity minus the flow on that edge. If that value is positive, we place
that edge into G with a “residual capacity” of c .u;/ c.u;/ f.u;/.
f f
D (cid:0)
The only edges of G that are in G are those that can admit more flow; those
f
edges .u;/ whose flowequals their capacity have c .u;/ 0, and they arenot
f
D
inG .
f
The residual network G may also contain edges that are not in G, however.
f
Asan algorithm manipulates the flow,withthe goal ofincreasing the total flow,it
might need to decrease the flow on a particular edge. In order to represent a pos-
sible decrease ofapositive flowf.u;/onanedge inG, weplace anedge .;u/
intoG withresidualcapacityc .;u/ f.u;/—thatis,anedgethatcanadmit
f f
D
flow in the opposite direction to .u;/, at most canceling out the flow on .u;/.
These reverse edges in the residual network allow an algorithm to send back flow
716 Chapter26 MaximumFlow
it has already sent along an edge. Sending flow back along an edge is equiva-
lent to decreasing the flow on the edge, which is a necessary operation in many
algorithms.
Moreformally, suppose thatwehaveaflownetworkG .V;E/withsource s
D
and sink t. Let f be a flow in G, and consider a pair of vertices u; V. We
2
definetheresidualcapacity c .u;/by
f
c.u;/ f.u;/ if.u;/ E ;
(cid:0) 2
c .u;/ f.;u/ if.;u/ E ; (26.2)
f
D 2
0 otherwise:

Becauseofourassumptionthat.u;/ E implies.;u/ E,exactlyonecasein
2 62
equation (26.2)applies toeachordered pairofvertices.
As an example of equation (26.2), if c.u;/ 16 and f.u;/ 11, then we
D D
can increase f.u;/ by up to c .u;/ 5 units before we exceed the capacity
f
D
constraint on edge .u;/. We also wish to allow an algorithm to return up to 11
unitsofflowfrom tou,andhencec .;u/ 11.
f
D
Given a flow network G .V;E/ and a flow f, the residual network of G
D
induced byf isG .V;E /,where
f f
D
E .u;/ V V c .u;/ > 0 : (26.3)
f f
D f 2  W g
That is, as promised above, each edge of the residual network, or residual edge,
can admit aflowthat isgreater than 0. Figure 26.4(a) repeats the flownetwork G
andflowf ofFigure26.1(b),andFigure26.4(b)showsthecorrespondingresidual
networkG . TheedgesinE areeitheredgesinE ortheirreversals, andthus
f f
E 2 E :
f
j j  j j
ObservethattheresidualnetworkG issimilartoaflownetworkwithcapacities
f
given by c . It does not satisfy our definition of a flow network because it may
f
contain both an edge .u;/ and its reversal .;u/. Other than this difference, a
residual network has the same properties as a flow network, and we can define a
flow in the residual network as one that satisfies the definition of a flow, but with
respecttocapacities c inthenetworkG .
f f
Aflowinaresidual networkprovides aroadmap foradding flowtotheoriginal
flow network. If f is a flow in G and f is a flow in the corresponding residual
0
networkG ,wedefinef f ,theaugmentationofflowf byf ,tobeafunction
f 0 0
fromV V toR ,defined" by

f.u;/ f .u;/ f .;u/ if.u;/ E ;
.f f /.u;/ C 0 (cid:0) 0 2 (26.4)
0
" D
(
0 otherwise:
26.2 TheFord-Fulkersonmethod 717
12
5 5
s
15
t
9
7 5 1 3
4
8
11
12/12
1
1/1 6 19/20
s t 7/7 9 4/1
12/12
1
1/1 6 15/20
1 1 4
s t 5
3
(b)
12/13 4/4
11/14
(c)
7/7 9 4/ 4/1
8/13 4/4
11/14
12
5 1
s
19
t
(d)
1 3 7
v v v v
1 3 1 3
v v v v
2 4 2 4
(a)
v v v v
1 3 1 3
1 1
1
3 4
v v 12 v v
2 4 2 4
11
Figure26.4 (a)TheflownetworkG andflowf ofFigure26.1(b). (b)TheresidualnetworkG f
with augmenting path p shaded; its residual capacity is c f.p/ c f.2;3/ 4. Edges with
D D
residualcapacityequalto0,suchas.1;3/,arenotshown,aconventionwefollowintheremainder
ofthissection.(c)TheflowinGthatresultsfromaugmentingalongpathpbyitsresidualcapacity4.
Edgescarryingnoflow,suchas.3;2/,arelabeledonlybytheircapacity,anotherconventionwe
followthroughout.(d)Theresidualnetworkinducedbytheflowin(c).
Theintuitionbehindthisdefinitionfollowsthedefinitionoftheresidualnetwork.
We increase the flow on .u;/ by f .u;/ but decrease it by f .;u/ because
0 0
pushing flow on the reverse edge in the residual network signifies decreasing the
flow in the original network. Pushing flow on the reverse edge in the residual
networkisalsoknownascancellation. Forexample,ifwesend5cratesofhockey
pucks from uto  and send 2crates from  to u, wecould equivalently (from the
perspective ofthefinalresult)justsend3cratesfromuto andnonefrom tou.
Cancellation ofthistypeiscrucialforanymaximum-flowalgorithm.
Lemma26.1
Let G .V;E/ be a flow network with source s and sink t, and let f be a flow
D
in G. Let G be the residual network of G induced by f, and let f be a flow
f 0
in G . Then the function f f defined in equation (26.4) is a flow in G with
f 0
"
value f f f f .
0 0
j " j D j jCj j
Proof Wefirstverifythatf f obeysthecapacityconstraintforeachedgeinE
0
"
andflowconservation ateachvertexinV s;t .
(cid:0)f g
718 Chapter26 MaximumFlow
For the capacity constraint, first observe that if .u;/ E, then c .;u/
f
2 D
f.u;/. Therefore,wehavef .;u/ c .;u/ f.u;/,andhence
0 f
 D
.f f /.u;/ f.u;/ f .u;/ f .;u/ (byequation(26.4))
0 0 0
" D C (cid:0)
f.u;/ f .u;/ f.u;/ (becausef .;u/ f.u;/)
0 0
 C (cid:0) 
f .u;/
0
D
0:

Inaddition,
.f f /.u;/
0
"
f.u;/ f .u;/ f .;u/ (byequation (26.4))
0 0
D C (cid:0)
f.u;/ f .u;/ (becauseflowsarenonnegative)
0
 C
f.u;/ c .u;/ (capacityconstraint)
f
 C
f.u;/ c.u;/ f.u;/ (definitionofc )
f
D C (cid:0)
c.u;/:
D
Forflowconservation, because both f andf obeyflowconservation, wehave
0
thatforallu V s;t ,
2 (cid:0)f g
.f f /.u;/ .f.u;/ f .u;/ f .;u//
0 0 0
" D C (cid:0)
 V  V
X2 X2
f.u;/ f .u;/ f .;u/
0 0
D C (cid:0)
 V  V  V
X2 X2 X2
f.;u/ f .;u/ f .u;/
0 0
D C (cid:0)
 V  V  V
X2 X2 X2
.f.;u/ f .;u/ f .u;//
0 0
D C (cid:0)
 V
X2
.f f /.;u/;
0
D "
 V
X2
wherethethirdlinefollowsfromthesecondbyflowconservation.
Finally, we compute the value of f f . Recall that we disallow antiparallel
0
"
edges in G (but not in G ), and hence for each vertex  V, weknow that there
f
2
can be an edge .s;/ or .;s/, but never both. We define V  .s;/ E
1
D f W 2 g
to be the set of vertices with edges from s, and V  .;s/ E to be the
2
D f W 2 g
set of vertices with edges to s. We have V V V and, because we disallow
1 2
[ 
antiparallel edges,V V . Wenowcompute
1 2
\ D ;
f f .f f /.s;/ .f f /.;s/
0 0 0
j " j D " (cid:0) "
 V  V
X2 X2
.f f /.s;/ .f f /.;s/; (26.5)
0 0
D " (cid:0) "
 X2V1  X2V2
26.2 TheFord-Fulkersonmethod 719
wherethesecondlinefollowsbecause .f f /.w;x/is0if.w;x/ E. Wenow
0
" 62
applythedefinitionoff f toequation(26.5),andthenreorderandgroupterms
0
"
toobtain
f f
0
j " j
.f.s;/ f .s;/ f .;s// .f.;s/ f .;s/ f .s;//
0 0 0 0
D C (cid:0) (cid:0) C (cid:0)
 X2V1  X2V2
f.s;/ f .s;/ f .;s/
0 0
D C (cid:0)
 X2V1  X2V1  X2V1
f.;s/ f .;s/ f .s;/
0 0
(cid:0) (cid:0) C
 X2V2  X2V2  X2V2
f.s;/ f.;s/
D (cid:0)
 X2V1  X2V2
f .s;/ f .s;/ f .;s/ f .;s/
0 0 0 0
C C (cid:0) (cid:0)
 X2V1  X2V2  X2V1  X2V2
f.s;/ f.;s/ f .s;/ f .;s/: (26.6)
0 0
D (cid:0) C (cid:0)
 X2V1  X2V2  2XV1 [V2  2XV1 [V2
In equation (26.6), we can extend all four summations to sum over V, since each
additional termhasvalue0. (Exercise 26.2-1asksyoutoprovethisformally.) We
thushave
f f f.s;/ f.;s/ f .s;/ f .;s/ (26.7)
0 0 0
j " j D (cid:0) C (cid:0)
 V  V  V  V
X2 X2 X2 X2
f f :
0
D j jCj j
Augmentingpaths
Given a flow network G .V;E/ and a flow f, an augmenting path p is a
D
simple path from s to t inthe residual network G . Bythedefinition ofthe resid-
f
ual network, we may increase the flow on an edge .u;/ of an augmenting path
by up to c .u;/ without violating the capacity constraint on whichever of .u;/
f
and.;u/isintheoriginalflownetworkG.
Theshaded path in Figure 26.4(b) is an augmenting path. Treating the residual
networkG inthefigureasaflownetwork, wecanincreasetheflowthrougheach
f
edgeofthispath byupto4units withoutviolating acapacity constraint, sincethe
smallest residual capacity on this path is c . ; / 4. We call the maximum
f 2 3
D
amount by which wecan increase the flowoneach edge inan augmenting path p
theresidualcapacity ofp,givenby
c .p/ min c .u;/ .u;/isonp :
f f
D f W g
720 Chapter26 MaximumFlow
The following lemma, whose proof weleave as Exercise 26.2-7, makes the above
argument moreprecise.
Lemma26.2
LetG .V;E/beaflownetwork,letf beaflowinG,andletpbeanaugmenting
pathinD G . Defineafunction f V V R by
f p
W  !
c .p/ if.u;/isonp ;
f .u;/ f (26.8)
p
D ( 0 otherwise :
Then,f isaflowinG withvalue f c .p/> 0.
p f p f
j j D
Thefollowingcorollaryshowsthatifweaugmentf byf ,wegetanotherflow
p
in G whose value is closer to the maximum. Figure 26.4(c) shows the result of
augmenting the flow f from Figure 26.4(a) by the flow f in Figure 26.4(b), and
p
Figure26.4(d)showstheensuingresidual network.
Corollary 26.3
Let G .V;E/ be a flow network, let f be a flow in G, and let p be an
D
augmenting path in G . Let f be defined as in equation (26.8), and suppose
f p
that we augment f by f . Then the function f f is a flow in G with value
p p
"
f f f f > f .
p p
j " j D j jCj j j j
Proof ImmediatefromLemmas26.1and26.2.
Cutsofflownetworks
TheFord-Fulkersonmethodrepeatedlyaugmentstheflowalongaugmentingpaths
until it has found a maximum flow. How do we know that when the algorithm
terminates, wehaveactuallyfoundamaximumflow? Themax-flowmin-cuttheo-
rem,whichweshallproveshortly,tellsusthataflowismaximumifandonlyifits
residual network contains noaugmenting path. Toprovethistheorem, though, we
mustfirstexplorethenotionofacutofaflownetwork.
A cut .S;T/ of flow network G .V;E/ is a partition of V into S and
D
T V S such that s S and t T. (This definition is similar to the def-
D (cid:0) 2 2
inition of “cut” that we used for minimum spanning trees in Chapter 23, except
that here we are cutting a directed graph rather than an undirected graph, and we
insist that s S and t T.) If f is a flow, then the net flow f.S;T/ across the
2 2
cut.S;T/isdefinedtobe
f.S;T/ f.u;/ f.;u/: (26.9)
D (cid:0)
u S  T u S  T
X2 X2 X2 X2
26.2 TheFord-Fulkersonmethod 721
12/12
1
1/1 6 15/20
s t 7/7 9
4/
4/1
v v
1 3
8/13
v v
4/4
2 4
11/14
S T
Figure 26.5 A cut .S;T/ in the flow network of Figure 26.1(b), where S s;1;2 and
D f g
T 3;4;t . The vertices in S are black, and the vertices in T are white. The net flow
Df g
across.S;T/isf.S;T/ 19,andthecapacityisc.S;T/ 26.
D D
Thecapacity ofthecut.S;T/is
c.S;T/ c.u;/: (26.10)
D
u S  T
X2 X2
Aminimumcutofa network is acut whose capacity is minimum over all cuts of
thenetwork.
The asymmetry between the definitions of flow and capacity of a cut is inten-
tional and important. For capacity, we count only the capacities of edges going
from S to T, ignoring edges in the reverse direction. For flow, we consider the
flowgoingfromS toT minustheflowgoinginthereversedirectionfromT toS.
Thereasonforthisdifference willbecomeclearlaterinthissection.
Figure 26.5 shows the cut . s; ; ;  ; ;t / in the flow network of Fig-
1 2 3 4
f g f g
ure26.1(b). Thenetflowacrossthiscutis
f. ; / f. ; / f. ; / 12 11 4
1 3 2 4 3 2
C (cid:0) D C (cid:0)
19;
D
andthecapacityofthiscutis
c. ; / c. ; / 12 14
1 3 2 4
C D C
26:
D
Thefollowinglemmashowsthat,foragivenflowf,thenetflowacrossanycut
isthesame,anditequals f ,thevalueoftheflow.
j j
Lemma26.4
Letf beaflowinaflownetworkG withsourcesandsinkt,andlet.S;T/beany
cutofG. Thenthenetflowacross.S;T/isf.S;T/ f .
Dj j
722 Chapter26 MaximumFlow
Proof Wecanrewritetheflow-conservationconditionforanynodeu V s;t
2 (cid:0)f g
as
f.u;/ f.;u/ 0: (26.11)
(cid:0) D
 V  V
X2 X2
Taking the definition of f from equation (26.1) and adding the left-hand side of
j j
equation (26.11), whichequals0,summedoverallvertices inS s ,gives
(cid:0)f g
f f.s;/ f.;s/ f.u;/ f.;u/ :
j j D (cid:0) C (cid:0)
!
 V  V u S s  V  V
X2 X2 2X(cid:0)f g X2 X2
Expanding theright-hand summationandregrouping termsyields
f f.s;/ f.;s/ f.u;/ f.;u/
j j D (cid:0) C (cid:0)
 V  V u S s  V u S s  V
X2 X2 2X(cid:0)f gX2 2X(cid:0)f gX2
f.s;/ f.u;/ f.;s/ f.;u/
D C (cid:0) C
! !
 V u S s  V u S s
X2 2X(cid:0)f g X2 2X(cid:0)f g
f.u;/ f.;u/:
D (cid:0)
 V u S  V u S
X2 X2 X2 X2
Because V S T and S T , we can split each summation over V into
D [ \ D ;
summationsoverS andT toobtain
f f.u;/ f.u;/ f.;u/ f.;u/
j j D C (cid:0) (cid:0)
 S u S  T u S  S u S  T u S
X2 X2 X2 X2 X2 X2 X2 X2
f.u;/ f.;u/
D (cid:0)
 T u S  T u S
X2 X2 X2 X2
f.u;/ f.;u/ :
C (cid:0)
!
 S u S  S u S
X2 X2 X2 X2
The two summations within the parentheses are actually the same, since for all
verticesx;y S,thetermf.x;y/appearsonceineachsummation. Hence,these
2
summationscancel, andwehave
f f.u;/ f.;u/
j j D (cid:0)
u S  T u S  T
X2 X2 X2 X2
f.S;T/:
D
A corollary to Lemma 26.4 shows how we can use cut capacities to bound the
valueofaflow.
26.2 TheFord-Fulkersonmethod 723
Corollary26.5
Thevalueofanyflowf inaflownetworkGisboundedfromabovebythecapacity
ofanycutofG.
Proof Let.S;T/beanycutofG andletf beanyflow. ByLemma26.4andthe
capacityconstraint,
f f.S;T/
j j D
f.u;/ f.;u/
D (cid:0)
u S  T u S  T
X2 X2 X2 X2
f.u;/

u S  T
X2 X2
c.u;/

u S  T
X2 X2
c.S;T/:
D
Corollary 26.5 yields the immediate consequence that the value of a maximum
flow in a network is bounded from above by the capacity of a minimum cut of
the network. The important max-flow min-cut theorem, which we now state and
prove, says that the value of a maximum flow is in fact equal to the capacity of a
minimumcut.
Theorem26.6(Max-flowmin-cuttheorem)
If f is a flow in a flow network G .V;E/ with source s and sink t, then the
D
followingconditions areequivalent:
1. f isamaximumflowinG.
2. Theresidual networkG contains noaugmenting paths.
f
3. f c.S;T/forsomecut.S;T/ofG.
j j D
Proof .1/ .2/: Suppose for the sake of contradiction that f is a maximum
)
flow in G but that G has an augmenting path p. Then, by Corollary 26.3, the
f
flowfoundbyaugmentingf byf ,wheref isgivenbyequation(26.8),isaflow
p p
in G with value strictly greater than f , contradicting the assumption that f is a
j j
maximumflow.
.2/ .3/: Suppose that G has no augmenting path, that is, that G contains
f f
)
nopathfroms tot. Define
S  V thereexistsapathfroms to inG
f
D f 2 W g
and T V S. Thepartition .S;T/ isacut: wehave s S trivially and t S
D (cid:0) 2 62
because there is no path from s to t in G . Now consider a pair of vertices
f
724 Chapter26 MaximumFlow
u S and  T. If .u;/ E, we must have f.u;/ c.u;/, since
2 2 2 D
otherwise .u;/ E , which would place  in set S. If .;u/ E, we must
f
2 2
have f.;u/ 0, because otherwise c .u;/ f.;u/ would be positive and
f
D D
wewouldhave.u;/ E ,whichwouldplace inS. Ofcourse,ifneither.u;/
f
2
nor.;u/isinE,thenf.u;/ f.;u/ 0. Wethushave
D D
f.S;T/ f.u;/ f.;u/
D (cid:0)
u S  T  T u S
X2 X2 X2 X2
c.u;/ 0
D (cid:0)
u S  T  T u S
X2 X2 X2 X2
c.S;T/:
D
ByLemma26.4,therefore, f f.S;T/ c.S;T/.
j j D D
.3/ .1/: ByCorollary26.5, f c.S;T/forallcuts.S;T/. Thecondition
) j j 
f c.S;T/thusimpliesthatf isamaximumflow.
j j D
ThebasicFord-Fulkersonalgorithm
In each iteration of the Ford-Fulkerson method, we find some augmenting path p
and use p to modify the flow f. As Lemma26.2 and Corollary 26.3 suggest, we
replacef byf f ,obtaining anewflowwhosevalueis f f . Thefollow-
p p
" j jCj j
ingimplementation ofthemethodcomputes themaximum flowinaflownetwork
G .V;E/ by updating the flow attribute .u;/:f for each edge .u;/ E.1
D 2
If .u;/ E, we assume implicitly that .u;/:f 0. We also assume that we
62 D
are given the capacities c.u;/ along with the flow network, and c.u;/ 0
D
if .u;/ E. We compute the residual capacity c .u;/ in accordance with the
f
62
formula (26.2). Theexpression c .p/inthe code is justatemporary variable that
f
storestheresidualcapacity ofthepathp.
FORD-FULKERSON.G;s;t/
1 foreachedge.u;/ G:E
2
2 .u;/:f 0
D
3 whilethereexistsapathp froms tot intheresidualnetworkG
f
4 c .p/ min c .u;/ .u;/isinp
f f
D f W g
5 foreachedge.u;/inp
6 if.u;/ E
2
7 .u;/:f .u;/:f c .p/
f
D C
8 else.;u/:f .;u/:f c .p/
f
D (cid:0)
1Recall from Section22.1 that werepresent an attributef for edge .u;/ withthesame styleof
notation—.u;/:f—thatweuseforanattributeofanyotherobject.
26.2 TheFord-Fulkersonmethod 725
The FORD-FULKERSON algorithm simply expands on the FORD-FULKERSON-
METHOD pseudocode given earlier. Figure 26.6showsthe result of each iteration
ina sample run. Lines 1–2 initialize the flow f to 0. Thewhileloop oflines 3–8
repeatedly finds an augmenting path p in G and augments flow f along p by
f
the residual capacity c .p/. Each residual edge in path p is either an edge in the
f
original network or the reversal of an edge in the original network. Lines 6–8
update the flow in each case appropriately, adding flow when the residual edge is
anoriginaledgeandsubtractingitotherwise. Whennoaugmentingpathsexist,the
flowf isamaximumflow.
AnalysisofFord-Fulkerson
Therunning timeof FORD-FULKERSON depends onhowwefindtheaugmenting
pathpinline3. Ifwechooseitpoorly,thealgorithmmightnoteventerminate: the
valueoftheflowwillincreasewithsuccessive augmentations, butitneednoteven
converge to the maximum flow value.2 If we find the augmenting path by using a
breadth-firstsearch(whichwesawinSection22.2),however,thealgorithmrunsin
polynomialtime. Beforeprovingthisresult,weobtainasimpleboundforthecase
inwhichwechoosetheaugmenting patharbitrarily andallcapacities areintegers.
In practice, the maximum-flow problem often arises with integral capacities. If
the capacities are rational numbers, we can apply an appropriate scaling transfor-
mationtomakethemallintegral. Iff denotesamaximumflowinthetransformed

network, then a straightforward implementation of FORD-FULKERSON executes
thewhileloopoflines3–8atmost f times,sincetheflowvalueincreases byat

j j
leastoneunitineachiteration.
Wecanperformtheworkdonewithinthewhileloopefficientlyifweimplement
theflownetworkG .V;E/withtherightdatastructure andfindanaugmenting
D
path by a linear-time algorithm. Let us assume that we keep a data structure cor-
responding toadirected graph G .V;E /, where E .u;/ .u;/ E or
0 0 0
D D f W 2
.;u/ E . Edges in the network G are also edges in G , and therefore we can
0
2 g
easily maintain capacities and flows in this data structure. Given a flow f on G,
the edges in the residual network G consist of all edges .u;/ of G such that
f 0
c .u;/ > 0, where c conforms to equation (26.2). The time to find a path in
f f
a residual network is therefore O.V E / O.E/ if we use either depth-first
0
C D
search or breadth-first search. Each iteration of the while loop thus takes O.E/
time, as does the initialization in lines 1–2, making the total running time of the
FORD-FULKERSON algorithm O.E f

/.
j j
2TheFord-Fulkersonmethodmightfailtoterminateonlyifedgecapacitiesareirrationalnumbers.
726 Chapter26 MaximumFlow
1 2
4
4
4/4
4
12
v
1
1 6 20
s t
10
4
16
4
7 9 4
4/12
v
1
4/1 6 20
s t
13 4
14
7 9 4/
13 4/4
4/14
s t 7
8
8/12
v v
1 1 4 20 4/1 6 4/20
4
s t 5
13 4
7 9 4/
4/13 4/4
4/14
s t
10
4
7
4 8/12
v v
1 1 8 8/1 6 8/20
s t 5
9
4
7
v v
3 3
(a)
v v v v
2 4 2 4
v v
1 2 3 3
(b)
v v v v
2 4 2 4
v v
3 3
(c) 9
v v
4/13
v v
4/4
2 4 2 4
4/14
4
4
4
4
4
Figure26.6 TheexecutionofthebasicFord-Fulkersonalgorithm. (a)–(e)Successiveiterationsof
thewhileloop. TheleftsideofeachpartshowstheresidualnetworkG f fromline3withashaded
augmentingpathp.Therightsideofeachpartshowsthenewflowf thatresultsfromaugmentingf
byfp.Theresidualnetworkin(a)istheinputnetworkG.
When the capacities are integral and the optimal flow value f is small, the

j j
runningtimeoftheFord-Fulkersonalgorithmisgood. Figure26.7(a)showsanex-
ampleofwhatcanhappenonasimpleflownetworkforwhich f islarge. Amax-

j j
imum flow in this network has value 2,000,000: 1,000,000 units of flow traverse
the path s u t, and another 1,000,000 units traverse the path s  t. If
! ! ! !
thefirstaugmentingpathfoundbyFORD-FULKERSON iss u  t,shown
! ! !
in Figure 26.7(a), theflowhas value 1 after thefirst iteration. Theresulting resid-
ual network appears in Figure 26.7(b). If the second iteration finds the augment-
ing path s  u t, as shown in Figure 26.7(b), the flow then has value 2.
! ! !
Figure 26.7(c) shows the resulting residual network. We can continue, choosing
theaugmentingpaths u  t intheodd-numbered iterations andtheaug-
! ! !
mentingpaths  u t intheeven-numberediterations. Wewouldperform
! ! !
atotalof2,000,000augmentations,increasingtheflowvaluebyonly1unitineach.
26.2 TheFord-Fulkersonmethod 727
9
4
8
8
2
11
4
1 2
2
11
4
4
8
9 4
12
8 s t
9
7
8/12
v 8/1 6 1 15/20
s t
4
4
7/7
v 1
9
10 11/13 4/4
11/14
12
1
s
19
t 7
v v 3 3
(d)
v 4 v 2 v 4
v
3
(f)
3 4
v
4
11
4
4
8
9 4
5
s
15
t 7
12/12
v 1 2/1 6 1 19/20
s t
4
11
7/7
v v v 1 3 3
(e) 9
3
v 4
11/13
v 2 11/14 v 4
4/4
4
8
8
v 2
v 2
v
1
v
2
Figure26.6,continued (f)Theresidualnetworkatthelastwhilelooptest. Ithasnoaugmenting
paths,andtheflowf shownin(e)isthereforeamaximumflow. Thevalueofthemaximumflow
foundis23.
TheEdmonds-Karpalgorithm
We can improve the bound on FORD-FULKERSON by finding the augmenting
path p in line 3 with a breadth-first search. That is, we choose the augmenting
path as a shortest path from s to t in the residual network, where each edge has
unit distance (weight). We call the Ford-Fulkerson method so implemented the
Edmonds-Karp algorithm. Wenow prove that the Edmonds-Karp algorithm runs
inO.VE2/time.
The analysis depends on the distances to vertices in the residual network G .
f
The following lemma uses the notation ı .u;/ for the shortest-path distance
f
fromuto inG ,whereeachedgehasunitdistance.
f
Lemma26.7
IftheEdmonds-KarpalgorithmisrunonaflownetworkG .V;E/withsources
D
and sink t, then for all vertices  V s;t , the shortest-path distance ı .s;/
f
2 (cid:0)f g
intheresidualnetworkG increasesmonotonically witheachflowaugmentation.
f
728 Chapter26 MaximumFlow
1,0 0
0,0 0 0 1,000,000 999,999
1
s t 9 9 9,9 9 9
1
1
u
9 9
9,9 9 9 1,000,000
1
1,000,000
v 1,0 0
0,0 0 0 s t 99 19,999 1
u
9 9
9,9 9 9
1
1,000,000
v
s 9 9 9,9 9 19 t 1
u
v
(a) (b) (c)
Figure 26.7 (a) A flow network for which FORD-FULKERSON can take ‚.E jf j/ time,
where f is a maximum flow, shown here with f 2,000,000. The shaded path is an aug-
 j j D
mentingpathwithresidualcapacity1. (b)Theresultingresidualnetwork,withanotheraugmenting
pathwhoseresidualcapacityis1.(c)Theresultingresidualnetwork.
Proof We will suppose that for some vertex  V s;t , there is a flow aug-
2 (cid:0)f g
mentation that causes the shortest-path distance from s to  to decrease, and then
wewillderiveacontradiction. Letf betheflowjustbeforethefirstaugmentation
that decreases some shortest-path distance, and let f be the flow just afterward.
0
Let  be the vertex with the minimum ı f0.s;/ whose distance was decreased by
the augmentation, sothat ı f0.s;/ < ı f.s;/. Letp s ; u  be ashortest
D !
pathfroms to inG f0,sothat.u;/ E f0 and
2
ı f0.s;u/ ı f0.s;/ 1: (26.12)
D (cid:0)
Becauseofhowwechose,weknowthatthedistanceofvertexufromthesources
didnotdecrease, i.e.,
ı f0.s;u/ ı f.s;u/: (26.13)

Weclaimthat.u;/ E . Why? Ifwehad.u;/ E ,thenwewouldalsohave
f f
62 2
ı .s;/ ı .s;u/ 1 (byLemma24.10,thetriangleinequality)
f f
 C
ı f0.s;u/ 1 (byinequality (26.13))
 C
ı f0.s;/ (byequation(26.12)) ,
D
whichcontradicts ourassumption thatı f0.s;/ < ı f.s;/.
How can wehave .u;/ E
f
and .u;/ E f0? The augmentation must have
62 2
increased the flow from  to u. The Edmonds-Karp algorithm always augments
flowalong shortest paths, and therefore it augmented along ashortest path from s
touinG thathas.;u/asitslastedge. Therefore,
f
ı .s;/ ı .s;u/ 1
f f
D (cid:0)
ı f0.s;u/ 1 (byinequality (26.13))
 (cid:0)
ı f0.s;/ 2 (byequation (26.12)) ,
D (cid:0)
26.2 TheFord-Fulkersonmethod 729
which contradicts our assumption that ı f0.s;/ < ı f.s;/. We conclude that our
assumption thatsuchavertex existsisincorrect.
The next theorem bounds the number of iterations of the Edmonds-Karp algo-
rithm.
Theorem26.8
IftheEdmonds-KarpalgorithmisrunonaflownetworkG .V;E/withsources
D
andsinkt,thenthetotalnumberofflowaugmentationsperformedbythealgorithm
isO.VE/.
Proof Wesay that anedge .u;/ inaresidual network G iscritical on anaug-
f
menting path p ifthe residual capacity of p isthe residual capacity of .u;/, that
is,ifc .p/ c .u;/. Afterwehaveaugmented flowalonganaugmenting path,
f f
D
any critical edge on the path disappears from the residual network. Moreover, at
leastoneedgeonanyaugmenting pathmustbecritical. Wewillshowthateachof
the E edgescanbecomecritical atmost V =2times.
j j j j
Letuand beverticesinV thatareconnectedbyanedgeinE. Sinceaugment-
ingpathsareshortest paths,when.u;/iscriticalforthefirsttime,wehave
ı .s;/ ı .s;u/ 1:
f f
D C
Oncetheflowisaugmented, theedge.u;/disappears fromtheresidual network.
Itcannotreappearlateronanotheraugmentingpathuntilaftertheflowfromuto
is decreased, which occurs only if .;u/ appears on an augmenting path. If f is
0
theflowinG whenthiseventoccurs, thenwehave
ı .s;u/ ı .s;/ 1:
f0 f0
D C
Sinceı f.s;/ ı f0.s;/byLemma26.7,wehave

ı .s;u/ ı .s;/ 1
f0 f0
D C
ı .s;/ 1
f
 C
ı .s;u/ 2:
f
D C
Consequently, from the time .u;/ becomes critical to the time when it next
becomes critical, the distance of u from the source increases by at least 2. The
distance ofufrom the source is initially atleast 0. The intermediate vertices on a
shortest path from s to u cannot contain s, u, or t (since .u;/ on an augmenting
pathimpliesthatu t). Therefore, untilubecomesunreachable fromthesource,
¤
ifever,itsdistanceisatmost V 2. Thus,afterthefirsttimethat.u;/becomes
j j(cid:0)
critical, itcanbecomecritical atmost. V 2/=2 V =2 1timesmore,fora
j j(cid:0) D j j (cid:0)
totalofatmost V =2times. SincethereareO.E/pairsofverticesthatcanhavean
j j
edgebetweentheminaresidualnetwork,thetotalnumberofcriticaledgesduring
730 Chapter26 MaximumFlow
the entire execution ofthe Edmonds-Karp algorithm isO.VE/. Eachaugmenting
pathhasatleastonecriticaledge,andhencethetheorem follows.
Because wecanimplement each iteration of FORD-FULKERSON inO.E/time
whenwefindtheaugmentingpathbybreadth-firstsearch,thetotalrunningtimeof
theEdmonds-KarpalgorithmisO.VE2/. Weshallseethatpush-relabelalgorithms
can yield even better bounds. The algorithm of Section 26.4 gives a method for
achieving an O.V2E/ running time, which forms the basis for the O.V3/-time
algorithm ofSection26.5.
Exercises
26.2-1
Prove that the summations in equation (26.6) equal the summations in equa-
tion(26.7).
26.2-2
InFigure26.1(b), whatistheflowacross thecut. s; ; ;  ; ;t /? Whatis
2 4 1 3
f g f g
thecapacity ofthiscut?
26.2-3
Show the execution of the Edmonds-Karp algorithm on the flow network of Fig-
ure26.1(a).
26.2-4
IntheexampleofFigure26.6,whatistheminimumcutcorresponding tothemax-
imumflowshown? Oftheaugmenting paths appearing intheexample, whichone
cancels flow?
26.2-5
Recallthattheconstruction inSection26.1thatconvertsaflownetworkwithmul-
tiple sources and sinks into a single-source, single-sink network adds edges with
infinite capacity. Prove that any flow in the resulting network has a finite value
if the edges of the original network with multiple sources and sinks have finite
capacity.
26.2-6
Suppose that each source s in a flow network with multiple sources and sinks
i
produces exactly p units of flow, so that f.s ;/ p . Suppose also
i  V i D i
that each sink t consumes exactly q units, so2that f.;t / q , where
p
qj
. Showhow
toconverj
t
theprP
oblem
offi n2dV
ing
afloj wD
f
thj
atobeys
i i D j j P
P P
26.2 TheFord-Fulkersonmethod 731
theseadditionalconstraintsintotheproblemoffindingamaximumflowinasingle-
source,single-sink flownetwork.
26.2-7
ProveLemma26.2.
26.2-8
Supposethatweredefinetheresidualnetworktodisallow edgesintos. Arguethat
theprocedure FORD-FULKERSON stillcorrectly computesamaximumflow.
26.2-9
Supposethatbothf andf areflowsinanetworkG andwecomputeflowf f .
0 0
"
Does the augmented flow satisfy the flow conservation property? Does it satisfy
thecapacityconstraint?
26.2-10
Show how to find a maximum flow in a network G .V;E/ by a sequence of at
D
most E augmentingpaths. (Hint:Determinethepathsafterfindingthemaximum
j j
flow.)
26.2-11
The edge connectivity of an undirected graph is the minimum number k of edges
thatmustberemovedtodisconnect thegraph. Forexample, theedgeconnectivity
of a tree is 1, and the edge connectivity of a cyclic chain of vertices is 2. Show
how to determine the edge connectivity of an undirected graph G .V;E/ by
D
running a maximum-flow algorithm on at most V flow networks, each having
j j
O.V/verticesandO.E/edges.
26.2-12
Suppose that you are given a flow network G, and G has edges entering the
sources. Letf beaflowinG inwhichoneoftheedges.;s/enteringthesource
has f.;s/ 1. Prove that there must exist another flow f with f .;s/ 0
0 0
D D
such that f f . Givean O.E/-time algorithm tocompute f , given f, and
0 0
j j D j j
assumingthatalledgecapacities areintegers.
26.2-13
Suppose thatyouwishtofind,amongallminimum cutsinaflownetwork G with
integral capacities, one that contains the smallest number of edges. Show how to
modifythecapacitiesofGtocreateanewflownetworkG inwhichanyminimum
0
cutinG isaminimumcutwiththesmallestnumberofedgesinG.
0
732 Chapter26 MaximumFlow
26.3 Maximum bipartite matching
Somecombinatorial problems caneasilybecastasmaximum-flowproblems. The
multiple-source, multiple-sink maximum-flowproblem from Section26.1gaveus
oneexample. Someothercombinatorialproblemsseemonthesurfacetohavelittle
to do with flow networks, but can in fact be reduced to maximum-flow problems.
Thissectionpresentsonesuchproblem: findingamaximummatchinginabipartite
graph. In order to solve this problem, we shall take advantage of an integrality
property provided by the Ford-Fulkerson method. We shall also see how to use
the Ford-Fulkerson method to solve the maximum-bipartite-matching problem on
agraphG .V;E/inO.VE/time.
D
Themaximum-bipartite-matchingproblem
Given an undirected graph G .V;E/, a matching is a subset of edges M E
D 
such that for all vertices  V, at most one edge of M is incident on . We
2
say that a vertex  V is matched by the matching M if some edge in M is
2
incident on ; otherwise,  is unmatched. A maximum matching is a matching
of maximum cardinality, that is, a matching M such that for any matching M ,
0
we have M M . In this section, we shall restrict our attention to finding
0
j j  j j
maximum matchings in bipartite graphs: graphs in which the vertex set can be
partitioned into V L R, where L and R are disjoint and all edges in E
D [
go between L and R. We further assume that every vertex in V has at least one
incident edge. Figure26.8illustrates thenotion ofamatchinginabipartitegraph.
The problem of finding a maximum matching in a bipartite graph has many
practical applications. Asanexample,wemightconsider matching asetLofma-
chineswithasetRoftaskstobeperformedsimultaneously. Wetakethepresence
of edge .u;/ in E to mean that a particular machine u L is capable of per-
2
formingaparticulartask R. Amaximummatchingprovidesworkforasmany
2
machinesaspossible.
Findingamaximumbipartitematching
We can use the Ford-Fulkerson method to find a maximum matching in an undi-
rected bipartite graph G .V;E/intimepolynomial in V and E . Thetrickis
D j j j j
to construct a flow network in which flows correspond to matchings, as shown in
Figure 26.8(c). We define the corresponding flow network G .V ;E / for the
0 0 0
D
bipartite graph G as follows. We let the source s and sink t be new vertices not
in V, and welet V V s;t . Ifthe vertex partition of G is V L R, the
0
D [f g D [
26.3 Maximumbipartitematching 733
s t
L R L R L R
(a) (b) (c)
Figure26.8 A bipartite graph G .V;E/ with vertex partition V L R. (a) A matching
D D [
withcardinality2,indicatedbyshadededges. (b)Amaximummatchingwithcardinality3. (c)The
correspondingflownetworkG withamaximumflowshown. Eachedgehasunitcapacity. Shaded
0
edgeshaveaflowof1,andallotheredgescarrynoflow.TheshadededgesfromLtoRcorrespond
tothoseinthemaximummatchingfrom(b).
directededgesofG aretheedgesofE,directed fromLtoR,alongwith V new
0
j j
directededges:
E .s;u/ u L .u;/ .u;/ E .;t/  R :
0
D f W 2 g[f W 2 g[f W 2 g
To complete the construction, we assign unit capacity to each edge in E . Since
0
eachvertexinV hasatleastoneincident edge, E V =2. Thus, E E
0
j j  j j j j  j j D
E V 3 E ,andso E ‚.E/.
0
j jCj j  j j j j D
ThefollowinglemmashowsthatamatchinginG corresponds directlytoaflow
in G’s corresponding flow network G . We say that a flow f on a flow network
0
G .V;E/isinteger-valued iff.u;/isanintegerforall.u;/ V V.
D 2 
Lemma26.9
Let G .V;E/ be a bipartite graph with vertex partition V L R, and let
D D [
G .V ;E / be its corresponding flow network. If M is a matching in G, then
0 0 0
D
there is an integer-valued flow f in G with value f M . Conversely, if f
0
j j D j j
is an integer-valued flow in G , then there is a matching M in G with cardinality
0
M f .
j j D j j
Proof We first show that a matching M in G corresponds to an integer-valued
flow f in G . Define f as follows. If .u;/ M, then f.s;u/ f.u;/
0
2 D D
f.;t/ 1. For all other edges .u;/ E , we define f.u;/ 0. It is simple
0
D 2 D
toverifythatf satisfiesthecapacity constraint andflowconservation.
734 Chapter26 MaximumFlow
Intuitively, each edge .u;/ M corresponds to one unit of flow in G that
0
2
traverses the path s u  t. Moreover, the paths induced by edges in M
! ! !
are vertex-disjoint, except fors andt. Thenet flowacross cut .L s ;R t /
[f g [f g
isequalto M ;thus,byLemma26.4,thevalueoftheflowis f M .
j j j j D j j
Toprovetheconverse, letf beaninteger-valued flowinG ,andlet
0
M .u;/ u L;  R; andf.u;/ > 0 :
D f W 2 2 g
Each vertex u L has only one entering edge, namely .s;u/, and its capacity
2
is 1. Thus, each u L has at most one unit of flow entering it, and if one unit of
2
flow does enter, by flow conservation, one unit of flow must leave. Furthermore,
sincef isinteger-valued, foreachu L,theoneunitofflowcanenteronatmost
2
oneedgeandcanleaveonatmostoneedge. Thus,oneunitofflowentersuifand
only if there is exactly one vertex  R such that f.u;/ 1, and at most one
2 D
edge leaving each u L carries positive flow. A symmetric argument applies to
2
each R. ThesetM istherefore amatching.
2
To see that M f , observe that for every matched vertex u L, we have
j j D j j 2
f.s;u/ 1, and for every edge .u;/ E M, we have f.u;/ 0. Conse-
D 2 (cid:0) D
quently, f.L s ;R t /,thenetflowacross cut.L s ;R t /,isequal
[f g [f g [f g [f g
to M . ApplyingLemma26.4,wehavethat f f.L s ;R t / M .
j j j j D [f g [f g D j j
Based on Lemma 26.9, we would like to conclude that a maximum matching
in a bipartite graph G corresponds to a maximum flow in its corresponding flow
networkG ,andwecantherefore computeamaximummatchinginG byrunning
0
a maximum-flow algorithm on G . The only hitch in this reasoning is that the
0
maximum-flow algorithm might return a flow in G for which some f.u;/ is
0
not an integer, even though the flow value f must be an integer. The following
j j
theorem shows that if we use the Ford-Fulkerson method, this difficulty cannot
arise.
Theorem26.10(Integrality theorem)
Ifthecapacity function c takes ononlyintegral values, thenthemaximumflowf
produced by the Ford-Fulkerson method has the property that f is an integer.
j j
Moreover, forallverticesuand,thevalueoff.u;/isaninteger.
Proof The proof is by induction on the number of iterations. We leave it as
Exercise26.3-2.
WecannowprovethefollowingcorollarytoLemma26.9.
26.3 Maximumbipartitematching 735
Corollary26.11
ThecardinalityofamaximummatchingM inabipartitegraphG equalsthevalue
ofamaximumflowf initscorresponding flownetworkG .
0
Proof We use the nomenclature from Lemma 26.9. Suppose that M is a max-
imum matching in G and that the corresponding flow f in G is not maximum.
0
Then there is a maximum flow f in G such that f > f . Since the ca-
0 0 0
j j j j
pacities in G are integer-valued, by Theorem 26.10, we can assume that f is
0 0
integer-valued. Thus, f corresponds to a matching M in G with cardinality
0 0
M f > f M , contradicting our assumption that M is a maximum
0 0
j j D j j j j D j j
matching. Inasimilarmanner,wecanshowthatiff isamaximumflowinG ,its
0
corresponding matchingisamaximummatchingonG.
Thus,givenabipartiteundirectedgraphG,wecanfindamaximummatchingby
creatingtheflownetworkG ,runningtheFord-Fulkersonmethod,anddirectlyob-
0
tainingamaximummatchingM fromtheinteger-valued maximumflowf found.
Sinceanymatchinginabipartitegraphhascardinalityatmostmin.L;R/ O.V/,
D
thevalue of themaximum flowin G isO.V/. Wecantherefore findamaximum
0
matchinginabipartite graphintimeO.VE / O.VE/,since E ‚.E/.
0 0
D j j D
Exercises
26.3-1
RuntheFord-Fulkersonalgorithm ontheflownetworkinFigure26.8(c)andshow
the residual network after each flow augmentation. Number the vertices in L top
tobottom from 1to5and inR topto bottom from 6to9. Foreach iteration, pick
theaugmenting paththatislexicographically smallest.
26.3-2
ProveTheorem26.10.
26.3-3
LetG .V;E/be abipartite graph withvertex partition V L R,and letG
0
D D [
beits corresponding flownetwork. Giveagood upper bound on thelength of any
augmenting pathfoundinG
0
duringtheexecutionof FORD-FULKERSON.
26.3-4 ?
A perfect matching is a matching in which every vertex is matched. Let G
D
.V;E/ be an undirected bipartite graph with vertex partition V L R, where
D [
L R . ForanyX V,definetheneighborhood ofX as
j j D j j 
N.X/ y V .x;y/ E forsomex X ;
D f 2 W 2 2 g
736 Chapter26 MaximumFlow
that is, the set of vertices adjacent to some member of X. Prove Hall’s theorem:
there exists a perfect matching in G if and only if A N.A/ for every subset
j j  j j
A L.

26.3-5 ?
WesaythatabipartitegraphG .V;E/,whereV L R,isd-regularifevery
D D [
vertex V hasdegreeexactlyd. Everyd-regularbipartitegraphhas L R .
2 j j D j j
Prove that every d-regular bipartite graph has a matching of cardinality L by
j j
arguing thataminimumcutofthecorresponding flownetworkhascapacity L .
j j
? 26.4 Push-relabel algorithms
In this section, we present the “push-relabel” approach to computing maximum
flows. To date, many of the asymptotically fastest maximum-flow algorithms are
push-relabel algorithms, and thefastest actual implementations of maximum-flow
algorithms are based on the push-relabel method. Push-relabel methods also effi-
ciently solve other flow problems, such as the minimum-cost flow problem. This
section introduces Goldberg’s “generic” maximum-flow algorithm, which has a
simple implementation that runs in O.V2E/ time, thereby improving upon the
O.VE2/ bound of the Edmonds-Karp algorithm. Section 26.5 refines the generic
algorithm toobtainanotherpush-relabel algorithm thatrunsinO.V3/time.
Push-relabel algorithms work in a more localized manner than the Ford-
Fulkersonmethod. Ratherthanexaminetheentireresidualnetworktofindanaug-
menting path, push-relabel algorithms work on one vertex at a time, looking only
at the vertex’s neighbors in the residual network. Furthermore, unlike the Ford-
Fulkerson method, push-relabel algorithms do not maintain the flow-conservation
property throughout theirexecution. Theydo,however, maintainapreflow,which
isafunctionf V V R thatsatisfiesthecapacityconstraintandthefollowing
W  !
relaxation offlowconservation:
f.;u/ f.u;/ 0
(cid:0) 
 V  V
X2 X2
for all vertices u V s . That is, the flow into a vertex may exceed the flow
2 (cid:0)f g
out. Wecallthequantity
e.u/ f.;u/ f.u;/ (26.14)
D (cid:0)
 V  V
X2 X2
the excess flow into vertex u. The excess at a vertex is the amount by which the
flowinexceeds theflowout. Wesaythatavertexu V s;t isoverflowingif
2 (cid:0)f g
e.u/ > 0.
26.4 Push-relabelalgorithms 737
We shall begin this section by describing the intuition behind the push-relabel
method. We shall then investigate the two operations employed by the method:
“pushing” preflow and “relabeling” a vertex. Finally, we shall present a generic
push-relabel algorithm andanalyzeitscorrectness andrunningtime.
Intuition
Youcan understand the intuition behind the push-relabel method in terms of fluid
flows: we consider a flow network G .V;E/ to be a system of interconnected
D
pipes of given capacities. Applying this analogy to the Ford-Fulkerson method,
wemight saythat each augmenting path inthe network gives riseto anadditional
stream of fluid, with no branch points, flowing from the source to the sink. The
Ford-Fulkersonmethoditerativelyaddsmorestreamsofflowuntilnomorecanbe
added.
The generic push-relabel algorithm has a rather different intuition. As before,
directed edges correspond to pipes. Vertices, which are pipe junctions, have two
interesting properties. First, to accommodate excess flow, each vertex has an out-
flowpipeleadingtoanarbitrarilylargereservoirthatcanaccumulatefluid. Second,
eachvertex,itsreservoir,andallitspipeconnectionssitonaplatformwhoseheight
increases asthealgorithm progresses.
Vertexheights determine howflowispushed: wepush flowonly downhill, that
is,fromahighervertextoalowervertex. Theflowfromalowervertextoahigher
vertex may be positive, but operations that push flow push it only downhill. We
fixthe height of the source at V and the height of the sink at 0. All other vertex
j j
heightsstartat0andincreasewithtime. Thealgorithm firstsendsasmuchflowas
possible downhill from thesource toward thesink. Theamount itsends isexactly
enough to fill each outgoing pipe from the source to capacity; that is, it sends the
capacity of the cut .s;V s /. When flow first enters an intermediate vertex, it
(cid:0)f g
collectsinthevertex’sreservoir. Fromthere,weeventually pushitdownhill.
We may eventually find that the only pipes that leave a vertex u and are not
already saturated with flow connect to vertices that are on the same level as u or
areuphillfromu. Inthiscase,toridanoverflowingvertexuofitsexcessflow,we
must increase its height—an operation called “relabeling” vertex u. We increase
its height to one unit more than the height of the lowest of its neighbors to which
ithasanunsaturated pipe. Afteravertex isrelabeled, therefore, ithasatleast one
outgoing pipethrough whichwecanpushmoreflow.
Eventually,alltheflowthatcanpossiblygetthroughtothesinkhasarrivedthere.
Nomorecanarrive,becausethepipesobeythecapacityconstraints; theamountof
flowacross anycut isstill limited by thecapacity ofthe cut. Tomakethe preflow
a “legal” flow, the algorithm then sends the excess collected in the reservoirs of
overflowing vertices back to the source by continuing to relabel vertices to above
738 Chapter26 MaximumFlow
the fixed height V of the source. As we shall see, once we have emptied all the
j j
reservoirs, thepreflowisnotonlya“legal”flow,itisalsoamaximumflow.
Thebasicoperations
Fromthepreceding discussion, weseethatapush-relabel algorithm performs two
basic operations: pushing flow excess from a vertex to one of its neighbors and
relabeling a vertex. The situations in which these operations apply depend on the
heights ofvertices, whichwenowdefineprecisely.
Let G .V;E/ be a flow network with source s and sink t, and let f be a
preflow inD G. A function h V N is a height function3 if h.s/ V ,
W ! D j j
h.t/ 0,and
D
h.u/ h./ 1
 C
foreveryresidual edge.u;/ E . Weimmediatelyobtainthefollowinglemma.
f
2
Lemma26.12
Let G .V;E/ be a flownetwork, let f be apreflow in G, and let hbe a height
D
function on V. For any two vertices u; V, if h.u/ > h./ 1, then .u;/ is
2 C
notanedgeintheresidual network.
Thepushoperation
ThebasicoperationPUSH.u;/appliesifuisanoverflowingvertex,c f.u;/ > 0,
andh.u/ h./ 1. Thepseudocodebelowupdatesthepreflowf andtheexcess
D C
flows for u and . It assumes that we can compute residual capacity c .u;/ in
f
constant time given c and f. We maintain the excess flow stored at a vertex u as
the attribute u:eand theheight ofuastheattribute u:h. Theexpression  .u;/
f
isatemporaryvariablethatstorestheamountofflowthatwecanpushfromuto.
3Intheliterature,aheightfunctionistypicallycalleda“distancefunction,”andtheheightofavertex
iscalleda“distancelabel.” Weusetheterm“height”becauseitismoresuggestiveoftheintuition
behindthealgorithm. Weretaintheuseoftheterm“relabel”torefertotheoperationthatincreases
theheightofavertex. Theheightofavertexisrelatedtoitsdistancefromthesinkt,aswouldbe
foundinabreadth-firstsearchofthetransposeGT.
26.4 Push-relabelalgorithms 739
PUSH.u;/
1 //Applieswhen: uisoverflowing,c .u;/ >0,andu:h :h 1.
f
D C
2 //Action: Push .u;/ min.u:e;c .u;//unitsofflowfromuto.
f f
D
3  .u;/ min.u:e;c .u;//
f f
D
4 if.u;/ E
2
5 .u;/:f .u;/:f  .u;/
f
D C
6 else.;u/:f .;u/:f  .u;/
f
D (cid:0)
7 u:e u:e  .u;/
f
D (cid:0)
8 :e :e  .u;/
f
D C
ThecodeforPUSHoperatesasfollows. Becausevertexuhasapositiveexcessu:e
andtheresidualcapacityof.u;/ispositive,wecanincreasetheflowfromuto
by  .u;/ min.u:e;c .u;// without causing u:e to become negative or the
f f
D
capacityc.u;/tobeexceeded. Line3computesthevalue .u;/,andlines4–6
f
update f. Line 5 increases the flow on edge .u;/, because we are pushing flow
over a residual edge that is also an original edge. Line 6 decreases the flow on
edge .;u/, because the residual edge is actually the reverse of an edge in the
original network. Finally, lines 7–8 update the excess flowsinto vertices uand .
Thus,iff isapreflowbefore PUSH iscalled, itremainsapreflowafterward.
Observe that nothing in the code for PUSH depends on the heights of u and ,
yetweprohibititfrombeinginvokedunlessu:h :h 1. Thus,wepushexcess
D C
flow downhill only by a height differential of 1. By Lemma 26.12, no residual
edges exist between two vertices whose heights differ by more than 1, and thus,
as long as the attribute h is indeed a height function, we would gain nothing by
allowingflowtobepushed downhillbyaheightdifferential ofmorethan1.
We call the operation PUSH.u;/ a push from u to . If a push operation ap-
plies to some edge .u;/ leaving a vertex u, we also say that the push operation
appliestou. Itisasaturatingpushifedge.u;/intheresidualnetworkbecomes
saturated (c .u;/ 0 afterward); otherwise, it is a nonsaturating push. If an
f
D
edgebecomessaturated, itdisappears fromtheresidual network. Asimplelemma
characterizes oneresultofanonsaturating push.
Lemma26.13
Afteranonsaturating pushfromuto,thevertexuisnolongeroverflowing.
Proof Since the push was nonsaturating, the amount of flow  .u;/ actually
f
pushed must equal u:e prior to the push. Since u:e is reduced by this amount, it
becomes0afterthepush.
740 Chapter26 MaximumFlow
Therelabel operation
Thebasic operation RELABEL.u/applies ifuisoverflowingand ifu:h :hfor

all edges .u;/ E . In other words, we can relabel an overflowing vertex u if
f
2
for everyvertex  forwhichthere isresidual capacity from uto,flowcannot be
pushed from u to  because  is not downhill from u. (Recall that by definition,
neitherthesources northesinkt canbeoverflowing,andsos andt areineligible
forrelabeling.)
RELABEL.u/
1 //Applieswhen: uisoverflowingandforall V suchthat.u;/ E ,
f
2 2
wehaveu:h :h.

2 //Action: Increase theheightofu.
3 u:h 1 min :h .u;/ E
f
D C f W 2 g
Whenwecall theoperation RELABEL.u/,wesaythat vertexuisrelabeled. Note
that when u is relabeled, E must contain at least one edge that leaves u, so that
f
the minimization in the code is over a nonempty set. This property follows from
theassumption thatuisoverflowing,whichinturntellsusthat
u:e f.;u/ f.u;/> 0:
D (cid:0)
 V  V
X2 X2
Since allflowsarenonnegative, wemusttherefore have atleast one vertex  such
that .;u/:f > 0. But then, c .u;/ > 0, which implies that .u;/ E . The
f f
2
operation RELABEL.u/thusgivesuthegreatest height allowedbytheconstraints
onheightfunctions.
Thegenericalgorithm
The generic push-relabel algorithm uses the following subroutine to create an ini-
tialpreflowintheflownetwork.
INITIALIZE-PREFLOW.G;s/
1 foreachvertex G:V
2
2 :h 0
D
3 :e 0
D
4 foreachedge.u;/ G:E
2
5 .u;/:f 0
D
6 s:h G:V
D j j
7 foreachvertex s:Adj
2
8 .s;/:f c.s;/
D
9 :e c.s;/
D
10 s:e s:e c.s;/
D (cid:0)
26.4 Push-relabelalgorithms 741
INITIALIZE-PREFLOW createsaninitialpreflowf definedby
c.u;/ ifu s ;
.u;/:f D (26.15)
D
(
0 otherwise:
Thatis,wefilltocapacityeachedgeleavingthesources,andallotheredgescarry
noflow. Foreach vertex  adjacent to the source, weinitially have :e c.s;/,
D
and we initialize s:e to the negative of the sum of these capacities. The generic
algorithm alsobeginswithaninitialheightfunctionh,givenby
V ifu s ;
u:h j j D (26.16)
D
(
0 otherwise:
Equation(26.16)definesaheightfunctionbecausetheonlyedges.u;/forwhich
u:h > :h 1 are those for which u s, and those edges are saturated, which
C D
meansthattheyarenotintheresidualnetwork.
Initialization, followed by a sequence of push and relabel operations, executed
innoparticular order, yieldsthe GENERIC-PUSH-RELABEL algorithm:
GENERIC-PUSH-RELABEL.G/
1 INITIALIZE-PREFLOW.G;s/
2 whilethereexistsanapplicable pushorrelabeloperation
3 selectanapplicable pushorrelabeloperation andperformit
The following lemma tells us that as long as an overflowing vertex exists, at least
oneofthetwobasicoperations applies.
Lemma26.14(Anoverflowingvertexcanbeeitherpushedorrelabeled)
Let G .V;E/ be a flow network with source s and sink t, let f be a preflow,
D
andlethbeanyheightfunctionforf. Ifuisanyoverflowingvertex,theneithera
pushorrelabeloperation appliestoit.
Proof For any residual edge .u;/, we have h.u/ h./ 1 because h is a
 C
height function. If a push operation does not apply to an overflowing vertex u,
then for all residual edges .u;/, we must have h.u/ < h./ 1, which implies
C
h.u/ h./. Thus,arelabeloperation appliestou.

Correctnessofthepush-relabelmethod
To show that the generic push-relabel algorithm solves the maximum-flow prob-
lem, we shall first prove that if it terminates, the preflow f is a maximum flow.
Weshall later prove that it terminates. We start with some observations about the
heightfunctionh.
742 Chapter26 MaximumFlow
Lemma26.15(Vertexheightsneverdecrease)
During the execution of the GENERIC-PUSH-RELABEL procedure on a flow net-
work G .V;E/, for each vertex u V, the height u:h never decreases. More-
D 2
over,wheneverarelabeloperation isappliedtoavertexu,itsheightu:hincreases
byatleast1.
Proof Because vertex heights change only during relabel operations, it suffices
to prove the second statement of the lemma. If vertex u is about to be rela-
beled, then for all vertices  such that .u;/ E , we have u:h :h. Thus,
f
2 
u:h < 1 min :h .u;/ E ,andsotheoperation mustincrease u:h.
f
C f W 2 g
Lemma26.16
LetG .V;E/beaflownetworkwithsources andsinkt. Thentheexecutionof
D
GENERIC-PUSH-RELABEL onG maintainstheattribute hasaheightfunction.
Proof The proof is by induction on the number of basic operations performed.
Initially, hisaheightfunction, aswehavealreadyobserved.
Weclaimthatifhisaheightfunction, thenanoperation RELABEL.u/leaves h
a height function. If we look at a residual edge .u;/ E that leaves u, then
f
2
the operation RELABEL.u/ ensures that u:h :h 1 afterward. Now consider
 C
aresidual edge .w;u/that enters u. ByLemma26.15, w:h u:h 1before the
 C
operation RELABEL.u/ implies w:h < u:h 1 afterward. Thus, the operation
C
RELABEL.u/leaveshaheightfunction.
Now,consideranoperation PUSH.u;/. Thisoperationmayaddtheedge.;u/
to E , and it may remove .u;/ from E . In the former case, we have
f f
:h u:h 1 < u:h 1, and so h remains a height function. In the latter case,
D (cid:0) C
removing .u;/ from the residual network removes the corresponding constraint,
andhagainremainsaheightfunction.
Thefollowinglemmagivesanimportantproperty ofheightfunctions.
Lemma26.17
Let G .V;E/ be a flow network with source s and sink t, let f be a preflow
D
inG, andlethbeaheight function onV. Thenthere isnopath fromthesource s
tothesinkt intheresidual networkG .
f
Proof AssumeforthesakeofcontradictionthatG containsapathpfromstot,
f
where p  ; ;:::; ,  s, and  t. Without loss of generality, p
0 1 k 0 k
D h i D D
is a simple path, and so k < V . For i 0;1;:::;k 1, edge . ; / E .
i i 1 f
Becausehisaheightfunctionj , hj . / hD . / 1for(cid:0) i 0;1;:::;k C 1.2 Com-
i i 1
biningtheseinequalitiesoverpathpy ieldshC.s/ C h.t/ kD . Butbecause(cid:0) h.t/ 0,
 C D
26.4 Push-relabelalgorithms 743
we have h.s/ k < V , which contradicts the requirement that h.s/ V in a
 j j D j j
heightfunction.
Wearenowreadytoshowthatifthegeneric push-relabel algorithm terminates,
thepreflowitcomputesisamaximumflow.
Theorem26.18(Correctness ofthegenericpush-relabel algorithm)
If the algorithm GENERIC-PUSH-RELABEL terminates when run on a flow net-
work G .V;E/ with source s and sink t, then the preflow f it computes is a
D
maximumflowforG.
Proof Weusethefollowingloopinvariant:
Each time the while loop test in line 2 in GENERIC-PUSH-RELABEL is
executed, f isapreflow.
Initialization: INITIALIZE-PREFLOW makesf apreflow.
Maintenance: Theonlyoperationswithinthewhileloopoflines2–3arepushand
relabel. Relabeloperationsaffectonlyheightattributesandnottheflowvalues;
hencetheydonotaffectwhetherf isapreflow. Asarguedonpage739,iff is
apreflowpriortoapushoperation, itremainsapreflowafterward.
Termination: Attermination, eachvertexinV s;t musthaveanexcessof0,
(cid:0)f g
becausebyLemma26.14andtheinvariantthatf isalwaysapreflow,thereare
no overflowing vertices. Therefore, f is a flow. Lemma26.16 shows that h is
aheightfunction attermination, andthusLemma26.17tellsusthatthereisno
pathfroms tot intheresidual networkG . Bythemax-flowmin-cut theorem
f
(Theorem26.6),therefore, f isamaximumflow.
Analysisofthepush-relabelmethod
Toshowthatthegenericpush-relabel algorithmindeedterminates,weshallbound
thenumberofoperations itperforms. Weboundseparately eachofthethreetypes
of operations: relabels, saturating pushes, and nonsaturating pushes. With knowl-
edgeofthesebounds,itisastraightforward problemtoconstructanalgorithmthat
runs in O.V2E/ time. Before beginning the analysis, however, we prove an im-
portantlemma. Recallthatweallowedgesintothesourceintheresidual network.
Lemma26.19
LetG .V;E/beaflownetworkwithsources andsinkt,andletf beapreflow
D
inG. Then, foranyoverflowingvertexx,thereisasimplepathfromx tos inthe
residualnetworkG .
f
744 Chapter26 MaximumFlow
Proof Foranoverflowingvertexx,letU  thereexistsasimplepathfromx
D f W
to inG ,andsupposeforthesakeofcontradictionthats U. LetU V U.
f
g 62 D (cid:0)
We take the definition of excess from equation (26.14), sum over all vertices
inU,andnotethatV U U,toobtain
D [
e.u/
u U
X2
f.;u/ f.u;/
D (cid:0)
!
u U  V  V
X2 X2 X2
f.;u/ f.;u/ f.u;/ f.u;/
D C (cid:0) C
! !!
u X2U X 2U X 2U X 2U X 2U
f.;u/ f.;u/ f.u;/ f.u;/
D C (cid:0) (cid:0)
u X2U X 2U uX 2UX 2U u X2U X 2U uX 2UX 2U
f.;u/ f.u;/:
D (cid:0)
uXUX U uXUX U
2 2 2 2
Weknowthatthequantity e.u/mustbepositive because e.x/ > 0,x U,
u U 2
allverticesotherthanshaveno2nnegativeexcess,and,byassumption,s U. Thus,
P 62
wehave
f.;u/ f.u;/ > 0: (26.17)
(cid:0)
uXUXU uXUXU
2 2 2 2
Alledgeflowsarenonnegative, andsoforequation (26.17) tohold, wemusthave
f.;u/ > 0. Hence, there must exist at least one pair of vertices
u U  U
u 2 U an2d  U with f. ;u/ > 0. But, if f. ;u/ > 0, there must be a
0 0 0 0 0 0
P2 P 2
residual edge .u; /, which means that there is a simple path from x to  (the
0 0 0
pathx ; u  ),thuscontradicting thedefinition ofU.
0 0
!
The next lemma bounds the heights of vertices, and its corollary bounds the
numberofrelabeloperations thatareperformedintotal.
Lemma26.20
Let G .V;E/ be a flow network with source s and sink t. At any time during
D
theexecutionofGENERIC-PUSH-RELABEL onG,wehaveu:h 2 V 1forall
 j j(cid:0)
verticesu V.
2
Proof The heights of the source s and the sink t never change because these
vertices are by definition not overflowing. Thus, we always have s:h V and
D j j
t:h 0,bothofwhicharenogreaterthan2 V 1.
D j j(cid:0)
Nowconsideranyvertexu V s;t . Initially,u:h 0 2 V 1. Weshall
2 (cid:0)f g D  j j(cid:0)
showthataftereachrelabelingoperation, westillhaveu:h 2 V 1. Whenuis
 j j(cid:0)
26.4 Push-relabelalgorithms 745
relabeled, itisoverflowing,andLemma26.19tellsusthatthereisasimplepathp
fromutosinG . Letp  ; ;:::; ,where u,  s,andk V 1
f 0 1 k 0 k
Dh i D D  j j(cid:0)
because p is simple. For i 0;1;:::;k 1, we have . ; / E , and
i i 1 f
therefore, byLemma26.16,D :h  :h (cid:0) 1. ExpandingtheseiC nequ2 alities over
i i 1
pathp yieldsu:h  :h  :h kC s:hC . V 1/ 2 V 1.
0 k
D  C  C j j(cid:0) D j j(cid:0)
Corollary26.21(Boundonrelabeloperations)
Let G .V;E/ be a flow network with source s and sink t. Then, during the
D
executionofGENERIC-PUSH-RELABEL onG,thenumberofrelabeloperationsis
atmost2 V 1pervertexandatmost.2 V 1/. V 2/ <2 V 2 overall.
j j(cid:0) j j(cid:0) j j(cid:0) j j
Proof Onlythe V 2verticesinV s;t mayberelabeled. Letu V s;t .
j j(cid:0) (cid:0)f g 2 (cid:0)f g
The operation RELABEL.u/ increases u:h. The value of u:h is initially 0 and by
Lemma 26.20, it grows to at most 2 V 1. Thus, each vertex u V s;t
j j (cid:0) 2 (cid:0) f g
is relabeled at most 2 V 1 times, and the total number of relabel operations
performedisatmost.2j Vj (cid:0) 1/. V 2/ < 2 V 2 .
j j(cid:0) j j(cid:0) j j
Lemma26.20alsohelpsustoboundthenumberofsaturating pushes.
Lemma26.22(Boundonsaturatingpushes)
During the execution of GENERIC-PUSH-RELABEL on any flow network G
D
.V;E/,thenumberofsaturating pushesislessthan2 V E .
j jj j
Proof For any pair of vertices u; V, we will count the saturating pushes
2
fromuto andfrom toutogether, callingthemthesaturatingpushesbetweenu
and . If there are any such pushes, at least one of .u;/ and .;u/ is actually
an edge in E. Now, suppose that a saturating push from u to  has occurred.
At that time, :h u:h 1. In order for another push from u to  to occur
D (cid:0)
later, the algorithm must first push flow from  to u, which cannot happen until
:h u:h 1. Since u:h never decreases, in order for :h u:h 1, the
D C D C
value of:hmustincrease by atleast 2. Likewise, u:hmust increase byatleast 2
between saturating pushes from  to u. Heights start at 0 and, by Lemma 26.20,
neverexceed2 V 1,whichimpliesthatthenumberoftimesanyvertexcanhave
j j(cid:0)
its height increase by 2 is less than V . Since at least one of u:h and :h must
j j
increase by2betweenanytwosaturating pushes betweenuand,therearefewer
than2 V saturatingpushesbetweenuand. Multiplyingbythenumberofedges
j j
givesaboundoflessthan2 V E onthetotalnumberofsaturating pushes.
j jj j
Thefollowinglemmaboundsthenumberofnonsaturating pushesinthegeneric
push-relabel algorithm.
746 Chapter26 MaximumFlow
Lemma26.23(Boundonnonsaturatingpushes)
During the execution of GENERIC-PUSH-RELABEL on any flow network G
.V;E/,thenumberofnonsaturating pushesislessthan4 V 2. V E /. D
j j j jCj j
Proof Define a potential function ˆ :h. Initially, ˆ 0, and the
D e./>0 D
value of ˆ may change after each relabelingW, saturating push, and nonsaturating
P
push. We will bound the amount that saturating pushes and relabelings can con-
tribute totheincrease ofˆ. Thenwewillshowthateachnonsaturating pushmust
decreaseˆbyatleast1,andwillusetheseboundstoderiveanupperboundonthe
numberofnonsaturating pushes.
Let us examine the two ways in which ˆ might increase. First, relabeling a
vertexuincreasesˆbylessthan2 V ,sincethesetoverwhichthesumistakenis
j j
the sameandtherelabeling cannot increase u’sheight bymorethan itsmaximum
possibleheight,which,byLemma26.20,isatmost2 V 1. Second,asaturating
j j(cid:0)
push fromavertexutoavertex increases ˆbylessthan2 V ,sincenoheights
j j
change andonlyvertex,whoseheight isatmost2 V 1,canpossibly become
j j(cid:0)
overflowing.
Now we show that a nonsaturating push from u to  decreases ˆ by at least 1.
Why? Before the nonsaturating push, u was overflowing, and  may or may not
have been overflowing. By Lemma 26.13, u is no longer overflowing after the
push. In addition, unless  is the source, it may or may not be overflowing after
the push. Therefore, the potential function ˆ hasdecreased byexactly u:h, and it
has increased by either 0 or :h. Since u:h :h 1, the net effect is that the
(cid:0) D
potential function hasdecreased byatleast1.
Thus, during the course of the algorithm, the total amount of increase in ˆ is
due to relabelings and saturated pushes, and Corollary 26.21 and Lemma 26.22
constrain the increase to be less than .2 V /.2 V 2/ .2 V /.2 V E /
4 V 2. V E /. Since ˆ 0, the total aj mj ountj ofj decC reasej , aj nd tj herj ej forj e thD e
toj talj nuj mbj eC roj fnj onsaturating pushes, islessthan4 V 2. V E /.
j j j jCj j
Having bounded the number of relabelings, saturating pushes, and nonsaturat-
ing pushes, we have set the stage for the following analysis of the GENERIC-
PUSH-RELABEL procedure, andhenceofanyalgorithm basedonthepush-relabel
method.
Theorem26.24
During the execution of GENERIC-PUSH-RELABEL on any flow network G
D
.V;E/,thenumberofbasicoperations isO.V2E/.
Proof ImmediatefromCorollary26.21andLemmas26.22and26.23.
26.4 Push-relabelalgorithms 747
Thus, the algorithm terminates after O.V2E/ operations. All that remains is
to give an efficient method for implementing each operation and for choosing an
appropriate operation toexecute.
Corollary26.25
There is an implementation of the generic push-relabel algorithm that runs in
O.V2E/timeonanyflownetworkG .V;E/.
D
Proof Exercise26.4-2asksyoutoshowhowtoimplementthegenericalgorithm
with an overhead of O.V/ per relabel operation and O.1/ per push. It also asks
you to design a data structure that allows you to pick an applicable operation in
O.1/time. Thecorollary thenfollows.
Exercises
26.4-1
Prove that, after the procedure INITIALIZE-PREFLOW.G;s/ terminates, we have
s:e f ,wheref isamaximumflowforG.
 
 (cid:0)j j
26.4-2
Show how to implement the generic push-relabel algorithm using O.V/ time per
relabeloperation, O.1/timeperpush, andO.1/timetoselectanapplicable oper-
ation,foratotaltimeofO.V2E/.
26.4-3
Prove that the generic push-relabel algorithm spends a total of only O.VE/ time
inperforming alltheO.V2/relabeloperations.
26.4-4
SupposethatwehavefoundamaximumflowinaflownetworkG .V;E/using
D
apush-relabel algorithm. Giveafastalgorithm tofindaminimumcutinG.
26.4-5
Giveanefficientpush-relabelalgorithmtofindamaximummatchinginabipartite
graph. Analyzeyouralgorithm.
26.4-6
Suppose that all edge capacities in a flow network G .V;E/ are in the set
D
1;2;:::;k . Analyze the running time of the generic push-relabel algorithm in
f g
termsof V , E ,andk. (Hint: Howmanytimescaneachedge support anonsat-
j j j j
uratingpushbeforeitbecomessaturated?)
748 Chapter26 MaximumFlow
26.4-7
Showthatwecouldchange line6ofINITIALIZE-PREFLOW to
6 s:h G:V 2
D j j(cid:0)
without affecting the correctness or asymptotic performance of the generic push-
relabelalgorithm.
26.4-8
Let ı .u;/ be the distance (number of edges) from u to  in the residual net-
f
work G f. Show that the GENERIC-PUSH-RELABEL procedure maintains the
properties that u:h < V implies u:h ı .u;t/ and that u:h V implies
f
j j   j j
u:h V ı .u;s/.
f
(cid:0)j j 
26.4-9 ?
Asintheprevious exercise, letı .u;/bethedistance from uto intheresidual
f
network G . Show how to modify the generic push-relabel algorithm to maintain
f
the property that u:h < V implies u:h ı .u;t/ and that u:h V implies
f
j j D  j j
u:h V ı .u;s/. Thetotal timethat yourimplementation dedicates tomain-
f
(cid:0)j j D
taining thispropertyshould beO.VE/.
26.4-10
Show that the number of nonsaturating pushes executed by the GENERIC-PUSH-
RELABEL procedure on a flow network G .V;E/ is at most 4 V 2 E for
D j j j j
V 4.
j j 
? 26.5 The relabel-to-front algorithm
The push-relabel method allows us to apply the basic operations in any order at
all. Bychoosing theorder carefully andmanaging thenetwork data structure effi-
ciently,however,wecansolvethemaximum-flowproblemfasterthantheO.V2E/
bound given by Corollary 26.25. We shall now examine the relabel-to-front algo-
rithm, apush-relabel algorithm whoserunning timeisO.V3/,whichisasymptot-
icallyatleastasgoodasO.V2E/,andevenbetterfordensenetworks.
The relabel-to-front algorithm maintains a list of the vertices in the network.
Beginning at the front, the algorithm scans the list, repeatedly selecting an over-
flowing vertex u and then “discharging” it, that is, performing push and relabel
operations until u no longer has a positive excess. Whenever we relabel a ver-
tex, we move it to the front of the list (hence the name “relabel-to-front”) and the
algorithm beginsitsscananew.
26.5 Therelabel-to-frontalgorithm 749
The correctness and analysis of the relabel-to-front algorithm depend on the
notion of “admissible” edges: those edges in the residual network through which
flowcanbepushed. Afterprovingsomepropertiesaboutthenetworkofadmissible
edges,weshallinvestigatethedischargeoperationandthenpresentandanalyzethe
relabel-to-front algorithm itself.
Admissibleedgesandnetworks
IfG .V;E/isaflownetworkwithsourcesandsinkt,f isapreflowinG,andh
D
is a height function, then we say that .u;/ is an admissible edge if c .u;/ > 0
f
andh.u/ h./ 1. Otherwise, .u;/isinadmissible. Theadmissible network
D C
isG .V;E /,whereE isthesetofadmissible edges.
f;h f;h f;h
D
Theadmissiblenetworkconsistsofthoseedgesthroughwhichwecanpushflow.
Thefollowinglemmashowsthatthisnetworkisadirected acyclicgraph(dag).
Lemma26.26(Theadmissible networkisacyclic)
If G .V;E/ is a flow network, f is a preflow in G, and h is a height function
D
onG,thentheadmissiblenetworkG .V;E /isacyclic.
f;h f;h
D
Proof The proof is by contradiction. Suppose that G contains a cycle p
f;h
D
 ; ;:::; ,where  andk > 0. Sinceeachedgeinp isadmissible, we
0 1 k 0 k
h i D
haveh. / h. / 1fori 1;2;:::;k. Summingaroundthecyclegives
i 1 i
(cid:0) D C D
k k
h. / .h. / 1/
i 1 i
(cid:0) D C
i 1 i 1
XD XD
k
h. / k :
i
D C
i 1
XD
Becauseeachvertexincyclep appearsonceineachofthesummations,wederive
thecontradiction that0 k.
D
Thenext twolemmasshow howpush andrelabel operations change the admis-
siblenetwork.
Lemma26.27
LetG .V;E/ be a flow network, let f be a preflow in G, and suppose that the
D
attribute h is a height function. If a vertex u is overflowing and .u;/ is an ad-
missible edge, then PUSH.u;/ applies. The operation does not create any new
admissibleedges, butitmaycause.u;/tobecomeinadmissible.
750 Chapter26 MaximumFlow
Proof By the definition of an admissible edge, we can push flow from u to .
Since u is overflowing, the operation PUSH.u;/ applies. The only new residual
edge that pushing flow from u to  can create is .;u/. Since :h u:h 1,
D (cid:0)
edge .;u/ cannot become admissible. If the operation is a saturating push, then
c .u;/ 0afterwardand.u;/becomesinadmissible.
f
D
Lemma26.28
Let G .V;E/ be a flow network, let f be a preflow in G, and suppose that
D
the attribute h is a height function. If a vertex u is overflowing and there are no
admissibleedgesleavingu,thenRELABEL.u/applies. Aftertherelabeloperation,
there is at least one admissible edge leaving u, but there are no admissible edges
entering u.
Proof If u is overflowing, then by Lemma 26.14, either a push or a relabel op-
eration applies to it. If there are no admissible edges leaving u, then no flow
can be pushed from u and so RELABEL.u/ applies. After the relabel operation,
u:h 1 min :h .u;/ E . Thus, if  is a vertex that realizes the mini-
f
D C f W 2 g
muminthisset,theedge.u;/becomesadmissible. Hence,aftertherelabel,there
isatleastoneadmissible edgeleaving u.
Toshowthatnoadmissible edgesenter uafterarelabel operation, suppose that
there is a vertex  such that .;u/ is admissible. Then, :h u:h 1 after the
D C
relabel, and so :h > u:h 1 just before the relabel. But by Lemma 26.12, no
C
residual edges exist between vertices whose heights differ by more than 1. More-
over, relabeling avertex does not change the residual network. Thus, .;u/is not
intheresidualnetwork, andhenceitcannotbeintheadmissible network.
Neighborlists
Edges in the relabel-to-front algorithm are organized into “neighbor lists.” Given
a flow network G .V;E/, the neighbor list u:N for a vertex u V is a singly
D 2
linked list of the neighbors of u in G. Thus, vertex  appears in the list u:N if
.u;/ E or .;u/ E. Theneighbor listu:N contains exactly those vertices 
2 2
for which there may be a residual edge .u;/. The attribute u:N:head points to
the first vertex in u:N, and :next-neighbor points to the vertex following  in a
neighbor list;thispointer isNIL if isthelastvertexintheneighbor list.
The relabel-to-front algorithm cycles through each neighbor list in an arbitrary
order that is fixed throughout the execution of the algorithm. For each vertex u,
the attribute u:current points to the vertex currently under consideration in u:N.
Initially, u:current issettou:N:head.
26.5 Therelabel-to-frontalgorithm 751
Discharginganoverflowingvertex
An overflowing vertex u is discharged by pushing all of its excess flow through
admissible edgestoneighboring vertices, relabeling uasnecessary tocauseedges
leavingutobecomeadmissible. Thepseudocode goesasfollows.
DISCHARGE.u/
1 whileu:e >0
2  u:current
D
3 if == NIL
4 RELABEL.u/
5 u:current u:N:head
D
6 elseifc .u;/ > 0andu:h ==:h 1
f
C
7 PUSH.u;/
8 elseu:current :next-neighbor
D
Figure26.9stepsthroughseveraliterationsofthewhileloopoflines1–8,which
executes as long as vertex u has positive excess. Each iteration performs exactly
oneofthreeactions, depending onthecurrentvertex intheneighbor listu:N.
1. If  is NIL, then we have run off the end of u:N. Line 4 relabels vertex u,
and then line 5 resets the current neighbor of u to be the first one in u:N.
(Lemma26.29belowstatesthattherelabel operation appliesinthissituation.)
2. If  is non-NIL and .u;/ is an admissible edge (determined by the test in
line6),thenline7pushessome(orpossiblyall)ofu’sexcesstovertex.
3. If  is non-NIL but .u;/ is inadmissible, then line 8 advances u:current one
position furtherintheneighbor listu:N.
Observe that if DISCHARGE is called on an overflowing vertex u, then the last
action performed by DISCHARGE must be a push from u. Why? The procedure
terminates only when u:e becomes zero, and neither the relabel operation nor ad-
vancingthepointeru:current affectsthevalueofu:e.
We must be sure that when PUSH or RELABEL is called by DISCHARGE, the
operation applies. Thenextlemmaprovesthisfact.
Lemma26.29
If DISCHARGE calls PUSH.u;/inline 7, then apush operation applies to .u;/.
IfDISCHARGE calls RELABEL.u/inline4,thenarelabeloperation appliestou.
Proof The tests in lines 1 and 6 ensure that a push operation occurs only if the
operation applies, whichprovesthefirststatementinthelemma.
752 Chapter26 MaximumFlow
6
s
5 1 2 3 4
–26
4 s s s s
(a) 3
14/14
x x x x
2 z z z z
x
1
0
0 5/5 y 8 z
19 0
6
s
5 5 6 7
–26
4 s s s
(b) 3
14/14
x x x
2 z z z
x 5/5 y
1
0 19 8
z
0
0
6
s
5 8 9
–26
4 s s
(c) 3
14/14
x x
2 z z
x 5/5 y
1 0 11 8/8
z
0
8
Figure26.9 Dischargingavertexy.Ittakes15iterationsofthewhileloopofDISCHARGEtopush
alltheexcessflowfromy.Onlytheneighborsofyandedgesoftheflownetworkthatenterorleavey
areshown. Ineachpartofthefigure,thenumberinsideeachvertexisitsexcessatthebeginningof
thefirstiterationshowninthepart,andeachvertexisshownatitsheightthroughoutthepart. The
neighborlisty:N atthebeginningofeachiterationappearsontheright,withtheiterationnumber
ontop. Theshadedneighborisy:current. (a)Initially,thereare19unitsofexcesstopushfromy,
andy:current s.Iterations1,2,and3justadvancey:current,sincetherearenoadmissibleedges
D
leavingy. Initeration4, y:current NIL (shownbytheshading beingbelow theneighbor list),
D
and so y isrelabeled and y:current isreset tothe head of the neighbor list. (b)After relabeling,
vertexyhasheight1.Initerations5and6,edges.y;s/and.y;x/arefoundtobeinadmissible,but
iteration7pushes8unitsofexcessflowfromyto´.Becauseofthepush,y:currentdoesnotadvance
inthisiteration. (c)Becausethepushiniteration7saturatededge.y;´/,itisfoundinadmissiblein
iteration8.Initeration9,y:current NIL,andsovertexyisagainrelabeledandy:currentisreset.
D
26.5 Therelabel-to-frontalgorithm 753
6
s
5 10 11
–26
4 14/14 s s
(d) 3 x x
y
2 5/5 11 z z
1
x 8/8
0
z
0
8
6
s
5 12 13 14
–26
4 14/14 s s s
(e) 3 x x x
y
2 5 6 z z z
1
x 8/8
5
z
0
8
y
6 14/14
6
s
5 15
–26
4 s
(f) 3
5 8/
8 x
2 z
x
1
5
z
0
8
y
6 8/14 0
s
5
–20
4
(g) 3
5 8/
8
2
x
1
5
z
0
8
Figure26.9, continued (d)Initeration10, .y;s/isinadmissible, butiteration11pushes5units
of excess flow from y to x. (e) Because y:current did not advance in iteration 11, iteration 12
finds.y;x/tobeinadmissible. Iteration13finds.y;´/inadmissible,anditeration14relabelsver-
texy andresetsy:current. (f)Iteration15pushes6unitsofexcessflowfromy tos. (g)Vertexy
nowhasnoexcessflow,andDISCHARGEterminates. Inthisexample,DISCHARGEbothstartsand
finisheswiththecurrentpointerattheheadoftheneighborlist,butingeneralthisneednotbethe
case.
754 Chapter26 MaximumFlow
Toprovethesecondstatement, accordingtothetestinline1andLemma26.28,
we need only show that all edges leaving u are inadmissible. If a call to
DISCHARGE.u/ starts with the pointer u:current at the head of u’s neighbor list
and finishes with it off the end of the list, then all of u’s outgoing edges are in-
admissible and a relabel operation applies. It is possible, however, that during a
call to DISCHARGE.u/, the pointer u:current traverses only part of the list be-
fore the procedure returns. Calls to DISCHARGE on other vertices may then oc-
cur, but u:current will continue moving through the list during the next call to
DISCHARGE.u/. We now consider what happens during a complete pass through
thelist,whichbeginsattheheadofu:N andfinisheswithu:current NIL. Once
D
u:current reaches the end of the list, the procedure relabels u and begins a new
pass. Fortheu:currentpointertoadvancepastavertex u:N duringapass,the
2
edge .u;/ must be deemed inadmissible by the test in line 6. Thus, by the time
the pass completes, every edge leaving u has been determined to be inadmissible
at some time during the pass. The key observation is that at the end of the pass,
everyedge leaving uisstill inadmissible. Why? ByLemma26.27, pushes cannot
create any admissible edges, regardless of which vertex the flow is pushed from.
Thus,anyadmissibleedgemustbecreatedbyarelabeloperation. Butthevertexu
is not relabeled during the pass, and by Lemma 26.28, any other vertex  that is
relabeledduringthepass(resultingfromacallofDISCHARGE./)hasnoentering
admissible edges after relabeling. Thus, attheendofthepass, alledges leaving u
remaininadmissible, whichcompletes theproof.
Therelabel-to-front algorithm
In the relabel-to-front algorithm, wemaintain alinked list Lconsisting ofall ver-
ticesinV s;t . Akeyproperty isthattheverticesinLaretopologically sorted
(cid:0)f g
according tothe admissible network, asweshall see intheloop invariant that fol-
lows. (RecallfromLemma26.26thattheadmissible networkisadag.)
The pseudocode for the relabel-to-front algorithm assumes that the neighbor
lists u:N have already been created for each vertex u. It also assumes that u:next
points tothevertexthatfollowsuinlistLandthat, asusual, u:next NIL ifuis
D
thelastvertexinthelist.
26.5 Therelabel-to-frontalgorithm 755
RELABEL-TO-FRONT.G;s;t/
1 INITIALIZE-PREFLOW.G;s/
2 L G:V s;t ,inanyorder
D (cid:0)f g
3 foreachvertexu G:V s;t
2 (cid:0)f g
4 u:current u:N:head
D
5 u L:head
D
6 whileu NIL
¤
7 old-height u:h
D
8 DISCHARGE.u/
9 ifu:h >old-height
10 moveutothefrontoflistL
11 u u:next
D
The relabel-to-front algorithm works as follows. Line 1 initializes the preflow
and heights to the same values as in the generic push-relabel algorithm. Line 2
initializes the list L to contain all potentially overflowing vertices, in any order.
Lines 3–4 initialize the current pointer of each vertex u to the first vertex in u’s
neighbor list.
AsFigure26.10illustrates, thewhileloopoflines6–11runsthrough thelistL,
discharging vertices. Line 5 makes it start with the first vertex in the list. Each
time through the loop, line 8 discharges a vertex u. If u was relabeled by the
DISCHARGE procedure, line 10 moves it to the front of list L. We can determine
whether u was relabeled by comparing its height before the discharge operation,
saved into the variable old-height in line 7, with its height afterward, in line 9.
Line 11 makes the next iteration of the while loop use the vertex following u in
listL. Ifline10movedutothefrontofthelist,thevertexusedinthenextiteration
istheonefollowinguinitsnewpositioninthelist.
Toshow that RELABEL-TO-FRONT computes a maximum flow, weshall show
that it is an implementation of the generic push-relabel algorithm. First, ob-
serve that it performs push and relabel operations only when they apply, since
Lemma 26.29 guarantees that DISCHARGE performs them only when they apply.
It remains to show that when RELABEL-TO-FRONT terminates, no basic opera-
tions apply. The remainder of the correctness argument relies on the following
loopinvariant:
At each test in line 6 of RELABEL-TO-FRONT, list L is a topological sort
of the vertices in the admissible network G .V;E /, and no vertex
f;h f;h
D
beforeuinthelisthasexcessflow.
Initialization: Immediatelyafter INITIALIZE-PREFLOW hasbeenrun, s:h V
D j j
and :h 0 for all  V s . Since V 2 (because V contains at
D 2 (cid:0) f g j j 
756 Chapter26 MaximumFlow
6
s
5 L: x y z
–26
4 N: s s x
(a) 3
1
2/
14/14 y x y
2 1 z z t
2
1 t
x 5 y 8 z 10 t
0
12 14 0 0
7
16
6
s
5 L: x y z
–26
4 N: s s x
(b) 3 1 2/
1
2
14/14 y x y
2 7/16 z z t
1 x 7 t
0
0 5/5 y 8 z 10 t
19 0 7
y
6 8/14 0
s
5 L: y x z
–20
4 N: s s x
(c) 3
1
2/
5 8/
8 x y y
1
2
2 7/16 z z t
x
1 t
5 7
z 10 t
0
8 7
Figure26.10 TheactionofRELABEL-TO-FRONT.(a)Aflownetworkjustbeforethefirstiteration
of the whileloop. Initially, 26 units of flow leave source s. On the right isshown the initial list
L x;y;´ ,whereinitiallyu x.UndereachvertexinlistLisitsneighborlist,withthecurrent
Dh i D
neighborshaded.Vertexxisdischarged.Itisrelabeledtoheight1,5unitsofexcessflowarepushed
toy,andthe7remainingunitsofexcessarepushedtothesinkt. Becausex isrelabeled,itmoves
totheheadofL,whichinthiscasedoesnotchangethestructureofL. (b)Afterx,thenextvertex
inLthatisdischargedisy. Figure26.9showsthedetailedactionofdischargingyinthissituation.
Becausey isrelabeled,itismovedtotheheadofL. (c)Vertexx nowfollowsy inL,andsoitis
againdischarged, pushingall5unitsofexcessflowtot. Becausevertexx isnotrelabeledinthis
dischargeoperation,itremainsinplaceinlistL.
26.5 Therelabel-to-frontalgorithm 757
y
6 8/14 0
s
5 L: y x z
–20
4 N: s s x
(d) 3
1
2/
5 8/
8 x y y
1
2
2 12/16 z z t
x
1 t
0 7
z 10 t
0
8 12
y
6 8/14 0
s
5 L: z y x
–20
4 N: x s s
(e) 3 1 2/
5 8/
8 y x y
1
2
2 t z z
x 7 z
1 0 12/16 0 8/10 t t
0
20
Figure26.10,continued (d)Sincevertex´followsvertexxinL,itisdischarged. Itisrelabeled
toheight 1andall 8unitsof excessflowarepushed tot. Because´isrelabeled, itmoves tothe
frontofL.(e)Vertexynowfollowsvertex´inLandisthereforedischarged.Butbecauseyhasno
excess,DISCHARGEimmediatelyreturns,andyremainsinplaceinL.Vertexxisthendischarged.
Becauseit,too,hasnoexcess,DISCHARGEagainreturns,andxremainsinplaceinL. RELABEL-
TO-FRONThasreachedtheendoflistLandterminates. Therearenooverflowingvertices,andthe
preflowisamaximumflow.
least s andt), noedgecanbeadmissible. Thus, E ,andanyordering of
f;h
D ;
V s;t isatopological sortofG .
f;h
(cid:0)f g
Because u is initially the head of the list L, there are no vertices before it and
sotherearenonebeforeitwithexcessflow.
Maintenance: Toseethat eachiteration ofthewhileloop maintains thetopolog-
ical sort, westart byobserving thattheadmissible network ischanged only by
push and relabel operations. By Lemma 26.27, push operations do not cause
edges tobecome admissible. Thus, only relabel operations cancreate admissi-
bleedges. Afteravertexuisrelabeled,however,Lemma26.28statesthatthere
arenoadmissibleedgesenteringubuttheremaybeadmissibleedgesleavingu.
Thus,bymovingutothefrontofL,thealgorithm ensures thatanyadmissible
edgesleaving usatisfythetopological sortordering.
758 Chapter26 MaximumFlow
To see that no vertex preceding u in L has excess flow, we denote the vertex
thatwillbeuinthenextiterationbyu. Theverticesthatwillprecedeu inthe
0 0
nextiteration include thecurrent u(duetoline11)andeither noother vertices
(ifuisrelabeled) orthesameverticesasbefore(ifuisnotrelabeled). Whenu
is discharged, it has no excess flow afterward. Thus, if u is relabeled during
the discharge, no vertices preceding u have excess flow. If u is not relabeled
0
duringthedischarge,noverticesbeforeitonthelistacquiredexcessflowduring
thisdischarge, because Lremained topologically sorted atalltimesduring the
discharge (asjustpointed out, admissible edgesarecreated onlybyrelabeling,
not pushing), and so each push operation causes excess flow to move only to
verticesfurtherdownthelist(ortosort). Again,noverticesprecedingu have
0
excessflow.
Termination: When the loop terminates, u is just past the end of L, and so the
loopinvariant ensures thattheexcessofeveryvertexis0. Thus,nobasicoper-
ationsapply.
Analysis
We shall now show that RELABEL-TO-FRONT runs in O.V3/ time on any flow
network G .V;E/. Since the algorithm is an implementation of the generic
D
push-relabel algorithm, we shall take advantage of Corollary 26.21, which pro-
videsanO.V/boundonthenumberofrelabeloperations executedpervertexand
anO.V2/boundonthetotalnumberofrelabeloperationsoverall. Inaddition,Ex-
ercise26.4-3providesanO.VE/boundonthetotaltimespentperformingrelabel
operations, and Lemma 26.22 provides an O.VE/ bound on the total number of
saturating pushoperations.
Theorem26.30
The running time of RELABEL-TO-FRONT on any flow network G .V;E/
D
isO.V3/.
Proof Let us consider a “phase” of the relabel-to-front algorithm to be the time
between two consecutive relabel operations. There are O.V2/ phases, since there
are O.V2/ relabel operations. Each phase consists of at most V calls to DIS-
j j
CHARGE, which we can see as follows. If DISCHARGE does not perform a re-
label operation, then the next call to DISCHARGE is further down the list L, and
the length of L is less than V . If DISCHARGE does perform a relabel, the next
j j
call to DISCHARGE belongs to a different phase. Since each phase contains at
most V calls to DISCHARGE and there are O.V2/ phases, the number of times
j j
DISCHARGE iscalledinline8ofRELABEL-TO-FRONT isO.V3/. Thus,thetotal
26.5 Therelabel-to-frontalgorithm 759
work performed by the while loop in RELABEL-TO-FRONT, excluding the work
performedwithin DISCHARGE, isatmostO.V3/.
We must now bound the work performed within DISCHARGE during the ex-
ecution of the algorithm. Each iteration of the while loop within DISCHARGE
performsoneofthreeactions. Weshallanalyze thetotalamountofworkinvolved
inperforming eachoftheseactions.
Westartwithrelabeloperations(lines4–5). Exercise26.4-3providesanO.VE/
timeboundonalltheO.V2/relabelsthatareperformed.
Now,supposethattheactionupdatestheu:current pointerinline8. Thisaction
occursO.degree.u//timeseachtimeavertexuisrelabeled,andO.V degree.u//

times overall for the vertex. For all vertices, therefore, the total amount of work
done inadvancing pointers inneighbor lists isO.VE/bythehandshaking lemma
(ExerciseB.4-1).
Thethirdtype ofaction performed by DISCHARGE isapushoperation (line7).
We already know that the total number of saturating push operations is O.VE/.
Observethatifanonsaturatingpushisexecuted,DISCHARGEimmediatelyreturns,
sincethepushreducestheexcessto0. Thus,therecanbeatmostonenonsaturating
pushpercalltoDISCHARGE. Aswehaveobserved, DISCHARGE iscalledO.V3/
times,andthusthetotaltimespentperformingnonsaturating pushesisO.V3/.
The running time of RELABEL-TO-FRONT is therefore O.V3 VE/, which
C
isO.V3/.
Exercises
26.5-1
IllustratetheexecutionofRELABEL-TO-FRONT inthemannerofFigure26.10for
theflownetworkinFigure26.1(a). AssumethattheinitialorderingofverticesinL
is  ; ; ; andthattheneighbor listsare
1 2 3 4
h i
 :N s; ; ;
1 2 3
D h i
 :N s; ; ; ;
2 1 3 4
D h i
 :N  ; ; ;t ;
3 1 2 4
D h i
 :N  ; ;t :
4 2 3
D h i
26.5-2 ?
Wewouldliketoimplementapush-relabelalgorithminwhichwemaintainafirst-
in,first-outqueueofoverflowingvertices. Thealgorithmrepeatedlydischargesthe
vertex at the head ofthe queue, and any vertices that were not overflowing before
the discharge but are overflowing afterward are placed at the end of the queue.
After the vertex at the head of the queue is discharged, it is removed. When the
760 Chapter26 MaximumFlow
queue is empty, the algorithm terminates. Show how to implement this algorithm
tocomputeamaximumflowinO.V3/time.
26.5-3
Show that the generic algorithm still works if RELABEL updates u:h by sim-
ply computing u:h u:h 1. How would this change affect the analysis of
D C
RELABEL-TO-FRONT?
26.5-4 ?
Show that if we always discharge a highest overflowing vertex, we can make the
push-relabel methodruninO.V3/time.
26.5-5
Supposethatatsomepointintheexecutionofapush-relabelalgorithm,thereexists
an integer 0 < k V 1 for which no vertex has :h k. Show that all
 j j (cid:0) D
vertices with:h > k areonthe source side ofaminimum cut. Ifsuch ak exists,
the gap heuristic updates every vertex  V s for which :h > k, to set
2 (cid:0) f g
:h max.:h; V 1/. Showthattheresulting attribute hisaheightfunction.
D j jC
(Thegapheuristiciscrucialinmakingimplementationsofthepush-relabelmethod
perform wellinpractice.)
Problems
26-1 Escapeproblem
Ann ngridisanundirectedgraphconsistingofnrowsandncolumnsofvertices,

asshowninFigure26.11. Wedenotethevertexintheithrowandthejthcolumn
by.i;j/. Allverticesinagridhaveexactlyfourneighbors,exceptfortheboundary
vertices, whicharethepoints.i;j/forwhichi 1,i n,j 1,orj n.
D D D D
Given m n2 starting points .x ;y /;.x ;y /;:::;.x ;y / in the grid, the
1 1 2 2 m m

escape problem is to determine whether or not there are m vertex-disjoint paths
from the starting points to any m different points on the boundary. For example,
thegridinFigure26.11(a)hasanescape, butthegridinFigure26.11(b)doesnot.
a. Consider a flow network in which vertices, as well as edges, have capacities.
Thatis,thetotalpositiveflowentering anygivenvertexissubjecttoacapacity
constraint. Show that determining the maximum flow in a network with edge
andvertexcapacitiescanbereducedtoanordinarymaximum-flowproblemon
aflownetworkofcomparable size.
ProblemsforChapter26 761
(a) (b)
Figure26.11 Gridsfortheescapeproblem. Startingpointsareblack,andothergridverticesare
white.(a)Agridwithanescape,shownbyshadedpaths.(b)Agridwithnoescape.
b. Describe an efficient algorithm to solve the escape problem, and analyze its
running time.
26-2 Minimumpathcover
A path cover of a directed graph G .V;E/ is a set P of vertex-disjoint paths
D
such that every vertex in V is included in exactly one path in P. Paths may start
and end anywhere, and they may be ofany length, including 0. Aminimumpath
coverofG isapathcovercontaining thefewestpossible paths.
a. Give an efficient algorithm to finda minimum path cover of adirected acyclic
graph G .V;E/. (Hint: Assuming that V 1;2;:::;n , construct the
D D f g
graphG .V ;E /,where
0 0 0
D
V x ;x ;:::;x y ;y ;:::;y ;
0 0 1 n 0 1 n
D f g[f g
E .x ;x / i V .y ;y / i V .x ;y / .i;j/ E ;
0 0 i i 0 i j
D f W 2 g[f W 2 g[f W 2 g
andrunamaximum-flowalgorithm.)
b. Doesyouralgorithm workfordirectedgraphsthatcontain cycles? Explain.
26-3 Algorithmicconsulting
ProfessorGorewantstoopenupanalgorithmicconsultingcompany. Hehasiden-
tified n important subareas of algorithms (roughly corresponding to different por-
tions of this textbook), which he represents by the set A A ;A ;:::;A . In
1 2 n
D f g
each subarea A , he can hire an expert in that area for c dollars. The consulting
k k
company has lined up a set J J ;J ;:::;J of potential jobs. In order to
1 2 m
D f g
perform job J , the company needs to have hired experts in a subset R A of
i i

762 Chapter26 MaximumFlow
subareas. Each expert can work on multiple jobs simultaneously. If the company
choosestoacceptjobJ ,itmusthavehiredexpertsinallsubareasinR ,anditwill
i i
takeinrevenueofp dollars.
i
ProfessorGore’sjobistodeterminewhichsubareastohireexpertsinandwhich
jobstoacceptinordertomaximizethenetrevenue,whichisthetotalincomefrom
jobsacceptedminusthetotalcostofemploying theexperts.
Consider the following flow network G. It contains a source vertex s, vertices
A ;A ;:::;A , vertices J ;J ;:::;J , and a sink vertex t. For k 1;2:::;n,
1 2 n 1 2 m
D
the flow network contains an edge .s;A / with capacity c.s;A / c , and
k k k
D
for i 1;2;:::;m, the flow network contains an edge .J ;t/ with capacity
i
D
c.J ;t/ p . For k 1;2;:::;n and i 1;2;:::;m, if A R , then G
i i k i
D D D 2
contains anedge.A ;J /withcapacityc.A ;J / .
k i k i
D1
a. ShowthatifJ T forafinite-capacity cut.S;T/ofG,thenA T foreach
i k
2 2
A R .
k i
2
b. Showhowtodeterminethemaximumnetrevenuefromthecapacityofamini-
mumcutofG andthegivenp values.
i
c. Giveanefficientalgorithmtodeterminewhichjobstoacceptandwhichexperts
to hire. Analyze the running time of your algorithm in terms of m, n, and
r m R .
D i 1j i j
D
P
26-4 Updatingmaximumflow
Let G .V;E/ be a flow network with source s, sink t, and integer capacities.
D
SupposethatwearegivenamaximumflowinG.
a. Suppose that we increase the capacity of a single edge .u;/ E by 1. Give
2
anO.V E/-timealgorithm toupdatethemaximumflow.
C
b. Suppose that wedecrease the capacity of a single edge .u;/ E by 1. Give
2
anO.V E/-timealgorithm toupdatethemaximumflow.
C
26-5 Maximumflowbyscaling
Let G .V;E/ be a flow network with source s, sink t, and an integer capac-
D
ityc.u;/oneachedge.u;/ E. LetC max c.u;/.
.u;/ E
2 D 2
a. ArguethataminimumcutofG hascapacity atmostC E .
j j
b. For a given number K, show how to find an augmenting path of capacity at
leastK inO.E/time,ifsuchapathexists.
ProblemsforChapter26 763
We can use the following modification of FORD-FULKERSON-METHOD to com-
puteamaximumflowinG:
MAX-FLOW-BY-SCALING.G;s;t/
1 C max c.u;/
.u;/ E
2 initD ialize flow2f to0
3 K 2 lgC
b c
D
4 whileK 1

5 whilethereexistsanaugmenting pathp ofcapacity atleastK
6 augmentflowf alongp
7 K K=2
D
8 returnf
c. Arguethat MAX-FLOW-BY-SCALING returns amaximumflow.
d. Showthatthecapacity ofaminimumcutoftheresidualnetworkG isatmost
f
2K E eachtimeline4isexecuted.
j j
e. Arguethattheinnerwhileloopoflines5–6executesO.E/timesforeachvalue
ofK.
f. Conclude that MAX-FLOW-BY-SCALING can be implemented so that it runs
inO.E2lgC/time.
26-6 TheHopcroft-Karp bipartite matchingalgorithm
In this problem, we describe a faster algorithm, due to Hopcroft and Karp, for
findingamaximummatchinginabipartitegraph. ThealgorithmrunsinO.pVE/
time. Given an undirected, bipartite graph G .V;E/, where V L R and
D D [
all edges have exactly one endpoint in L, let M be a matching in G. We say that
a simple path P in G is an augmenting path with respect to M if it starts at an
unmatched vertex in L, ends at an unmatched vertex in R, and its edges belong
alternately to M and E M. (This definition of an augmenting path is related
(cid:0)
to, but different from, an augmenting path in a flow network.) In this problem,
we treat a path as a sequence of edges, rather than as a sequence of vertices. A
shortest augmenting path with respect to a matching M is an augmenting path
withaminimumnumberofedges.
GiventwosetsAandB,thesymmetricdifferenceA B isdefinedas.A B/
˚ (cid:0) [
.B A/,thatis,theelementsthatareinexactlyoneofthetwosets.
(cid:0)
764 Chapter26 MaximumFlow
a. ShowthatifM isamatching andP isanaugmenting pathwithrespect toM,
thenthesymmetricdifferenceM P isamatchingand M P M 1.
˚ j ˚ jD j jC
Show that if P ;P ;:::;P are vertex-disjoint augmenting paths with respect
1 2 k
toM,thenthesymmetricdifference M .P P P /isamatching
1 2 k
˚ [ [[
withcardinality M k.
j jC
Thegeneral structureofouralgorithm isthefollowing:
HOPCROFT-KARP.G/
1 M
D ;
2 repeat
3 letP P ;P ;:::;P beamaximalsetofvertex-disjoint
1 2 k
D f g
shortestaugmenting pathswithrespect toM
4 M M .P P P /
1 2 k
PD ˚ [ [[
5 until ==
;
6 returnM
The remainder of this problem asks you to analyze the number of iterations in
the algorithm (that is, the number ofiterations in the repeat loop) and todescribe
animplementation ofline3.
b. Given two matchings M and M in G, show that every vertex in the graph

G .V;M M / has degree at most 2. Conclude that G is a disjoint
0  0
D ˚
union of simple paths or cycles. Argue that edges in each such simple path
or cycle belong alternately to M or M . Prove that if M M , then
 
j j  j j
M M contains at least M M vertex-disjoint augmenting paths with
 
˚ j j(cid:0)j j
respecttoM.
Letl bethelengthofashortestaugmentingpathwithrespecttoamatchingM,and
letP ;P ;:::;P beamaximalsetofvertex-disjointaugmentingpathsoflengthl
1 2 k
withrespecttoM. LetM M .P P /,andsupposethatP isashortest
0 1 k
D ˚ [[
augmenting pathwithrespecttoM .
0
c. Show that ifP isvertex-disjoint from P ;P ;:::;P , then P has morethan l
1 2 k
edges.
d. Now suppose that P is not vertex-disjoint from P ;P ;:::;P . Let A be the
1 2 k
set of edges .M M / P. Show that A .P P P / P and
0 1 2 k
˚ ˚ D [ [[ ˚
that A .k 1/l. Conclude thatP hasmorethanl edges.
j j  C
e. ProvethatifashortestaugmentingpathwithrespecttoM hasl edges,thesize
ofthemaximummatchingisatmost M V =.l 1/.
j jCj j C
NotesforChapter26 765
f. Show that the number of repeat loop iterations in the algorithm is at
most2 V . (Hint:ByhowmuchcanM growafteriteration number V ?)
j j j j
p p
g. Give an algorithm that runs in O.E/ time to find a maximal set of vertex-
disjoint shortest augmenting paths P ;P ;:::;P for a given matching M.
1 2 k
ConcludethatthetotalrunningtimeofHOPCROFT-KARP isO.pVE/.
Chapter notes
Ahuja,Magnanti,andOrlin[7],Even[103],Lawler[224],PapadimitriouandStei-
glitz[271],andTarjan[330]aregoodreferencesfornetworkflowandrelatedalgo-
rithms. Goldberg,Tardos,andTarjan[139]alsoprovideanicesurveyofalgorithms
for network-flow problems, and Schrijver [304] has written an interesting review
ofhistorical developments inthefieldofnetworkflows.
TheFord-Fulkerson methodisduetoFordandFulkerson[109],whooriginated
the formal study of many of the problems in the area of network flow, including
themaximum-flowandbipartite-matching problems. Manyearlyimplementations
of the Ford-Fulkerson method found augmenting paths using breadth-first search;
Edmonds and Karp [102], and independently Dinic [89], proved that this strategy
yieldsapolynomial-time algorithm. Arelatedidea,thatofusing“blockingflows,”
wasalso firstdeveloped by Dinic[89]. Karzanov [202]firstdeveloped theidea of
preflows. Thepush-relabelmethodisduetoGoldberg[136]andGoldbergandTar-
jan[140]. GoldbergandTarjangaveanO.V3/-timealgorithmthatusesaqueueto
maintainthesetofoverflowingvertices,aswellasanalgorithm thatusesdynamic
treestoachievearunningtimeofO.VElg.V2=E 2//. Severalotherresearchers
C
have developed push-relabel maximum-flow algorithms. Ahuja and Orlin [9] and
Ahuja, Orlin, and Tarjan [10] gave algorithms that used scaling. Cheriyan and
Maheshwari[62]proposedpushingflowfromtheoverflowingvertexofmaximum
height. Cheriyan and Hagerup [61] suggested randomly permuting the neighbor
lists, and several researchers [14,204,276]developed clever derandomizations of
this idea, leading to asequence of faster algorithms. Thealgorithm of King, Rao,
and Tarjan [204] is the fastest such algorithm and runs in O.VElog V/
E=.VlgV/
time.
Theasymptotically fastestalgorithm todateforthemaximum-flowproblem,by
Goldberg and Rao [138], runs in time O.min.V2=3;E1=2/Elg.V2=E 2/lgC/,
C
where C max c.u;/. This algorithm does not use the push-relabel
.u;/ E
D 2
method but instead is based on finding blocking flows. All previous maximum-
flow algorithms, including the ones in this chapter, use some notion of distance
(thepush-relabelalgorithmsusetheanalogousnotionofheight),withalengthof1
766 Chapter26 MaximumFlow
assigned implicitly to each edge. This new algorithm takes a different approach
and assigns a length of 0to high-capacity edges and a length of 1to low-capacity
edges. Informally, with respect to these lengths, shortest paths from the source
to the sink tend to have high capacity, which means that fewer iterations need be
performed.
In practice, push-relabel algorithms currently dominate augmenting-path or
linear-programming based algorithms for the maximum-flow problem. A study
byCherkasskyandGoldberg[63]underscorestheimportanceofusingtwoheuris-
tics when implementing a push-relabel algorithm. The first heuristic is to peri-
odically perform a breadth-first search of the residual network in order to obtain
moreaccurateheightvalues. Thesecondheuristicisthegapheuristic,describedin
Exercise 26.5-5. Cherkassky and Goldberg conclude that the best choice of push-
relabelvariantsistheonethatchoosestodischargetheoverflowingvertexwiththe
maximumheight.
The best algorithm to date for maximum bipartite matching, discovered by
HopcroftandKarp[176],runsinO.pVE/timeandisdescribedinProblem26-6.
The book by Lova´sz and Plummer [239] is an excellent reference on matching
problems.
VII Selected Topics
Introduction
This part contains a selection of algorithmic topics that extend and complement
earliermaterialinthisbook. Somechapters introduce newmodelsofcomputation
such as circuits or parallel computers. Others cover specialized domains such as
computational geometry or number theory. Thelast two chapters discuss some of
theknownlimitationstothedesignofefficientalgorithmsandintroducetechniques
forcoping withthoselimitations.
Chapter 27 presents an algorithmic model for parallel computing based on dy-
namic multithreading. The chapter introduces the basics of the model, showing
how to quantify parallelism in terms of the measures of work and span. It then
investigates several interesting multithreaded algorithms, including algorithms for
matrixmultiplication andmergesorting.
Chapter 28 studies efficient algorithms for operating on matrices. It presents
two general methods—LU decomposition and LUP decomposition—for solving
linear equations byGaussian elimination inO.n3/ time. Italso shows that matrix
inversion and matrix multiplication can be performed equally fast. The chapter
concludes byshowing howtocompute aleast-squares approximate solution when
asetoflinearequations hasnoexactsolution.
Chapter29studieslinearprogramming, inwhichwewishtomaximizeormini-
mizeanobjective, given limited resources and competing constraints. Linear pro-
grammingarisesinavarietyofpracticalapplicationareas. Thischaptercovershow
to formulate and solve linear programs. The solution method covered is the sim-
plex algorithm, which is the oldest algorithm for linear programming. In contrast
tomanyalgorithmsinthisbook,thesimplexalgorithmdoesnotruninpolynomial
timeintheworstcase,butitisfairlyefficientandwidelyusedinpractice.
770 PartVII SelectedTopics
Chapter 30 studies operations on polynomials and shows how to use a well-
known signal-processing technique—the fast Fourier transform (FFT)—to multi-
ply two degree-n polynomials in O.nlgn/ time. It also investigates efficient im-
plementations oftheFFT,including aparallelcircuit.
Chapter 31 presents number-theoretic algorithms. After reviewing elementary
number theory, it presents Euclid’s algorithm for computing greatest common di-
visors. Next, it studies algorithms for solving modular linear equations and for
raisingonenumbertoapowermoduloanothernumber. Then,itexploresanimpor-
tantapplicationofnumber-theoreticalgorithms: theRSApublic-keycryptosystem.
This cryptosystem can be used not only to encrypt messages so that an adversary
cannot readthem, butalsotoprovide digital signatures. Thechapter thenpresents
the Miller-Rabin randomized primality test, with which we can find large primes
efficiently—anessentialrequirementfortheRSAsystem. Finally,thechaptercov-
ersPollard’s“rho”heuristic forfactoring integersanddiscusses thestateoftheart
ofintegerfactorization.
Chapter 32 studies the problem of finding all occurrences of a given pattern
string in a given text string, a problem that arises frequently in text-editing pro-
grams. After examining the naive approach, the chapter presents an elegant ap-
proach due to Rabin and Karp. Then, after showing an efficient solution based
on finite automata, the chapter presents the Knuth-Morris-Pratt algorithm, which
modifies the automaton-based algorithm to save space by cleverly preprocessing
thepattern.
Chapter 33 considers a few problems in computational geometry. After dis-
cussing basic primitivesofcomputational geometry, thechapter showshowtouse
a “sweeping” method to efficiently determine whether aset of line segments con-
tainsanyintersections. Twocleveralgorithmsforfindingtheconvexhullofasetof
points—Graham’s scan and Jarvis’s march—also illustrate thepower ofsweeping
methods. Thechaptercloseswithanefficientalgorithm forfindingtheclosestpair
fromamongagivensetofpointsintheplane.
Chapter 34 concerns NP-complete problems. Many interesting computational
problemsareNP-complete,butnopolynomial-timealgorithmisknownforsolving
any of them. This chapter presents techniques for determining when a problem is
NP-complete. SeveralclassicproblemsareprovedtobeNP-complete: determining
whether a graph has a hamiltonian cycle, determining whether a boolean formula
is satisfiable, and determining whether a given set of numbers has a subset that
adds uptoagiventarget value. Thechapter alsoprovesthat thefamous traveling-
salesmanproblem isNP-complete.
Chapter 35 shows how to find approximate solutions to NP-complete problems
efficiently by using approximation algorithms. For some NP-complete problems,
approximatesolutionsthatarenearoptimalarequiteeasytoproduce,butforothers
eventhebestapproximation algorithmsknownworkprogressively morepoorlyas
PartVII SelectedTopics 771
theproblemsizeincreases. Then,therearesomeproblemsforwhichwecaninvest
increasing amounts of computation time in return for increasingly better approx-
imate solutions. This chapter illustrates these possibilities with the vertex-cover
problem (unweighted and weighted versions), an optimization version of 3-CNF
satisfiability, the traveling-salesman problem, the set-covering problem, and the
subset-sum problem.
27 Multithreaded Algorithms
The vast majority of algorithms in this book are serial algorithms suitable for
running on a uniprocessor computer in which only one instruction executes at a
time. Inthischapter, weshallextendouralgorithmicmodeltoencompassparallel
algorithms, which can run on a multiprocessor computer that permits multiple
instructions to execute concurrently. In particular, we shall explore the elegant
model of dynamic multithreaded algorithms, which are amenable to algorithmic
designandanalysis, aswellastoefficientimplementation inpractice.
Parallel computers—computers with multiple processing units—have become
increasinglycommon,andtheyspanawiderangeofpricesandperformance. Rela-
tivelyinexpensivedesktopandlaptopchipmultiprocessorscontainasinglemulti-
coreintegrated-circuit chipthathousesmultipleprocessing “cores,”eachofwhich
is a full-fledged processor that can access a common memory. At an intermedi-
ate price/performance point are clusters built from individual computers—often
simple PC-class machines—with a dedicated network interconnecting them. The
highest-priced machines are supercomputers, which often use a combination of
custom architectures and custom networks to deliver the highest performance in
termsofinstructions executed persecond.
Multiprocessor computers have been around, in one form or another, for
decades. Although the computing community settled on the random-access ma-
chine model for serial computing early on in the history of computer science, no
single model for parallel computing has gained as wide acceptance. A major rea-
son is that vendors have not agreed on a single architectural model for parallel
computers. Forexample, some parallel computers feature shared memory, where
each processor can directly access any location of memory. Other parallel com-
putersemploydistributedmemory,whereeachprocessor’smemoryisprivate,and
anexplicit message mustbesentbetween processors inorderforoneprocessor to
access thememoryofanother. Withthe advent ofmulticore technology, however,
everynewlaptopanddesktopmachineisnowashared-memoryparallelcomputer,
Chapter27 MultithreadedAlgorithms 773
andthetrendappearstobetowardshared-memorymultiprocessing. Althoughtime
willtell,thatistheapproach weshalltakeinthischapter.
One common means of programming chip multiprocessors and other shared-
memoryparallelcomputersisbyusingstaticthreading,whichprovidesasoftware
abstraction of “virtual processors,” or threads, sharing a common memory. Each
thread maintains an associated program counter and can execute code indepen-
dently of the other threads. The operating system loads a thread onto a processor
for execution and switches it out when another thread needs to run. Although the
operating system allows programmers to create and destroy threads, these opera-
tions are comparatively slow. Thus, for most applications, threads persist for the
durationofacomputation, whichiswhywecallthem“static.”
Unfortunately, programming ashared-memory parallel computer directly using
static threads is difficult and error-prone. One reason is that dynamically parti-
tioning the work among the threads so that each thread receives approximately
the same load turns out to be a complicated undertaking. For any but the sim-
plestofapplications, theprogrammer mustusecomplexcommunication protocols
to implement a scheduler to load-balance the work. This state of affairs has led
toward the creation of concurrency platforms, which provide a layer of software
that coordinates, schedules, and manages the parallel-computing resources. Some
concurrencyplatformsarebuiltasruntimelibraries,butothersprovidefull-fledged
parallellanguages withcompilerandruntimesupport.
Dynamicmultithreadedprogramming
Oneimportantclassofconcurrency platformisdynamicmultithreading,whichis
themodelweshalladoptinthischapter. Dynamicmultithreading allowsprogram-
merstospecifyparallelisminapplicationswithoutworryingaboutcommunication
protocols, load balancing, and other vagaries of static-thread programming. The
concurrency platform contains a scheduler, which load-balances the computation
automatically, thereby greatly simplifying the programmer’s chore. Although the
functionality of dynamic-multithreading environments is still evolving, almost all
support two features: nested parallelism and parallel loops. Nested parallelism
allows a subroutine to be “spawned,” allowing the caller to proceed while the
spawned subroutine is computing its result. A parallel loop is like an ordinary
forloop,exceptthattheiterations oftheloopcanexecuteconcurrently.
Thesetwofeatures formthebasisofthemodelfordynamicmultithreading that
we shall study in this chapter. A key aspect of this model is that the programmer
needstospecifyonlythelogicalparallelism withinacomputation, andthethreads
withintheunderlyingconcurrencyplatformscheduleandload-balance thecompu-
tationamongthemselves. Weshallinvestigatemultithreadedalgorithmswrittenfor
774 Chapter27 MultithreadedAlgorithms
thismodel,aswellhowtheunderlying concurrencyplatformcanschedulecompu-
tationsefficiently.
Ourmodelfordynamicmultithreading offersseveralimportantadvantages:
 It is a simple extension of our serial programming model. We can describe a
multithreaded algorithm byadding toourpseudocode justthree“concurrency”
keywords: parallel, spawn, and sync. Moreover, if we delete these concur-
rency keywords from the multithreaded pseudocode, theresulting text isserial
pseudocodeforthesameproblem,whichwecallthe“serialization” ofthemul-
tithreaded algorithm.
 It provides a theoretically clean way to quantify parallelism based on the no-
tionsof“work”and“span.”
 Many multithreaded algorithms involving nested parallelism follow naturally
from the divide-and-conquer paradigm. Moreover, just as serial divide-and-
conquer algorithms lend themselves to analysis by solving recurrences, so do
multithreaded algorithms.
 Themodel isfaithful to how parallel-computing practice is evolving. Agrow-
ingnumberofconcurrencyplatformssupportonevariantoranotherofdynamic
multithreading, includingCilk[51,118],Cilk++[71],OpenMP[59],TaskPar-
allelLibrary[230],andThreadingBuildingBlocks[292].
Section27.1introducesthedynamicmultithreadingmodelandpresentsthemet-
rics of work, span, and parallelism, which we shall use to analyze multithreaded
algorithms. Section 27.2 investigates how to multiply matrices with multithread-
ing,andSection27.3tacklesthetougher problemofmultithreading mergesort.
27.1 The basics ofdynamicmultithreading
We shall begin our exploration of dynamic multithreading using the example of
computing Fibonacci numbers recursively. Recall that the Fibonacci numbers are
definedbyrecurrence (3.22):
F 0;
0
D
F 1;
1
D
F F F fori 2:
i i 1 i 2
D (cid:0) C (cid:0) 
Hereisasimple,recursive,serialalgorithm tocomputethenthFibonaccinumber:
27.1 Thebasicsofdynamicmultithreading 775
FIB.6/
FIB.5/ FIB.4/
FIB.4/ FIB.3/ FIB.3/ FIB.2/
FIB.3/ FIB.2/ FIB.2/ FIB.1/ FIB.2/ FIB.1/ FIB.1/ FIB.0/
FIB.2/ FIB.1/ FIB.1/ FIB.0/ FIB.1/ FIB.0/ FIB.1/ FIB.0/
FIB.1/ FIB.0/
Figure27.1 Thetreeofrecursiveprocedureinstanceswhencomputing FIB.6/. Eachinstanceof
FIBwiththesameargumentdoesthesameworktoproducethesameresult,providinganinefficient
butinterestingwaytocomputeFibonaccinumbers.
FIB.n/
1 ifn 1

2 returnn
3 elsex FIB.n 1/
D (cid:0)
4 y FIB.n 2/
D (cid:0)
5 returnx y
C
You would not really want to compute large Fibonacci numbers this way, be-
cause this computation does much repeated work. Figure 27.1 shows the tree of
recursive procedure instances that are created when computing F . For example,
6
a call to FIB.6/ recursively calls FIB.5/ and then FIB.4/. But, the call to FIB.5/
also results in a call to FIB.4/. Both instances of FIB.4/ return the same result
(F
4
3). Since the FIB procedure does not memoize, the second call to FIB.4/
D
replicates theworkthatthefirstcallperforms.
Let T.n/ denote the running time of FIB.n/. Since FIB.n/ contains two recur-
sivecallsplusaconstant amountofextrawork,weobtaintherecurrence
T.n/ T.n 1/ T.n 2/ ‚.1/:
D (cid:0) C (cid:0) C
Thisrecurrence hassolution T.n/ ‚.F /,whichwecanshowusing thesubsti-
n
D
tution method. For an inductive hypothesis, assume that T.n/ aF b, where
n
 (cid:0)
a >1andb >0areconstants. Substituting, weobtain
776 Chapter27 MultithreadedAlgorithms
T.n/ .aF b/ .aF b/ ‚.1/
n 1 n 2
 (cid:0) (cid:0) C (cid:0) (cid:0) C
a.F F / 2b ‚.1/
n 1 n 2
D (cid:0) C (cid:0) (cid:0) C
aF b .b ‚.1//
n
D (cid:0) (cid:0) (cid:0)
aF b
n
 (cid:0)
if we choose b large enough to dominate the constant in the ‚.1/. We can then
choosea largeenoughtosatisfytheinitialcondition. Theanalytical bound
T.n/ ‚. n/; (27.1)
D
where .1 p5/=2 is the golden ratio, now follows from equation (3.25).
D C
Since F grows exponentially in n, this procedure is a particularly slow way to
n
computeFibonaccinumbers. (SeeProblem31-3formuchfasterways.)
Although the FIB procedure is a poor way to compute Fibonacci numbers, it
makesagoodexampleforillustratingkeyconceptsintheanalysisofmultithreaded
algorithms. Observe that within FIB.n/, thetworecursive calls inlines 3and 4to
FIB.n 1/andFIB.n 2/,respectively, areindependent ofeachother: theycould
(cid:0) (cid:0)
be called ineither order, and the computation performed by one inno wayaffects
theother. Therefore, thetworecursivecallscanruninparallel.
Weaugmentourpseudocode toindicate parallelism byadding theconcurrency
keywords spawn and sync. Here is how we can rewrite the FIB procedure to use
dynamicmultithreading:
P-FIB.n/
1 ifn 1

2 returnn
3 elsex spawnP-FIB.n 1/
D (cid:0)
4 y P-FIB.n 2/
D (cid:0)
5 sync
6 returnx y
C
Notice that if we delete the concurrency keywords spawn and sync from P-FIB,
theresultingpseudocodetextisidenticaltoFIB(otherthanrenamingtheprocedure
intheheader andinthetworecursive calls). Wedefine theserialization ofamul-
tithreaded algorithm tobetheserialalgorithm thatresults fromdeleting themulti-
threaded keywords: spawn, sync, and when we examine parallel loops, parallel.
Indeed, our multithreaded pseudocode has the nice property that a serialization is
alwaysordinaryserialpseudocode tosolvethesameproblem.
Nested parallelism occurs whenthe keyword spawnprecedes aprocedure call,
as in line 3. The semantics of a spawn differs from an ordinary procedure call in
that the procedure instance that executes the spawn—the parent—may continue
to execute in parallel with the spawned subroutine—its child—instead of waiting
27.1 Thebasicsofdynamicmultithreading 777
for the child to complete, as would normally happen in a serial execution. In this
case, while the spawned child is computing P-FIB.n 1/, the parent may go on
(cid:0)
to compute P-FIB.n 2/ in line 4 in parallel with the spawned child. Since the
(cid:0)
P-FIB procedure is recursive, these two subroutine calls themselves create nested
parallelism,asdotheirchildren,therebycreatingapotentiallyvasttreeofsubcom-
putations, allexecuting inparallel.
Thekeyword spawndoes not say, however, that aprocedure must execute con-
currently with its spawned children, only that it may. The concurrency keywords
express the logical parallelism of the computation, indicating which parts of the
computation may proceed in parallel. Atruntime, it is up to a scheduler to deter-
minewhichsubcomputations actuallyrunconcurrently byassigningthemtoavail-
able processors as the computation unfolds. We shall discuss the theory behind
schedulers shortly.
Aprocedure cannot safelyuse thevalues returned byitsspawned children until
after it executes a sync statement, as in line 5. The keyword sync indicates that
the procedure must wait as necessary for all its spawned children to complete be-
fore proceeding to the statement after the sync. In the P-FIB procedure, a sync
is required before the return statement in line 6 to avoid the anomaly that would
occur if x and y were summed before x was computed. In addition to explicit
synchronization provided by the sync statement, every procedure executes a sync
implicitly before it returns, thus ensuring that all its children terminate before it
does.
Amodelformultithreadedexecution
It helps to think of a multithreaded computation—the set of runtime instruc-
tionsexecutedbyaprocessor onbehalfofamultithreaded program—asadirected
acyclicgraphG .V;E/,called acomputation dag. Asanexample, Figure27.2
D
shows the computation dag that results from computing P-FIB.4/. Conceptually,
the vertices in V are instructions, and the edges in E represent dependencies be-
tweeninstructions, where.u;/ E meansthatinstructionumustexecutebefore
2
instruction . For convenience, however, if a chain of instructions contains no
parallel control (no spawn, sync, or return from a spawn—via either an explicit
return statement or the return that happens implicitly upon reaching the end of
a procedure), we may group them into a single strand, each of which represents
one or more instructions. Instructions involving parallel control are not included
instrands, but arerepresented inthe structure ofthe dag. Forexample, ifastrand
has two successors, one of them must have been spawned, and a strand with mul-
tiple predecessors indicates the predecessors joined because of a sync statement.
Thus, in the general case, the set V forms the set of strands, and the set E of di-
rectededgesrepresents dependencies betweenstrands induced byparallel control.
778 Chapter27 MultithreadedAlgorithms
P-FIB(4)
P-FIB(3) P-FIB(2)
P-FIB(2) P-FIB(1) P-FIB(1) P-FIB(0)
P-FIB(1) P-FIB(0)
Figure27.2 AdirectedacyclicgraphrepresentingthecomputationofP-FIB.4/. Eachcirclerep-
resents one strand, with black circles representing either base cases or the part of the procedure
(instance)uptothespawnofP-FIB.n 1/inline3,shadedcirclesrepresentingthepartofthepro-
(cid:0)
cedurethatcallsP-FIB.n 2/inline4uptothesyncinline5,whereitsuspendsuntilthespawnof
(cid:0)
P-FIB.n 1/returns,andwhitecirclesrepresentingthepartoftheprocedureafterthesyncwhere
(cid:0)
itsumsx andy uptothepointwhereitreturnstheresult. Eachgroupofstrandsbelongingtothe
sameprocedure issurrounded by arounded rectangle, lightlyshaded forspawned procedures and
heavily shaded for called procedures. Spawn edges and call edges point downward, continuation
edgespointhorizontallytotheright,andreturnedgespointupward.Assumingthateachstrandtakes
unittime,theworkequals17timeunits,sincethereare17strands,andthespanis8timeunits,since
thecriticalpath—shownwithshadededges—contains8strands.
If G has adirected path from strand uto strand , wesaythat the twostrands are
(logically) inseries. Otherwise,strands uand are(logically) inparallel.
We can picture a multithreaded computation as a dag of strands embedded in a
treeofprocedure instances. Forexample, Figure27.1showsthetreeofprocedure
instances for P-FIB.6/withoutthedetailedstructure showingstrands. Figure27.2
zooms in on a section of that tree, showing the strands that constitute each proce-
dure. Alldirected edges connecting strands runeither withinaprocedure oralong
undirected edgesintheprocedure tree.
Wecanclassifytheedgesofacomputationdagtoindicatethekindofdependen-
cies betweenthevarious strands. Acontinuation edge.u;u/,drawnhorizontally
0
in Figure 27.2, connects a strand u to its successor u within the same procedure
0
instance. Whenastranduspawnsastrand,thedagcontainsaspawnedge.u;/,
which points downward in the figure. Call edges, representing normal procedure
calls, also point downward. Strand u spawning strand  differs from u calling 
inthataspawninduces ahorizontal continuation edgefrom utothestrand u fol-
0
27.1 Thebasicsofdynamicmultithreading 779
lowing u in its procedure, indicating that u is free to execute at the same time
0
as , whereas a call induces no such edge. When a strand u returns to its calling
procedure and x is the strand immediately following the next sync in the calling
procedure, thecomputationdagcontainsreturnedge.u;x/,whichpointsupward.
Acomputationstartswithasingleinitialstrand—theblackvertexintheprocedure
labeled P-FIB.4/ in Figure 27.2—and ends with a single final strand—the white
vertexintheprocedure labeled P-FIB.4/.
We shall study the execution of multithreaded algorithms on an ideal paral-
lel computer, which consists of a set of processors and a sequentially consistent
sharedmemory. Sequentialconsistencymeansthatthesharedmemory,whichmay
in reality be performing many loads and stores from the processors at the same
time,produces thesameresultsasifateachstep,exactlyoneinstruction fromone
of the processors is executed. That is, the memory behaves as if the instructions
wereexecutedsequentiallyaccordingtosomegloballinearorderthatpreservesthe
individualordersinwhicheachprocessorissuesitsowninstructions. Fordynamic
multithreaded computations, which are scheduled onto processors automatically
by the concurrency platform, the shared memory behaves as if the multithreaded
computation’sinstructionswereinterleavedtoproducealinearorderthatpreserves
the partial order of the computation dag. Depending on scheduling, the ordering
could differ from one run of the program to another, but the behavior of any exe-
cution can be understood by assuming that the instructions are executed in some
linearorderconsistent withthecomputation dag.
Inaddition tomakingassumptions aboutsemantics, theideal-parallel-computer
model makes some performance assumptions. Specifically, it assumes that each
processor in the machine has equal computing power, and it ignores the cost of
scheduling. Although this last assumption may sound optimistic, it turns out that
for algorithms with sufficient “parallelism” (a term we shall define precisely in a
moment),theoverheadofscheduling isgenerally minimalinpractice.
Performancemeasures
Wecangaugethetheoretical efficiencyofamultithreaded algorithm byusingtwo
metrics: “work”and“span.” Theworkofamultithreaded computation isthetotal
timetoexecutetheentire computation ononeprocessor. Inother words, thework
is the sum of the times taken by each of the strands. For a computation dag in
which each strand takes unit time, the work is just the number of vertices in the
dag. Thespanisthelongest timetoexecutethestrands alonganypathinthedag.
Again,foradaginwhicheachstrandtakesunittime,thespanequalsthenumberof
vertices onalongest orcritical pathinthedag. (RecallfromSection24.2thatwe
canfindacritical pathinadagG .V;E/in‚.V E/time.) Forexample,the
D C
computation dag of Figure 27.2 has 17 vertices in all and 8 vertices on its critical
780 Chapter27 MultithreadedAlgorithms
path, so that if each strand takes unit time, its work is 17 time units and its span
is8timeunits.
The actual running time of a multithreaded computation depends not only on
its work and its span, but also on how many processors are available and how
the scheduler allocates strands to processors. To denote the running time of a
multithreadedcomputationonP processors,weshallsubscriptbyP. Forexample,
we might denote the running time of an algorithm on P processors by T . The
P
workistherunningtimeonasingleprocessor, orT . Thespanistherunningtime
1
if we could run each strand on its own processor—in other words, if we had an
unlimited numberofprocessors—and sowedenotethespanbyT .
The work and span provide lower bounds on the running
time1T
of a multi-
P
threaded computation onP processors:
 In one step, an ideal parallel computer with P processors can do at most P
unitsofwork,andthusinT time,itcanperformatmostPT work. Sincethe
P P
totalworktodoisT ,wehavePT T . DividingbyP yieldstheworklaw:
1 P 1

T T =P : (27.2)
P 1

 A P-processor ideal parallel computer cannot run any faster than a machine
with an unlimited number of processors. Looked at another way, a machine
withanunlimitednumberofprocessorscanemulateaP-processormachineby
usingjustP ofitsprocessors. Thus,thespanlawfollows:
T T : (27.3)
P
 1
We define the speedup of a computation on P processors by the ratio T =T ,
1 P
which says how many times faster the computation is on P processors than
on1processor. By the work law, we have T T =P, which implies that
P 1

T =T P. Thus, the speedup on P processors can be at most P. When the
1 P

speedup is linear in the number of processors, that is, when T =T ‚.P/, the
1 P
D
computation exhibits linear speedup, and when T =T P, we have perfect
1 P
D
linearspeedup.
The ratio T =T of the work to the span gives the parallelism of the multi-
1
1
threaded computation. Wecan view the parallelism from three perspectives. Asa
ratio,theparallelismdenotestheaverageamountofworkthatcanbeperformedin
parallel for each step along the critical path. As an upper bound, the parallelism
gives the maximum possible speedup that can be achieved on any number of pro-
cessors. Finally, and perhaps most important, the parallelism provides a limit on
thepossibilityofattainingperfectlinearspeedup. Specifically,oncethenumberof
processors exceeds the parallelism, the computation cannot possibly achieve per-
fectlinearspeedup. Toseethislastpoint,supposethatP > T =T ,inwhichcase
1
1
27.1 Thebasicsofdynamicmultithreading 781
the span law implies that the speedup satisfies T =T T =T < P. Moreover,
1 P 1
if the number P of processors in the ideal parallel co mputer g1 reatly exceeds the
parallelism—that is, if P T =T —then T =T P, so that the speedup is
1 1 P
1
muchless than the number ofprocessors. Inother words, the moreprocessors we
usebeyondtheparallelism, thelessperfectthespeedup.
Asan example, consider the computation P-FIB.4/ in Figure 27.2, and assume
thateachstrandtakesunittime. SincetheworkisT 17andthespanisT 8,
1
the parallelism is T =T 17=8 2:125. ConseqD uently, achieving muc1 h mD ore
1
1 D D
than double the speedup is impossible, no matter how many processors we em-
ploytoexecutethecomputation. Forlarger input sizes, however, weshallseethat
P-FIB.n/exhibitssubstantial parallelism.
We define the (parallel) slackness of a multithreaded computation executed
on an ideal parallel computer with P processors to be the ratio .T =T /=P
1
T =.PT /, which is the factor by which the parallelism of the comput1 ation exD -
1
ceedsthe1 numberofprocessorsinthemachine. Thus,iftheslacknessislessthan1,
wecannothopetoachieveperfectlinearspeedup, becauseT =.PT / < 1andthe
1
spanlawimplythatthespeedup onP processors satisfiesT =T 1T =T < P.
1 P 1
Indeed,astheslacknessdecreasesfrom1toward0,thespeedupoft hecomp1
utation
divergesfurtherandfurtherfromperfectlinearspeedup. Iftheslackness isgreater
than1,however,theworkperprocessoristhelimitingconstraint. Asweshallsee,
as the slackness increases from 1, a good scheduler can achieve closer and closer
toperfectlinearspeedup.
Scheduling
Goodperformance depends onmorethanjustminimizing theworkandspan. The
strands must also be scheduled efficiently onto the processors of the parallel ma-
chine. Our multithreaded programming model provides no way to specify which
strands to execute on which processors. Instead, we rely on the concurrency plat-
form’sschedulertomapthedynamicallyunfolding computationtoindividualpro-
cessors. In practice, the scheduler maps the strands to static threads, and the op-
erating system schedules the threads on the processors themselves, but this extra
level of indirection is unnecessary for our understanding of scheduling. We can
justimagine thatthe concurrency platform’s scheduler mapsstrands toprocessors
directly.
A multithreaded scheduler must schedule the computation with no advance
knowledge of whenstrands willbespawned orwhen they willcomplete—it must
operate on-line. Moreover, a good scheduler operates in a distributed fashion,
wherethethreads implementing the scheduler cooperate toload-balance the com-
putation. Provably good on-line, distributed schedulers exist, but analyzing them
iscomplicated.
782 Chapter27 MultithreadedAlgorithms
Instead, tokeep our analysis simple, weshall investigate an on-line centralized
scheduler, which knows the global state of the computation at any given time. In
particular, we shall analyze greedy schedulers, which assign as many strands to
processors aspossible ineach timestep. Ifat least P strands areready toexecute
during atimestep, wesay thatthe stepisacomplete step, and agreedy scheduler
assigns anyP ofthereadystrandstoprocessors. Otherwise,fewerthanP strands
areready toexecute, inwhichcase wesaythat thestep isanincomplete step, and
thescheduler assigns eachreadystrandtoitsownprocessor.
From the work law, the best running time we can hope for on P processors
is T T =P, and from the span law the best we can hope for is T T .
P 1 P
D D 1
The following theorem shows that greedy scheduling is provably good in that it
achievesthesumofthesetwolowerboundsasanupperbound.
Theorem27.1
On an ideal parallel computer with P processors, a greedy scheduler executes a
multithreaded computation withworkT andspanT intime
1
1
T T =P T : (27.4)
P 1
 C 1
Proof We start by considering the complete steps. In each complete step, the
P processors together perform a total of P work. Suppose for the purpose of
contradiction that the number of complete steps is strictly greater than T =P .
1
b c
Then,thetotalworkofthecompletestepsisatleast
P . T =P 1/ P T =P P
1 1
 b cC D b cC
T .T modP/ P (byequation (3.8))
1 1
D (cid:0) C
> T (byinequality (3.9)) .
1
Thus,weobtainthecontradiction thattheP processorswouldperformmorework
than the computation requires, which allows us to conclude that the number of
completestepsisatmost T =P .
1
b c
Now, consider an incomplete step. Let G be the dag representing the entire
computation, and without loss of generality, assume that each strand takes unit
time. (We can replace each longer strand by a chain of unit-time strands.) Let G
0
bethesubgraphofG thathasyettobeexecutedatthestartoftheincompletestep,
and let G be the subgraph remaining to be executed after the incomplete step. A
00
longest path in a dag must necessarily start at a vertex with in-degree 0. Since an
incomplete step of a greedy scheduler executes all strands with in-degree 0 in G ,
0
the length of a longest path in G must be 1 less than the length of a longest path
00
inG . Inotherwords,anincompletestepdecreasesthespanoftheunexecuteddag
0
by1. Hence,thenumberofincompletestepsisatmostT .
1
Sinceeachstepiseithercompleteorincomplete, thetheorem follows.
27.1 Thebasicsofdynamicmultithreading 783
Thefollowing corollary to Theorem 27.1shows that agreedy scheduler always
performswell.
Corollary27.2
The running time T of any multithreaded computation scheduled by a greedy
P
scheduler on an ideal parallel computer with P processors is within a factor of 2
ofoptimal.
Proof LetT betherunningtimeproducedbyanoptimalscheduleronamachine
P
with P processors, and let T and T be the work and span of the computation,
1
1
respectively. Since the work and span laws—inequalities (27.2) and (27.3)—give
usT max.T =P;T /,Theorem27.1impliesthat
P

1
1
T T =P T
P 1
 C 1
2 max.T =P;T /
1
  1
2T :

P
Thenext corollary shows that, infact, a greedy scheduler achieves near-perfect
linearspeedup onanymultithreaded computation astheslackness grows.
Corollary27.3
LetT be the running time of amultithreaded computation produced by agreedy
P
scheduler on an ideal parallel computer with P processors, and let T and T be
1
the work and span of the computation, respectively. Then, if P T =T 1 , we
1
haveT T =P,orequivalently, aspeedup ofapproximately P. 1
P 1

Proof If we suppose that P T =T , then we also have T T =P, and
1 1
hence Theorem 27.1 gives us T T1=P T T =P.1 S ince the work
P 1 1
law (27.2) dictates that T T =P , we concC lude1 th at T T =P, or equiva-
P 1 P 1
 
lently,thatthespeedup isT =T P.
1 P

The symbol denotes “much less,” but how much is “much less”? As a rule
of thumb, a slackness of at least 10—that is, 10 times more parallelism than pro-
cessors—generally suffices to achieve good speedup. Then, the span term in the
greedy bound, inequality (27.4), is less than 10% of the work-per-processor term,
whichisgoodenoughformostengineering situations. Forexample, ifacomputa-
tion runs ononly 10 or 100 processors, itdoesn’t make sense to value parallelism
of, say 1,000,000 over parallelism of 10,000, even with the factor of 100 differ-
ence. As Problem 27-2 shows, sometimes by reducing extreme parallelism, we
canobtain algorithms thatarebetterwithrespect tootherconcerns andwhichstill
scaleupwellonreasonable numbersofprocessors.
784 Chapter27 MultithreadedAlgorithms
A
A B
B
Work:T 1.A [B/ DT 1.A/ CT 1.B/ Work:T 1.A [B/ DT 1.A/ CT 1.B/
Span:T .A B/ T .A/ T .B/ Span:T .A B/ max.T .A/;T .B/)
1 [ D 1 C 1 1 [ D 1 1
(a) (b)
Figure27.3 Thework andspanof composed subcomputations. (a) Whentwosubcomputations
are joined in series, the work of the composition is the sum of their work, and the span of the
composition is the sum of their spans. (b) When two subcomputations are joined in parallel, the
workofthecompositionremainsthesumoftheirwork,butthespanofthecompositionisonlythe
maximumoftheirspans.
Analyzingmultithreadedalgorithms
Wenowhaveallthetoolsweneedtoanalyzemultithreadedalgorithmsandprovide
good bounds on their running timeson various numbers ofprocessors. Analyzing
the work is relatively straightforward, since it amounts to nothing more than ana-
lyzing the running time of an ordinary serial algorithm—namely, the serialization
ofthemultithreaded algorithm—which youshould already befamiliar with,since
thatiswhatmostofthistextbookisabout! Analyzingthespanismoreinteresting,
but generally no harder once you get the hang of it. Weshall investigate the basic
ideasusingtheP-FIB program.
AnalyzingtheworkT 1.n/ofP-FIB.n/posesnohurdles, becausewe’vealready
done it. The original FIB procedure is essentially the serialization of P-FIB, and
henceT .n/ T.n/ ‚. n/fromequation (27.1).
1
D D
Figure 27.3 illustrates how to analyze the span. If two subcomputations are
joined in series, their spans add to form the span of their composition, whereas
if they are joined inparallel, the span of their composition is the maximum of the
spansofthetwosubcomputations. ForP-FIB.n/,thespawnedcalltoP-FIB.n 1/
(cid:0)
in line 3 runs in parallel with the call to P-FIB.n 2/ in line 4. Hence, we can
(cid:0)
expressthespanof P-FIB.n/astherecurrence
T .n/ max.T .n 1/;T .n 2// ‚.1/
1 D 1 (cid:0) 1 (cid:0) C
T .n 1/ ‚.1/;
D 1 (cid:0) C
whichhassolution T .n/ ‚.n/.
The parallelism of1 P-FID B.n/ is T 1.n/=T .n/ ‚. n=n/, which grows dra-
matically as n gets large. Thus, on even the1 largeD st parallel computers, a modest
27.1 Thebasicsofdynamicmultithreading 785
value for n suffices to achieve near perfect linear speedup for P-FIB.n/, because
thisprocedure exhibitsconsiderable parallel slackness.
Parallelloops
Many algorithms contain loops all of whose iterations can operate in parallel. As
we shall see, we can parallelize such loops using the spawn and sync keywords,
but itismuch moreconvenient tospecify directly that the iterations ofsuch loops
can run concurrently. Our pseudocode provides this functionality via the parallel
concurrency keyword, whichprecedestheforkeywordinaforloopstatement.
Asanexample, consider theproblem ofmultiplying ann nmatrixA .a /
ij
 D
byann-vectorx .x /. Theresulting n-vectory .y /isgivenbytheequation
j i
D D
n
y a x ;
i ij j
D
j 1
XD
fori 1;2;:::;n. Wecanperformmatrix-vectormultiplicationbycomputingall
D
theentriesofy inparallel asfollows:
MAT-VEC.A;x/
1 n A:rows
D
2 lety beanewvectoroflengthn
3 parallelfori 1ton
D
4 y 0
i
D
5 parallelfori 1ton
D
6 forj 1ton
D
7 y y a x
i i ij j
D C
8 returny
In this code, the parallel for keywords in lines 3 and 5 indicate that the itera-
tions of the respective loops may be run concurrently. A compiler can implement
eachparallelforloopasadivide-and-conquersubroutineusingnestedparallelism.
For example, the parallel for loop in lines 5–7 can be implemented with the call
MAT-VEC-MAIN-LOOP.A;x;y;n;1;n/, where the compiler produces the auxil-
iarysubroutine MAT-VEC-MAIN-LOOP asfollows:
786 Chapter27 MultithreadedAlgorithms
1,8
1,4 5,8
1,2 3,4 5,6 7,8
1,1 2,2 3,3 4,4 5,5 6,6 7,7 8,8
Figure27.4 AdagrepresentingthecomputationofMAT-VEC-MAIN-LOOP.A;x;y;8;1;8/. The
twonumberswithineachrounded rectanglegivethevaluesofthelasttwoparameters(i andi in
0
theprocedure header) intheinvocation(spawnor call)of theprocedure. Theblackcirclesrepre-
sent strandscorresponding toeither thebase case or thepart of theprocedure up tothespawn of
MAT-VEC-MAIN-LOOPinline5;theshadedcirclesrepresentstrandscorrespondingtothepartof
theprocedurethatcallsMAT-VEC-MAIN-LOOPinline6uptothesyncinline7,whereitsuspends
untilthespawnedsubroutineinline5returns;andthewhitecirclesrepresentstrandscorresponding
tothe(negligible)partoftheprocedureafterthesyncuptothepointwhereitreturns.
MAT-VEC-MAIN-LOOP.A;x;y;n;i;i 0/
1 ifi ==i
0
2 forj 1ton
D
3 y y a x
i i ij j
D C
4 elsemid .i i /=2
0
D b C c
5 spawnMAT-VEC-MAIN-LOOP.A;x;y;n;i;mid/
6 MAT-VEC-MAIN-LOOP.A;x;y;n;mid 1;i 0/
C
7 sync
This code recursively spawns the first half of the iterations of the loop to execute
in parallel with the second half of the iterations and then executes a sync, thereby
creating a binary tree of execution where the leaves are individual loop iterations,
asshowninFigure27.4.
TocalculatetheworkT 1.n/ofMAT-VEConann nmatrix,wesimplycompute

therunning timeofitsserialization, whichweobtainbyreplacing theparallelfor
loops with ordinary for loops. Thus, we have T .n/ ‚.n2/, because the qua-
1
D
draticrunningtimeofthedoublynestedloopsinlines5–7dominates. Thisanalysis
27.1 Thebasicsofdynamicmultithreading 787
seems to ignore the overhead for recursive spawning in implementing the parallel
loops,however. Infact,theoverheadofrecursivespawningdoesincreasethework
of a parallel loop compared with that of its serialization, but not asymptotically.
To see why, observe that since the tree of recursive procedure instances is a full
binarytree,thenumberofinternal nodesis1fewerthanthenumberofleaves(see
ExerciseB.5-3). Eachinternalnodeperformsconstant worktodividetheiteration
range, and each leaf corresponds to an iteration of the loop, which takes at least
constant time (‚.n/time inthis case). Thus, wecan amortize the overhead of re-
cursivespawningagainsttheworkoftheiterations,contributing atmostaconstant
factortotheoverallwork.
Asapracticalmatter,dynamic-multithreadingconcurrencyplatformssometimes
coarsen theleaves ofthe recursion byexecuting several iterations inasingle leaf,
either automatically or under programmer control, thereby reducing the overhead
ofrecursive spawning. Thisreduced overheadcomesattheexpense ofalsoreduc-
ing the parallelism, however, but if the computation has sufficient parallel slack-
ness,near-perfect linearspeedup neednotbesacrificed.
Wemustalsoaccountfortheoverheadofrecursivespawningwhenanalyzingthe
spanofaparallel-loopconstruct. Sincethedepthofrecursivecallingislogarithmic
in the number of iterations, for a parallel loop with n iterations in which the ith
iterationhasspaniter .i/,thespanis
1
T .n/ ‚.lgn/ max iter .i/:
1 D C1 i n 1
 
For example, for MAT-VEC on an n n matrix, the parallel initialization loop in

lines3–4hasspan‚.lgn/,becausetherecursivespawningdominatestheconstant-
time work of each iteration. The span of the doubly nested loops in lines 5–7
is‚.n/,because each iteration ofthe outer parallel for loop contains niterations
of the inner (serial) for loop. The span of the remaining code in the procedure
is constant, and thus the span is dominated by the doubly nested loops, yielding
an overall span of ‚.n/ for the whole procedure. Since the work is ‚.n2/, the
parallelism is ‚.n2/=‚.n/ ‚.n/. (Exercise 27.1-6 asks you to provide an
D
implementation withevenmoreparallelism.)
Raceconditions
Amultithreaded algorithm isdeterministic ifitalwaysdoes thesamething onthe
same input, no matter how the instructions are scheduled on the multicore com-
puter. It is nondeterministic if its behavior might vary from run to run. Often, a
multithreaded algorithm that is intended to be deterministic fails to be, because it
containsa“determinacy race.”
Race conditions are the bane of concurrency. Famous race bugs include the
Therac-25 radiation therapy machine, which killed three people and injured sev-
788 Chapter27 MultithreadedAlgorithms
eral others, and the North American Blackout of 2003, which left over 50 million
peoplewithoutpower. Theseperniciousbugsarenotoriouslyhardtofind. Youcan
run tests in the lab for days without a failure only to discover that your software
sporadically crashes inthefield.
A determinacy race occurs when two logically parallel instructions access the
same memory location and at least one of the instructions performs a write. The
followingprocedure illustrates aracecondition:
RACE-EXAMPLE./
1 x 0
D
2 parallelfori 1to2
D
3 x x 1
D C
4 printx
After initializing x to 0 in line 1, RACE-EXAMPLE creates two parallel strands,
each of which increments x in line 3. Although it might seem that RACE-
EXAMPLEshouldalwaysprintthevalue2(itsserializationcertainlydoes),itcould
instead printthevalue1. Let’sseehowthisanomalymightoccur.
Whenaprocessorincrementsx,theoperationisnotindivisible,butiscomposed
ofasequence ofinstructions:
1. Readx frommemoryintooneoftheprocessor’s registers.
2. Incrementthevalueintheregister.
3. Writethevalueintheregisterbackintox inmemory.
Figure 27.5(a) illustrates a computation dag representing the execution of RACE-
EXAMPLE, with the strands broken down to individual instructions. Recall that
since an ideal parallel computer supports sequential consistency, we can view the
parallel execution of a multithreaded algorithm as an interleaving of instructions
that respects the dependencies in the dag. Part (b) of the figure shows the values
in an execution of the computation that elicits the anomaly. The value x is stored
in memory, and r and r are processor registers. In step 1, one of the processors
1 2
sets x to 0. In steps 2and 3, processor 1 reads x from memory into its register r
1
and increments it, producing the value 1 in r . At that point, processor 2 comes
1
intothepicture,executinginstructions4–6. Processor2readsx frommemoryinto
register r ; increments it, producing the value 1 in r ; and then stores this value
2 2
into x, setting x to 1. Now, processor 1 resumes with step 7, storing the value 1
in r into x, which leaves the value of x unchanged. Therefore, step 8 prints the
1
value1,ratherthan2,astheserialization wouldprint.
We can see what has happened. If the effect of the parallel execution were that
processor 1 executed all its instructions before processor 2, the value 2 would be
27.1 Thebasicsofdynamicmultithreading 789
1 x = 0
step x r r
1 2
2 r = x 4 r = x 1 0 – –
1 2
2 0 0 –
3 incr r 5 incr r 3 0 1 –
1 2
4 0 1 0
7 x = r 6 x = r 5 0 1 1
1 2
6 1 1 1
8 print x 7 1 1 1
(a) (b)
Figure27.5 IllustrationofthedeterminacyraceinRACE-EXAMPLE.(a)Acomputationdagshow-
ingthedependenciesamongindividualinstructions. Theprocessorregistersarer1andr2. Instruc-
tionsunrelatedtotherace,suchastheimplementationofloopcontrol,areomitted.(b)Anexecution
sequencethatelicitsthebug, showingthevaluesofx inmemoryandregistersr1 andr2 foreach
stepintheexecutionsequence.
printed. Conversely, iftheeffectwerethatprocessor 2executedallitsinstructions
beforeprocessor 1,thevalue2wouldstillbeprinted. Whentheinstructions ofthe
twoprocessorsexecuteatthesametime,however,itispossible, asinthisexample
execution, thatoneoftheupdates tox islost.
Ofcourse, manyexecutions donot elicit the bug. Forexample, ifthe execution
order were 1;2;3;7;4;5;6;8 or 1;4;5;6;2;3;7;8 , we would get the cor-
h i h i
rect result. That’s the problem withdeterminacy races. Generally, most orderings
produce correct results—such as any in which the instructions on the left execute
before the instructions on the right, or vice versa. But some orderings generate
improper results when the instructions interleave. Consequently, races can be ex-
tremely hard to test for. You can run tests for days and never see the bug, only to
experience acatastrophic systemcrashinthefieldwhentheoutcomeiscritical.
Although wecan cope with races in a variety of ways, including using mutual-
exclusion locks and other methods of synchronization, for our purposes, we shall
simply ensure that strands that operate in parallel are independent: they have no
determinacyracesamongthem. Thus,inaparallelforconstruct, alltheiterations
should be independent. Between a spawn and the corresponding sync, the code
of the spawned child should be independent of the code of the parent, including
code executed by additional spawned or called children. Note that arguments to a
spawnedchildareevaluatedintheparentbeforetheactualspawnoccurs,andthus
theevaluation ofarguments toaspawnedsubroutine isinserieswithanyaccesses
tothosearguments afterthespawn.
790 Chapter27 MultithreadedAlgorithms
As an example of how easy it is to generate code with races, here is a faulty
implementation ofmultithreaded matrix-vector multiplication thatachievesaspan
of‚.lgn/byparallelizing theinnerforloop:
MAT-VEC-WRONG.A;x/
1 n A:rows
D
2 lety beanewvectoroflengthn
3 parallelfori 1ton
D
4 y 0
i
D
5 parallelfori 1ton
D
6 parallelforj 1ton
D
7 y y a x
i i ij j
D C
8 returny
This procedure is, unfortunately, incorrect due to races on updating y in line 7,
i
whichexecutesconcurrentlyforallnvaluesofj. Exercise27.1-6asksyoutogive
acorrectimplementation with‚.lgn/span.
A multithreaded algorithm with races can sometimes be correct. As an exam-
ple, two parallel threads might store the same value into a shared variable, and it
wouldn’tmatterwhichstoredthevaluefirst. Generally,however,weshallconsider
codewithracestobeillegal.
Achesslesson
We close this section with a true story that occurred during the development of
theworld-class multithreaded chess-playing program ?Socrates[80],although the
timings below have been simplified for exposition. The program was prototyped
ona32-processorcomputerbutwasultimatelytorunonasupercomputerwith512
processors. Atonepoint,thedevelopersincorporated anoptimizationintothepro-
gramthatreduceditsrunningtimeonanimportantbenchmarkonthe32-processor
machine from T 65 seconds to T 40 seconds. Yet, the developers used
32
D
302
D
the work and span performance measures to conclude that the optimized version,
whichwasfasteron32processors, wouldactually beslowerthantheoriginal ver-
sionon512processsors. Asaresult, theyabandoned the“optimization.”
Hereistheiranalysis. Theoriginalversionoftheprogram hadworkT 2048
1
D
seconds and span T 1 second. If we treat inequality (27.4) as an equation,
T T =P T , 1 andD use it as an approximation to the running time on P pro-
P 1
cessD ors, wesC eeth1 at indeed T 2048=32 1 65. Withthe optimization, the
32
D C D
work became T 1024 seconds and the span became T 8 seconds. Again
10
D
0
D
usingourapproximation, wegetT 1024=32 8 401.
302
D C D
The relative speeds of the two versions switch when we calculate the running
timeson512processors,however. Inparticular,wehaveT 2048=512 1 5
512
D C D
27.1 Thebasicsofdynamicmultithreading 791
seconds, and T 1024=512 8 10seconds. Theoptimization thatsped up
5012
D C D
theprogram on32processors wouldhavemadetheprogram twiceasslowon512
processors! Theoptimizedversion’sspanof8,whichwasnotthedominanttermin
the running time on 32 processors, became the dominant term on 512 processors,
nullifying theadvantage fromusingmoreprocessors.
The moral of the story is that work and span can provide a better means of
extrapolating performance thancanmeasuredrunning times.
Exercises
27.1-1
Suppose that we spawn P-FIB.n 2/ in line 4 of P-FIB, rather than calling it
(cid:0)
as is done in the code. What is the impact on the asymptotic work, span, and
parallelism?
27.1-2
Draw the computation dag that results from executing P-FIB.5/. Assuming that
each strand in the computation takes unit time, what are the work, span, and par-
allelism ofthecomputation? Showhowtoschedule thedagon3processors using
greedyschedulingbylabelingeachstrandwiththetimestepinwhichitisexecuted.
27.1-3
Provethatagreedyscheduler achievesthefollowingtimebound,whichisslightly
strongerthantheboundproveninTheorem27.1:
T T
1
T P (cid:0) 1 T : (27.5)
 P C 1
27.1-4
Construct a computation dag for which one execution of a greedy scheduler can
take nearly twice thetimeofanother execution of agreedy scheduler onthe same
numberofprocessors. Describehowthetwoexecutions wouldproceed.
27.1-5
Professor Karan measures her deterministic multithreaded algorithm on 4, 10,
and 64 processors of an ideal parallel computer using a greedy scheduler. She
claims that the three runs yielded T 80 seconds, T 42 seconds, and
4 10
D D
T 10seconds. Argue that the professor iseither lying or incompetent. (Hint:
64
D
Use the work law (27.2), the span law (27.3), and inequality (27.5) from Exer-
cise27.1-3.)
792 Chapter27 MultithreadedAlgorithms
27.1-6
Give a multithreaded algorithm to multiply an n n matrix by an n-vector that

achieves‚.n2=lgn/parallelism whilemaintaining ‚.n2/work.
27.1-7
Considerthefollowingmultithreadedpseudocodefortransposingann nmatrixA

inplace:
P-TRANSPOSE.A/
1 n A:rows
D
2 parallelforj 2ton
D
3 parallelfori 1toj 1
D (cid:0)
4 exchangea witha
ij ji
Analyzethework,span,andparallelism ofthisalgorithm.
27.1-8
Suppose thatwereplace theparallel forloopinline3of P-TRANSPOSE (seeEx-
ercise 27.1-7) with an ordinary for loop. Analyze the work, span, and parallelism
oftheresultingalgorithm.
27.1-9
For how many processors do the two versions of the chess program run equally
fast,assumingthatT T =P T ?
P 1
D C 1
27.2 Multithreaded matrix multiplication
In this section, we examine how to multithread matrix multiplication, a problem
whose serial running time we studied in Section 4.2. We’ll look at multithreaded
algorithms basedonthestandard triplynested loop,aswellasdivide-and-conquer
algorithms.
Multithreadedmatrixmultiplication
Thefirstalgorithmwestudyisthestraighforwardalgorithm basedonparallelizing
theloopsintheprocedure SQUARE-MATRIX-MULTIPLY onpage75:
27.2 Multithreadedmatrixmultiplication 793
P-SQUARE-MATRIX-MULTIPLY.A;B/
1 n A:rows
D
2 letC beanewn nmatrix

3 parallelfori 1ton
D
4 parallelforj 1ton
D
5 c 0
ij
D
6 fork 1ton
D
7 c c a b
ij ij ik kj
D C 
8 returnC
Toanalyzethisalgorithm,observethatsincetheserialization ofthealgorithmis
just SQUARE-MATRIX-MULTIPLY, the work is therefore simply T 1.n/ ‚.n3/,
D
the same as the running time of SQUARE-MATRIX-MULTIPLY. The span is
T .n/ ‚.n/, because it follows a path down the tree of recursion for the
1 D
parallelforloopstartinginline3,thendownthetreeofrecursion fortheparallel
forloopstartinginline4,andthenexecutesallniterationsoftheordinaryforloop
starting in line 6, resulting in a total span of ‚.lgn/ ‚.lgn/ ‚.n/ ‚.n/.
C C D
Thus, the parallelism is ‚.n3/=‚.n/ ‚.n2/. Exercise 27.2-3 asks you to par-
D
allelize theinnerlooptoobtain aparallelism of‚.n3=lgn/,whichyoucannot do
straightforwardly usingparallelfor,because youwouldcreateraces.
Adivide-and-conquermultithreadedalgorithm formatrixmultiplication
As we learned in Section 4.2, we can multiply n n matrices serially in time

‚.nlg7/ O.n2:81/usingStrassen’sdivide-and-conquerstrategy,whichmotivates
D
ustolookatmultithreadingsuchanalgorithm. Webegin,aswedidinSection4.2,
withmultithreading asimplerdivide-and-conquer algorithm.
Recallfrompage77thattheSQUARE-MATRIX-MULTIPLY-RECURSIVE proce-
dure,whichmultipliestwon nmatricesAandB toproducethen nmatrixC,
 
reliesonpartitioning eachofthethreematricesintofourn=2 n=2submatrices:

A A B B C C
A 11 12 ; B 11 12 ; C 11 12 :
D A 21 A 22 D B 21 B 22 D C 21 C 22
     
Then,wecanwritethematrixproduct as
C C A A B B
11 12 11 12 11 12
C 21 C 22 D A 21 A 22 B 21 B 22
    
A B A B A B A B
11 11 11 12 12 21 12 22 : (27.6)
D A 21B 11 A 21B 12 C A 22B 21 A 22B 22
   
Thus,tomultiplytwon nmatrices,weperformeightmultiplicationsofn=2 n=2
 
matricesandoneadditionofn nmatrices. Thefollowingpseudocodeimplements

794 Chapter27 MultithreadedAlgorithms
this divide-and-conquer strategy using nested parallelism. Unlike the SQUARE-
MATRIX-MULTIPLY-RECURSIVE procedure on which it is based, P-MATRIX-
MULTIPLY-RECURSIVE takestheoutput matrixasaparametertoavoidallocating
matricesunnecessarily.
P-MATRIX-MULTIPLY-RECURSIVE.C;A;B/
1 n A:rows
D
2 ifn ==1
3 c a b
11 11 11
D
4 elseletT beanewn nmatrix

5 partition A,B,C,andT inton=2 n=2submatrices

A ;A ;A ;A ;B ;B ;B ;B ;C ;C ;C ;C ;
11 12 21 22 11 12 21 22 11 12 21 22
andT ;T ;T ;T ;respectively
11 12 21 22
6 spawnP-MATRIX-MULTIPLY-RECURSIVE.C 11;A 11;B 11/
7 spawnP-MATRIX-MULTIPLY-RECURSIVE.C 12;A 11;B 12/
8 spawnP-MATRIX-MULTIPLY-RECURSIVE.C 21;A 21;B 11/
9 spawnP-MATRIX-MULTIPLY-RECURSIVE.C 22;A 21;B 12/
10 spawnP-MATRIX-MULTIPLY-RECURSIVE.T 11;A 12;B 21/
11 spawnP-MATRIX-MULTIPLY-RECURSIVE.T 12;A 12;B 22/
12 spawnP-MATRIX-MULTIPLY-RECURSIVE.T 21;A 22;B 21/
13 P-MATRIX-MULTIPLY-RECURSIVE.T 22;A 22;B 22/
14 sync
15 parallelfori 1ton
D
16 parallelforj 1ton
D
17 c c t
ij ij ij
D C
Line3handlesthebasecase,wherewearemultiplying 1 1matrices. Wehandle

the recursive case in lines 4–17. We allocate a temporary matrix T in line 4, and
line5partitions eachofthematrices A,B,C,andT inton=2 n=2submatrices.

(As with SQUARE-MATRIX-MULTIPLY-RECURSIVE on page 77, we gloss over
the minor issue of how to use index calculations to represent submatrix sections
of a matrix.) The recursive call in line 6 sets the submatrix C to the submatrix
11
product A B , so that C equals the first of the two terms that form its sum in
11 11 11
equation (27.6). Similarly, lines 7–9 set C , C , and C to the first of the two
12 21 22
terms that equal their sums in equation (27.6). Line 10 sets the submatrix T to
11
the submatrix product A B ,sothat T equals the second ofthe twoterms that
12 21 11
form C ’ssum. Lines 11–13 setT , T , and T tothe second ofthe twoterms
11 12 21 22
that form the sums of C , C , and C , respectively. The first seven recursive
12 21 22
calls are spawned, andthe last oneruns inthe mainstrand. Thesync statement in
line 14 ensures that all the submatrix products in lines 6–13 have been computed,
27.2 Multithreadedmatrixmultiplication 795
after which we add the products from T into C using the doubly nested parallel
forloopsinlines15–17.
We first analyze the work M 1.n/ of the P-MATRIX-MULTIPLY-RECURSIVE
procedure, echoing the serial running-time analysis of its progenitor SQUARE-
MATRIX-MULTIPLY-RECURSIVE. Intherecursivecase,wepartitionin‚.1/time,
perform eight recursive multiplications of n=2 n=2matrices, and finish up with

the ‚.n2/ work from adding two n n matrices. Thus, the recurrence for the

workM .n/is
1
M .n/ 8M .n=2/ ‚.n2/
1 1
D C
‚.n3/
D
bycase1ofthemastertheorem. Inotherwords,theworkofourmultithreaded al-
gorithmisasymptotically thesameastherunningtimeoftheprocedure SQUARE-
MATRIX-MULTIPLY inSection4.2,withitstriplynestedloops.
Todetermine the span M .n/ of P-MATRIX-MULTIPLY-RECURSIVE, we first
observe that the span for pa1 rtitioning is ‚.1/, which is dominated by the ‚.lgn/
span of the doubly nested parallel for loops in lines 15–17. Because the eight
parallelrecursivecallsallexecuteonmatricesofthesamesize,themaximumspan
for any recursive call is just the span of any one. Hence, the recurrence for the
spanM .n/ofP-MATRIX-MULTIPLY-RECURSIVE is
1
M .n/ M .n=2/ ‚.lgn/: (27.7)
1 D 1 C
This recurrence does not fall under any of the cases of the master theorem, but
it does meet the condition of Exercise 4.6-2. By Exercise 4.6-2, therefore, the
solutiontorecurrence (27.7)isM .n/ ‚.lg2n/.
1 D
Now that we know the work and span of P-MATRIX-MULTIPLY-RECURSIVE,
we can compute its parallelism as M .n/=M .n/ ‚.n3=lg2n/, which is very
1
1 D
high.
MultithreadingStrassen’smethod
To multithread Strassen’s algorithm, we follow the same general outline as on
page79,onlyusingnestedparallelism:
1. Divide the input matrices A and B and output matrix C into n=2 n=2 sub-

matrices, as in equation (27.6). This step takes ‚.1/ work and span by index
calculation.
2. Create10matrices S ;S ;:::;S ,eachofwhichisn=2 n=2andisthesum
1 2 10

or difference of two matrices created in step 1. We can create all 10 matrices
with‚.n2/workand‚.lgn/spanbyusingdoublynestedparallelforloops.
796 Chapter27 MultithreadedAlgorithms
3. Using the submatrices created in step 1 and the 10 matrices created in
step2,recursively spawnthecomputation ofsevenn=2 n=2matrixproducts

P ;P ;:::;P .
1 2 7
4. Compute the desired submatrices C ;C ;C ;C of the result matrix C by
11 12 21 22
adding and subtracting various combinations of the P matrices, once again
i
using doubly nested parallel for loops. We can compute all four submatrices
with‚.n2/workand‚.lgn/span.
To analyze this algorithm, we first observe that since the serialization is the
same as the original serial algorithm, the work is just the running time of the
serialization, namely, ‚.nlg7/. As for P-MATRIX-MULTIPLY-RECURSIVE, we
can devise a recurrence for the span. In this case, seven recursive calls exe-
cute in parallel, but since they all operate on matrices of the same size, we ob-
tainthesamerecurrence(27.7)aswedidforP-MATRIX-MULTIPLY-RECURSIVE,
which has solution ‚.lg2n/. Thus, the parallelism of multithreaded Strassen’s
method is ‚.nlg7=lg2n/, which is high, though slightly less than the parallelism
of P-MATRIX-MULTIPLY-RECURSIVE.
Exercises
27.2-1
DrawthecomputationdagforcomputingP-SQUARE-MATRIX-MULTIPLY on2 2

matrices, labeling how the vertices in your diagram correspond to strands in the
execution of the algorithm. Use the convention that spawn and call edges point
downward, continuation edges point horizontally to the right, and return edges
point upward. Assuming that each strand takes unit time, analyze the work, span,
andparallelism ofthiscomputation.
27.2-2
RepeatExercise27.2-1for P-MATRIX-MULTIPLY-RECURSIVE.
27.2-3
Givepseudocode foramultithreaded algorithm thatmultiplies twon nmatrices

withwork‚.n3/butspanonly‚.lgn/. Analyzeyouralgorithm.
27.2-4
Give pseudocode for an efficient multithreaded algorithm that multiplies a p q

matrix by a q r matrix. Your algorithm should be highly parallel even if any of

p,q,andr are1. Analyzeyouralgorithm.
27.3 Multithreadedmergesort 797
27.2-5
Givepseudocode foranefficientmultithreaded algorithm thattransposes ann n

matrix in place by using divide-and-conquer and no parallel for loops to divide
thematrixrecursively intofourn=2 n=2submatrices. Analyzeyouralgorithm.

27.2-6
Give pseudocode for an efficient multithreaded implementation of the Floyd-
Warshall algorithm (see Section 25.2), whichcomputes shortest paths between all
pairsofverticesinanedge-weighted graph. Analyzeyouralgorithm.
27.3 Multithreaded mergesort
WefirstsawserialmergesortinSection2.3.1,andinSection2.3.2weanalyzedits
running time and showed it to be ‚.nlgn/. Because merge sort already uses the
divide-and-conquer paradigm, it seems like a terrific candidate for multithreading
using nested parallelism. We can easily modify the pseudocode so that the first
recursivecallisspawned:
MERGE-SORT0.A;p;r/
1 ifp < r
2 q .p r/=2
D b C c
3 spawnMERGE-SORT0.A;p;q/
4 MERGE-SORT0.A;q 1;r/
C
5 sync
6 MERGE.A;p;q;r/
Like its serial counterpart, MERGE-SORT0 sorts the subarray AŒp::r. After the
tworecursivesubroutines inlines3and4havecompleted, whichisensuredbythe
sync statement in line 5, MERGE-SORT0 calls the same MERGE procedure as on
page31.
Letusanalyze MERGE-SORT0. Todoso, wefirstneed toanalyze MERGE. Re-
call that its serial running time to merge n elements is ‚.n/. Because MERGE is
serial, bothitsworkanditsspanare‚.n/. Thus,thefollowingrecurrence charac-
terizestheworkMS 01.n/ofMERGE-SORT0 onnelements:
MS .n/ 2MS .n=2/ ‚.n/
01
D
01
C
‚.nlgn/;
D
798 Chapter27 MultithreadedAlgorithms
p q r p q r
1 1 1 2 2 2
… … …
T x x x <x x
  
merge copy merge
… …
A x x x
 
p q r
3 3 3
Figure 27.6 The idea behind the multithreaded merging of two sorted subarrays TŒp1::r1
andTŒp2::r2intothesubarrayAŒp3::r3.Lettingx TŒq1bethemedianofTŒp1::r1andq2
D
betheplaceinTŒp2::r2suchthatx wouldfallbetweenTŒq2 1andTŒq2, everyelement in
(cid:0)
subarraysTŒp1::q1 1andTŒp2::q2 1(lightlyshaded)islessthanorequaltox,andevery
(cid:0) (cid:0)
elementinthesubarraysTŒq1 1::r1andTŒq2 1::r2(heavilyshaded)isatleastx.Tomerge,
C C
wecompute theindexq3 wherex belongsinAŒp3::r3, copyx intoAŒq3, andthenrecursively
mergeTŒp1::q1 1withTŒp2::q2 1intoAŒp3::q3 1andTŒq1 1::r1withTŒq2::r2
(cid:0) (cid:0) (cid:0) C
intoAŒq3 1::r3.
C
whichisthesameastheserialrunning timeofmergesort. Sincethetworecursive
callsofMERGE-SORT0canruninparallel,thespanMS
0
isgivenbytherecurrence
1
MS .n/ MS .n=2/ ‚.n/
0 0
D C
1 1
‚.n/:
D
Thus, the parallelism of MERGE-SORT0 comes to MS 01.n/=MS
0
.n/
D
‚.lgn/,
which is an unimpressive amount of parallelism. To sort 10 mil1lion elements, for
example, it might achieve linear speedup on a few processors, but it would not
scaleupeffectively tohundreds ofprocessors.
You probably have already figured out where the parallelism bottleneck is in
this multithreaded merge sort: the serial MERGE procedure. Although merging
mightinitiallyseemtobeinherentlyserial,wecan,infact,fashionamultithreaded
versionofitbyusingnestedparallelism.
Our divide-and-conquer strategy for multithreaded merging, which is illus-
trated in Figure 27.6, operates on subarrays of an array T. Suppose that we
are merging the two sorted subarrays TŒp ::r  of length n r p 1
1 1 1 1 1
D (cid:0) C
and TŒp ::r  of length n r p 1 into another subarray AŒp ::r , of
2 2 2 2 2 3 3
D (cid:0) C
lengthn r p 1 n n . Withoutlossofgenerality, wemakethesim-
3 3 3 1 2
D (cid:0) C D C
plifying assumption thatn n .
1 2

We first find the middle element x TŒq  of the subarray TŒp ::r ,
1 1 1
D
where q .p r /=2 . Because the subarray is sorted, x is a median
1 1 1
D b C c
of TŒp ::r : every element in TŒp ::q 1 is no more than x, and every el-
1 1 1 1
(cid:0)
ement in TŒq 1::r  is no less than x. We then use binary search to find the
1 1
C
27.3 Multithreadedmergesort 799
indexq inthesubarray TŒp ::r sothatthesubarray wouldstillbesorted ifwe
2 2 2
insertedx betweenTŒq 1andTŒq .
2 2
(cid:0)
Wenextmergetheoriginalsubarrays TŒp ::r andTŒp ::r intoAŒp ::r 
1 1 2 2 3 3
asfollows:
1. Setq p .q p / .q p /.
3 3 1 1 2 2
D C (cid:0) C (cid:0)
2. Copyx intoAŒq .
3
3. RecursivelymergeTŒp ::q 1withTŒp ::q 1,andplacetheresultinto
1 1 2 2
(cid:0) (cid:0)
thesubarray AŒp ::q 1.
3 3
(cid:0)
4. RecursivelymergeTŒq 1::r withTŒq ::r ,andplacetheresultintothe
1 1 2 2
C
subarray AŒq 1::r .
3 3
C
Whenwecomputeq ,thequantityq p isthenumberofelementsinthesubarray
3 1 1
(cid:0)
TŒp ::q 1,andthequantityq p isthenumberofelementsinthesubarray
1 1 2 2
(cid:0) (cid:0)
TŒp ::q 1. Thus,theirsumisthenumberofelementsthatendupbeforex in
2 2
(cid:0)
thesubarrayAŒp ::r .
3 3
The base case occurs when n n 0, in which case we have no work
1 2
D D
to do to merge the two empty subarrays. Since we have assumed that the sub-
array TŒp ::r  is at least as long as TŒp ::r , that is, n n , we can check
1 1 2 2 1 2

for the base case by just checking whether n 0. We must also ensure that the
1
D
recursion properly handles the case when only one of the two subarrays is empty,
which,byourassumption thatn n ,mustbethesubarrayTŒp ::r .
1 2 2 2

Now, let’s put these ideas into pseudocode. We start with the binary search,
which we express serially. The procedure BINARY-SEARCH.x;T;p;r/ takes a
keyx andasubarrayTŒp::r,anditreturnsoneofthefollowing:
 IfTŒp::risempty(r < p),thenitreturns theindexp.
 Ifx TŒp,and hence less than orequal toalltheelements ofTŒp::r,then

itreturnstheindexp.
 Ifx > TŒp,thenitreturnsthelargestindexq intherangep <q r 1such
 C
thatTŒq 1 < x.
(cid:0)
Hereisthepseudocode:
BINARY-SEARCH.x;T;p;r/
1 low p
D
2 high max.p;r 1/
D C
3 whilelow <high
4 mid .low high/=2
D b C c
5 ifx TŒmid

6 high mid
D
7 elselow mid 1
D C
8 returnhigh
800 Chapter27 MultithreadedAlgorithms
The call BINARY-SEARCH.x;T;p;r/ takes ‚.lgn/ serial time in the worst case,
where n r p 1 is the size of the subarray on which it runs. (See Exer-
D (cid:0) C
cise2.3-5.) Since BINARY-SEARCH isaserialprocedure, itsworst-case workand
spanareboth‚.lgn/.
We are now prepared to write pseudocode for the multithreaded merging pro-
cedure itself. Like the MERGE procedure on page 31, the P-MERGE procedure
assumes that the two subarrays to be merged lie within the same array. Un-
like MERGE, however, P-MERGE does not assume that the two subarrays to
be merged are adjacent within the array. (That is, P-MERGE does not require
that p
2
r
1
1.) Another difference between MERGE and P-MERGE is that
D C
P-MERGE takes as an argument an output subarray A into which the merged val-
uesshouldbestored. ThecallP-MERGE.T;p 1;r 1;p 2;r 2;A;p 3/mergesthesorted
subarrays TŒp ::r  and TŒp ::r  into the subarray AŒp ::r , where r
1 1 2 2 3 3 3
D
p .r p 1/ .r p 1/ 1 p .r p / .r p / 1 and
3 1 1 2 2 3 1 1 2 2
C (cid:0) C C (cid:0) C (cid:0) D C (cid:0) C (cid:0) C
isnotprovidedasaninput.
P-MERGE.T;p 1;r 1;p 2;r 2;A;p 3/
1 n r p 1
1 1 1
D (cid:0) C
2 n r p 1
2 2 2
D (cid:0) C
3 ifn < n //ensurethatn n
1 2 1 2

4 exchange p withp
1 2
5 exchange r withr
1 2
6 exchange n withn
1 2
7 ifn ==0 //bothempty?
1
8 return
9 elseq .p r /=2
1 1 1
D b C c
10 q
2
BINARY-SEARCH.TŒq 1;T;p 2;r 2/
D
11 q p .q p / .q p /
3 3 1 1 2 2
D C (cid:0) C (cid:0)
12 AŒq  TŒq 
3 1
D
13 spawnP-MERGE.T;p 1;q
1
1;p 2;q
2
1;A;p 3/
(cid:0) (cid:0)
14 P-MERGE.T;q
1
1;r 1;q 2;r 2;A;q
3
1/
C C
15 sync
The P-MERGE procedure works as follows. Lines 1–2 compute the lengths n
1
and n of the subarrays TŒp ::r  and TŒp ::r , respectively. Lines 3–6 en-
2 1 1 2 2
force the assumption that n n . Line 7 tests for the base case, where the
1 2

subarray TŒp ::r  is empty (and hence so is TŒp ::r ), in which case wesim-
1 1 2 2
ply return. Lines 9–15 implement the divide-and-conquer strategy. Line 9 com-
putes themidpoint ofTŒp ::r ,andline10findsthepoint q inTŒp ::r such
1 1 2 2 2
that all elements in TŒp ::q 1 are less than TŒq  (which corresponds to x)
2 2 1
(cid:0)
and all the elements in TŒq ::r  are at least as large as TŒq . Line 11 com-
2 2 1
27.3 Multithreadedmergesort 801
putes the index q of the element that divides the output subarray AŒp ::r  into
3 3 3
AŒp ::q 1andAŒq 1::r ,andthenline12copiesTŒq directlyintoAŒq .
3 3 3 3 1 3
(cid:0) C
Then,werecurseusingnestedparallelism. Line13spawnsthefirstsubproblem,
whileline14callsthesecondsubprobleminparallel. Thesyncstatementinline15
ensures thatthesubproblems havecompleted before theprocedure returns. (Since
everyprocedureimplicitlyexecutesasyncbeforereturning,wecouldhaveomitted
the sync statement in line 15, but including it is good coding practice.) There
is some cleverness in the coding to ensure that when the subarray TŒp ::r  is
2 2
empty,thecodeoperates correctly. Thewayitworksisthatoneachrecursivecall,
amedianelementofTŒp ::r isplacedintotheoutputsubarray, untilTŒp ::r 
1 1 1 1
itselffinallybecomesempty,triggering thebasecase.
Analysisofmultithreadedmerging
We first derive a recurrence for the span PM .n/ of P-MERGE, where the two
subarrayscontainatotalofn n n elemen1 ts. Becausethespawninline13and
1 2
D C
thecallinline14operatelogicallyinparallel,weneedexamineonlythecostlierof
thetwocalls. Thekeyistounderstandthatintheworstcase,themaximumnumber
of elements in either of the recursive calls can be at most 3n=4, which we see as
follows. Because lines 3–6 ensure that n n , it follows that n 2n =2
2 1 2 2
 D 
.n n /=2 n=2. In the worst case, one of the two recursive calls merges
1 2
C D
n =2 elements of TŒp ::r  with all n elements of TŒp ::r , and hence the
1 1 1 2 2 2
b c
numberofelementsinvolved inthecallis
n =2 n n =2 n =2 n =2
1 2 1 2 2
b cC  C C
.n n /=2 n =2
1 2 2
D C C
n=2 n=4
 C
3n=4:
D
Adding in the ‚.lgn/ cost of the call to BINARY-SEARCH in line 10, we obtain
thefollowingrecurrence fortheworst-case span:
PM .n/ PM .3n=4/ ‚.lgn/: (27.8)
1 D 1 C
(For the base case, the span is ‚.1/, since lines 1–8 execute in constant time.)
This recurrence does not fall under any of the cases of the master theorem, but it
meetsthecondition ofExercise4.6-2. Therefore, thesolution torecurrence (27.8)
isPM .n/ ‚.lg2n/.
We1 nowaD nalyzetheworkPM 1.n/ofP-MERGE onnelements,whichturnsout
tobe‚.n/. Sinceeachofthenelements mustbecopied fromarrayT toarrayA,
wehavePM .n/ .n/. Thus,itremainsonlytoshowthatPM .n/ O.n/.
1 1
D D
Weshall firstderive arecurrence for the worst-case work. Thebinary search in
line 10 costs ‚.lgn/ in the worst case, which dominates the other work outside
802 Chapter27 MultithreadedAlgorithms
of the recursive calls. For the recursive calls, observe that although the recursive
calls in lines 13 and 14 might merge different numbers of elements, together the
tworecursivecallsmergeatmostnelements(actuallyn 1elements,sinceTŒq 
1
(cid:0)
doesnotparticipate ineitherrecursivecall). Moreover,aswesawinanalyzing the
span, a recursive call operates on at most 3n=4 elements. We therefore obtain the
recurrence
PM .n/ PM .˛n/ PM ..1 ˛/n/ O.lgn/; (27.9)
1 1 1
D C (cid:0) C
where˛ liesintherange1=4 ˛ 3=4,andwhereweunderstand thattheactual
 
valueof˛ mayvaryforeachlevelofrecursion.
We prove that recurrence (27.9) has solution PM O.n/ via the substitution
1
D
method. AssumethatPM .n/ c n c lgnforsomepositiveconstantsc andc .
1 1 2 1 2
 (cid:0)
Substituting givesus
PM .n/ .c ˛n c lg.˛n// .c .1 ˛/n c lg..1 ˛/n// ‚.lgn/
1 1 2 1 2
 (cid:0) C (cid:0) (cid:0) (cid:0) C
c .˛ .1 ˛//n c .lg.˛n/ lg..1 ˛/n// ‚.lgn/
1 2
D C (cid:0) (cid:0) C (cid:0) C
c n c .lg˛ lgn lg.1 ˛/ lgn/ ‚.lgn/
1 2
D (cid:0) C C (cid:0) C C
c n c lgn .c .lgn lg.˛.1 ˛/// ‚.lgn//
1 2 2
D (cid:0) (cid:0) C (cid:0) (cid:0)
c n c lgn;
1 2
 (cid:0)
since we can choose c large enough that c .lgn lg.˛.1 ˛/// dominates the
2 2
C (cid:0)
‚.lgn/ term. Furthermore, we can choose c large enough to satisfy the base
1
conditions of the recurrence. Since the work PM 1.n/ of P-MERGE is both .n/
andO.n/,wehavePM .n/ ‚.n/.
1
D
Theparallelism ofP-MERGE isPM 1.n/=PM .n/ ‚.n=lg2n/.
1 D
Multithreadedmergesort
Now that we have a nicely parallelized multithreaded merging procedure, we can
incorporateitintoamultithreadedmergesort. Thisversionofmergesortissimilar
totheMERGE-SORT0procedurewesawearlier,butunlikeMERGE-SORT0,ittakes
as an argument an output subarray B, which will hold the sorted result. In par-
ticular, the call P-MERGE-SORT.A;p;r;B;s/ sorts the elements in AŒp::r and
storestheminBŒs::s r p.
C (cid:0)
27.3 Multithreadedmergesort 803
P-MERGE-SORT.A;p;r;B;s/
1 n r p 1
D (cid:0) C
2 ifn == 1
3 BŒs AŒp
D
4 elseletTŒ1::nbeanewarray
5 q .p r/=2
D b C c
6 q q p 1
0
D (cid:0) C
7 spawnP-MERGE-SORT.A;p;q;T;1/
8 P-MERGE-SORT.A;q 1;r;T;q
0
1/
C C
9 sync
10 P-MERGE.T;1;q 0;q
0
1;n;B;s/
C
After line 1 computes the number n of elements in the input subarray AŒp::r,
lines 2–3 handle the base case when the array has only 1 element. Lines 4–6 set
upfortherecursive spawninline7andcallinline8,whichoperateinparallel. In
particular, line4allocates atemporaryarrayT withnelementstostoretheresults
oftherecursive merge sorting. Line5calculates the index q ofAŒp::rto divide
the elements into the two subarrays AŒp::q and AŒq 1::r that will be sorted
C
recursively, and line 6 goes on to compute the number q of elements in the first
0
subarrayAŒp::q,whichline8usestodeterminethestartingindexinT ofwhere
to store the sorted result of AŒq 1::r. At that point, the spawn and recursive
C
call are made, followed by the sync in line 9, which forces the procedure to wait
until the spawned procedure is done. Finally, line 10 calls P-MERGE to merge
thesorted subarrays, now inTŒ1::q and TŒq 1::n, into theoutput subarray
0 0
C
BŒs::s r p.
C (cid:0)
Analysisofmultithreadedmergesort
We start by analyzing the work PMS 1.n/ of P-MERGE-SORT, which is consider-
ablyeasierthananalyzingtheworkofP-MERGE. Indeed,theworkisgivenbythe
recurrence
PMS .n/ 2PMS .n=2/ PM .n/
1 1 1
D C
2PMS .n=2/ ‚.n/:
1
D C
This recurrence is the same as the recurrence (4.4) for ordinary MERGE-SORT
fromSection2.3.1andhassolution PMS .n/ ‚.nlgn/bycase2ofthemaster
1
D
theorem.
Wenowderiveandanalyzearecurrence fortheworst-case spanPMS .n/. Be-
1
causethetworecursivecallstoP-MERGE-SORT onlines7and8operatelogically
inparallel, wecanignoreoneofthem,obtaining therecurrence
804 Chapter27 MultithreadedAlgorithms
PMS .n/ PMS .n=2/ PM .n/
1 D 1 C 1
PMS .n=2/ ‚.lg2n/: (27.10)
D 1 C
Asforrecurrence (27.8), themastertheorem doesnotapplytorecurrence (27.10),
butExercise4.6-2does. Thesolution isPMS .n/ ‚.lg3n/,andsothespanof
P-MERGE-SORT is‚.lg3n/. 1 D
ParallelmerginggivesP-MERGE-SORT asignificantparallelismadvantageover
MERGE-SORT0. Recall that the parallelism of MERGE-SORT0, which calls the se-
rial MERGE procedure, isonly‚.lgn/. ForP-MERGE-SORT, theparallelism is
PMS .n/=PMS .n/ ‚.nlgn/=‚.lg3n/
1
1 D
‚.n=lg2n/;
D
which is much better both in theory and in practice. A good implementation in
practice would sacrifice some parallelism by coarsening the base case in order to
reduce the constants hidden by the asymptotic notation. The straightforward way
to coarsen the base case is to switch to an ordinary serial sort, perhaps quicksort,
whenthesizeofthearrayissufficientlysmall.
Exercises
27.3-1
Explainhowtocoarsen thebasecaseofP-MERGE.
27.3-2
Insteadoffindingamedianelementinthelargersubarray,asP-MERGEdoes,con-
sider a variant that finds a median element of all the elements in the two sorted
subarrays using the result of Exercise 9.3-8. Give pseudocode for an efficient
multithreaded merging procedure that uses this median-finding procedure. Ana-
lyzeyouralgorithm.
27.3-3
Givean efficient multithreaded algorithm for partitioning an array around a pivot,
asisdonebythePARTITION procedureonpage171. Youneednotpartitionthear-
rayinplace. Makeyouralgorithmasparallelaspossible. Analyzeyouralgorithm.
(Hint:Youmayneedanauxiliary arrayandmayneedtomakemorethanonepass
overtheinputelements.)
27.3-4
GiveamultithreadedversionofRECURSIVE-FFT onpage911. Makeyourimple-
mentation asparallelaspossible. Analyzeyouralgorithm.
ProblemsforChapter27 805
27.3-5 ?
Giveamultithreaded version of RANDOMIZED-SELECT onpage 216. Makeyour
implementation as parallel as possible. Analyze your algorithm. (Hint: Use the
partitioning algorithm fromExercise27.3-3.)
27.3-6 ?
ShowhowtomultithreadSELECT fromSection9.3. Makeyourimplementationas
parallelaspossible. Analyzeyouralgorithm.
Problems
27-1 Implementingparallel loopsusingnestedparallelism
Consider the following multithreaded algorithm for performing pairwise addition
onn-elementarraysAŒ1::nandBŒ1::n,storingthesumsinCŒ1::n:
SUM-ARRAYS.A;B;C/
1 parallelfori 1toA:length
D
2 CŒi AŒi BŒi
D C
a. Rewrite the parallel loop in SUM-ARRAYS using nested parallelism (spawn
and sync) in the manner of MAT-VEC-MAIN-LOOP. Analyze the parallelism
ofyourimplementation.
Consider the following alternative implementation of the parallel loop, which
containsavaluegrain-size tobespecified:
SUM-ARRAYS0.A;B;C/
1 n A:length
D
2 grain-size ‹ //tobedetermined
D
3 r n=grain-size
D d e
4 fork 0tor 1
D (cid:0)
5 spawnADD-SUBARRAY.A;B;C;k grain-size 1;
 C
min..k 1/ grain-size;n//
C 
6 sync
ADD-SUBARRAY.A;B;C;i;j/
1 fork i toj
D
2 CŒk AŒk BŒk
D C
806 Chapter27 MultithreadedAlgorithms
b. Supposethatwesetgrain-size 1. Whatistheparallelism ofthisimplemen-
D
tation?
c. Give a formula for the span of SUM-ARRAYS0 in terms of n and grain-size.
Derivethebestvalueforgrain-size tomaximizeparallelism.
27-2 Savingtemporary spaceinmatrixmultiplication
The P-MATRIX-MULTIPLY-RECURSIVE procedure has the disadvantage that it
must allocate a temporary matrix T of size n n, which can adversely affect the

constantshiddenbythe‚-notation. TheP-MATRIX-MULTIPLY-RECURSIVE pro-
cedure does have high parallelism, however. For example, ignoring the constants
in the ‚-notation, the parallelism for multiplying 1000 1000 matrices comes to

approximately 10003=102 107, since lg1000 10. Most parallel computers
D 
havefarfewerthan10millionprocessors.
a. Describe a recursive multithreaded algorithm that eliminates the need for the
temporary matrix T at the cost of increasing the span to ‚.n/. (Hint: Com-
pute C C AB following the general strategy of P-MATRIX-MULTIPLY-
D C
RECURSIVE, but initialize C inparallel and insertasyncinajudiciously cho-
senlocation.)
b. Giveandsolverecurrences fortheworkandspanofyourimplementation.
c. Analyze the parallelism of your implementation. Ignoring the constants in the
‚-notation, estimate the parallelism on 1000 1000 matrices. Compare with

theparallelism of P-MATRIX-MULTIPLY-RECURSIVE.
27-3 Multithreadedmatrixalgorithms
a. Parallelize the LU-DECOMPOSITION procedure on page 821 by giving pseu-
docode for a multithreaded version of this algorithm. Make your implementa-
tionasparallel aspossible, andanalyzeitswork,span,andparallelism.
b. Dothesamefor LUP-DECOMPOSITION onpage824.
c. Dothesamefor LUP-SOLVE onpage817.
d. Do the same for a multithreaded algorithm based on equation (28.13) for in-
vertingasymmetricpositive-definite matrix.
ProblemsforChapter27 807
27-4 Multithreadingreductionsandprefixcomputations
A˝-reductionofanarrayxŒ1::n,where isanassociativeoperator,isthevalue
˝
y xŒ1 xŒ2 xŒn:
D ˝ ˝˝
Thefollowingprocedurecomputesthe -reductionofasubarrayxŒi::jserially.
˝
REDUCE.x;i;j/
1 y xŒi
D
2 fork i 1toj
D C
3 y y xŒk
D ˝
4 returny
a. Use nested parallelism to implement a multithreaded algorithm P-REDUCE,
whichperforms thesamefunction with‚.n/workand‚.lgn/span. Analyze
youralgorithm.
A related problem is that of computing a ˝-prefix computation, sometimes
called a ˝-scan, on an array xŒ1::n, where is once again an associative op-
˝
erator. The -scanproduces thearrayyŒ1::ngivenby
˝
yŒ1 xŒ1;
D
yŒ2 xŒ1 xŒ2;
D ˝
yŒ3 xŒ1 xŒ2 xŒ3;
D ˝ ˝
:
:
:
yŒn xŒ1 xŒ2 xŒ3 xŒn;
D ˝ ˝ ˝˝
that is, all prefixes of the array x “summed” using the operator. The following
˝
serialprocedure SCANperformsa -prefixcomputation:
˝
SCAN.x/
1 n x:length
D
2 letyŒ1::nbeanewarray
3 yŒ1 xŒ1
D
4 fori 2ton
D
5 yŒi yŒi 1 xŒi
D (cid:0) ˝
6 returny
Unfortunately,multithreadingSCANisnotstraightforward. Forexample,changing
the for loop to a parallel for loop would create races, since each iteration of the
loop body depends on theprevious iteration. Thefollowing procedure P-SCAN-1
performsthe -prefixcomputation inparallel, albeitinefficiently:
˝
808 Chapter27 MultithreadedAlgorithms
P-SCAN-1.x/
1 n x:length
D
2 letyŒ1::nbeanewarray
3 P-SCAN-1-AUX.x;y;1;n/
4 returny
P-SCAN-1-AUX.x;y;i;j/
1 parallelforl i toj
D
2 yŒl P-REDUCE.x;1;l/
D
b. Analyzethework,span,andparallelism of P-SCAN-1.
By using nested parallelism, we can obtain a more efficient -prefix computa-
˝
tion:
P-SCAN-2.x/
1 n x:length
D
2 letyŒ1::nbeanewarray
3 P-SCAN-2-AUX.x;y;1;n/
4 returny
P-SCAN-2-AUX.x;y;i;j/
1 ifi ==j
2 yŒi xŒi
D
3 elsek .i j/=2
D b C c
4 spawnP-SCAN-2-AUX.x;y;i;k/
5 P-SCAN-2-AUX.x;y;k 1;j/
C
6 sync
7 parallelforl k 1toj
D C
8 yŒl yŒk yŒl
D ˝
c. Arguethat P-SCAN-2 iscorrect, andanalyzeitswork,span,andparallelism.
Wecanimproveonboth P-SCAN-1 and P-SCAN-2 byperformingthe -prefix
˝
computation in two distinct passes over the data. On the first pass, we gather the
terms for various contiguous subarrays of x into a temporary array t, and on the
second pass we use the terms in t to compute the final result y. The following
pseudocode implementsthisstrategy, butcertainexpressions havebeenomitted:
ProblemsforChapter27 809
P-SCAN-3.x/
1 n x:length
D
2 letyŒ1::nandtŒ1::nbenewarrays
3 yŒ1 xŒ1
D
4 ifn >1
5 P-SCAN-UP.x;t;2;n/
6 P-SCAN-DOWN.xŒ1;x;t;y;2;n/
7 returny
P-SCAN-UP.x;t;i;j/
1 ifi ==j
2 returnxŒi
3 else
4 k .i j/=2
D b C c
5 tŒk spawnP-SCAN-UP.x;t;i;k/
D
6 right P-SCAN-UP.x;t;k 1;j/
D C
7 sync
8 return //fillintheblank
P-SCAN-DOWN.;x;t;y;i;j/
1 ifi ==j
2 yŒi  xŒi
D ˝
3 else
4 k .i j/=2
D b C c
5 spawnP-SCAN-DOWN. ;x;t;y;i;k/ //fillintheblank
6 P-SCAN-DOWN. ;x;t;y;k 1;j/ //fillintheblank
C
7 sync
d. Fillinthethreemissing expressions inline8of P-SCAN-UP andlines5and6
of P-SCAN-DOWN. Argue that with expressions you supplied, P-SCAN-3 is
correct. (Hint:Provethatthevalue passedtoP-SCAN-DOWN.;x;t;y;i;j/
satisfies xŒ1 xŒ2 xŒi 1.)
D ˝ ˝˝ (cid:0)
e. Analyzethework,span,andparallelism ofP-SCAN-3.
27-5 Multithreadingasimplestencilcalculation
Computationalscienceisrepletewithalgorithmsthatrequiretheentriesofanarray
to be filled in with values that depend on the values of certain already computed
neighboring entries, along with other information that does not change over the
course of the computation. The pattern of neighboring entries does not change
duringthecomputation andiscalled astencil. Forexample, Section15.4presents
810 Chapter27 MultithreadedAlgorithms
astencilalgorithm tocomputealongestcommonsubsequence, wherethevaluein
entrycŒi;jdependsonlyonthevaluesincŒi 1;j,cŒi;j 1,andcŒi 1;j 1,
(cid:0) (cid:0) (cid:0) (cid:0)
as well as the elements x and y within the two sequences given as inputs. The
i j
inputsequences arefixed,butthealgorithm fillsinthetwo-dimensional arrayc so
thatitcomputesentrycŒi;jaftercomputingallthreeentriescŒi 1;j,cŒi;j 1,
(cid:0) (cid:0)
andcŒi 1;j 1.
(cid:0) (cid:0)
In this problem, we examine how to use nested parallelism to multithread a
simple stencil calculation on an n n array A in which, of the values in A, the

value placed into entry AŒi;j depends only on values in AŒi ;j , where i i
0 0 0

and j j (and of course, i i or j j). In other words, the value in an
0 0 0
 ¤ ¤
entry depends only on values in entries that are above it and/or to its left, along
with static information outside of the array. Furthermore, we assume throughout
thisproblemthatoncewehavefilledintheentriesuponwhichAŒi;jdepends,we
canfillinAŒi;jin‚.1/time(asintheLCS-LENGTHprocedureofSection15.4).
Wecanpartition then narrayAintofourn=2 n=2subarrays asfollows:
 
A A
A 11 12 : (27.11)
D A 21 A 22
 
Observe now that wecan fillinsubarray A recursively, since itdoes not depend
11
ontheentriesoftheotherthreesubarrays. OnceA iscomplete, wecancontinue
11
to fill in A and A recursively in parallel, because although they both depend
12 21
onA ,theydonotdependoneachother. Finally,wecanfillinA recursively.
11 22
a. Give multithreaded pseudocode that performs this simple stencil calculation
using a divide-and-conquer algorithm SIMPLE-STENCIL based on the decom-
position(27.11)andthediscussionabove. (Don’tworryaboutthedetailsofthe
base case, which depends on the specific stencil.) Give and solve recurrences
fortheworkandspanofthisalgorithm intermsofn. Whatistheparallelism?
b. Modify your solution to part (a) to divide an n n array into nine n=3 n=3
 
subarrays, again recursing with as much parallelism as possible. Analyze this
algorithm. How much more or less parallelism does this algorithm have com-
paredwiththealgorithm frompart(a)?
c. Generalize your solutions to parts (a) and (b) as follows. Choose an integer
b 2. Divideann narrayintob2subarrays,eachofsizen=b n=b,recursing
  
with as much parallelism as possible. In terms of n and b, what are the work,
span, and parallelism of your algorithm? Argue that, using this approach, the
parallelismmustbeo.n/foranychoiceofb 2. (Hint:Forthislastargument,

show that the exponent of n in the parallelism is strictly less than 1 for any
choiceofb 2.)

NotesforChapter27 811
d. Give pseudocode for a multithreaded algorithm for this simple stencil calcu-
lation that achieves ‚.n=lgn/ parallelism. Argue using notions of work and
span that the problem, in fact, has ‚.n/ inherent parallelism. As it turns out,
thedivide-and-conquer natureofourmultithreaded pseudocode doesnotletus
achievethismaximalparallelism.
27-6 Randomizedmultithreadedalgorithms
Justaswithordinary serialalgorithms, wesometimes wanttoimplement random-
ized multithreaded algorithms. This problem explores how to adapt the various
performancemeasuresinordertohandletheexpectedbehaviorofsuchalgorithms.
It also asks you to design and analyze a multithreaded algorithm for randomized
quicksort.
a. Explainhowtomodifytheworklaw(27.2),spanlaw(27.3),andgreedysched-
uler bound (27.4) to work with expectations when T , T , and T are all ran-
P 1
1
domvariables.
b. Consider a randomized multithreaded algorithm for which 1% of the time we
have T 104 and T 1, but for 99% of the time we have T
1 10;000 1
D D D
T 109. Argue that the speedup of a randomized multithreaded algo-
10;000
D
rithmshouldbedefinedasEŒT =EŒT ,ratherthanEŒT =T .
1 P 1 P
c. Argue thattheparallelism ofarandomized multithreaded algorithm should be
definedastheratioEŒT =EŒT .
1
1
d. Multithread the RANDOMIZED-QUICKSORT algorithm on page 179 by using
nested parallelism. (Do not parallelize RANDOMIZED-PARTITION.) Give the
pseudocode foryour P-RANDOMIZED-QUICKSORT algorithm.
e. Analyze your multithreaded algorithm for randomized quicksort. (Hint: Re-
viewtheanalysis ofRANDOMIZED-SELECT onpage216.)
Chapter notes
Parallelcomputers,modelsforparallelcomputers,andalgorithmicmodelsforpar-
allel programming have been around in various forms for years. Prior editions of
thisbookincludedmaterialonsortingnetworksandthePRAM(ParallelRandom-
Access Machine) model. The data-parallel model [48, 168] is another popular al-
gorithmic programming model,whichfeatures operations onvectors andmatrices
asprimitives.
812 Chapter27 MultithreadedAlgorithms
Graham [149] and Brent [55] showed that there exist schedulers achieving the
bound of Theorem 27.1. Eager, Zahorjan, and Lazowska [98] showed that any
greedyschedulerachievesthisboundandproposedthemethodologyofusingwork
and span (although not by those names) to analyze parallel algorithms. Blelloch
[47]developedanalgorithmicprogrammingmodelbasedonworkandspan(which
hecalledthe“depth”ofthecomputation)fordata-parallel programming. Blumofe
and Leiserson [52] gave a distributed scheduling algorithm for dynamic multi-
threading based on randomized “work-stealing” and showed that it achieves the
bound EŒT  T =P O.T /. Arora, Blumofe,andPlaxton [19]andBlelloch,
P 1
 C 1
Gibbons, and Matias [49] also provided provably good algorithms for scheduling
dynamicmultithreaded computations.
Themultithreadedpseudocodeandprogrammingmodelwereheavilyinfluenced
by the Cilk [51, 118] project at MIT and the Cilk++ [71] extensions to C++ dis-
tributed by Cilk Arts, Inc. Many of the multithreaded algorithms in this chapter
appeared inunpublished lecture notes byC.E.Leiserson and H.Prokop and have
been implemented in Cilk or Cilk++. The multithreaded merge-sorting algorithm
wasinspired byanalgorithm ofAkl[12].
Thenotionofsequential consistency isduetoLamport[223].
28 Matrix Operations
Becauseoperationsonmatriceslieattheheartofscientificcomputing,efficiental-
gorithmsforworkingwithmatriceshavemanypracticalapplications. Thischapter
focuses on how to multiply matrices and solve sets of simultaneous linear equa-
tions. Appendix Dreviewsthebasicsofmatrices.
Section28.1showshowtosolveasetoflinearequationsusingLUPdecomposi-
tions. Then, Section 28.2 explores the close relationship between multiplying and
invertingmatrices. Finally,Section28.3discussestheimportantclassofsymmetric
positive-definite matrices and shows how we can use them to find a least-squares
solutiontoanoverdetermined setoflinearequations.
One important issue that arises in practice is numerical stability. Due to the
limited precision of floating-point representations in actual computers, round-off
errorsinnumericalcomputationsmaybecomeamplifiedoverthecourseofacom-
putation, leading to incorrect results; we call such computations numerically un-
stable. Although weshall briefly consider numerical stability on occasion, we do
not focus on it in this chapter. We refer you to the excellent book by Golub and
VanLoan[144]forathorough discussion ofstability issues.
28.1 Solvingsystems oflinearequations
Numerous applications need to solve sets of simultaneous linear equations. We
canformulate alinear system as amatrixequation inwhich each matrix or vector
R
elementbelongstoafield,typicallytherealnumbers . Thissectiondiscusseshow
tosolveasystemoflinearequations usingamethodcalledLUPdecomposition.
Westartwithasetoflinearequations innunknownsx ;x ;:::;x :
1 2 n
814 Chapter28 MatrixOperations
a x a x a x b ;
11 1 12 2 1n n 1
C C  C D
a x a x a x b ;
21 1 22 2 2n n 2
C C  C D
: (28.1)
:
:
a x a x a x b :
n1 1 n2 2 nn n n
C C  C D
A solution to the equations (28.1) is a set of values for x ;x ;:::;x that satisfy
1 2 n
alloftheequations simultaneously. Inthissection, wetreatonlythecaseinwhich
thereareexactlynequations innunknowns.
Wecanconveniently rewriteequations (28.1)asthematrix-vector equation
a a a x b
11 12 1n 1 1

a a a x b
21 22 2n 2 2
:
: :
:
: :
 :: : :
: :
:
: : D
:
: :
˙ a a a ˙ x  ˙ b 
n1 n2 nn n n

or,equivalently, lettingA .a /,x .x /,andb .b /,as
ij i i
D D D
Ax b : (28.2)
D
IfAisnonsingular, itpossesses aninverse A 1,and
(cid:0)
x A 1b (28.3)
(cid:0)
D
isthesolutionvector. Wecanprovethatx istheuniquesolutiontoequation(28.2)
asfollows. Iftherearetwosolutions, x andx ,thenAx Ax b and, lettingI
0 0
D D
denoteanidentitymatrix,
x Ix
D
.A 1A/x
(cid:0)
D
A 1.Ax/
(cid:0)
D
A 1.Ax /
(cid:0) 0
D
.A 1A/x
(cid:0) 0
D
x :
0
D
In this section, we shall be concerned predominantly with the case in which A
is nonsingular or, equivalently (by Theorem D.1), the rank of A is equal to the
numbernofunknowns. Thereareotherpossibilities, however,whichmeritabrief
discussion. Ifthenumberofequationsislessthanthenumbernofunknowns—or,
more generally, if the rank of A is less than n—then the system is underdeter-
mined. An underdetermined system typically has infinitely many solutions, al-
though it may have no solutions at all if the equations are inconsistent. If the
number ofequations exceeds the number nofunknowns, the system isoverdeter-
mined,andtheremaynotexistanysolutions. Section28.3addressestheimportant
28.1 Solvingsystemsoflinearequations 815
problemoffindinggoodapproximatesolutionstooverdeterminedsystemsoflinear
equations.
Letus return to our problem ofsolving the system Ax b ofn equations in n
D
unknowns. We could compute A 1 and then, using equation (28.3), multiply b
(cid:0)
by A 1, yielding x A 1b. This approach suffers in practice from numerical
(cid:0) (cid:0)
D
instability. Fortunately, another approach—LUP decomposition—is numerically
stableandhasthefurtheradvantage ofbeingfasterinpractice.
OverviewofLUPdecomposition
The idea behind LUP decomposition is to find three n n matrices L, U, and P

suchthat
PA LU ; (28.4)
D
where
 Lisaunitlower-triangular matrix,
 U isanupper-triangular matrix,and
 P isapermutation matrix.
We call matrices L, U, and P satisfying equation (28.4) an LUP decomposition
of the matrix A. We shall show that every nonsingular matrix A possesses such a
decomposition.
Computing an LUP decomposition for the matrix A has the advantage that we
can more easily solve linear systems when they are triangular, as is the case for
both matrices L and U. Once we have found an LUP decomposition for A, we
can solve equation (28.2), Ax b, by solving only triangular linear systems, as
D
follows. Multiplying both sides of Ax b by P yields the equivalent equation
D
PAx Pb,which,byExerciseD.1-4,amountstopermutingtheequations(28.1).
D
Usingourdecomposition (28.4),weobtain
LUx Pb :
D
We can now solve this equation by solving two triangular linear systems. Let us
define y Ux, where x is the desired solution vector. First, we solve the lower-
D
triangular system
Ly Pb (28.5)
D
fortheunknownvectorybyamethodcalled“forwardsubstitution.” Havingsolved
fory,wethensolvetheupper-triangular system
Ux y (28.6)
D
816 Chapter28 MatrixOperations
for the unknown x by a method called “back substitution.” Because the permu-
tation matrix P is invertible (Exercise D.2-3), multiplying both sides of equa-
tion(28.4)byP 1 givesP 1PA P 1LU,sothat
(cid:0) (cid:0) (cid:0)
D
A P 1LU : (28.7)
(cid:0)
D
Hence,thevectorx isoursolutiontoAx b:
D
Ax P 1LUx (byequation (28.7))
(cid:0)
D
P 1Ly (byequation (28.6))
(cid:0)
D
P 1Pb (byequation (28.5))
(cid:0)
D
b :
D
Ournextstepistoshowhowforwardandbacksubstitutionworkandthenattack
theproblem ofcomputing theLUPdecomposition itself.
Forwardandbacksubstitution
Forward substitution can solve the lower-triangular system (28.5) in ‚.n2/ time,
given L, P, and b. For convenience, we represent the permutation P compactly
by an array Œ1::n. Fori 1;2;:::;n, the entry Œiindicates that P 1
i;Œi
D D
and P 0 for j Œi. Thus, PA has a in row i and column j, and Pb
ij Œi;j
D ¤
has b as its ithelement. Since L isunit lower-triangular, we can rewrite equa-
Œi
tion(28.5)as
y b ;
1 Œ1
D
l y y b ;
21 1 2 Œ2
C D
l y l y y b ;
31 1 32 2 3 Œ3
C C D
:
:
:
l y l y l y y b :
n1 1 n2 2 n3 3 n Œn
C C C  C D
The first equation tells us that y b . Knowing the value of y , we can
1 Œ1 1
D
substitute itintothesecondequation, yielding
y b l y :
2 Œ2 21 1
D (cid:0)
Now,wecansubstitute bothy andy intothethirdequation, obtaining
1 2
y b .l y l y /:
3 Œ3 31 1 32 2
D (cid:0) C
In general, we substitute y ;y ;:::;y “forward” into the ith equation to solve
1 2 i 1
fory : (cid:0)
i
28.1 Solvingsystemsoflinearequations 817
i 1
(cid:0)
y b l y :
i Œi ij j
D (cid:0)
j 1
XD
Having solved fory, wesolve forx inequation (28.6) using back substitution,
which issimilar toforward substitution. Here, wesolve the nth equation firstand
work backward to the first equation. Like forward substitution, this process runs
in‚.n2/time. SinceU isupper-triangular, wecanrewritethesystem(28.6)as
u x u x u x u x u x y ;
11 1 12 2 1;n 2 n 2 1;n 1 n 1 1n n 1
C C C (cid:0) (cid:0) C (cid:0) (cid:0) C D
u x u x u x u x y ;
22 2 2;n 2 n 2 2;n 1 n 1 2n n 2
C C (cid:0) (cid:0) C (cid:0) (cid:0) C D
:
:
:
u x u x u x y ;
n 2;n 2 n 2 n 2;n 1 n 1 n 2;n n n 2
(cid:0) (cid:0) (cid:0) C (cid:0) (cid:0) (cid:0) C (cid:0) D (cid:0)
u x u x y ;
n 1;n 1 n 1 n 1;n n n 1
(cid:0) (cid:0) (cid:0) C (cid:0) D (cid:0)
u x y :
n;n n n
D
Thus,wecansolveforx ;x ;:::;x successively asfollows:
n n 1 1
(cid:0)
x y =u ;
n n n;n
D
x .y u x /=u ;
n 1 n 1 n 1;n n n 1;n 1
(cid:0) D (cid:0) (cid:0) (cid:0) (cid:0) (cid:0)
x .y .u x u x //=u ;
n 2 n 2 n 2;n 1 n 1 n 2;n n n 2;n 2
(cid:0) D (cid:0) (cid:0) (cid:0) (cid:0) (cid:0) C (cid:0) (cid:0) (cid:0)
:
:
:
or,ingeneral,
n
x y u x =u :
i i ij j ii
D (cid:0)
!
j i 1
DXC
Given P, L, U, and b, the procedure LUP-SOLVE solves for x by combining
forward and back substitution. Thepseudocode assumes that the dimension n ap-
pears in the attribute L:rows and that the permutation matrix P is represented by
thearray.
LUP-SOLVE.L;U;;b/
1 n L:rows
D
2 letx andy benewvectorsoflengthn
3 fori 1ton
D
4 y
i
D
b
Œi
(cid:0)
ji (cid:0)1 1l ijy
j
5 fori ndownto1 D
6
xD
y
Pn
u x =u
i D i (cid:0) j i 1 ij j ii
7 returnx D C
(cid:0) P 
818 Chapter28 MatrixOperations
Procedure LUP-SOLVE solves for y using forward substitution in lines 3–4, and
thenitsolvesforx usingbackwardsubstitution inlines5–6. Sincethesummation
withineachoftheforloopsincludes animplicitloop,therunning timeis‚.n2/.
Asanexampleofthesemethods,considerthesystemoflinearequationsdefined
by
1 2 0 3
3 4 4 x 7 ;
D
5 6 3 8
(cid:0)where  (cid:0) 
1 2 0
A 3 4 4 ;
D
5 6 3
(cid:0) 3 
b 7 ;
D
8
andwew(cid:0)ishtosolvefortheunknownx. TheLUPdecomposition is
1 0 0
L 0:2 1 0 ;
D
0:6 0:5 1
(cid:0) 5 6 3
U 0 0:8 0:6 ;
D (cid:0)
0 0 2:5
(cid:0) 0 0 1 
P 1 0 0 :
D
0 1 0
(You mig(cid:0)ht want to verify that PA LU.) Using forward substitution, we solve
D
Ly Pb fory:
D
1 0 0 y 8
1
0:2 1 0 y 3 ;
2
D
0:6 0:5 1 y 7
3
(cid:0)obtaining (cid:0)  (cid:0) 
8
y 1:4
D
1:5
by co(cid:0)mputing first y , then y , and finally y . Using back substitution, we solve
1 2 3
Ux y forx:
D
28.1 Solvingsystemsoflinearequations 819
5 6 3 x 8
1
0 0:8 0:6 x 1:4 ;
2
(cid:0) D
0 0 2:5 x 1:5
3
(cid:0) (cid:0)  (cid:0) 
therebyobtaining thedesiredanswer
1:4
(cid:0)
x 2:2
D
0:6
(cid:0) 
bycomputing firstx ,thenx ,andfinallyx .
3 2 1
ComputinganLUdecomposition
WehavenowshownthatifwecancreateanLUPdecomposition foranonsingular
matrix A, then forward and back substitution can solve the system Ax b of
D
linearequations. NowweshowhowtoefficientlycomputeanLUPdecomposition
for A. We start withthe case in which A is an n n nonsingular matrix and P is

absent (or, equivalently, P I ). In this case, we factor A LU. We call the
n
D D
twomatricesLandU anLUdecomposition ofA.
We use a process known as Gaussian elimination to create an LU decomposi-
tion. Westartbysubtractingmultiplesofthefirstequationfromtheotherequations
inorder toremove thefirstvariable from those equations. Then, wesubtract mul-
tiples ofthe second equation from the third and subsequent equations so that now
the first and second variables are removed from them. We continue this process
until the system that remains has an upper-triangular form—in fact, it is the ma-
trix U. The matrix L is made up of the row multipliers that cause variables to be
eliminated.
Our algorithm to implement this strategy is recursive. We wish to construct an
LUdecomposition forann nnonsingular matrixA. Ifn 1,thenwearedone,
 D
sincewecanchooseL I andU A. Forn > 1,webreakAintofourparts:
1
D D
a a a
11 12 1n

a a a
21 22 2n
A
D
:
: :
:
: :
 :: : :
: :
˙ a a a 
n1 n2 nn

a wT
11 ;
D  A
0
 
where  . ; ;:::; / .a ;a ;:::;a / is a column .n 1/-vector,
2 3 n 21 22 n1
D D (cid:0)
wT .w ;w ;:::;w /T .a ;a ;:::;a /T is arow .n 1/-vector, and A is
2 3 n 12 13 1n 0
D D (cid:0)
an .n 1/ .n 1/ matrix. Then, using matrix algebra (verify the equations by
(cid:0)  (cid:0)
820 Chapter28 MatrixOperations
simplymultiplying through), wecanfactorAas
a wT
A 11
D  A
0
 
1 0 a wT
11 : (28.8)
D =a 11 I n 1 0 A 0 wT=a 11
 (cid:0)  (cid:0) 
The 0s in the first and second matrices of equation (28.8) are row and col-
umn .n 1/-vectors, respectively. The term wT=a , formed by taking the
11
(cid:0)
outer product of  and w and dividing each element of the result by a , is an
11
.n 1/ .n 1/matrix,whichconformsinsizetothematrixA fromwhichitis
0
(cid:0)  (cid:0)
subtracted. Theresulting .n 1/ .n 1/matrix
(cid:0)  (cid:0)
A wT=a (28.9)
0 11
(cid:0)
iscalledtheSchurcomplementofAwithrespecttoa .
11
We claim that if A is nonsingular, then the Schur complement is nonsingular,
too. Why? Suppose that the Schur complement, which is .n 1/ .n 1/, is
(cid:0)  (cid:0)
singular. Then by Theorem D.1, it has row rank strictly less than n 1. Because
(cid:0)
thebottomn 1entriesinthefirstcolumnofthematrix
(cid:0)
a wT
11
0 A wT=a
0 11
 (cid:0) 
are all 0, the bottom n 1 rows of this matrix must have row rank strictly less
(cid:0)
than n 1. The row rank of the entire matrix, therefore, is strictly less than n.
(cid:0)
Applying Exercise D.2-8 to equation (28.8), A has rank strictly less than n, and
fromTheoremD.1wederivethecontradiction thatAissingular.
Because the Schur complement is nonsingular, we can now recursively find an
LUdecomposition forit. Letussaythat
A wT=a LU ;
0 11 0 0
(cid:0) D
where L is unit lower-triangular and U is upper-triangular. Then, using matrix
0 0
algebra, wehave
1 0 a wT
A 11
D =a 11 I n 1 0 A 0 wT=a 11
 (cid:0)  (cid:0) 
1 0 a wT
11
D =a 11 I n 1 0 L 0U 0
 (cid:0)  
1 0 a wT
11
D =a 11 L 0 0 U 0
  
LU ;
D
thereby providing our LU decomposition. (Note that because L is unit lower-
0
triangular, soisL,andbecauseU isupper-triangular, soisU.)
0
28.1 Solvingsystemsoflinearequations 821
Ofcourse, ifa 0,thismethoddoesn’t work,becauseitdivides by0. Italso
11
D
doesn’t work if the upper leftmost entry of the Schur complement A wT=a
0 11
(cid:0)
is 0, since we divide by it in the next step of the recursion. The elements by
which we divide during LU decomposition are called pivots, and they occupy the
diagonalelementsofthematrixU. ThereasonweincludeapermutationmatrixP
duringLUPdecompositionisthatitallowsustoavoiddividingby0. Whenweuse
permutations toavoiddivisionby0(orbysmallnumbers, whichwouldcontribute
tonumericalinstability), wearepivoting.
Animportant class ofmatrices for which LUdecomposition always works cor-
rectly is the class of symmetric positive-definite matrices. Such matrices require
no pivoting, and thus we can employ the recursive strategy outlined above with-
out fear of dividing by 0. We shall prove this result, as well as several others, in
Section28.3.
OurcodeforLUdecompositionofamatrixAfollowstherecursivestrategy,ex-
ceptthataniterationloopreplacestherecursion. (Thistransformationisastandard
optimization fora“tail-recursive” procedure—one whoselastoperation isarecur-
sive call to itself. See Problem 7-4.) It assumes that the attribute A:rows gives
the dimension of A. We initialize the matrix U with 0s below the diagonal and
matrix L with 1s on its diagonal and 0s above the diagonal. Each iteration works
onasquaresubmatrix,usingitsupperleftmostelementasthepivottocomputethe
 and w vectors and the Schur complement, which becomes the square submatrix
workedonbythenextiteration.
LU-DECOMPOSITION.A/
1 n A:rows
D
2 letLandU benewn nmatrices

3 initializeU with0sbelowthediagonal
4 initializeLwith1sonthediagonaland0sabovethediagonal
5 fork 1ton
D
6 u a
kk kk
D
7 fori k 1ton
D C
8 l a =a //a holds
ik ik kk ik i
D
9 u a //a holdsw
ki ki ki i
D
10 fori k 1ton
D C
11 forj k 1ton
D C
12 a a l u
ij ij ik kj
D (cid:0)
13 returnLandU
Theouterforloopbeginninginline5iteratesonceforeachrecursivestep. Within
this loop, line 6 determines the pivot to be u a . The for loop in lines 7–9
kk kk
D
(whichdoesnotexecutewhenk n)usesthe andwvectorstoupdateLandU.
D
822 Chapter28 MatrixOperations
2 3 1 5 2 3 1 5 2 3 1 5 2 3 1 5
6 13 5 19 3 4 2 4 3 4 2 4 3 4 2 4
2 19 10 23 1 16 9 18 1 4 1 2 1 4 1 2
4 10 11 31 2 4 9 21 2 1 7 17 2 1 7 3
(a) (b) (c) (d)
2 3 1 5 1 0 0 0 2 3 1 5
6 13 5 19 3 1 0 0 0 4 2 4
2 19 10 23 D 1 4 1 0 0 0 1 2
4 10 11 31 2 1 7 1 0 0 0 3
 ˘  ˘  ˘
A L U
(e)
Figure28.1 TheoperationofLU-DECOMPOSITION.(a)ThematrixA.(b)Theelementa11 2
D
intheblackcircleisthepivot,theshadedcolumnis=a11,andtheshadedrowiswT.Theelements
of U computed thusfar areabovethehorizontal line, andtheelementsof Laretotheleftof the
verticalline. TheSchurcomplement matrixA 0(cid:0)wT=a11 occupiesthelowerright. (c)Wenow
operateontheSchurcomplementmatrixproducedfrompart(b). Theelementa22 4intheblack
D
circleisthepivot,andtheshadedcolumnandroware=a22andwT(inthepartitioningoftheSchur
complement),respectively. LinesdividethematrixintotheelementsofU computedsofar(above),
theelementsofLcomputedsofar(left),andthenewSchurcomplement(lowerright).(d)Afterthe
nextstep,thematrixAisfactored.(Theelement3inthenewSchurcomplementbecomespartofU
whentherecursionterminates.)(e)ThefactorizationA LU.
D
Line 8 determines the below-diagonal elements of L, storing  =a in l , and
i kk ik
line 9 computes the above-diagonal elements of U, storing w in u . Finally,
i ki
lines10–12 compute the elements of the Schur complement and store them back
into the matrix A. (We don’t need to divide by a in line 12 because we already
kk
did so when we computed l in line 8.) Because line 12 is triply nested, LU-
ik
DECOMPOSITION runsintime‚.n3/.
Figure 28.1 illustrates the operation of LU-DECOMPOSITION. It shows a stan-
dardoptimization oftheprocedureinwhichwestorethesignificantelementsofL
and U in place in the matrix A. That is, wecan set up a correspondence between
each element a and either l (if i > j) or u (if i j) and update the ma-
ij ij ij

trix A so that it holds both L and U when the procedure terminates. To obtain
thepseudocode forthisoptimizationfromtheabovepseudocode, justreplaceeach
reference to l or u by a; you can easily verify that this transformation preserves
correctness.
28.1 Solvingsystemsoflinearequations 823
ComputinganLUPdecomposition
Generally, in solving a system of linear equations Ax b, we must pivot on off-
D
diagonal elements of A to avoid dividing by 0. Dividing by 0 would, of course,
be disastrous. But we also want to avoid dividing by a small value—even if A is
nonsingular—because numerical instabilities can result. We therefore try to pivot
onalargevalue.
The mathematics behind LUP decomposition is similar to that of LU decom-
position. Recall that we are given an n n nonsingular matrix A, and we wish

to find a permutation matrix P, a unit lower-triangular matrix L, and an upper-
triangular matrixU suchthatPA LU. Beforewepartition thematrixA,aswe
D
didforLUdecomposition, wemoveanonzero element, saya ,from somewhere
k1
in the first column to the .1;1/ position of the matrix. For numerical stability, we
choosea astheelementinthefirstcolumnwiththegreatestabsolutevalue. (The
k1
first column cannot contain only 0s, for then A would be singular, because its de-
terminant would be 0, by Theorems D.4 and D.5.) In order to preserve the set of
equations, weexchangerow1withrowk,whichisequivalenttomultiplyingAby
apermutation matrixQontheleft(ExerciseD.1-4). Thus,wecanwriteQAas
a wT
QA k1 ;
D  A
0
 
where  .a ;a ;:::;a /, except that a replaces a ; wT .a ;a ;
21 31 n1 11 k1 k2 k3
D D
:::;a /T; and A is an .n 1/ .n 1/ matrix. Since a 0, we can now
kn 0 k1
(cid:0)  (cid:0) ¤
perform much the same linear algebra as for LU decomposition, but now guaran-
teeingthatwedonotdivideby0:
a wT
QA k1
D  A 0
 
1 0 a wT
k1 :
D =a k1 I n 1 0 A 0 wT=a k1
 (cid:0)  (cid:0) 
As we saw for LU decomposition, if A is nonsingular, then the Schur comple-
ment A wT=a is nonsingular, too. Therefore, we can recursively find an
0 k1
(cid:0)
LUP decomposition for it, with unit lower-triangular matrix L, upper-triangular
0
matrixU ,andpermutation matrixP ,suchthat
0 0
P .A wT=a / LU :
0 0 k1 0 0
(cid:0) D
Define
1 0
P Q;
D 0 P
0
 
which isapermutation matrix, since it isthe product oftwo permutation matrices
(ExerciseD.1-4). Wenowhave
824 Chapter28 MatrixOperations
1 0
PA QA
D 0 P
0
 
1 0 1 0 a wT
k1
D 0 P 0 =a k1 I n 1 0 A 0 wT=a k1
  (cid:0)  (cid:0) 
1 0 a wT
k1
D P 0=a k1 P 0 0 A 0 wT=a k1
  (cid:0) 
1 0 a wT
k1
D P 0=a k1 I n 1 0 P 0.A 0 wT=a k1/
 (cid:0)  (cid:0) 
1 0 a wT
k1
D P 0=a k1 I n 1 0 L 0U 0
 (cid:0)  
1 0 a wT
k1
D P 0=a k1 L 0 0 U 0
  
LU ;
D
yielding theLUPdecomposition. Because L isunitlower-triangular, soisL,and
0
because U isupper-triangular, soisU.
0
Notice that in this derivation, unlike the one for LU decomposition, we must
multiply both the column vector =a and the Schur complement A wT=a
k1 0 k1
(cid:0)
bythepermutation matrixP . Hereisthepseudocode forLUPdecomposition:
0
LUP-DECOMPOSITION.A/
1 n A:rows
D
2 letŒ1::nbeanewarray
3 fori 1ton
D
4 Œi i
D
5 fork 1ton
D
6 p 0
D
7 fori k ton
D
8 if a > p
ik
j j
9 p a
ik
D j j
10 k i
0
D
11 ifp == 0
12 error“singular matrix”
13 exchange ŒkwithŒk 
0
14 fori 1ton
D
15 exchange a ki witha k0i
16 fori k 1ton
D C
17 a a =a
ik ik kk
D
18 forj k 1ton
D C
19 a a a a
ij ij ik kj
D (cid:0)
28.1 Solvingsystemsoflinearequations 825
Like LU-DECOMPOSITION, our LUP-DECOMPOSITION procedure replaces
the recursion with an iteration loop. As an improvement over a direct implemen-
tation of the recursion, we dynamically maintain the permutation matrix P as an
array , where Œi j means that the ith row of P contains a 1 in column j.
D
WealsoimplementthecodetocomputeLandU “inplace”inthematrixA. Thus,
whentheprocedure terminates,
l ifi > j ;
a ij
ij
D
(
u
ij
ifi j :

Figure28.2illustrates how LUP-DECOMPOSITION factors amatrix. Lines3–4
initialize the array  to represent the identity permutation. The outer for loop
beginning in line 5 implements the recursion. Each time through the outer loop,
lines 6–10 determine the element a k0k with largest absolute value of those in the
current first column (column k) of the .n k 1/ .n k 1/ matrix whose
(cid:0) C  (cid:0) C
LUPdecomposition we are finding. If all elements in the current first column are
zero, lines 11–12 report that the matrix is singular. To pivot, we exchange Œk 
0
with Œk in line 13 and exchange the kth and k th rows of A in lines 14–15,
0
thereby making the pivot element a . (The entire rows are swapped because in
kk
thederivationofthemethodabove,notonlyisA wT=a multipliedbyP ,but
0 k1 0
(cid:0)
so is =a .) Finally, the Schur complement is computed by lines 16–19 in much
k1
thesamewayasitiscomputedbylines7–12ofLU-DECOMPOSITION, exceptthat
heretheoperation iswrittentoworkinplace.
Because of its triply nested loop structure, LUP-DECOMPOSITION has a run-
ning time of ‚.n3/, which is the same as that of LU-DECOMPOSITION. Thus,
pivotingcostsusatmostaconstantfactorintime.
Exercises
28.1-1
Solvetheequation
1 0 0 x 3
1
4 1 0 x 14
2
D
6 5 1 x 7
3
(cid:0) (cid:0)
(cid:0)byusingforwar(cid:0)dsubstituti(cid:0)on. 
28.1-2
FindanLUdecomposition ofthematrix
4 5 6
(cid:0)
8 6 7 :
(cid:0)
12 7 12
(cid:0)
(cid:0) 
826 Chapter28 MatrixOperations
1 2 0 2 0.6 3 5 5 4 2 3 5 5 4 2
2 3 3 4 –2 2 3 3 4 –2 2 0.6 0 1.6 –3.2
3 5 5 4 2 1 2 0 2 0.6 1 0.4 –2 0.4 –.2
4 –1 –2 3.4 –1 4 –1 –2 3.4 –1 4 –0.2 –1 4.2 –0.6
(a) (b) (c)
3 5 5 4 2 3 5 5 4 2 3 5 5 4 2
2 0.6 0 1.6 –3.2 1 0.4 –2 0.4 –0.2 1 0.4 –2 0.4 –0.2
1 0.4 –2 0.4 –0.2 2 0.6 0 1.6 –3.2 2 0.6 0 1.6 –3.2
4 –0.2 –1 4.2 –0.6 4 –0.2 –1 4.2 –0.6 4 –0.2 0.5 4 –0.5
(d) (e) (f)
3 5 5 4 2 3 5 5 4 2 3 5 5 4 2
1 0.4 –2 0.4 –0.2 1 0.4 –2 0.4 –0.2 1 0.4 –2 0.4 –0.2
2 0.6 0 1.6 –3.2 4 –0.2 0.5 4 –0.5 4 –0.2 0.5 4 –0.5
4 –0.2 0.5 4 –0.5 2 0.6 0 1.6 –3.2 2 0.6 0 0.4 –3
(g) (h) (i)
0 0 1 0 2 0 2 0:6 1 0 0 0 5 5 4 2
1 0 0 0 3 3 4 2 0:4 1 0 0 0 2 0:4 0:2
(cid:0) (cid:0) (cid:0)
0 0 0 1 5 5 4 2 D 0:2 0:5 1 0 0 0 4 0:5
(cid:0) (cid:0)
0 1 0 0 1 2 3:4 1 0:6 0 0:4 1 0 0 0 3
 ˘  ˘  ˘  ˘
(cid:0) (cid:0) (cid:0) (cid:0)
P A L U
(j)
Figure28.2 Theoperationof LUP-DECOMPOSITION. (a)Theinput matrixAwiththeidentity
permutation of therows on theleft. Thefirststep of the algorithmdetermines that theelement 5
intheblackcircleinthethirdrowisthepivotforthefirstcolumn. (b)Rows1and3areswapped
andthepermutationisupdated. Theshadedcolumnandrowrepresent andwT. (c)Thevector
isreplacedby=5,andthelowerrightofthematrixisupdatedwiththeSchurcomplement. Lines
dividethematrixintothreeregions:elementsofU (above),elementsofL(left),andelementsofthe
Schurcomplement(lowerright).(d)–(f)Thesecondstep.(g)–(i)Thethirdstep.Nofurtherchanges
occuronthefourth(final)step.(j)TheLUPdecompositionPA LU.
D
28.2 Invertingmatrices 827
28.1-3
Solvetheequation
1 5 4 x 12
1
2 0 3 x 9
2
D
5 8 2 x 5
3
(cid:0) (cid:0)  (cid:0) 
byusinganLUPdecomposition.
28.1-4
DescribetheLUPdecomposition ofadiagonal matrix.
28.1-5
Describe the LUP decomposition of a permutation matrix A, and prove that it is
unique.
28.1-6
Showthatforalln 1,thereexistsasingularn nmatrixthathasanLUdecom-
 
position.
28.1-7
In LU-DECOMPOSITION, is it necessary to perform the outermost for loop itera-
tionwhenk n? Howaboutin LUP-DECOMPOSITION?
D
28.2 Inverting matrices
Although in practice we do not generally use matrix inverses to solve systems of
linearequations,preferringinsteadtousemorenumericallystabletechniquessuch
as LUP decomposition, sometimes we need to compute a matrix inverse. In this
section, we show how to use LUP decomposition to compute a matrix inverse.
We also prove that matrix multiplication and computing the inverse of a matrix
are equivalently hard problems, in that (subject to technical conditions) we can
use an algorithm for one to solve the other in the same asymptotic running time.
Thus, we can use Strassen’s algorithm (see Section 4.2) for matrix multiplication
toinvertamatrix. Indeed, Strassen’s original paperwasmotivated bytheproblem
of showing that a set of a linear equations could be solved more quickly than by
theusualmethod.
828 Chapter28 MatrixOperations
ComputingamatrixinversefromanLUPdecomposition
Suppose that we have an LUP decomposition of a matrix A in the form of three
matrices L, U, and P such that PA LU. Using LUP-SOLVE, we can solve
D
an equation of the form Ax b in time ‚.n2/. Since the LUP decomposition
D
depends on A but not b, we can run LUP-SOLVE on a second set of equations of
the form Ax b in additional time ‚.n2/. In general, once we have the LUP
0
D
decomposition of A, we can solve, in time ‚.kn2/, k versions of the equation
Ax b thatdifferonlyinb.
D
Wecanthinkoftheequation
AX I ; (28.10)
n
D
whichdefinesthematrixX,theinverseofA,asasetofndistinctequations ofthe
formAx b. Tobeprecise,letX denotetheithcolumnofX,andrecallthatthe
i
D
unitvectore istheithcolumnofI . Wecanthensolveequation(28.10)forX by
i n
usingtheLUPdecomposition forAtosolveeachequation
AX e
i i
D
separately forX . OncewehavetheLUPdecomposition, wecancomputeeachof
i
thencolumnsX intime‚.n2/,andsowecancomputeX fromtheLUPdecom-
i
positionofAintime‚.n3/. SincewecandeterminetheLUPdecomposition ofA
intime‚.n3/,wecancomputetheinverseA 1 ofamatrixAintime‚.n3/.
(cid:0)
Matrixmultiplicationandmatrixinversion
We now show that the theoretical speedups obtained for matrix multiplication
translate to speedups for matrix inversion. In fact, we prove something stronger:
matrix inversion is equivalent to matrix multiplication, in the following sense.
If M.n/ denotes the time to multiply two n n matrices, then we can invert a

nonsingular n n matrix in time O.M.n//. Moreover, if I.n/ denotes the time

to invert a nonsingular n n matrix, then we can multiply two n n matrices in
 
timeO.I.n//. Weprovetheseresultsastwoseparate theorems.
Theorem28.1(Multiplication isnoharderthaninversion)
If we can invert an n n matrix in time I.n/, where I.n/ .n2/ and I.n/
 D
satisfiestheregularityconditionI.3n/ O.I.n//,thenwecanmultiplytwon n
D 
matricesintimeO.I.n//.
Proof LetAand B ben nmatrices whosematrix product C wewishtocom-

pute. Wedefinethe3n 3nmatrixD by

28.2 Invertingmatrices 829
I A 0
n
D 0 I B :
n
D
0 0 I
n
Thein(cid:0) verseofD is 
I A AB
n
(cid:0)
D 1 0 I B ;
(cid:0) n
D (cid:0)
0 0 I
n
andthus(cid:0) wecancomputet heproductAB bytakingtheupperrightn nsubmatrix

ofD 1.
(cid:0)
WecanconstructmatrixD in‚.n2/time,whichisO.I.n//becauseweassume
that I.n/ .n2/, and we can invert D in O.I.3n// O.I.n// time, by the
D D
regularity condition onI.n/. WethushaveM.n/ O.I.n//.
D
Note that I.n/ satisfies the regularity condition whenever I.n/ ‚.nclgd n/
D
foranyconstants c > 0andd 0.

The proof that matrix inversion is no harder than matrix multiplication relies
on some properties of symmetric positive-definite matrices that we will prove in
Section28.3.
Theorem28.2(Inversionisnoharderthanmultiplication)
Suppose we can multiply two n n real matrices in time M.n/, where M.n/
 D
.n2/andM.n/satisfiesthetworegularityconditions M.n k/ O.M.n//for
C D
anyk inthe range 0 k n and M.n=2/ cM.n/for someconstant c < 1=2.
  
Then we can compute the inverse of any real nonsingular n n matrix in time

O.M.n//.
Proof Weprove the theorem here for real matrices. Exercise 28.2-6 asks you to
generalize theproofformatriceswhoseentriesarecomplexnumbers.
Wecanassumethatnisanexactpowerof2,sincewehave
A 0 (cid:0)1 A (cid:0)1 0
0 I k D 0 I k
   
for any k > 0. Thus, by choosing k such that n k is a power of 2, we enlarge
C
the matrix to a size that is the next power of 2 and obtain the desired answer A 1
(cid:0)
from the answer to the enlarged problem. The first regularity condition on M.n/
ensures thatthis enlargement does not cause therunning timetoincrease bymore
thanaconstant factor.
Forthemoment,letusassumethatthen nmatrixAissymmetricandpositive-

definite. Wepartition eachofAanditsinverseA 1 intofourn=2 n=2submatri-
(cid:0)

ces:
830 Chapter28 MatrixOperations
B CT R T
A and A 1 : (28.11)
(cid:0)
D C D D U V
   
Then,ifwelet
S D CB 1CT (28.12)
(cid:0)
D (cid:0)
betheSchurcomplementofAwithrespecttoB (weshallseemoreaboutthisform
ofSchurcomplementinSection28.3),wehave
R T B 1 B 1CTS 1CB 1 B 1CTS 1
A 1 (cid:0) C (cid:0) (cid:0) (cid:0) (cid:0) (cid:0) (cid:0) ; (28.13)
(cid:0) D U V D S 1CB 1 S 1
   (cid:0) (cid:0) (cid:0) (cid:0) 
since AA 1 I ,as you canverify byperforming thematrix multiplication. Be-
(cid:0) n
D
causeAissymmetricandpositive-definite, Lemmas28.4and28.5inSection28.3
imply that B and S are both symmetric and positive-definite. By Lemma 28.3 in
Section 28.3, therefore, the inverses B 1 and S 1 exist, and by Exercise D.2-6,
(cid:0) (cid:0)
B 1 and S 1 are symmetric, so that .B 1/T B 1 and .S 1/T S 1. There-
(cid:0) (cid:0) (cid:0) (cid:0) (cid:0) (cid:0)
D D
fore, we can compute the submatrices R, T, U, and V of A 1 as follows, where
(cid:0)
allmatricesmentionedaren=2 n=2:

1. Formthesubmatrices B,C,CT,andD ofA.
2. RecursivelycomputetheinverseB 1 ofB.
(cid:0)
3. Compute thematrix product W CB 1, and then compute its transpose WT,
(cid:0)
D
whichequalsB 1CT (byExerciseD.1-2and.B 1/T B 1).
(cid:0) (cid:0) (cid:0)
D
4. Compute the matrix product X WCT, which equals CB 1CT, and then
(cid:0)
D
computethematrixS D X D CB 1CT.
(cid:0)
D (cid:0) D (cid:0)
5. RecursivelycomputetheinverseS 1 ofS,andsetV toS 1.
(cid:0) (cid:0)
6. Compute the matrix product Y S 1W, which equals S 1CB 1, and
(cid:0) (cid:0) (cid:0)
D
then compute its transpose YT, which equals B 1CTS 1 (by Exercise D.1-2,
(cid:0) (cid:0)
.B 1/T B 1,and.S 1/T S 1). SetT to YT andU to Y.
(cid:0) (cid:0) (cid:0) (cid:0)
D D (cid:0) (cid:0)
7. Compute the matrix product Z WTY, which equals B 1CTS 1CB 1, and
(cid:0) (cid:0) (cid:0)
D
setRtoB 1 Z.
(cid:0)
C
Thus,wecaninvertann nsymmetricpositive-definitematrixbyinvertingtwo

n=2 n=2matricesinsteps2and5;performingfourmultiplications ofn=2 n=2
 
matrices in steps 3, 4, 6, and 7; plus an additional cost of O.n2/ for extracting
submatrices from A, inserting submatrices into A 1, and performing a constant
(cid:0)
number of additions, subtractions, and transposes on n=2 n=2 matrices. Weget

therecurrence
I.n/ 2I.n=2/ 4M.n=2/ O.n2/
 C C
2I.n=2/ ‚.M.n//
D C
O.M.n//:
D
28.2 Invertingmatrices 831
The second line holds because the second regularity condition in the statement
of the theorem implies that 4M.n=2/ < 2M.n/ and because we assume that
M.n/ .n2/. The third line follows because the second regularity condition
D
allowsustoapplycase3ofthemastertheorem (Theorem4.1).
Itremainstoprovethatwecanobtainthesameasymptoticrunningtimeforma-
trix multiplication as for matrix inversion when A is invertible but not symmetric
andpositive-definite. Thebasic idea isthat forany nonsingular matrix A, thema-
trixATAissymmetric(byExerciseD.1-2)andpositive-definite(byTheoremD.6).
The trick, then, is to reduce the problem of inverting A to the problem of invert-
ingATA.
The reduction is based on the observation that when A is an n n nonsingular

matrix,wehave
A 1 .ATA/ 1AT ;
(cid:0) (cid:0)
D
since ..ATA/ 1AT/A .ATA/ 1.ATA/ I and a matrix inverse is unique.
(cid:0) (cid:0) n
D D
Therefore, wecan compute A 1 byfirstmultiplying AT by Atoobtain ATA, then
(cid:0)
inverting the symmetric positive-definite matrix ATA using the above divide-and-
conquer algorithm, and finally multiplying the result by AT. Each of these three
stepstakesO.M.n//time,andthuswecaninvertanynonsingularmatrixwithreal
entriesinO.M.n//time.
The proof of Theorem 28.2 suggests a means of solving the equation Ax b
D
by using LU decomposition without pivoting, so long as A is nonsingular. We
multiply both sides of the equation by AT, yielding .ATA/x ATb. This trans-
D
formation doesn’t affect the solution x, since AT is invertible, and so we can fac-
tor the symmetric positive-definite matrix ATA by computing an LU decomposi-
tion. Wethenuseforward andback substitution tosolve forx withtheright-hand
side ATb. Although this method is theoretically correct, in practice the procedure
LUP-DECOMPOSITION works much better. LUP decomposition requires fewer
arithmetic operations by a constant factor, and it has somewhat better numerical
properties.
Exercises
28.2-1
LetM.n/bethetimetomultiplytwon nmatrices,andletS.n/denotethetime

required to square an n n matrix. Show that multiplying and squaring matri-

ces have essentially the same difficulty: an M.n/-time matrix-multiplication al-
gorithm impliesanO.M.n//-timesquaring algorithm, andanS.n/-timesquaring
algorithm impliesanO.S.n//-timematrix-multiplication algorithm.
832 Chapter28 MatrixOperations
28.2-2
Let M.n/ be the time to multiply two n n matrices. Show that an M.n/-time

matrix-multiplication algorithmimpliesanO.M.n//-timeLUP-decompositional-
gorithm.
28.2-3
Let M.n/ be the time to multiply two n n matrices, and let D.n/ denote the

time required to find the determinant of an n n matrix. Show that multiply-

ing matrices and computing the determinant have essentially the same difficulty:
anM.n/-timematrix-multiplication algorithm impliesanO.M.n//-timedetermi-
nant algorithm, and aD.n/-timedeterminant algorithm implies anO.D.n//-time
matrix-multiplication algorithm.
28.2-4
LetM.n/bethetimetomultiply twon nboolean matrices, andletT.n/bethe

time to findthe transitive closure of an n n boolean matrix. (See Section 25.2.)

Show that an M.n/-time boolean matrix-multiplication algorithm implies an
O.M.n/lgn/-timetransitive-closurealgorithm,andaT.n/-timetransitive-closure
algorithm impliesanO.T.n//-timebooleanmatrix-multiplication algorithm.
28.2-5
Does the matrix-inversion algorithm based on Theorem 28.2 work when matrix
elementsaredrawnfromthefieldofintegersmodulo2? Explain.
28.2-6 ?
Generalize the matrix-inversion algorithm of Theorem 28.2 to handle matrices of
complex numbers, and prove that your generalization works correctly. (Hint: In-
steadofthetransposeofA,usetheconjugatetransposeA ,whichyouobtainfrom

thetranspose ofAbyreplacing everyentrywithitscomplexconjugate. Insteadof
symmetric matrices, consider Hermitian matrices, whicharematricesAsuch that
A A .)

D
28.3 Symmetric positive-definite matrices and least-squares approximation
Symmetric positive-definite matrices have many interesting and desirable proper-
ties. For example, they are nonsingular, and we can perform LU decomposition
on them without having to worry about dividing by 0. In this section, we shall
28.3 Symmetricpositive-definitematricesandleast-squaresapproximation 833
prove several other important properties of symmetric positive-definite matrices
and show an interesting application to curve fitting by a least-squares approxima-
tion.
Thefirstproperty weproveisperhapsthemostbasic.
Lemma28.3
Anypositive-definite matrixisnonsingular.
Proof Suppose thatamatrixAissingular. ThenbyCorollary D.3,there existsa
nonzero vectorx suchthatAx 0. Hence, xTAx 0,andAcannot bepositive-
D D
definite.
The proof that we can perform LU decomposition on a symmetric positive-
definite matrix A without dividing by 0 is more involved. We begin by proving
properties about certain submatrices ofA. Definethe kthleading submatrix ofA
to be the matrix A consisting of the intersection of the first k rows and first k
k
columnsofA.
Lemma28.4
If A is a symmetric positive-definite matrix, then every leading submatrix of A is
symmetricandpositive-definite.
Proof ThateachleadingsubmatrixA issymmetricisobvious. ToprovethatA
k k
ispositive-definite, weassumethatitisnotandderiveacontradiction. IfA isnot
k
positive-definite, thenthereexistsak-vectorx 0suchthatxTA x 0. LetA
k ¤ k k k 
ben n,and

A BT
A k (28.14)
D B C
 
forsubmatricesB (whichis.n k/ k)andC (whichis.n k/ .n k/). Define
(cid:0)  (cid:0)  (cid:0)
then-vectorx . xT 0/T,wheren k 0sfollowx . Thenwehave
D k (cid:0) k
A BT x
xTAx . xT 0 / k k
D k B C 0
  
A x
. xT 0 / k k
D k Bx k
 
xTA x
D k k k
0;

whichcontradicts Abeingpositive-definite.
834 Chapter28 MatrixOperations
We now turn to some essential properties of the Schur complement. Let A be
a symmetric positive-definite matrix, and let A be a leading k k submatrix
k

of A. Partition A once again according to equation (28.14). We generalize equa-
tion(28.9)todefinetheSchurcomplementS ofAwithrespecttoA as
k
S C BA 1BT : (28.15)
D (cid:0)
(cid:0)k
(By Lemma 28.4, A is symmetric and positive-definite; therefore, A 1 exists by
k (cid:0)k
Lemma 28.3, and S is well defined.) Note that our earlier definition (28.9) of the
Schurcomplementisconsistent withequation (28.15), bylettingk 1.
D
Thenextlemmashowsthat theSchur-complement matrices ofsymmetric posi-
tive-definitematricesarethemselvessymmetricandpositive-definite. Weusedthis
result in Theorem 28.2, and we need its corollary to prove the correctness of LU
decomposition forsymmetricpositive-definite matrices.
Lemma28.5(Schurcomplementlemma)
If A is a symmetric positive-definite matrix and A is a leading k k submatrix
k

of A, then the Schur complement S of A with respect to A is symmetric and
k
positive-definite.
Proof Because A is symmetric, so is the submatrix C. By Exercise D.2-6, the
product BA 1BT issymmetric,andbyExerciseD.1-1,S issymmetric.
(cid:0)k
ItremainstoshowthatS ispositive-definite. ConsiderthepartitionofAgivenin
equation(28.14). Foranynonzerovectorx,wehavexTAx > 0bytheassumption
that A ispositive-definite. Letusbreak x into twosubvectors y and ´compatible
withA andC,respectively. BecauseA 1 exists,wehave
k (cid:0)k
A BT y
xTAx . yT ´T / k
D B C ´
  
A y BT´
. yT ´T / k C
D By C´
 C 
yTA y yTBT´ ´TBy ´TC´
k
D C C C
.y A 1BT´/TA .y A 1BT´/ ´T.C BA 1BT/´; (28.16)
D C
(cid:0)k k
C
(cid:0)k
C (cid:0)
(cid:0)k
by matrix magic. (Verify by multiplying through.) This last equation amounts to
“completing thesquare” ofthequadratic form. (SeeExercise28.3-2.)
Since xTAx > 0 holds for any nonzero x, let us pick any nonzero ´ and then
choose y A 1BT´, which causes the first term in equation (28.16) to vanish,
D (cid:0)
(cid:0)k
leaving
´T.C BA 1BT/´ ´TS´
(cid:0)
(cid:0)k
D
as the value of the expression. For any ´ 0, we therefore have ´TS´
¤ D
xTAx > 0,andthusS ispositive-definite.
28.3 Symmetricpositive-definitematricesandleast-squaresapproximation 835
Corollary28.6
LUdecomposition ofasymmetric positive-definite matrix nevercauses adivision
by0.
Proof Let A be a symmetric positive-definite matrix. Weshall prove something
strongerthanthestatementofthecorollary: everypivotisstrictlypositive. Thefirst
pivotisa . Lete bethefirstunitvector,fromwhichweobtaina eTAe >0.
11 1 11 D 1 1
Since the first step of LU decomposition produces the Schur complement of A
with respect to A .a /, Lemma 28.5 implies by induction that all pivots are
1 11
D
positive.
Least-squaresapproximation
One important application of symmetric positive-definite matrices arises in fitting
curvestogivensetsofdatapoints. Supposethatwearegivenasetofmdatapoints
.x ;y /;.x ;y /;:::;.x ;y /;
1 1 2 2 m m
where we know that the y are subject to measurement errors. We would like to
i
determineafunction F.x/suchthattheapproximation errors
 F.x / y (28.17)
i i i
D (cid:0)
aresmallfori 1;2;:::;m. TheformofthefunctionF dependsontheproblem
D
athand. Here,weassumethatithastheformofalinearlyweightedsum,
n
F.x/ c f .x/;
j j
D
j 1
XD
where the number of summands n and the specific basis functions f are chosen
j
based on knowledge of the problem at hand. A common choice is f .x/ xj 1,
j (cid:0)
D
whichmeansthat
F.x/ c c x c x2 c xn 1
1 2 3 n (cid:0)
D C C CC
isapolynomial ofdegree n 1inx. Thus,given mdata points .x ;y /;.x ;y /;
1 1 2 2
(cid:0)
:::;.x ;y /, we wish to calculate n coefficients c ;c ;:::;c that minimize the
m m 1 2 n
approximation errors ; ;:::; .
1 2 m
Bychoosing n m,wecancalculate eachy exactlyinequation(28.17). Such
i
D
a high-degree F “fits the noise” as well as the data, however, and generally gives
poor results when used to predict y for previously unseen values of x. It is usu-
ally better to choose n significantly smaller than m and hope that by choosing the
coefficients c well, we can obtain a function F that finds the significant patterns
j
in the data points without paying undue attention to the noise. Some theoretical
836 Chapter28 MatrixOperations
principles exist for choosing n, but they are beyond the scope of this text. In any
case, once we choose a value of n that is less than m, we end up with an overde-
termined set of equations whose solution we wish to approximate. We now show
howtodoso.
Let
f .x / f .x / ::: f .x /
1 1 2 1 n 1
f .x / f .x / ::: f .x /
1 2 2 2 n 2
A D : : : : : : ::: : : :
˙f .x / f .x / ::: f .x /
1 m 2 m n m
denote the matrix of values of the basis functions at the given points; that is,
a f .x /. Letc .c /denote thedesired n-vectorofcoefficients. Then,
ij j i k
D D
f .x / f .x / ::: f .x / c
1 1 2 1 n 1 1
f .x / f .x / ::: f .x / c
1 2 2 2 n 2 2
Ac D : : : : : : ::: : : : : : :
˙ f .x / f .x / ::: f .x / ˙c 
1 m 2 m n m n
F.x /
1
F.x /
2
:
D : :
˙ F.x / 
m
isthem-vectorof“predicted values”fory. Thus,
 Ac y
D (cid:0)
isthem-vectorofapproximation errors.
Tominimizeapproximation errors,wechoosetominimizethenormoftheerror
vector,whichgivesusaleast-squares solution,since
m 1=2
 2 :
k kD i
!
i 1
XD
Because
m n 2
 2 Ac y 2 a c y ;
ij j i
k k D k (cid:0) k D (cid:0)
!
i 1 j 1
XD XD
we can minimize  by differentiating  2 with respect to each c and then
k
k k k k
settingtheresultto0:
28.3 Symmetricpositive-definitematricesandleast-squaresapproximation 837
d  2 m n
k k 2 a c y a 0: (28.18)
ij j i ik
dc D (cid:0) D
k !
i 1 j 1
XD XD
The n equations (28.18) for k 1;2;:::;n are equivalent to the single matrix
D
equation
.Ac y/TA 0
(cid:0) D
or,equivalently (usingExerciseD.1-2),to
AT.Ac y/ 0;
(cid:0) D
whichimplies
ATAc ATy : (28.19)
D
In statistics, this is called the normal equation. The matrix ATA is symmetric
by Exercise D.1-2, and if A has full column rank, then by Theorem D.6, ATA
is positive-definite as well. Hence, .ATA/ 1 exists, and the solution to equa-
(cid:0)
tion(28.19)is
c .ATA/ 1AT y
(cid:0)
D
A y ; (28.20)
(cid:0) C 
D
where the matrix A ..ATA/ 1AT/ is the pseudoinverse of the matrix A. The
C (cid:0)
D
pseudoinverse naturally generalizes the notion of a matrix inverse to the case in
which A is not square. (Compare equation (28.20) as the approximate solution to
Ac y withthesolution A 1b astheexactsolution toAx b.)
(cid:0)
D D
As an example of producing a least-squares fit, suppose that we have five data
points
.x ;y / . 1;2/;
1 1
D (cid:0)
.x ;y / .1;1/;
2 2
D
.x ;y / .2;1/;
3 3
D
.x ;y / .3;0/;
4 4
D
.x ;y / .5;3/;
5 5
D
shown as black dots in Figure 28.3. We wish to fit these points with a quadratic
polynomial
F.x/ c c x c x2 :
1 2 3
D C C
Westartwiththematrixofbasis-function values
838 Chapter28 MatrixOperations
y
3.0
2.5
F(x) = 1.2 – 0.757x + 0.214x2
2.0
1.5
1.0
0.5
0.0
x
–2 –1 0 1 2 3 4 5
Figure 28.3 The least-squares fit of a quadratic polynomial to the set of five data points
. 1;2/;.1;1/;.2;1/;.3;0/;.5;3/ . Theblackdotsarethedatapoints,andthewhitedotsaretheir
f es(cid:0) timatedvaluespredictedbythepg olynomialF.x/ 1:2 0:757x 0:214x2,thequadraticpoly-
D (cid:0) C
nomialthatminimizesthesumofthesquarederrors. Eachshadedlineshowstheerrorforonedata
point.
1 x x2 1 1 1
1 1 (cid:0)
1 x x2 1 1 1
2 2
A 1 x x2 1 2 4 ;
D 3 3 D
1 x x2 1 3 9
4 4
(cid:0)1 x x2  (cid:0)1 5 25 
5 5
whosepseudoinverse is
0:500 0:300 0:200 0:100 0:100
(cid:0)
A 0:388 0:093 0:190 0:193 0:088 :
C
D (cid:0) (cid:0)
0:060 0:036 0:048 0:036 0:060
(cid:0) (cid:0) (cid:0)
(cid:0) 
Multiplying y byA ,weobtainthecoefficientvector
C
1:200
c 0:757 ;
D (cid:0)
0:214
(cid:0) 
whichcorresponds tothequadratic polynomial
28.3 Symmetricpositive-definitematricesandleast-squaresapproximation 839
F.x/ 1:200 0:757x 0:214x2
D (cid:0) C
astheclosest-fitting quadratic tothegivendata,inaleast-squares sense.
As a practical matter, we solve the normal equation (28.19) by multiplying y
by AT and then finding an LU decomposition of ATA. If A has full rank, the
matrixATAisguaranteed tobenonsingular, because itissymmetric andpositive-
definite. (SeeExerciseD.1-2andTheoremD.6.)
Exercises
28.3-1
Provethat every diagonal element of a symmetric positive-definite matrix is posi-
tive.
28.3-2
a b
Let A be a 2 2 symmetric positive-definite matrix. Prove that its
D b c 
 
determinant ac b2 ispositiveby“completing thesquare” inamannersimilarto
(cid:0)
thatusedintheproofofLemma28.5.
28.3-3
Prove that the maximum element in a symmetric positive-definite matrix lies on
thediagonal.
28.3-4
Prove that the determinant of each leading submatrix of a symmetric positive-
definitematrixispositive.
28.3-5
LetA denotethekthleadingsubmatrixofasymmetricpositive-definitematrixA.
k
Prove that det.A /=det.A / is the kth pivot during LU decomposition, where,
k k 1
byconvention, det.A / 1(cid:0) .
0
D
28.3-6
Findthefunction oftheform
F.x/ c c xlgx c ex
1 2 3
D C C
thatisthebestleast-squares fittothedatapoints
.1;1/;.2;1/;.3;3/;.4;8/ :
840 Chapter28 MatrixOperations
28.3-7
Showthatthepseudoinverse A satisfiesthefollowingfourequations:
C
AA A A;
C
D
A AA A ;
C C C
D
.AA /T AA ;
C C
D
.A A/T A A:
C C
D
Problems
28-1 Tridiagonalsystemsoflinearequations
Considerthetridiagonal matrix
1 1 0 0 0
(cid:0)
1 2 1 0 0
(cid:0) (cid:0)
A 0 1 2 1 0 :
D (cid:0) (cid:0)
0 0 1 2 1
ˇ
(cid:0) (cid:0)
0 0 0 1 2
(cid:0)
a. FindanLUdecomposition ofA.
b. Solvethe equation Ax 1 1 1 1 1 T byusing forward and back sub-
D
stitution.
(cid:0) 
c. FindtheinverseofA.
d. Showhow,foranyn nsymmetricpositive-definite, tridiagonal matrixAand

any n-vector b, to solve the equation Ax b in O.n/ time by performing an
D
LUdecomposition. ArguethatanymethodbasedonformingA 1 isasymptot-
(cid:0)
icallymoreexpensiveintheworstcase.
e. Showhow,foranyn nnonsingular,tridiagonalmatrixAandanyn-vectorb,to

solvetheequationAx binO.n/timebyperforminganLUPdecomposition.
D
28-2 Splines
A practical method for interpolating a set of points with a curve is to use cu-
bic splines. We are given a set .x ;y / i 0;1;:::;n of n 1 point-value
i i
f W D g C
pairs, where x < x < < x . We wish to fit a piecewise-cubic curve
0 1 n

(spline) f.x/tothepoints. Thatis, thecurve f.x/ismadeupofncubic polyno-
mialsf .x/ a b x c x2 d x3 fori 0;1;:::;n 1,whereifx fallsin
i i i i i
D C C C D (cid:0)
ProblemsforChapter28 841
therangex x x ,thenthevalueofthecurveisgivenbyf.x/ f .x x /.
i i 1 i i
Thepointsx atw hichC thecubicpolynomialsare“pasted”togetherareD calledk(cid:0)
nots.
i
Forsimplicity, weshallassumethatx i fori 0;1;:::;n.
i
D D
Toensurecontinuity off.x/,werequire that
f.x / f .0/ y ;
i i i
D D
f.x / f .1/ y
i 1 i i 1
C D D C
for i 0;1;:::;n 1. Toensure that f.x/is sufficiently smooth, we also insist
D (cid:0)
thatthefirstderivativebecontinuous ateachknot:
f .x / f .1/ f .0/
0 i C1 D i0 D i0 C1
fori 0;1;:::;n 2.
D (cid:0)
a. Suppose that for i 0;1;:::;n, we are given not only the point-value pairs
D
.x ;y / but alsothefirstderivatives D f .x /ateachknot. Express each
i i i 0 i
f g D
coefficient a , b , c , and d in terms of the values y , y , D , and D .
i i i i i i 1 i i 1
(Remember that x i.) How quickly can we compute C the 4n coefficieC nts
i
D
fromthepoint-value pairsandfirstderivatives?
The question remains of how to choose the first derivatives of f.x/ at the knots.
Onemethodistorequirethesecondderivatives tobecontinuous attheknots:
f .x / f .1/ f .0/
00 i C1
D
i00
D
i0 C01
for i 0;1;:::;n 2. At the first and last knots, we assume that f .x /
00 0
D (cid:0) D
f .0/ 0 and f .x / f .1/ 0; these assumptions make f.x/a natural
000 D 00 n D n00 1 D
cubicspline. (cid:0)
b. Use the continuity constraints on the second derivative to show that for i
D
1;2;:::;n 1,
(cid:0)
D 4D D 3.y y /: (28.21)
i 1 i i 1 i 1 i 1
(cid:0) C C C D C (cid:0) (cid:0)
c. Showthat
2D D 3.y y /; (28.22)
0 1 1 0
C D (cid:0)
D 2D 3.y y /: (28.23)
n 1 n n n 1
(cid:0) C D (cid:0) (cid:0)
d. Rewrite equations (28.21)–(28.23) as a matrix equation involving the vector
D D ;D ;:::;D ofunknowns. Whatattributes does the matrix inyour
0 1 n
D h i
equation have?
e. Arguethatanaturalcubicsplinecaninterpolateasetofn 1point-valuepairs
C
inO.n/time(seeProblem28-1).
842 Chapter28 MatrixOperations
f. Show how to determine a natural cubic spline that interpolates a set of n 1
C
points .x ;y / satisfying x < x < < x , even when x is not necessarily
i i 0 1 n i

equal to i. What matrix equation must your method solve, and how quickly
doesyouralgorithm run?
Chapter notes
Manyexcellenttextsdescribenumericalandscientificcomputationinmuchgreater
detail than wehave room for here. Thefollowing are especially readable: George
and Liu[132], Golub andVanLoan[144],Press, Teukolsky, Vetterling, andFlan-
nery[283,284],andStrang[323,324].
Golub and Van Loan [144] discuss numerical stability. They show why det.A/
isnotnecessarily agoodindicator ofthestability ofamatrixA,proposing instead
to use A A 1 , where A max n a . They also address
thequek stiok n1 ok fh(cid:0) owk 1 tocomputek thk is1 vaD luewit1 h oi u tn actuj D al1 lyj cij oj mputing A 1.
P (cid:0)
Gaussian elimination, upon which the LU and LUP decompositions are based,
wasthefirstsystematicmethodforsolvinglinearsystemsofequations. Itwasalso
one of the earliest numerical algorithms. Although it was known earlier, its dis-
covery is commonly attributed to C. F. Gauss (1777–1855). In his famous paper
[325],Strassenshowedthatann nmatrixcanbeinvertedinO.nlg7/time. Wino-

grad [358] originally proved that matrix multiplication is no harder than matrix
inversion, andtheconverse isduetoAho,Hopcroft,andUllman[5].
Another important matrix decomposition is the singular value decomposition,
or SVD. The SVD factors an m n matrix A into A Q †QT, where † is an
 D 1 2
m nmatrixwithnonzerovaluesonlyonthediagonal,Q ism mwithmutually
1
 
orthonormal columns, and Q is n n, also with mutually orthonormal columns.
2

Twovectorsareorthonormaliftheirinnerproductis0andeachvectorhasanorm
of1. ThebooksbyStrang[323,324]andGolubandVanLoan[144]containgood
treatments oftheSVD.
Strang [324]hasanexcellent presentation ofsymmetric positive-definite matri-
cesandoflinearalgebra ingeneral.
29 Linear Programming
Many problems take the form of maximizing or minimizing an objective, given
limited resources and competing constraints. If we can specify the objective as
a linear function of certain variables, and if we can specify the constraints on
resources as equalities or inequalities on those variables, then we have a linear-
programming problem. Linear programs arise in a variety of practical applica-
tions. Webeginbystudying anapplication inelectoralpolitics.
Apoliticalproblem
Suppose that you are apolitician trying towin an election. Your district has three
different types of areas—urban, suburban, and rural. These areas have, respec-
tively, 100,000, 200,000, and 50,000 registered voters. Although not all the reg-
istered voters actually go to the polls, you decide that to govern effectively, you
wouldlikeatleasthalftheregisteredvotersineachofthethreeregionstovotefor
you. Youarehonorableandwouldneverconsidersupportingpoliciesinwhichyou
do not believe. You realize, however, that certain issues may be more effective in
winning votes incertain places. Yourprimary issues arebuilding moreroads, gun
control, farm subsidies, and a gasoline tax dedicated to improved public transit.
According to your campaign staff’s research, you can estimate how many votes
you winor lose from each population segment byspending $1,000 on advertising
on each issue. This information appears in the table of Figure 29.1. In this table,
each entry indicates the number of thousands of either urban, suburban, or rural
voters who would be won over by spending $1,000 on advertising in support of a
particular issue. Negative entries denote votes that would be lost. Your task is to
figure out the minimum amount of money that you need to spend in order to win
50,000urbanvotes, 100,000suburban votes, and25,000ruralvotes.
You could, by trial and error, devise a strategy that wins the required number
of votes, but the strategy you come up with might not be the least expensive one.
Forexample,youcoulddevote$20,000ofadvertising tobuilding roads,$0togun
control, $4,000 to farm subsidies, and $9,000 to a gasoline tax. In this case, you
844 Chapter29 LinearProgramming
policy urban suburban rural
buildroads 2 5 3
(cid:0)
guncontrol 8 2 5
(cid:0)
farmsubsidies 0 0 10
gasolinetax 10 0 2
(cid:0)
Figure29.1 Theeffectsof policieson voters. Eachentrydescribes the number of thousands of
urban,suburban,orruralvoterswhocouldbewonoverbyspending$1,000onadvertisingsupport
ofapolicyonaparticularissue.Negativeentriesdenotevotesthatwouldbelost.
wouldwin20. 2/ 0.8/ 4.0/ 9.10/ 50thousandurbanvotes,20.5/ 0.2/
(cid:0) C C C D C C
4.0/ 9.0/ 100thousandsuburbanvotes,and20.3/ 0. 5/ 4.10/ 9. 2/
C D C (cid:0) C C (cid:0) D
82 thousand rural votes. You would win the exact number of votes desired in the
urban and suburban areas and more than enough votes in the rural area. (In fact,
in the rural area, you would receive more votes than there are voters.) In order to
garnerthesevotes,youwouldhavepaidfor20 0 4 9 33thousanddollars
C C C D
ofadvertising.
Naturally, you may wonder whether this strategy is the best possible. That is,
could you achieve your goals while spending less on advertising? Additional trial
and error might help you to answer this question, but wouldn’t you rather have a
systematicmethodforansweringsuchquestions? Inordertodevelopone,weshall
formulatethisquestionmathematically. Weintroduce 4variables:
 x isthenumberofthousandsofdollarsspentonadvertisingonbuildingroads,
1
 x isthenumberofthousands ofdollars spentonadvertising onguncontrol,
2
 x isthenumberofthousandsofdollarsspentonadvertisingonfarmsubsidies,
3
and
 x isthenumberofthousands ofdollarsspentonadvertising onagasolinetax.
4
Wecanwritetherequirement thatwewinatleast50,000urbanvotesas
2x 8x 0x 10x 50: (29.1)
1 2 3 4
(cid:0) C C C 
Similarly, we can write the requirements that we win at least 100,000 suburban
votesand25,000ruralvotesas
5x 2x 0x 0x 100 (29.2)
1 2 3 4
C C C 
and
3x 5x 10x 2x 25: (29.3)
1 2 3 4
(cid:0) C (cid:0) 
Any setting of the variables x ;x ;x ;x that satisfies inequalities (29.1)–(29.3)
1 2 3 4
yields a strategy that wins a sufficient number of each type of vote. In order to
Chapter29 LinearProgramming 845
keep costs as small as possible, you would like to minimize the amount spent on
advertising. Thatis,youwanttominimizetheexpression
x x x x : (29.4)
1 2 3 4
C C C
Althoughnegativeadvertisingoftenoccursinpoliticalcampaigns,thereisnosuch
thingasnegative-cost advertising. Consequently, werequirethat
x 0; x 0; x 0; and x 0: (29.5)
1 2 3 4
   
Combining inequalities (29.1)–(29.3) and (29.5) with the objective of minimiz-
ing(29.4),weobtainwhatisknownasa“linearprogram.” Weformatthisproblem
as
minimize x x x x (29.6)
1 2 3 4
C C C
subjectto
2x 8x 0x 10x 50 (29.7)
1 2 3 4
(cid:0) C C C 
5x 2x 0x 0x 100 (29.8)
1 2 3 4
C C C 
3x 5x 10x 2x 25 (29.9)
1 2 3 4
(cid:0) C (cid:0) 
x ;x ;x ;x 0 : (29.10)
1 2 3 4

Thesolution ofthislinearprogram yieldsyouroptimalstrategy.
Generallinearprograms
Inthegeneral linear-programming problem, wewishtooptimize alinear function
subjecttoasetoflinearinequalities. Givenasetofrealnumbersa ;a ;:::;a and
1 2 n
aset of variables x ;x ;:::;x , wedefine alinear function f on those variables
1 2 n
by
n
f.x ;x ;:::;x / a x a x a x a x :
1 2 n 1 1 2 2 n n j j
D C CC D
j 1
XD
Ifb isarealnumberandf isalinearfunction, thentheequation
f.x ;x ;:::;x / b
1 2 n
D
isalinearequalityandtheinequalities
f.x ;x ;:::;x / b
1 2 n

and
f.x ;x ;:::;x / b
1 2 n

846 Chapter29 LinearProgramming
arelinearinequalities. Weusethegeneraltermlinearconstraintstodenoteeither
linear equalities or linear inequalities. In linear programming, we do not allow
strict inequalities. Formally, a linear-programming problem is the problem of
either minimizing or maximizing a linear function subject to a finite set of linear
constraints. Ifwearetominimize,thenwecallthelinearprogram aminimization
linear program, and if we are to maximize, then we call the linear program a
maximization linearprogram.
The remainder of this chapter covers how to formulate and solve linear pro-
grams. Althoughseveralpolynomial-timealgorithmsforlinearprogramminghave
beendeveloped, wewillnotstudytheminthischapter. Instead, weshallstudythe
simplexalgorithm,whichistheoldestlinear-programmingalgorithm. Thesimplex
algorithmdoesnotruninpolynomialtimeintheworstcase,butitisfairlyefficient
andwidelyusedinpractice.
Anoverview oflinearprogramming
In order to describe properties of and algorithms for linear programs, we find it
convenient toexpress them incanonical forms. Weshall usetwo forms, standard
and slack, in this chapter. We will define them precisely in Section 29.1. Infor-
mally, a linear program in standard form is the maximization of a linear function
subject to linear inequalities, whereas a linear program in slack form is the max-
imization of a linear function subject to linear equalities. We shall typically use
standard form for expressing linear programs, but we find it more convenient to
useslackformwhenwedescribethedetailsofthesimplexalgorithm. Fornow,we
restrictourattentiontomaximizingalinearfunctiononnvariablessubjecttoaset
ofmlinearinequalities.
Letusfirstconsider thefollowinglinearprogram withtwovariables:
maximize x x (29.11)
1 2
C
subject to
4x x 8 (29.12)
1 2
(cid:0) 
2x x 10 (29.13)
1 2
C 
5x 2x 2 (29.14)
1 2
(cid:0)  (cid:0)
x ;x 0 : (29.15)
1 2

We call any setting of the variables x and x that satisfies all the constraints
1 2
(29.12)–(29.15) a feasible solution to the linear program. If we graph the con-
straints in the .x ;x /-Cartesian coordinate system, as in Figure 29.2(a), we see
1 2
Chapter29 LinearProgramming 847
x
2
8
£ x2
–
x1
4
2
x
1 +
x 2 £
1
0
x x ‡ 0 1
2
0
‡
x
1
x
2 2
–
‡
x2
2
–
x1
5
x
1 +
x 2 =
8
x
1 + x
2 =
4
x
1 + x x 1
2 =
0
(a) (b)
Figure29.2 (a) The linear program given in(29.12)–(29.15). Eachconstraint isrepresented by
alineandadirection. Theintersection of theconstraints, whichisthefeasibleregion, isshaded.
(b)Thedottedlinesshow,respectively,thepointsforwhichtheobjectivevalueis0,4,and8. The
optimalsolutiontothelinearprogramisx1 2andx2 6withobjectivevalue8.
D D
that the set of feasible solutions (shaded in the figure) forms a convex region1 in
thetwo-dimensional space. Wecallthisconvexregion thefeasible region andthe
functionwewishtomaximizetheobjectivefunction. Conceptually,wecouldeval-
uatetheobjectivefunction x x ateachpointinthefeasibleregion;wecallthe
1 2
C
value of the objective function at a particular point the objective value. We could
thenidentify apointthathasthemaximum objective valueasanoptimal solution.
For this example (and for most linear programs), the feasible region contains an
infinite number of points, and so we need to determine an efficient way to find a
point that achieves the maximum objective value without explicitly evaluating the
objectivefunction ateverypointinthefeasible region.
Intwodimensions, wecanoptimizeviaagraphicalprocedure. Thesetofpoints
forwhichx x ´,forany´,isalinewithaslopeof 1. Ifweplotx x 0,
1 2 1 2
C D (cid:0) C D
we obtain the line with slope 1 through the origin, as in Figure 29.2(b). The
(cid:0)
intersection of this line and the feasible region is the set of feasible solutions that
have an objective value of 0. In this case, that intersection of the line with the
feasibleregionisthesinglepoint.0;0/. Moregenerally, forany´,theintersection
1Anintuitivedefinitionofaconvexregionisthatitfulfillstherequirementthatforanytwopointsin
theregion,allpointsonalinesegmentbetweenthemarealsointheregion.
848 Chapter29 LinearProgramming
oftheline x x ´and thefeasible region isthesetoffeasible solutions that
1 2
C D
haveobjectivevalue´. Figure29.2(b)showsthelinesx x 0,x x 4,
1 2 1 2
C D C D
and x x 8. Because the feasible region in Figure 29.2 is bounded, there
1 2
C D
mustbesomemaximumvalue´forwhichtheintersectionofthelinex x ´
1 2
C D
and the feasible region is nonempty. Any point at which this occurs is an optimal
solution to the linear program, which in this case is the point x 2 and x 6
1 2
D D
withobjectivevalue8.
Itisnoaccidentthatanoptimalsolutiontothelinearprogramoccursatavertex
of the feasible region. The maximum value of ´ for which the line x x ´
1 2
C D
intersects the feasible region must be on the boundary of the feasible region, and
thustheintersection ofthislinewiththeboundary ofthefeasibleregioniseithera
single vertex or a line segment. If the intersection is a single vertex, then there is
justoneoptimalsolution, anditisthatvertex. Iftheintersection isalinesegment,
everypoint onthatlinesegmentmusthavethesameobjective value; inparticular,
bothendpoints ofthelinesegmentareoptimalsolutions. Sinceeachendpoint ofa
linesegmentisavertex,thereisanoptimalsolutionatavertexinthiscaseaswell.
Althoughwecannoteasilygraphlinearprogramswithmorethantwovariables,
the same intuition holds. If we have three variables, then each constraint corre-
sponds to a half-space in three-dimensional space. The intersection of these half-
spacesformsthefeasibleregion. Thesetofpointsforwhichtheobjectivefunction
obtains agiven value ´isnowaplane (assuming nodegenerate conditions). Ifall
coefficientsoftheobjectivefunctionarenonnegative, andiftheoriginisafeasible
solutiontothelinearprogram,thenaswemovethisplaneawayfromtheorigin,in
adirection normaltotheobjective function, wefindpoints ofincreasing objective
value. (Iftheorigin isnotfeasible orifsomecoefficients intheobjective function
are negative, the intuitive picture becomes slightly more complicated.) As in two
dimensions, because the feasible region is convex, the set of points that achieve
the optimal objective value must include a vertex of the feasible region. Simi-
larly, ifwehavenvariables, eachconstraint definesahalf-space inn-dimensional
space. Wecallthefeasibleregionformedbytheintersection ofthesehalf-spaces a
simplex. Theobjectivefunction isnowahyperplane and,becauseofconvexity, an
optimalsolution stilloccursatavertexofthesimplex.
The simplex algorithm takes as input a linear program and returns an optimal
solution. It starts at some vertex of the simplex and performs a sequence of itera-
tions. Ineachiteration,itmovesalonganedgeofthesimplexfromacurrentvertex
toaneighboring vertexwhoseobjectivevalueisnosmallerthanthatofthecurrent
vertex (and usually is larger.) The simplex algorithm terminates when it reaches
a local maximum, which is a vertex from which all neighboring vertices have a
smaller objective value. Because the feasible region is convex and the objective
functionislinear,thislocaloptimumisactuallyaglobaloptimum. InSection29.4,
Chapter29 LinearProgramming 849
we shall use a concept called “duality” to show that the solution returned by the
simplexalgorithm isindeedoptimal.
Althoughthegeometricviewgivesagoodintuitiveviewoftheoperationsofthe
simplex algorithm, we shall not refer to it explicitly when developing the details
of the simplex algorithm in Section 29.3. Instead, we take an algebraic view. We
firstwritethegivenlinearprograminslackform,whichisasetoflinearequalities.
These linear equalities express some of the variables, called “basic variables,” in
terms of other variables, called “nonbasic variables.” We move from one vertex
to another by making a basic variable become nonbasic and making a nonbasic
variable become basic. Wecall this operation a“pivot” and, viewedalgebraically,
itisnothing morethanrewritingthelinearprogram inanequivalent slackform.
The two-variable example described above was particularly simple. We shall
need to address several more details in this chapter. These issues include iden-
tifying linear programs that have no solutions, linear programs that have no finite
optimalsolution,andlinearprogramsforwhichtheoriginisnotafeasiblesolution.
Applicationsoflinearprogramming
Linear programming has a large number of applications. Any textbook on opera-
tions research is filled with examples of linear programming, and linear program-
minghasbecome astandard tool taught tostudents inmostbusiness schools. The
election scenario is one typical example. Two more examples of linear program-
mingarethefollowing:
 An airline wishes to schedule its flight crews. The Federal Aviation Adminis-
tration imposes many constraints, such as limiting the number of consecutive
hoursthateachcrewmembercanworkandinsistingthataparticularcrewwork
onlyononemodelofaircraftduringeachmonth. Theairlinewantstoschedule
crewsonallofitsflightsusingasfewcrewmembersaspossible.
 Anoilcompanywantstodecidewheretodrillforoil. Sitingadrillataparticu-
larlocationhasanassociatedcostand,basedongeologicalsurveys,anexpected
payoffofsomenumberofbarrelsofoil. Thecompanyhasalimitedbudget for
locating new drills and wants to maximize the amount ofoil itexpects tofind,
giventhisbudget.
With linear programs, we also model and solve graph and combinatorial prob-
lems, such as those appearing in this textbook. We have already seen a special
caseoflinearprogrammingusedtosolvesystemsofdifference constraints inSec-
tion 24.4. In Section 29.2, we shall study how to formulate several graph and
network-flow problems as linear programs. In Section 35.4, we shall use linear
programmingasatooltofindanapproximate solution toanothergraphproblem.
850 Chapter29 LinearProgramming
Algorithmsforlinearprogramming
This chapter studies the simplex algorithm. This algorithm, when implemented
carefully, often solves general linear programs quickly in practice. With some
carefullycontrivedinputs,however,thesimplexalgorithm canrequireexponential
time. Thefirstpolynomial-timealgorithmforlinearprogrammingwastheellipsoid
algorithm,whichrunsslowlyinpractice. Asecondclassofpolynomial-timealgo-
rithms areknown asinterior-point methods. Incontrast tothesimplex algorithm,
whichmovesalongtheexteriorofthefeasibleregionandmaintainsafeasiblesolu-
tionthatisavertexofthesimplexateachiteration,thesealgorithmsmovethrough
the interior of the feasible region. The intermediate solutions, while feasible, are
not necessarily vertices of the simplex, but the finalsolution isa vertex. Forlarge
inputs, interior-point algorithms can run as fast as, and sometimes faster than, the
simplex algorithm. The chapter notes point you to more information about these
algorithms.
If we add to a linear program the additional requirement that all variables take
on integer values, we have an integer linear program. Exercise 34.5-3 asks you
to show that just finding a feasible solution to this problem is NP-hard; since
no polynomial-time algorithms are known for any NP-hard problems, there is no
knownpolynomial-time algorithm forintegerlinearprogramming. Incontrast, we
cansolveagenerallinear-programming problem inpolynomial time.
Inthis chapter, ifwehave alinear program withvariables x .x ;x ;:::;x /
1 2 n
D
and wish to refer to a particular setting of the variables, we shall use the notation
x .x ;x ;:::;x /.
1 2 n
N D N N N
29.1 Standard andslackforms
This section describes two formats, standard form and slack form, that are use-
ful when we specify and work with linear programs. In standard form, all the
constraints are inequalities, whereas in slack form, all constraints are equalities
(exceptforthosethatrequirethevariablestobenonnegative).
Standardform
In standard form, we are given n real numbers c ;c ;:::;c ; m real numbers
1 2 n
b ;b ;:::;b ; and mn realnumbers a for i 1;2;:::;m and j 1;2;:::;n.
1 2 m ij
D D
Wewishtofindnrealnumbersx ;x ;:::;x that
1 2 n
29.1 Standardandslackforms 851
n
maximize c x (29.16)
j j
j 1
XD
subjectto
n
a x b fori 1;2;:::;m (29.17)
ij j i
 D
j 1
XD
x 0 forj 1;2;:::;n: (29.18)
j
 D
Generalizingtheterminologyweintroducedforthetwo-variablelinearprogram,
we call expression (29.16) the objective function and the n m inequalities in
C
lines (29.17) and (29.18) theconstraints. Thenconstraints in line(29.18) arethe
nonnegativityconstraints. Anarbitrary linearprogram neednothavenonnegativ-
ity constraints, but standard form requires them. Sometimes wefindit convenient
toexpress alinearprogram inamorecompact form. Ifwecreateanm nmatrix

A .a /,anm-vectorb .b /,ann-vectorc .c /,andann-vectorx .x /,
ij i j j
D D D D
thenwecanrewritethelinearprogram definedin(29.16)–(29.18) as
maximize cTx (29.19)
subjectto
Ax b (29.20)

x 0: (29.21)

In line (29.19), cTx is the inner product of two vectors. In inequality (29.20), Ax
isamatrix-vector product, andininequality (29.21), x 0means that each entry

ofthevector x mustbenonnegative. Weseethat wecan specify alinear program
instandard form by atuple .A;b;c/, and weshall adopt the convention that A, b,
andc alwayshavethedimensions givenabove.
Wenowintroduceterminologytodescribesolutionstolinearprograms. Weused
some of this terminology in the earlier example of a two-variable linear program.
Wecallasetting ofthevariables x thatsatisfiesalltheconstraints afeasible solu-
N
tion,whereas asetting ofthevariables x thatfailstosatisfy atleastoneconstraint
N
isaninfeasible solution. Wesaythatasolution x hasobjective valuecTx. Afea-
N N
siblesolutionx whoseobjectivevalueismaximumoverallfeasiblesolutionsisan
N
optimal solution, and we call its objective value cTx the optimal objective value.
N
Ifalinear program has no feasible solutions, wesay that the linear program isin-
feasible; otherwise it is feasible. If a linear program has some feasible solutions
but does not have a finite optimal objective value, we say that the linear program
isunbounded. Exercise 29.1-9 asks you toshow that alinear program can have a
finiteoptimalobjectivevalueevenifthefeasible regionisnotbounded.
852 Chapter29 LinearProgramming
Convertinglinearprogramsintostandardform
It is always possible to convert a linear program, given as minimizing or maxi-
mizing alinear function subject to linear constraints, into standard form. A linear
program mightnotbeinstandard formforanyoffourpossible reasons:
1. Theobjectivefunction mightbeaminimizationratherthanamaximization.
2. Theremightbevariables withoutnonnegativity constraints.
3. There might be equality constraints, which have an equal sign rather than a
less-than-or-equal-to sign.
4. There might be inequality constraints, but instead of having a less-than-or-
equal-tosign,theyhaveagreater-than-or-equal-to sign.
WhenconvertingonelinearprogramLintoanotherlinearprogramL,wewould
0
liketheproperty thatanoptimal solution toL yieldsanoptimalsolution toL. To
0
capture this idea, we say that two maximization linear programs L and L are
0
equivalent if for each feasible solution x to L with objective value ´, there is
N
a corresponding feasible solution x to L with objective value ´, and for each
0 0
N
feasible solution x to L with objective value ´, there is a corresponding feasible
0 0
N
solution x to L with objective value ´. (This definition does not imply a one-to-
N
onecorrespondencebetweenfeasiblesolutions.) AminimizationlinearprogramL
andamaximizationlinearprogramL areequivalentifforeachfeasiblesolutionx
0
N
toLwithobjectivevalue´,thereisacorresponding feasiblesolutionx toL with
0 0
N
objective value ´, and for each feasible solution x to L with objective value ´,
0 0
(cid:0) N
thereisacorresponding feasiblesolution x toLwithobjectivevalue ´.
N (cid:0)
Wenow show how toremove, oneby one, each ofthe possible problems inthe
listabove. Afterremovingeachone,weshallarguethatthenewlinearprogram is
equivalent totheoldone.
ToconvertaminimizationlinearprogramLintoanequivalentmaximizationlin-
ear program L,wesimply negate the coefficients inthe objective function. Since
0
LandL haveidenticalsetsoffeasiblesolutionsand,foranyfeasiblesolution,the
0
objective value in L is the negative of the objective value in L, these two linear
0
programsareequivalent. Forexample,ifwehavethelinearprogram
minimize 2x 3x
1 2
(cid:0) C
subject to
x x 7
1 2
C D
x 2x 4
1 2
(cid:0) 
x 0 ;
1

andwenegatethecoefficients oftheobjectivefunction, weobtain
29.1 Standardandslackforms 853
maximize 2x 3x
1 2
(cid:0)
subjectto
x x 7
1 2
C D
x 2x 4
1 2
(cid:0) 
x 0 :
1

Next, we show how to convert a linear program in which some of the variables
do not have nonnegativity constraints into one in which each variable has a non-
negativityconstraint. Supposethatsomevariablex doesnothaveanonnegativity
j
constraint. Then, we replace each occurrence of x by x x , and add the non-
j j0
(cid:0)
j00
negativity constraints x 0 and x 0. Thus, if the objective function has a
j0

j00

term c x , we replace it by c x c x , and if constraint i has a term a x , we
j j j j0
(cid:0)
j j00 ij j
replaceitbya x a x . Anyfeasiblesolution x tothenewlinearprogramcor-
ij j0
(cid:0)
ij j00
y
responds toafeasible solution x totheoriginal linear program withx x x
N
Nj
D
yj0
(cid:0)
yj00
and with the same objective value. Also, any feasible solution x to the original
N
linearprogramcorresponds toafeasiblesolutionx tothenewlinearprogramwith
y
x x and x 0ifx 0, orwithx x and x 0ifx < 0. Thetwo
yj0
D
Nj yj00
D
Nj

yj00
D
(cid:0)Nj yj0
D
Nj
linear programs have the same objective value regardless of the sign of x . Thus,
j
N
the two linear programs are equivalent. We apply this conversion scheme to each
variable that does not have anonnegativity constraint to yield an equivalent linear
programinwhichallvariables havenonnegativity constraints.
Continuingtheexample,wewanttoensurethateachvariablehasacorrespond-
ingnonnegativityconstraint. Variablex hassuchaconstraint,butvariablex does
1 2
not. Therefore,wereplacex bytwovariablesx andx ,andwemodifythelinear
2 20 200
programtoobtain
maximize 2x 3x 3x
1
(cid:0)
20
C
200
subjectto
x x x 7 (29.22)
1
C
20
(cid:0)
200
D
x 2x 2x 4
1
(cid:0)
20
C
200

x ;x ;x 0 :
1 20 200

Next,weconvert equalityconstraints intoinequality constraints. Supposethata
linear program has an equality constraint f.x ;x ;:::;x / b. Since x y if
1 2 n
D D
and only if both x y and x y, we can replace this equality constraint by the
 
pair of inequality constraints f.x ;x ;:::;x / b and f.x ;x ;:::;x / b.
1 2 n 1 2 n
 
Repeating this conversion for each equality constraint yields a linear program in
whichallconstraints areinequalities.
Finally, we can convert the greater-than-or-equal-to constraints to less-than-or-
equal-to constraints by multiplying these constraints through by 1. That is, any
(cid:0)
inequality oftheform
854 Chapter29 LinearProgramming
n
a x b
ij j i

j 1
XD
isequivalent to
n
a x b :
ij j i
(cid:0)  (cid:0)
j 1
XD
Thus,byreplacingeachcoefficienta by a andeachvalueb by b ,weobtain
ij ij i i
(cid:0) (cid:0)
anequivalent less-than-or-equal-to constraint.
Finishing our example, we replace the equality in constraint (29.22) by two in-
equalities, obtaining
maximize 2x 3x 3x
1
(cid:0)
20
C
200
subject to
x x x 7
1
C
20
(cid:0)
200

x x x 7 (29.23)
1
C
20
(cid:0)
200

x 2x 2x 4
1
(cid:0)
20
C
200

x ;x ;x 0 :
1 20 200

Finally, we negate constraint (29.23). For consistency in variable names, we re-
namex tox andx tox ,obtaining thestandard form
20 2 200 3
maximize 2x 3x 3x (29.24)
1 2 3
(cid:0) C
subject to
x x x 7 (29.25)
1 2 3
C (cid:0) 
x x x 7 (29.26)
1 2 3
(cid:0) (cid:0) C  (cid:0)
x 2x 2x 4 (29.27)
1 2 3
(cid:0) C 
x ;x ;x 0 : (29.28)
1 2 3

Convertinglinearprogramsintoslackform
To efficiently solve a linear program with the simplex algorithm, we prefer to ex-
press it in a form in which some of the constraints are equality constraints. More
precisely, weshallconvertitintoaforminwhichthenonnegativityconstraints are
theonlyinequality constraints, andtheremainingconstraints areequalities. Let
n
a x b (29.29)
ij j i

j 1
XD
29.1 Standardandslackforms 855
be an inequality constraint. We introduce a new variable s and rewrite inequal-
ity(29.29)asthetwoconstraints
n
s b a x ; (29.30)
i ij j
D (cid:0)
j 1
XD
s 0: (29.31)

We call s a slack variable because it measures the slack, or difference, between
theleft-hand andright-hand sides ofequation (29.29). (Weshall soonseewhywe
find it convenient to write the constraint with only the slack variable on the left-
hand side.) Because inequality (29.29) istrue ifand only if both equation (29.30)
and inequality (29.31) are true, wecan convert each inequality constraint ofalin-
ear program in this way to obtain an equivalent linear program in which the only
inequality constraints are the nonnegativity constraints. When converting from
standardtoslackform,weshallusex (insteadofs)todenotetheslackvariable
n i
associated withtheithinequality. TheCithconstraint istherefore
n
x b a x ; (29.32)
n i i ij j
C D (cid:0)
j 1
XD
alongwiththenonnegativity constraint x 0.
n i
C 
Byconverting eachconstraint ofalinearprogram instandard form,weobtaina
linear program in a different form. Forexample, for the linear program described
in(29.24)–(29.28), weintroduce slackvariablesx ,x ,andx ,obtaining
4 5 6
maximize 2x 3x 3x (29.33)
1 2 3
(cid:0) C
subjectto
x 7 x x x (29.34)
4 1 2 3
D (cid:0) (cid:0) C
x 7 x x x (29.35)
5 1 2 3
D (cid:0) C C (cid:0)
x 4 x 2x 2x (29.36)
6 1 2 3
D (cid:0) C (cid:0)
x ;x ;x ;x ;x ;x 0 : (29.37)
1 2 3 4 5 6

In this linear program, all the constraints except for the nonnegativity constraints
areequalities, and each variable issubject toanonnegativity constraint. Wewrite
eachequalityconstraintwithoneofthevariablesontheleft-handsideoftheequal-
ityandallothers ontheright-hand side. Furthermore, each equation hasthe same
set of variables on the right-hand side, and these variables are also the only ones
thatappear intheobjectivefunction. Wecallthevariables ontheleft-hand sideof
theequalities basicvariables andthoseontheright-hand sidenonbasicvariables.
For linear programs that satisfy these conditions, we shall sometimes omit the
words “maximize” and “subject to,” as well as the explicit nonnegativity con-
straints. Weshall alsousethevariable ´todenote thevalueoftheobjective func-
856 Chapter29 LinearProgramming
tion. Wecalltheresulting formatslackform. Ifwewritethelinearprogram given
in(29.33)–(29.37) inslackform,weobtain
´ 2x 3x 3x (29.38)
1 2 3
D (cid:0) C
x 7 x x x (29.39)
4 1 2 3
D (cid:0) (cid:0) C
x 7 x x x (29.40)
5 1 2 3
D (cid:0) C C (cid:0)
x 4 x 2x 2x : (29.41)
6 1 2 3
D (cid:0) C (cid:0)
As with standard form, we find it convenient to have a more concise notation
for describing aslack form. Asweshall see in Section 29.3, the sets of basic and
nonbasic variableswillchangeasthesimplexalgorithm runs. WeuseN todenote
the set of indices of the nonbasic variables and B to denote the set of indices of
the basic variables. We always have that N n, B m, and N B
j j D j j D [ D
1;2;:::;n m . TheequationsareindexedbytheentriesofB,andthevariables
f C g
ontheright-handsidesareindexedbytheentriesofN. Asinstandardform,weuse
b , c , and a to denote constant terms and coefficients. We also use  to denote
i j ij
an optional constant term inthe objective function. (Weshall see alittle later that
includingtheconstanttermintheobjectivefunctionmakesiteasytodeterminethe
value of the objective function.) Thus we can concisely define a slack form by a
tuple.N;B;A;b;c;/,denoting theslackform
´  c x (29.42)
j j
D C
j N
X2
x b a x fori B ; (29.43)
i i ij j
D (cid:0) 2
j N
X2
in which all variables x are constrained to be nonnegative. Because we subtract
the sum a x in (29.43), the values a are actually the negatives of the
j N ij j ij
coefficients a2sthey“appear” intheslackform.
P
Forexample,intheslackform
x x 2x
´ 28 3 5 6
D (cid:0) 6 (cid:0) 6 (cid:0) 3
x x x
x 8 3 5 6
1
D C 6 C 6 (cid:0) 3
8x 2x x
3 5 6
x 4
2
D (cid:0) 3 (cid:0) 3 C 3
x x
3 5
x 18 ;
4
D (cid:0) 2 C 2
wehaveB 1;2;4 ,N 3;5;6 ,
D f g D f g
29.1 Standardandslackforms 857
a a a 1=6 1=6 1=3
13 15 16
(cid:0) (cid:0)
A a a a 8=3 2=3 1=3 ;
23 25 26
D D (cid:0)
a a a 1=2 1=2 0
43 45 46
(cid:0)
(cid:0)  (cid:0) 
b 8
1
b b 4 ;
2
D D
b 18
4
(cid:0)  (cid:0) 
c c c c T 1=6 1=6 2=3 T , and  28. Note that the
3 5 6
D D (cid:0) (cid:0) (cid:0) D
indicesintoA,b,andc arenotnecessarilysetsofcontiguousintegers;theydepend
(cid:0)  (cid:0) 
onthe index sets B and N. Asanexample of theentries ofA being the negatives
ofthecoefficientsastheyappearintheslackform,observethattheequationforx
1
includesthetermx =6,yetthecoefficienta isactually 1=6ratherthan 1=6.
3 13
(cid:0) C
Exercises
29.1-1
If we express the linear program in (29.24)–(29.28) in the compact notation of
(29.19)–(29.21), whataren,m,A,b,andc?
29.1-2
Givethreefeasible solutions tothelinear program in(29.24)–(29.28). Whatisthe
objectivevalueofeachone?
29.1-3
Fortheslackformin(29.38)–(29.41), whatareN,B,A,b,c,and?
29.1-4
Convertthefollowinglinearprogram intostandard form:
minimize 2x 7x x
1 2 3
C C
subjectto
x x 7
1 3
(cid:0) D
3x x 24
1 2
C 
x 0
2

x 0 :
3

858 Chapter29 LinearProgramming
29.1-5
Convertthefollowinglinearprogram intoslackform:
maximize 2x 6x
1 3
(cid:0)
subject to
x x x 7
1 2 3
C (cid:0) 
3x x 8
1 2
(cid:0) 
x 2x 2x 0
1 2 3
(cid:0) C C 
x ;x ;x 0 :
1 2 3

Whatarethebasicandnonbasic variables?
29.1-6
Showthatthefollowinglinearprogram isinfeasible:
maximize 3x 2x
1 2
(cid:0)
subject to
x x 2
1 2
C 
2x 2x 10
1 2
(cid:0) (cid:0)  (cid:0)
x ;x 0 :
1 2

29.1-7
Showthatthefollowinglinearprogram isunbounded:
maximize x x
1 2
(cid:0)
subject to
2x x 1
1 2
(cid:0) C  (cid:0)
x 2x 2
1 2
(cid:0) (cid:0)  (cid:0)
x ;x 0 :
1 2

29.1-8
Supposethatwehaveagenerallinearprogramwithnvariablesandmconstraints,
and suppose that we convert it into standard form. Give an upper bound on the
numberofvariables andconstraints intheresulting linearprogram.
29.1-9
Giveanexampleofalinearprogram forwhichthefeasible regionisnotbounded,
buttheoptimalobjectivevalueisfinite.
29.2 Formulatingproblemsaslinearprograms 859
29.2 Formulatingproblems aslinearprograms
Although weshall focus onthesimplexalgorithm inthischapter, itisalso impor-
tanttobeabletorecognize whenwecanformulateaproblem asalinearprogram.
Once we cast a problem as a polynomial-sized linear program, we can solve it
in polynomial time by the ellipsoid algorithm or interior-point methods. Several
linear-programming softwarepackagescansolveproblemsefficiently,sothatonce
theproblemisintheformofalinearprogram,suchapackagecansolveit.
Weshalllookatseveralconcreteexamplesoflinear-programmingproblems. We
start with two problems that we have already studied: the single-source shortest-
pathsproblem (seeChapter24)andthemaximum-flowproblem (seeChapter 26).
We then describe the minimum-cost-flow problem. Although the minimum-cost-
flowproblemhasapolynomial-timealgorithmthatisnotbasedonlinearprogram-
ming,wewon’tdescribe thealgorithm. Finally, wedescribe themulticommodity-
flow problem, for which the only known polynomial-time algorithm is based on
linearprogramming.
When we solved graph problems in Part VI, we used attribute notation, such
as :d and .u;/:f. Linear programs typically use subscripted variables rather
than objects with attached attributes, however. Therefore, when we express vari-
ables in linear programs, we shall indicate vertices and edges through subscripts.
Forexample,wedenotetheshortest-path weightforvertex notby:dbutbyd .

Similarly,wedenote theflowfromvertexutovertex notby.u;/:f butbyf .
u
Forquantities thataregivenasinputstoproblems, suchasedgeweightsorcapac-
ities,weshallcontinue tousenotations suchasw.u;/andc.u;/.
Shortestpaths
We can formulate the single-source shortest-paths problem as a linear program.
In this section, we shall focus on how to formulate the single-pair shortest-path
problem, leaving the extension to the more general single-source shortest-paths
problemasExercise29.2-3.
Inthesingle-pairshortest-pathproblem,wearegivenaweighted,directedgraph
G .V;E/, with weight function w E R mapping edges to real-valued
D W !
weights, a source vertex s, and destination vertex t. We wish to compute the
value d , which is the weight of a shortest path from s to t. Toexpress this prob-
t
lemasalinearprogram,weneedtodetermineasetofvariablesandconstraintsthat
definewhenwehaveashortest pathfroms tot. Fortunately, theBellman-Fordal-
gorithm does exactly this. When the Bellman-Ford algorithm terminates, it has
computed, for each vertex , a value d (using subscript notation here rather than

attributenotation)suchthatforeachedge.u;/ E,wehaved d w.u;/.
 u
2  C
860 Chapter29 LinearProgramming
The source vertex initially receives a value d 0, which never changes. Thus
s
D
weobtainthefollowinglinearprogramtocomputetheshortest-path weightfroms
tot:
maximize d (29.44)
t
subject to
d d w.u;/ foreachedge.u;/ E ; (29.45)
 u
 C 2
d 0: (29.46)
s
D
You might be surprised that this linear program maximizes an objective function
when it is supposed to compute shortest paths. We do not want to minimize the
objective function, sincethensetting d 0forall V wouldyieldanoptimal
N
D 2
solution to the linear program without solving the shortest-paths problem. We
maximize because an optimal solution to the shortest-paths problem sets each d
N
to min d w.u;/ , so that d is the largest value that is less than or
u W.u;/ 2E Nu
C
N
equal to all of the values in the set d w.u;/ . We want to maximize d
˚ (cid:9) Nu C 
for all vertices  on a shortest path from s to t subject to these constraints on all
˚ (cid:9)
vertices,andmaximizingd achievesthisgoal.
t
This linear program has V variables d , one for each vertex  V. It also

j j 2
has E 1 constraints: one for each edge, plus the additional constraint that the
j jC
sourcevertex’sshortest-path weightalwayshasthevalue0.
Maximumflow
Next, weexpress the maximum-flow problem asa linear program. Recall that we
are given a directed graph G .V;E/ in which each edge .u;/ E has a
D 2
nonnegative capacity c.u;/ 0, and two distinguished vertices: a source s and

a sink t. As defined in Section 26.1, a flow is a nonnegative real-valued function
f V V R that satisfies the capacity constraint and flow conservation. A
W  !
maximum flow is a flow that satisfies these constraints and maximizes the flow
value,whichisthetotalflowcomingoutofthesourceminusthetotalflowintothe
source. A flow, therefore, satisfies linear constraints, and the value of a flow is a
linear function. Recalling also that weassume that c.u;/ 0 if.u;/ E and
D 62
thattherearenoantiparallel edges, wecanexpressthemaximum-flowproblem as
alinearprogram:
maximize f f (29.47)
s s
(cid:0)
 V  V
X2 X2
subject to
f c.u;/ foreachu; V ; (29.48)
u
 2
f f foreachu V s;t ; (29.49)
u u
D 2 (cid:0)f g
 V  V
X2 X2
f 0 foreachu; V : (29.50)
u
 2
29.2 Formulatingproblemsaslinearprograms 861
This linear program has V 2 variables, corresponding to the flow between each
pairofvertices, andithasj 2j V 2 V 2constraints.
j j Cj j(cid:0)
It is usually more efficient to solve a smaller-sized linear program. The linear
program in (29.47)–(29.50) has, for ease of notation, a flow and capacity of 0 for
eachpairofverticesu; with.u;/ E. Itwouldbemoreefficienttorewritethe
62
linear program so that it has O.V E/ constraints. Exercise 29.2-5 asks you to
C
doso.
Minimum-costflow
Inthis section, wehave used linear programming to solve problems for which we
already knew efficient algorithms. In fact, an efficient algorithm designed specif-
ically for a problem, such as Dijkstra’s algorithm for the single-source shortest-
paths problem, or the push-relabel method for maximum flow, will often be more
efficientthanlinearprogramming, bothintheoryandinpractice.
Therealpoweroflinearprogrammingcomesfromtheabilitytosolvenewprob-
lems. Recall the problem faced by the politician in the beginning of this chapter.
The problem of obtaining a sufficient number of votes, while not spending too
much money, is not solved by any of the algorithms that we have studied in this
book, yet we can solve it by linear programming. Books abound with such real-
world problems that linear programming can solve. Linear programming is also
particularly useful for solving variants of problems for which wemay not already
knowofanefficientalgorithm.
Consider,forexample,thefollowinggeneralization ofthemaximum-flowprob-
lem. Suppose that, in addition to a capacity c.u;/ for each edge .u;/, we are
givenareal-valuedcosta.u;/. Asinthemaximum-flowproblem,weassumethat
c.u;/ 0if.u;/ E,and that there arenoantiparallel edges. Ifwesend f
u
D 62
units of flow over edge .u;/, we incur a cost of a.u;/f . We are also given a
u
flowdemandd. Wewishtosendd unitsofflowfroms tot whileminimizingthe
total cost a.u;/f incurred bythe flow. Thisproblem isknown asthe
.u;/ E u
minimum-cost-fl2owproblem.
P
Figure 29.3(a) shows an example of the minimum-cost-flow problem. Wewish
to send 4 units of flow from s to t while incurring the minimum total cost. Any
particular legal flow, that is, a function f satisfying constraints (29.48)–(29.50),
incurs a total cost of a.u;/f . We wish to find the particular 4-unit
.u;/ E u
flowthatminimizesthiscost2. Figure29.3(b)showsanoptimalsolution, withtotal
P
cost a.u;/f .2 2/ .5 2/ .3 1/ .7 1/ .1 3/ 27:
.u;/ E u D  C  C  C  C  D
There are2 polynomial-time algorithms specifically designed for the minimum-
P
cost-flow problem, but they are beyond the scope of this book. We can, however,
express the minimum-cost-flow problem as a linear program. The linear program
looks similar to the one for the maximum-flow problem with the additional con-
862 Chapter29 LinearProgramming
c a= =5 2 x a c = = 7 2 2 a/ 5 = 2 x a =1 / 72
c = 1 1/1
s t s t
a = 3 a = 3
a c = = 5 2 y c a= =4 1 a =2 / 52 y 3 a/ 4 = 1
(a) (b)
Figure29.3 (a)Anexampleofaminimum-cost-flowproblem. Wedenotethecapacitiesbycand
thecostsbya. Vertexs isthesourceandvertext isthesink,andwewishtosend4unitsofflow
fromstot.(b)Asolutiontotheminimum-costflowprobleminwhich4unitsofflowaresentfroms
tot.Foreachedge,theflowandcapacityarewrittenasflow/capacity.
straint that the value of the flow be exactly d units, and with the new objective
function ofminimizingthecost:
minimize a.u;/f (29.51)
u
.u;/ E
subject to X2
f c.u;/ foreachu; V ;
u
 2
f f 0 foreachu V s;t ;
u u
(cid:0) D 2 (cid:0)f g
 V  V
X2 X2
f f d ;
s s
(cid:0) D
 V  V
X2 X2
f 0 foreachu; V : (29.52)
u
 2
Multicommodityflow
As a final example, we consider another flow problem. Suppose that the Lucky
Puck company from Section 26.1 decides to diversify its product line and ship
not only hockey pucks, but also hockey sticks and hockey helmets. Each piece of
equipment is manufactured in its own factory, has its own warehouse, and must
be shipped, each day, from factory to warehouse. The sticks are manufactured in
VancouverandmustbeshippedtoSaskatoon,andthehelmetsaremanufacturedin
Edmonton and must be shipped to Regina. The capacity of the shipping network
doesnotchange, however, andthedifferent items,orcommodities, mustsharethe
samenetwork.
Thisexampleisaninstanceofamulticommodity-flowproblem. Inthisproblem,
we are again given a directed graph G .V;E/ in which each edge .u;/ E
D 2
hasanonnegativecapacityc.u;/ 0. Asinthemaximum-flowproblem,weim-

plicitly assume thatc.u;/ 0for .u;/ E,and thatthegraph hasnoantipar-
D 62
29.2 Formulatingproblemsaslinearprograms 863
allel edges. In addition, we are given k different commodities, K ;K ;:::;K ,
1 2 k
where we specify commodity i by the triple K .s ;t ;d /. Here, vertex s is
i i i i i
D
the source of commodity i, vertex t is the sink of commodity i, and d is the de-
i i
mandforcommodityi,whichisthedesiredflowvalueforthecommodityfroms
i
tot . Wedefineaflowforcommodity i,denoted byf ,(sothatf istheflowof
i i iu
commodity i from vertex u to vertex ) to be a real-valued function that satisfies
theflow-conservation andcapacity constraints. Wenow definef , theaggregate
u
flow,tobethesumofthevariouscommodityflows,sothatf k f . The
u D i 1 iu
aggregate flow on edge .u;/ must be no more than the capacity ofDedge .u;/.
P
We are not trying to minimize any objective function in this problem; we need
onlydeterminewhethersuchaflowexists. Thus,wewritealinearprogram witha
“null”objectivefunction:
minimize 0
subjectto
k
f c.u;/ foreachu; V ;
iu
 2
i 1
XD
f f 0 foreachi 1;2;:::;k and
iu iu
(cid:0) D D
X 2V X 2V foreachu
2
V (cid:0)fs i;t
i
g
;
f f d foreachi 1;2;:::;k ;
i;si;
(cid:0)
i;;si
D
i
D
 V  V
X2 X2
f 0 foreachu; V and
iu
 2
foreachi 1;2;:::;k :
D
Theonlyknownpolynomial-timealgorithmforthisproblemexpressesitasalinear
programandthensolvesitwithapolynomial-time linear-programming algorithm.
Exercises
29.2-1
Putthesingle-pairshortest-pathlinearprogramfrom(29.44)–(29.46)intostandard
form.
29.2-2
Write out explicitly the linear program corresponding to finding the shortest path
fromnodes tonodey inFigure24.2(a).
29.2-3
In the single-source shortest-paths problem, we want to find the shortest-path
weights from a source vertex s to all vertices  V. Given a graph G, write a
2
864 Chapter29 LinearProgramming
linear program for which the solution has the property that d is the shortest-path

weightfroms to foreachvertex V.
2
29.2-4
Writeoutexplicitlythelinearprogramcorrespondingtofindingthemaximumflow
inFigure26.1(a).
29.2-5
Rewritethelinearprogram formaximumflow(29.47)–(29.50) sothatitusesonly
O.V E/constraints.
C
29.2-6
Write alinear program that, given abipartite graph G .V;E/, solves the maxi-
D
mum-bipartite-matching problem.
29.2-7
In the minimum-costmulticommodity-flow problem, weare given directed graph
G .V;E/inwhicheachedge.u;/ E hasanonnegativecapacityc.u;/ 0
D 2 
and a cost a.u;/. As in the multicommodity-flow problem, we are given k dif-
ferent commodities, K ;K ;:::;K , where wespecify commodity i bythe triple
1 2 k
K .s ;t ;d /. Wedefinetheflowf forcommodityi andtheaggregateflowf
i i i i i u
D
on edge .u;/ as in the multicommodity-flow problem. A feasible flow is one
in which the aggregate flow on each edge .u;/ is no more than the capacity of
edge .u;/. The cost of a flow is a.u;/f , and the goal is to find the
u; V u
feasible flowofminimumcost. Expresst2hisproblemasalinearprogram.
P
29.3 The simplex algorithm
Thesimplexalgorithm istheclassicalmethodforsolvinglinearprograms. Incon-
trasttomostoftheotheralgorithmsinthisbook,itsrunningtimeisnotpolynomial
in theworst case. It does yield insight into linear programs, however, and isoften
remarkably fastinpractice.
Inadditiontohavingageometricinterpretation, describedearlierinthischapter,
the simplex algorithm bears some similarity toGaussian elimination, discussed in
Section28.1. Gaussianeliminationbeginswithasystemoflinearequalitieswhose
solution is unknown. In each iteration, we rewrite this system in an equivalent
formthathassomeadditional structure. Aftersomenumberofiterations, wehave
rewritten the system so that the solution is simple to obtain. The simplex algo-
rithmproceedsinasimilarmanner,andwecanviewitasGaussianeliminationfor
inequalities.
29.3 Thesimplexalgorithm 865
We now describe the main idea behind an iteration of the simplex algorithm.
Associated with each iteration willbe a“basic solution” that wecan easily obtain
from the slack form of the linear program: set each nonbasic variable to 0 and
computethevaluesofthebasicvariablesfromtheequalityconstraints. Aniteration
converts one slack form into an equivalent slack form. The objective value of the
associated basicfeasiblesolution willbenolessthanthatatthepreviousiteration,
and usually greater. To achieve this increase in the objective value, we choose a
nonbasicvariablesuchthatifweweretoincreasethatvariable’svaluefrom0,then
the objective value would increase, too. The amount by which we can increase
thevariable islimitedbytheotherconstraints. Inparticular, weraiseituntilsome
basic variable becomes 0. We then rewrite the slack form, exchanging the roles
of that basic variable and the chosen nonbasic variable. Although we have used a
particular settingofthevariables toguidethealgorithm, andweshalluseitinour
proofs, the algorithm does not explicitly maintain this solution. Itsimply rewrites
thelinearprogram untilanoptimalsolutionbecomes“obvious.”
Anexampleofthesimplexalgorithm
We begin with an extended example. Consider the following linear program in
standardform:
maximize 3x x 2x (29.53)
1 2 3
C C
subjectto
x x 3x 30 (29.54)
1 2 3
C C 
2x 2x 5x 24 (29.55)
1 2 3
C C 
4x x 2x 36 (29.56)
1 2 3
C C 
x ;x ;x 0 : (29.57)
1 2 3

In order to use the simplex algorithm, we must convert the linear program into
slackform;wesawhowtodosoinSection29.1. Inadditiontobeinganalgebraic
manipulation, slack is a useful algorithmic concept. Recalling from Section 29.1
that each variable has a corresponding nonnegativity constraint, we say that an
equality constraint is tight for a particular setting of its nonbasic variables if they
cause the constraint’s basic variable to become 0. Similarly, a setting of the non-
basic variables that would make a basic variable become negative violates that
constraint. Thus, the slack variables explicitly maintain howfar each constraint is
from being tight, and so they help to determine how muchwecan increase values
ofnonbasic variables withoutviolating anyconstraints.
Associatingtheslackvariables x ,x ,andx withinequalities (29.54)–(29.56),
4 5 6
respectively, andputtingthelinearprogram intoslackform,weobtain
866 Chapter29 LinearProgramming
´ 3x x 2x (29.58)
1 2 3
D C C
x 30 x x 3x (29.59)
4 1 2 3
D (cid:0) (cid:0) (cid:0)
x 24 2x 2x 5x (29.60)
5 1 2 3
D (cid:0) (cid:0) (cid:0)
x 36 4x x 2x : (29.61)
6 1 2 3
D (cid:0) (cid:0) (cid:0)
The system of constraints (29.59)–(29.61) has 3 equations and 6 variables. Any
setting of the variables x , x , and x defines values for x , x , and x ; therefore,
1 2 3 4 5 6
we have an infinite number of solutions to this system of equations. A solution is
feasible if all of x ;x ;:::;x are nonnegative, and there can be an infinite num-
1 2 6
ber of feasible solutions as well. The infinite number of possible solutions to a
system such asthis onewillbeuseful inlater proofs. Wefocus on thebasic solu-
tion: set all the (nonbasic) variables on the right-hand side to 0 and then compute
the values of the (basic) variables on the left-hand side. In this example, the ba-
sic solution is .x ;x ;:::;x / .0;0;0;30;24;36/ and it has objective value
1 2 6
N N N D
´ .3 0/ .1 0/ .2 0/ 0. Observe that this basic solution sets x b
i i
D  C  C  D N D
foreachi B. Aniteration ofthesimplexalgorithm rewritesthesetofequations
2
and the objective function so as to put a different set of variables on the right-
handside. Thus,adifferentbasicsolutionisassociatedwiththerewrittenproblem.
We emphasize that the rewrite does not in any way change the underlying linear-
programmingproblem;theproblematoneiterationhastheidenticalsetoffeasible
solutions as the problem at the previous iteration. The problem does, however,
haveadifferentbasicsolutionthanthatofthepreviousiteration.
Ifabasicsolutionisalsofeasible,wecallitabasicfeasiblesolution. Aswerun
thesimplexalgorithm,thebasicsolutionisalmostalwaysabasicfeasiblesolution.
WeshallseeinSection29.5,however,thatforthefirstfewiterationsofthesimplex
algorithm, thebasicsolutionmightnotbefeasible.
Ourgoal,ineachiteration, istoreformulate thelinearprogram sothatthebasic
solution has a greater objective value. We select a nonbasic variable x whose
e
coefficient in the objective function ispositive, and weincrease the value ofx as
e
muchaspossiblewithoutviolatinganyoftheconstraints. Thevariablex becomes
e
basic, and some other variable x becomes nonbasic. The values of other basic
l
variables andoftheobjective functionmayalsochange.
To continue the example, let’s think about increasing the value of x . As we
1
increase x ,thevaluesofx ,x ,andx alldecrease. Becausewehaveanonnega-
1 4 5 6
tivityconstraintforeachvariable,wecannotallowanyofthemtobecomenegative.
If x increases above 30, then x becomes negative, and x and x become nega-
1 4 5 6
tivewhenx increasesabove12and9,respectively. Thethirdconstraint(29.61)is
1
the tightest constraint, and it limits how much we can increase x . Therefore, we
1
switchtherolesofx andx . Wesolveequation(29.61)forx andobtain
1 6 1
x x x
2 3 6
x 9 : (29.62)
1
D (cid:0) 4 (cid:0) 2 (cid:0) 4
29.3 Thesimplexalgorithm 867
Torewrite the other equations with x on theright-hand side, wesubstitute for x
6 1
usingequation (29.62). Doingsoforequation(29.59), weobtain
x 30 x x 3x
4 1 2 3
D (cid:0) (cid:0) x (cid:0) x x
30 9 2 3 6 x 3x
2 3
D (cid:0) (cid:0) 4 (cid:0) 2 (cid:0) 4 (cid:0) (cid:0)
3x 5x x 
21 2 3 6 : (29.63)
D (cid:0) 4 (cid:0) 2 C 4
Similarly,wecombineequation (29.62)withconstraint (29.60)andwithobjective
function(29.58)torewriteourlinearprogram inthefollowingform:
x x 3x
2 3 6
´ 27 (29.64)
D C 4 C 2 (cid:0) 4
x x x
2 3 6
x 9 (29.65)
1
D (cid:0) 4 (cid:0) 2 (cid:0) 4
3x 5x x
2 3 6
x 21 (29.66)
4
D (cid:0) 4 (cid:0) 2 C 4
3x x
2 6
x 6 4x : (29.67)
5 3
D (cid:0) 2 (cid:0) C 2
Wecall thisoperation apivot. Asdemonstrated above, apivotchooses anonbasic
variablex ,calledtheenteringvariable,andabasicvariablex ,calledtheleaving
e l
variable, andexchanges theirroles.
The linear program described in equations (29.64)–(29.67) is equivalent to the
linearprogramdescribedinequations(29.58)–(29.61). Weperformtwooperations
inthesimplexalgorithm: rewriteequationssothatvariablesmovebetweentheleft-
handsideandtheright-handside,andsubstituteoneequationintoanother. Thefirst
operation trivially creates an equivalent problem, and the second, by elementary
linearalgebra, alsocreatesanequivalent problem. (SeeExercise29.3-3.)
To demonstrate this equivalence, observe that our original basic solution .0;0;
0;30;24;36/ satisfies the new equations (29.65)–(29.67) and has objective value
27 .1=4/ 0 .1=2/ 0 .3=4/ 36 0. Thebasicsolutionassociatedwiththe
C  C  (cid:0)  D
new linear program sets the nonbasic values to 0and is.9;0;0;21;6;0/, withob-
jective value ´ 27. Simple arithmetic verifies that this solution also satisfies
D
equations (29.59)–(29.61) and, when plugged into objective function (29.58), has
objectivevalue.3 9/ .1 0/ .2 0/ 27.
 C  C  D
Continuingtheexample,wewishtofindanewvariablewhosevaluewewishto
increase. Wedonotwanttoincrease x ,since asitsvalue increases, theobjective
6
valuedecreases. Wecanattempttoincreaseeitherx orx ;letuschoosex . How
2 3 3
farcanweincreasex withoutviolating anyoftheconstraints? Constraint(29.65)
3
limits it to 18, constraint (29.66) limits it to 42=5, and constraint (29.67) limits
it to 3=2. The third constraint is again the tightest one, and therefore we rewrite
the third constraint so that x is on the left-hand side and x is on the right-hand
3 5
868 Chapter29 LinearProgramming
side. Wethensubstitutethisnewequation, x 3=2 3x =8 x =4 x =8,into
3 2 5 6
D (cid:0) (cid:0) C
equations (29.64)–(29.66) andobtainthenew,butequivalent, system
111 x x 11x
2 5 6
´ (29.68)
D 4 C 16 (cid:0) 8 (cid:0) 16
33 x x 5x
2 5 6
x (29.69)
1
D 4 (cid:0) 16 C 8 (cid:0) 16
3 3x x x
2 5 6
x (29.70)
3
D 2 (cid:0) 8 (cid:0) 4 C 8
69 3x 5x x
x 2 5 6 : (29.71)
4
D 4 C 16 C 8 (cid:0) 16
This system has the associated basic solution .33=4;0;3=2;69=4;0;0/, with ob-
jective value 111=4. Now the only way to increase the objective value is to in-
crease x . Thethreeconstraints giveupperbounds of132, 4,and ,respectively.
2
1
(Wegetanupper bound of from constraint (29.71) because, asweincrease x ,
2
1
the value ofthe basic variable x increases also. Thisconstraint, therefore, places
4
no restriction on how much we can increase x .) We increase x to 4, and it be-
2 2
comesnonbasic. Thenwesolveequation (29.70)forx andsubstitute intheother
2
equations toobtain
x x 2x
3 5 6
´ 28 (29.72)
D (cid:0) 6 (cid:0) 6 (cid:0) 3
x x x
3 5 6
x 8 (29.73)
1
D C 6 C 6 (cid:0) 3
8x 2x x
3 5 6
x 4 (29.74)
2
D (cid:0) 3 (cid:0) 3 C 3
x x
3 5
x 18 : (29.75)
4
D (cid:0) 2 C 2
Atthispoint,allcoefficientsintheobjectivefunctionarenegative. Asweshallsee
later in this chapter, this situation occurs only when we have rewritten the linear
program so that the basic solution is an optimal solution. Thus, for this problem,
the solution .8;4;0;18;0;0/, with objective value 28, is optimal. We can now
return to our original linear program given in (29.53)–(29.57). Theonly variables
in the original linear program are x , x , and x , and so our solution is x 8,
1 2 3 1
D
x 4, and x 0, with objective value .3 8/ .1 4/ .2 0/ 28. Note
2 3
D D  C  C  D
thatthevaluesoftheslack variables inthefinalsolution measurehowmuchslack
remains in each inequality. Slack variable x is 18, and in inequality (29.54), the
4
left-handside,withvalue8 4 0 12,is18lessthantheright-handsideof30.
C C D
Slack variables x and x are 0 and indeed, in inequalities (29.55) and (29.56),
5 6
the left-hand and right-hand sides are equal. Observe also that even though the
coefficients in the original slack form are integral, the coefficients in the other
linear programsarenotnecessarily integral, andtheintermediate solutions arenot
29.3 Thesimplexalgorithm 869
necessarily integral. Furthermore, the final solution to a linear program need not
beintegral; itispurelycoincidental thatthisexamplehasanintegralsolution.
Pivoting
We now formalize the procedure for pivoting. The procedure PIVOT takes as in-
put a slack form, given by the tuple .N;B;A;b;c;/, the index l of the leav-
ing variable x , and the index e of the entering variable x . It returns the tuple
l e
.N;B;A;b;c;/ describing the new slack form. (Recall again that the entries of
y y y y y y
them nmatricesAandAareactuallythenegativesofthecoefficientsthatappear
 y
intheslackform.)
PIVOT.N;B;A;b;c;;l;e/
1 //Computethecoefficients oftheequation fornewbasicvariablex .
e
2 letAbeanewm nmatrix
y 
3 b b =a
ye
D
l le
4 foreachj N e
2 (cid:0)f g
5 a a =a
ej lj le
y D
6 a 1=a
el le
y D
7 //Computethecoefficients oftheremainingconstraints.
8 foreachi B l
2 (cid:0)f g
9 b b a b
yi
D
i
(cid:0)
ieye
10 foreachj N e
2 (cid:0)f g
11 a a a a
ij ij ie ej
y D (cid:0) y
12 a a a
il ie el
y D (cid:0) y
13 //Computetheobjectivefunction.
14   c b
y D C
eye
15 foreachj N e
2 (cid:0)f g
16 c c c a
j j e ej
y D (cid:0) y
17 c c a
l e el
y D (cid:0) y
18 //Computenewsetsofbasicandnonbasic variables.
19 N N e l
y D (cid:0)f g[f g
20 B B l e
y D (cid:0)f g[f g
21 return.N;B;A;b;c;/
y y y y y y
PIVOTworksasfollows. Lines3–6computethecoefficientsinthenewequation
forx byrewritingtheequation thathasx ontheleft-handsidetoinsteadhavex
e l e
on the left-hand side. Lines 8–12 update the remaining equations by substituting
the right-hand side of this new equation for each occurrence of x . Lines 14–17
e
dothesamesubstitution fortheobjective function, andlines 19and 20update the
870 Chapter29 LinearProgramming
setsofnonbasicandbasicvariables. Line21returnsthenewslackform. Asgiven,
ifa
le
0,PIVOT wouldcauseanerrorbydividingby0,butasweshallseeinthe
D
proofsofLemmas29.2and29.12,wecall PIVOT onlywhena
le
0.
¤
We now summarize the effect that PIVOT has on the values of the variables in
thebasicsolution.
Lemma29.1
Consider a call to PIVOT.N;B;A;b;c;;l;e/ in which a
le
0. Let the values
¤
returnedfromthecallbe.N;B;A;b;c;/,andletxdenotethebasicsolutionafter
y y y y y y N
thecall. Then
1. x 0foreachj N.
Nj D 2 y
2. x b =a .
e l le
N D
3. x b a b foreachi B e .
Ni D i (cid:0) ieye 2 y(cid:0)f g
Proof The first statement is true because the basic solution always sets all non-
basicvariablesto0. Whenweseteachnonbasic variableto0inaconstraint
x b a x ;
i
D
yi
(cid:0)
yij j
jX 2N
y
wehavethatx
Ni
D
b
yi
foreachi
2
B y. Sincee 2B y,line3of PIVOT gives
x b b =a ;
Ne
D
ye
D
l le
which proves the second statement. Similarly, using line 9 for each i B e ,
2 y (cid:0)f g
wehave
x b b a b ;
Ni
D
yi
D
i
(cid:0)
ieye
whichprovesthethirdstatement.
Theformalsimplexalgorithm
Weare now ready to formalize the simplex algorithm, which wedemonstrated by
example. Thatexamplewasaparticularly niceone,andwecouldhavehadseveral
otherissuestoaddress:
 Howdowedetermine whetheralinearprogram isfeasible?
 Whatdowedoifthelinearprogram isfeasible, buttheinitialbasic solution is
notfeasible?
 Howdowedetermine whetheralinearprogram isunbounded?
 Howdowechoose theentering andleaving variables?
29.3 Thesimplexalgorithm 871
In Section 29.5, we shall show how to determine whether a problem is feasible,
and if so, how to find a slack form in which the initial basic solution is feasible.
Therefore, let us assume that wehave aprocedure INITIALIZE-SIMPLEX.A;b;c/
that takes as input a linear program in standard form, that is, an m n matrix

A .a /, an m-vector b .b /, and an n-vector c .c /. If the problem is
ij i j
D D D
infeasible, theprocedure returnsamessagethattheprogram isinfeasible andthen
terminates. Otherwise, the procedure returns a slack form for which the initial
basicsolution isfeasible.
TheprocedureSIMPLEX takesasinputalinearprograminstandardform,asjust
described. It returns an n-vector x .x / that is an optimal solution to the linear
j
N D N
programdescribed in(29.19)–(29.21).
SIMPLEX.A;b;c/
1 .N;B;A;b;c;/ INITIALIZE-SIMPLEX.A;b;c/
D
2 letbeanewvectoroflengthm
3 whilesomeindexj N hasc > 0
j
2
4 chooseanindexe N forwhichc > 0
e
2
5 foreachindexi B
2
6 ifa > 0
ie
7  b =a
i i ie
D
8 else
i
D 1
9 chooseanindexl B thatminimizes
l
2
10 if ==
l
1
11 return“unbounded”
12 else.N;B;A;b;c;/ PIVOT.N;B;A;b;c;;l;e/
D
13 fori 1ton
D
14 ifi B
2
15 x b
i i
N D
16 elsex 0
i
N D
17 return.x ;x ;:::;x /
1 2 n
N N N
The SIMPLEX procedure works as follows. In line 1, it calls the procedure
INITIALIZE-SIMPLEX.A;b;c/, described above, which either determines that the
linear program is infeasible or returns aslack form forwhich the basic solution is
feasible. Thewhileloop of lines 3–12 formsthe mainpart ofthealgorithm. Ifall
coefficients in the objective function are negative, then the while loop terminates.
Otherwise, line 4selects avariable x , whose coefficient in the objective function
e
ispositive, astheentering variable. Althoughwemaychooseanysuchvariable as
theentering variable, weassume thatweusesomeprespecified deterministic rule.
Next, lines 5–9 check each constraint and pick the one that most severely limits
the amount by which we can increase x without violating any of the nonnegativ-
e
872 Chapter29 LinearProgramming
ity constraints; the basic variable associated with this constraint is x . Again, we
l
are free to choose one of several variables as the leaving variable, but we assume
that we use some prespecified deterministic rule. If none of the constraints lim-
its the amount by which the entering variable can increase, the algorithm returns
“unbounded” in line 11. Otherwise, line 12 exchanges the roles of the entering
and leaving variables by calling PIVOT.N;B;A;b;c;;l;e/, as described above.
Lines13–16computeasolutionx ;x ;:::;x fortheoriginallinear-programming
1 2 n
N N N
variables bysettingallthenonbasicvariables to0andeachbasicvariablex tob ,
i i
N
andline17returnsthesevalues.
To show that SIMPLEX is correct, we first show that if SIMPLEX has an initial
feasiblesolutionandeventuallyterminates,theniteitherreturnsafeasiblesolution
ordeterminesthatthelinearprogramisunbounded. Then,weshowthatSIMPLEX
terminates. Finally, in Section 29.4 (Theorem 29.10) we show that the solution
returned isoptimal.
Lemma29.2
Givenalinearprogram .A;b;c/,suppose thatthecalltoINITIALIZE-SIMPLEX in
line 1 of SIMPLEX returns a slack form for which the basic solution is feasible.
ThenifSIMPLEX returnsasolutioninline17,thatsolutionisafeasiblesolutionto
thelinearprogram. IfSIMPLEX returns“unbounded”inline11,thelinearprogram
isunbounded.
Proof Weusethefollowingthree-part loopinvariant:
Atthestartofeachiteration ofthewhileloopoflines3–12,
1. the slack form is equivalent to the slack form returned by the call of
INITIALIZE-SIMPLEX,
2. foreachi B,wehaveb 0,and
i
2 
3. thebasicsolution associated withtheslackformisfeasible.
Initialization: The equivalence of the slack forms is trivial for the first itera-
tion. We assume, in the statement of the lemma, that the call to INITIALIZE-
SIMPLEXinline1ofSIMPLEXreturnsaslackformforwhichthebasicsolution
is feasible. Thus, the third part of the invariant is true. Because the basic so-
lutionisfeasible, eachbasicvariable x isnonnegative. Furthermore, sincethe
i
basic solution sets each basic variable x to b , we have that b 0 for all
i i i

i B. Thus,thesecondpartoftheinvariant holds.
2
Maintenance: We shall show that each iteration of the while loop maintains the
loop invariant, assuming that the return statement inline 11does notexecute.
Weshallhandlethecaseinwhichline11executeswhenwediscusstermination.
29.3 Thesimplexalgorithm 873
An iteration of the while loop exchanges the role of a basic and a nonbasic
variable bycalling the PIVOT procedure. ByExercise29.3-3, theslackformis
equivalent to the one from the previous iteration which, by the loop invariant,
isequivalent totheinitialslackform.
We now demonstrate the second part of the loop invariant. We assume that at
thestartofeachiterationofthewhileloop,b 0foreachi B,andweshall
i
 2
showthattheseinequalitiesremaintrueafterthecalltoPIVOTinline12. Since
theonlychangestothevariablesb andthesetB ofbasicvariablesoccurinthis
i
assignment, it suffices to show that line 12 maintains this part of the invariant.
We let b i, a ij, and B refer to values before the call of PIVOT, and b
yi
refer to
valuesreturned from PIVOT.
First, weobserve that b 0because b 0bytheloop invariant, a > 0by
ye

l

le
lines6and9of SIMPLEX, andb
ye
D
b l=a
le
byline3ofPIVOT.
Fortheremainingindicesi B l ,wehavethat
2 (cid:0)f g
b
yi
D
b
i
(cid:0)a ieb
ye
(byline9of PIVOT)
b
i
a ie.b l=a le/ (byline3of PIVOT) . (29.76)
D (cid:0)
We have two cases to consider, depending on whether a > 0 or a 0.
ie ie

Ifa > 0,thensincewechosel suchthat
ie
b =a b =a foralli B ; (29.77)
l le i ie
 2
wehave
b b a .b =a / (byequation (29.76))
yi
D
i
(cid:0)
ie l le
b a .b =a / (byinequality (29.77))
i ie i ie
 (cid:0)
b b
i i
D (cid:0)
0;
D
and thus b 0. If a 0, then because a , b , and b are all nonnegative,
yi

ie

le i l
equation (29.76)impliesthatb mustbenonnegative, too.
yi
Wenowarguethatthebasicsolutionisfeasible,i.e.,thatallvariableshavenon-
negative values. The nonbasic variables are set to 0 and thus are nonnegative.
Eachbasicvariable x isdefinedbytheequation
i
x b a x :
i i ij j
D (cid:0)
j N
X2
Thebasicsolutionsetsx b . Usingthesecondpartoftheloopinvariant, we
i i
N D
conclude thateachbasicvariablex isnonnegative.
i
N
874 Chapter29 LinearProgramming
Termination: The while loop can terminate in one of two ways. If it terminates
becauseoftheconditioninline3,thenthecurrentbasicsolutionisfeasibleand
line 17 returns this solution. The other way it terminates is by returning “un-
bounded” inline11. Inthiscase,foreachiteration oftheforloopinlines5–8,
whenline6isexecuted,wefindthata 0. Considerthesolutionxdefinedas
ie
 N
ifi e ;
1 D
x 0 ifi N e ;
i
N D 2 (cid:0)f g
b a x ifi B :
 i (cid:0) j 2N ij Nj 2
P
We now show that this solution is feasible, i.e., that all variables are nonneg-
ative. The nonbasic variables other than x are 0, and x > 0; thus all
e e
N N D 1
nonbasic variablesarenonnegative. Foreachbasicvariable x ,wehave
i
N
x b a x
i i ij j
N D (cid:0) N
j N
X2
b a x :
i ie e
D (cid:0) N
Theloopinvariant impliesthatb 0,andwehavea 0andx > 0.
i ie e
  N D 1
Thus,x 0.
i
N 
Now we show that the objective value for the solution x is unbounded. From
N
equation (29.42),theobjectivevalueis
´  c x
j j
D C N
j N
X2
 c x :
e e
D C N
Since c
e
> 0 (by line 4 of SIMPLEX) and x
e
, the objective value is ,
N D 1 1
andthusthelinearprogramisunbounded.
It remains to show that SIMPLEX terminates, and when it does terminate, the
solutionitreturnsisoptimal. Section29.4willaddressoptimality. Wenowdiscuss
termination.
Termination
Intheexamplegiveninthebeginning ofthissection, eachiteration ofthesimplex
algorithm increased theobjective value associated withthebasic solution. AsEx-
ercise29.3-2asksyoutoshow,noiterationofSIMPLEX candecreasetheobjective
value associated withthebasic solution. Unfortunately, itispossible that anitera-
tionleavestheobjectivevalueunchanged. Thisphenomenoniscalleddegeneracy,
andweshallnowstudyitingreaterdetail.
29.3 Thesimplexalgorithm 875
Theassignmentinline14ofPIVOT,
y D
 Cc eb ye,changestheobjectivevalue.
Since SIMPLEX calls PIVOT only when c
e
> 0, the only way for the objective
value to remain unchanged (i.e.,  ) is for b to be 0. This value is assigned
y D
ye
asb
ye
D
b l=a
le
in line 3of PIVOT. Since wealways call PIVOT with a
le
¤
0, we
seethatforb toequal0,andhence theobjective valuetobeunchanged, wemust
ye
haveb 0.
l
D
Indeed,thissituationcanoccur. Considerthelinearprogram
´ x x x
1 2 3
D C C
x 8 x x
4 1 2
D (cid:0) (cid:0)
x x x :
5 2 3
D (cid:0)
Suppose thatwechoose x astheentering variable andx astheleaving variable.
1 4
Afterpivoting, weobtain
´ 8 x x
3 4
D C (cid:0)
x 8 x x
1 2 4
D (cid:0) (cid:0)
x x x :
5 2 3
D (cid:0)
At this point, our only choice is to pivot with x entering and x leaving. Since
3 5
b 0,theobjectivevalueof8remainsunchanged afterpivoting:
5
D
´ 8 x x x
2 4 5
D C (cid:0) (cid:0)
x 8 x x
1 2 4
D (cid:0) (cid:0)
x x x :
3 2 5
D (cid:0)
The objective value has not changed, but our slack form has. Fortunately, if we
pivot again, withx entering andx leaving, the objective value increases (to 16),
2 1
andthesimplexalgorithm cancontinue.
Degeneracy canpreventthesimplexalgorithm from terminating, because itcan
lead to a phenomenon known as cycling: the slack forms at two different itera-
tions of SIMPLEX are identical. Because ofdegeneracy, SIMPLEX could choose a
sequence of pivot operations that leave the objective value unchanged but repeat
a slack form within the sequence. Since SIMPLEX is a deterministic algorithm, if
it cycles, then it will cycle through the same series of slack forms forever, never
terminating.
CyclingistheonlyreasonthatSIMPLEX mightnotterminate. Toshowthisfact,
wemustfirstdevelopsomeadditional machinery.
At each iteration, SIMPLEX maintains A, b, c, and  in addition to the sets
N and B. Although we need to explicitly maintain A, b, c, and  in order to
implement the simplex algorithm efficiently, we can get by without maintaining
them. In other words, the sets of basic and nonbasic variables suffice to uniquely
determine the slack form. Before proving this fact, we prove a useful algebraic
lemma.
876 Chapter29 LinearProgramming
Lemma29.3
LetI beasetofindices. Foreachj I,let˛ andˇ berealnumbers,andletx
j j j
2
be areal-valued variable. Let be anyreal number. Suppose that for anysettings
ofthex ,wehave
j
˛ x ˇ x : (29.78)
j j j j
D C
j I j I
X2 X2
Then˛ ˇ foreachj I,and 0.
j j
D 2 D
Proof Sinceequation(29.78)holdsforanyvaluesofthex ,wecanuseparticular
j
values to draw conclusions about ˛, ˇ, and . If we let x 0 for each j I,
j
D 2
we conclude that 0. Now pick an arbitrary index j I, and set x 1 and
j
D 2 D
x 0 for all k j. Then we must have ˛ ˇ . Since we picked j as any
k j j
D ¤ D
indexinI,weconclude that˛ ˇ foreachj I.
j j
D 2
Aparticularlinearprogramhasmanydifferentslackforms;recallthateachslack
form has the same set of feasible and optimal solutions as the original linear pro-
gram. Wenowshowthattheslackformofalinearprogramisuniquelydetermined
bythesetofbasicvariables. Thatis,giventhesetofbasicvariables,auniqueslack
form(uniquesetofcoefficientsandright-handsides)isassociatedwiththosebasic
variables.
Lemma29.4
Let.A;b;c/bealinearprograminstandardform. GivenasetB ofbasicvariables,
theassociated slackformisuniquely determined.
Proof Assume forthe purpose of contradiction that there are twodifferent slack
forms with the same set B of basic variables. The slack forms must also have
identicalsetsN 1;2;:::;n m B ofnonbasicvariables. Wewritethefirst
D f C g(cid:0)
slackformas
´  c x (29.79)
j j
D C
j N
X2
x b a x fori B ; (29.80)
i i ij j
D (cid:0) 2
j N
X2
andthesecondas
´  c x (29.81)
D
0
C
j0 j
j N
X2
x b a x fori B : (29.82)
i
D
i0
(cid:0)
i0j j
2
j N
X2
29.3 Thesimplexalgorithm 877
Consider the system of equations formed by subtracting each equation in
line (29.82) from the corresponding equation in line (29.80). The resulting sys-
temis
0 .b b / .a a /x fori B
D
i
(cid:0)
i0
(cid:0)
ij
(cid:0)
i0j j
2
j N
X2
or,equivalently,
a x .b b / a x fori B :
ij j
D
i
(cid:0)
i0
C
i0j j
2
j N j N
X2 X2
Now,foreachi B,applyLemma29.3with˛ a ,ˇ a , b b ,and
2
j
D
ij j
D
i0j
D
i
(cid:0)
i0
I N. Since˛ ˇ ,wehavethata a foreachj N,andsince 0,
D
j
D
j ij
D
i0j
2 D
we have that b b . Thus, for the two slack forms, A and b are identical to A
i
D
i0 0
and b . Using a similar argument, Exercise 29.3-1 shows that it must also be the
0
casethatc c and  ,andhencethattheslackformsmustbeidentical.
0 0
D D
Wecan now show that cycling is the only possible reason that SIMPLEX might
notterminate.
Lemma29.5
n m
IfSIMPLEX failstoterminateinatmost C iterations, thenitcycles.
m
(cid:0) 
Proof ByLemma29.4, the set B ofbasic variables uniquely determines a slack
form. There are n m variables and B m, and therefore, there are at most
n Cm ways to chooC se B. Thus, there aj rej oD nly at most n Cm unique slack forms.
m m
n m
(cid:0)There fore, ifSIMPLEX runsformorethan C
m
iteratio (cid:0)ns, it mustcycle.
(cid:0) 
Cyclingistheoreticallypossible,butextremelyrare. Wecanpreventitbychoos-
ing the entering and leaving variables somewhat more carefully. One option is to
perturb the input slightly so that it is impossible to have two solutions with the
sameobjective value. Anotheroption istobreak tiesbyalwayschoosing thevari-
ablewiththesmallest index, astrategy knownasBland’srule. Weomittheproof
thatthesestrategies avoidcycling.
Lemma29.6
If lines 4 and 9 of SIMPLEX always break ties by choosing the variable with the
smallestindex, then SIMPLEX mustterminate.
Weconclude thissection withthefollowinglemma.
878 Chapter29 LinearProgramming
Lemma29.7
Assuming that INITIALIZE-SIMPLEX returns aslack form for which thebasic so-
lutionisfeasible, SIMPLEX eitherreportsthatalinearprogram isunbounded, orit
n m
terminates withafeasible solutioninatmost Cm iterations.
(cid:0) 
Proof Lemmas29.2and 29.6show that if INITIALIZE-SIMPLEX returns aslack
form for which the basic solution is feasible, SIMPLEX either reports that a linear
program is unbounded, or it terminates with a feasible solution. By the contra-
positive of Lemma 29.5, if SIMPLEX terminates with a feasible solution, then it
n m
terminates inatmost Cm iterations.
(cid:0) 
Exercises
29.3-1
CompletetheproofofLemma29.4byshowingthatitmustbethecasethatc c
0
D
and  .
0
D
29.3-2
ShowthatthecalltoPIVOT inline12ofSIMPLEX neverdecreases thevalueof.
29.3-3
ProvethattheslackformgiventothePIVOT procedure andtheslackformthatthe
procedure returns areequivalent.
29.3-4
Suppose we convert a linear program .A;b;c/ in standard form to slack form.
Showthatthebasicsolution isfeasible ifandonlyifb 0fori 1;2;:::;m.
i
 D
29.3-5
Solvethefollowinglinearprogram using SIMPLEX:
maximize 18x 12:5x
1 2
C
subject to
x x 20
1 2
C 
x 12
1

x 16
2

x ;x 0 :
1 2

29.4 Duality 879
29.3-6
Solvethefollowinglinearprogramusing SIMPLEX:
maximize 5x 3x
1 2
(cid:0)
subjectto
x x 1
1 2
(cid:0) 
2x x 2
1 2
C 
x ;x 0 :
1 2

29.3-7
Solvethefollowinglinearprogramusing SIMPLEX:
minimize x x x
1 2 3
C C
subjectto
2x 7:5x 3x 10000
1 2 3
C C 
20x 5x 10x 30000
1 2 3
C C 
x ;x ;x 0 :
1 2 3

29.3-8
m n
IntheproofofLemma29.5,wearguedthatthereareatmost C waystochoose
n
asetB ofbasicvariables. Giveanexampleofalinearprogram inwhichthere are
strictlyfewerthan m Cnn waystochoosethesetB. (cid:0) 
(cid:0) 
29.4 Duality
Wehaveproventhat,undercertainassumptions,SIMPLEXterminates. Wehavenot
yet shown that it actually finds an optimal solution to a linear program, however.
In order to do so, we introduce a powerful concept called linear-programming
duality.
Duality enables usto prove thatasolution isindeed optimal. Wesawan exam-
ple of duality in Chapter 26 with Theorem 26.6, the max-flow min-cut theorem.
Suppose that, given an instance of a maximum-flow problem, we find a flow f
withvalue f . Howdoweknowwhetherf isamaximumflow? Bythemax-flow
j j
min-cut theorem, if we can find a cut whose value is also f , then we have ver-
j j
ified that f is indeed a maximum flow. This relationship provides an example of
duality: givenamaximization problem, wedefinearelated minimization problem
suchthatthetwoproblemshavethesameoptimalobjective values.
Givenalinearprogram inwhichtheobjective istomaximize,weshalldescribe
how toformulate adual linear program in which the objective isto minimize and
880 Chapter29 LinearProgramming
whoseoptimalvalueisidenticaltothatoftheoriginallinearprogram. Whenrefer-
ringtoduallinearprograms, wecalltheoriginal linearprogramtheprimal.
Givenaprimallinearprograminstandardform,asin(29.16)–(29.18),wedefine
theduallinearprogram as
m
minimize b y (29.83)
i i
i 1
XD
subject to
m
a y c forj 1;2;:::;n; (29.84)
ij i j
 D
i 1
XD
y 0 fori 1;2;:::;m: (29.85)
i
 D
Toform thedual, wechange themaximization toaminimization, exchange the
rolesofcoefficientsontheright-hand sidesandtheobjectivefunction, andreplace
eachless-than-or-equal-to byagreater-than-or-equal-to. Eachofthemconstraints
intheprimalhasanassociatedvariabley inthedual,andeachofthenconstraints
i
in the dual has an associated variable x in the primal. For example, consider the
j
linearprogram givenin(29.53)–(29.57). Thedualofthislinearprogram is
minimize 30y 24y 36y (29.86)
1 2 3
C C
subject to
y 2y 4y 3 (29.87)
1 2 3
C C 
y 2y y 1 (29.88)
1 2 3
C C 
3y 5y 2y 2 (29.89)
1 2 3
C C 
y ;y ;y 0 : (29.90)
1 2 3

We shall show in Theorem 29.10 that the optimal value of the dual linear pro-
gram is always equal to the optimal value of the primal linear program. Further-
more,thesimplexalgorithmactuallyimplicitlysolvesboththeprimalandthedual
linearprogramssimultaneously, thereby providingaproofofoptimality.
We begin by demonstrating weak duality, which states that any feasible solu-
tion to the primal linear program has a value no greater than that of any feasible
solution totheduallinearprogram.
Lemma29.8(Weaklinear-programming duality)
Letx beanyfeasible solution totheprimallinear program in(29.16)–(29.18) and
N
lety beanyfeasible solution totheduallinear program in(29.83)–(29.85). Then,
N
wehave
n m
c x b y :
j j i i
N  N
j 1 i 1
XD XD
29.4 Duality 881
Proof Wehave
n n m
c x a y x (byinequalities (29.84))
j j ij i j
N  N N
!
j 1 j 1 i 1
XD XD XD
m n
a x y
ij j i
D N N
!
i 1 j 1
XD XD
m
b y (byinequalities (29.17)).
i i
 N
i 1
XD
Corollary29.9
Let x be a feasible solution to a primal linear program .A;b;c/, and let y be a
N N
feasiblesolution tothecorresponding duallinearprogram. If
n m
c x b y ;
j j i i
N D N
j 1 i 1
XD XD
thenx andy areoptimalsolutions totheprimalanddual linearprograms, respec-
N N
tively.
Proof By Lemma 29.8, the objective value of a feasible solution to the primal
cannot exceed that ofafeasible solution tothedual. Theprimal linear program is
amaximization problem andthedualisaminimization problem. Thus, iffeasible
solutions x andy havethesameobjective value,neithercanbeimproved.
N N
Before proving that there always is a dual solution whose value is equal to that
of an optimal primal solution, we describe how to find such a solution. When
we ran the simplex algorithm on the linear program in (29.53)–(29.57), the final
iteration yielded the slack form (29.72)–(29.75) with objective ´ 28 x =6
3
D (cid:0) (cid:0)
x =6 2x =3,B 1;2;4 ,andN 3;5;6 . Asweshallshowbelow,thebasic
5 6
(cid:0) D f g D f g
solution associated with the final slack form is indeed an optimal solution to the
linear program; anoptimal solution tolinear program (29.53)–(29.57) istherefore
.x ;x ;x / .8;4;0/, with objective value .3 8/ .1 4/ .2 0/ 28. As
1 2 3
N N N D  C  C  D
wealsoshowbelow,wecanreadoffanoptimaldualsolution: thenegativesofthe
coefficients of the primal objective function are the values of the dual variables.
Moreprecisely, suppose thatthelastslackformoftheprimalis
´  c x
D
0
C
j0 j
j N
X2
x b a x fori B :
i
D
i0
(cid:0)
i0j j
2
j N
X2
882 Chapter29 LinearProgramming
Then,toproduce anoptimaldualsolution, weset
c if.n i/ N ;
y (cid:0) n0 i C 2 (29.91)
i C
N D
(
0 otherwise:
Thus, anoptimal solution tothedual linear program defined in(29.86)–(29.90)
is y 0 (since n 1 4 B), y c 1=6, and y c 2=3.
N1
D C D 2
N2
D (cid:0)
50
D
N3
D (cid:0)
60
D
Evaluating the dual objective function (29.86), we obtain an objective value of
.30 0/ .24 .1=6// .36 .2=3// 28,whichconfirmsthattheobjectivevalue
 C  C  D
of the primal is indeed equal to the objective value of the dual. Combining these
calculationswithLemma29.8yieldsaproofthattheoptimalobjectivevalueofthe
primal linear program is 28. We now show that this approach applies in general:
wecanfindanoptimalsolutiontothedualandsimultaneouslyprovethatasolution
totheprimalisoptimal.
Theorem29.10(Linear-programming duality)
Suppose that SIMPLEX returns values x .x 1;x 2;:::;x n/ for the primal lin-
N D N N N
ear program .A;b;c/. Let N and B denote the nonbasic and basic variables for
the final slack form, let c denote the coefficients in the final slack form, and let
0
y .y ;y ;:::;y / be defined by equation (29.91). Then x is an optimal so-
1 2 m
N D N N N N
lution to the primal linear program, y is an optimal solution to the dual linear
N
program, and
n m
c x b y : (29.92)
j j i i
N D N
j 1 i 1
XD XD
Proof By Corollary 29.9, if we can find feasible solutions x and y that satisfy
N N
equation (29.92), then x and y must be optimal primal and dual solutions. We
N N
shallnowshowthatthesolutionsxandy describedinthestatementofthetheorem
N N
satisfyequation (29.92).
Suppose that we run SIMPLEX on a primal linear program, as given in lines
(29.16)–(29.18). The algorithm proceeds through a series of slack forms until it
terminates withafinalslackformwithobjectivefunction
´  c x : (29.93)
D
0
C
j0 j
j N
X2
SinceSIMPLEX terminatedwithasolution,bytheconditioninline3weknowthat
c 0 forallj N : (29.94)
j0
 2
29.4 Duality 883
Ifwedefine
c 0 forallj B ; (29.95)
j0
D 2
wecanrewriteequation (29.93)as
´  c x
D
0
C
j0 j
j N
X2
 c x c x (because c 0ifj B)
D
0
C
j0 j
C
j0 j j0
D 2
j N j B
X2 X2
n m
C
 c x (because N B 1;2;:::;n m ). (29.96)
D
0
C
j0 j
[ D f C g
j 1
XD
Forthebasicsolutionxassociatedwiththisfinalslackform,x 0forallj N,
j
N N D 2
and´  . Sinceallslackformsareequivalent, ifweevaluatetheoriginal objec-
0
D
tivefunction onx,wemustobtainthesameobjectivevalue:
N
n n m
C
c x  c x (29.97)
j Nj
D
0
C
j0 Nj
j 1 j 1
XD XD
 c x c x
D
0
C
j0 Nj
C
j0 Nj
j N j B
X2 X2
 .c 0/ .0 x / (29.98)
D
0
C
j0
 C 
Nj
j N j B
X2 X2
 :
0
D
We shall now show that y, defined by equation (29.91), is feasible for the dual
linear program and that itsN objective value m b y equals n c x . Equa-
i 1 i Ni j 1 j Nj
tion(29.97)saysthatthefirstandlastslackformDs,evaluated atx,aDreequal. More
P PN
generally, the equivalence of all slack forms implies that for any set of values
x .x ;x ;:::;x /,wehave
1 2 n
D
n n m
C
c x  c x :
j j
D
0
C
j0 j
j 1 j 1
XD XD
Therefore, foranyparticular setofvaluesx .x ;x ;:::;x /,wehave
1 2 n
N D N N N
884 Chapter29 LinearProgramming
n
c x
j j
N
j 1
XD
n m
C
 c x
D
0
C
j0 Nj
j 1
XD
n n m
C
 c x c x
D
0
C
j0 Nj
C
j0 Nj
j 1 j n 1
XD DXC
n m
 c x c x
D
0
C
j0 Nj
C
n0 CiNn Ci
j 1 i 1
XD XD
n m
 c x . y /x (byequations (29.91)and(29.95))
D
0
C
j0 Nj
C
(cid:0)Ni Nn Ci
j 1 i 1
XD XD
n m n
 c x . y / b a x (byequation (29.32))
D
0
C
j0 Nj
C
(cid:0)Ni i
(cid:0)
ij Nj
!
j 1 i 1 j 1
XD XD XD
n m m n
 c x b y .a x /y
D
0
C
j0 Nj
(cid:0)
i Ni
C
ij Nj Ni
j 1 i 1 i 1j 1
XD XD XD XD
n m n m
 c x b y .a y /x
D
0
C
j0 Nj
(cid:0)
i Ni
C
ij Ni Nj
j 1 i 1 j 1 i 1
XD XD XD XD
m n m
 b y c a y x ;
D
0
(cid:0)
i Ni
C
j0
C
ij Ni Nj
! !
i 1 j 1 i 1
XD XD XD
sothat
n m n m
c x  b y c a y x : (29.99)
j Nj
D
0
(cid:0)
i Ni
C
j0
C
ij Ni Nj
! !
j 1 i 1 j 1 i 1
XD XD XD XD
ApplyingLemma29.3toequation (29.99), weobtain
m
 b y 0; (29.100)
0 i i
(cid:0) N D
i 1
XD
m
c a y c forj 1;2;:::;n: (29.101)
j0
C
ij Ni
D
j
D
i 1
XD
Byequation(29.100),wehavethat m b y  ,andhencetheobjectivevalue
i 1 i Ni D 0
of the dual m b y is equal to thD at of the primal ( ). It remains to show
i 1 i Ni P 0
D
 
P
29.4 Duality 885
that the solution y is feasible for the dual problem. From inequalities (29.94) and
N
equations(29.95),wehavethatc 0forallj 1;2;:::;n m. Hence,forany
j0
 D C
j 1;2;:::;n,equations (29.101) implythat
D
m
c c a y
j
D
j0
C
ij Ni
i 1
XD
m
a y ;
ij i
 N
i 1
XD
which satisfies the constraints (29.84) of the dual. Finally, since c 0 for each
j0

j N B,whenwesetyaccordingtoequation(29.91),wehavethateachy 0,
i
2 [ N N 
andsothenonnegativity constraints aresatisfiedaswell.
We have shown that, given a feasible linear program, if INITIALIZE-SIMPLEX
returns a feasible solution, and if SIMPLEX terminates without returning “un-
bounded,” then the solution returned is indeed an optimal solution. We have also
shownhowtoconstruct anoptimalsolution totheduallinearprogram.
Exercises
29.4-1
Formulatethedualofthelinearprogram giveninExercise29.3-5.
29.4-2
Suppose that we have a linear program that is not in standard form. We could
produce the dual by first converting it to standard form, and then taking the dual.
It would be more convenient, however, to be able to produce the dual directly.
Explainhowwecandirectlytakethedualofanarbitrarylinearprogram.
29.4-3
Write down the dual of the maximum-flow linear program, as given in lines
(29.47)–(29.50) on page 860. Explain how to interpret this formulation as a
minimum-cutproblem.
29.4-4
Write down the dual of the minimum-cost-flow linear program, as given in lines
(29.51)–(29.52) on page 862. Explain how to interpret this problem in terms of
graphsandflows.
29.4-5
Showthatthedualofthedualofalinearprogram istheprimallinearprogram.
886 Chapter29 LinearProgramming
29.4-6
WhichresultfromChapter26canbeinterpretedasweakdualityforthemaximum-
flowproblem?
29.5 The initialbasicfeasiblesolution
In this section, we first describe how to test whether a linear program is feasible,
and if it is, how to produce a slack form for which the basic solution is feasible.
We conclude by proving the fundamental theorem of linear programming, which
saysthattheSIMPLEX procedure alwaysproduces thecorrectresult.
Findinganinitialsolution
In Section 29.3, we assumed that we had a procedure INITIALIZE-SIMPLEX that
determineswhetheralinearprogramhasanyfeasiblesolutions,andifitdoes,gives
a slack form for which the basic solution is feasible. We describe this procedure
here.
A linear program can be feasible, yet the initial basic solution might not be
feasible. Consider, forexample,thefollowinglinearprogram:
maximize 2x x (29.102)
1 2
(cid:0)
subject to
2x x 2 (29.103)
1 2
(cid:0) 
x 5x 4 (29.104)
1 2
(cid:0)  (cid:0)
x ;x 0 : (29.105)
1 2

Ifweweretoconvertthislinearprogramtoslackform,thebasicsolutionwould
setx 0andx 0. Thissolutionviolatesconstraint(29.104),andsoitisnota
1 2
D D
feasiblesolution. Thus,INITIALIZE-SIMPLEX cannotjustreturntheobviousslack
form. In order to determine whether a linear program has any feasible solutions,
wewillformulate anauxiliary linear program. Forthis auxiliary linear program,
wecanfind(withalittlework)aslackformforwhichthebasicsolutionisfeasible.
Furthermore, the solution of this auxiliary linear program determines whether the
initiallinearprogramisfeasibleandifso,itprovidesafeasiblesolutionwithwhich
wecaninitialize SIMPLEX.
Lemma29.11
LetLbealinearprograminstandard form,givenasin(29.16)–(29.18). Letx be
0
anewvariable, andletL bethefollowinglinearprogramwithn 1variables:
aux C
29.5 Theinitialbasicfeasiblesolution 887
maximize x (29.106)
0
(cid:0)
subjectto
n
a x x b fori 1;2;:::;m; (29.107)
ij j 0 i
(cid:0)  D
j 1
XD
x 0 forj 0;1;:::;n: (29.108)
j
 D
ThenLisfeasible ifandonlyiftheoptimalobjective valueofL is0.
aux
Proof Suppose that L has a feasible solution x .x ;x ;:::;x /. Then the
1 2 n
N D N N N
solution x 0 combined with x is a feasible solution to L with objective
N0 D N aux
value 0. Since x 0 is a constraint of L and the objective function is to
0  aux
maximize x ,thissolution mustbeoptimalforL .
(cid:0) 0 aux
Conversely, suppose thattheoptimal objective valueofL is0. Thenx 0,
aux N0 D
andtheremainingsolution valuesofx satisfytheconstraints ofL.
N
Wenowdescribeourstrategytofindaninitialbasicfeasiblesolutionforalinear
programLinstandardform:
INITIALIZE-SIMPLEX.A;b;c/
1 letk betheindexoftheminimumb
i
2 ifb 0 //istheinitialbasicsolution feasible?
k

3 return. 1;2;:::;n ; n 1;n 2;:::;n m ;A;b;c;0/
f g f C C C g
4 formL byadding x totheleft-hand sideofeachconstraint
aux (cid:0) 0
andsettingtheobjectivefunction to x
0
(cid:0)
5 let.N;B;A;b;c;/betheresulting slackformforL
aux
6 l n k
D C
7 //L hasn 1nonbasic variables andmbasicvariables.
aux C
8 .N;B;A;b;c;/ PIVOT.N;B;A;b;c;;l;0/
D
9 //ThebasicsolutionisnowfeasibleforL .
aux
10 iteratethewhileloopoflines3–12ofSIMPLEX untilanoptimalsolution
toL isfound
aux
11 iftheoptimalsolution toL setsx to0
aux N0
12 ifx isbasic
0
N
13 performone(degenerate) pivottomakeitnonbasic
14 fromthefinalslackformofL ,removex fromtheconstraints and
aux 0
restoretheoriginalobjective functionofL,butreplaceeachbasic
variableinthisobjectivefunction bytheright-hand sideofits
associated constraint
15 returnthemodifiedfinalslackform
16 elsereturn“infeasible”
888 Chapter29 LinearProgramming
INITIALIZE-SIMPLEX works as follows. In lines 1–3, we implicitly test the
basic solution to the initial slack form for L given by N 1;2;:::;n , B
D f g D
n 1;n 2;:::;n m , x b for all i B, and x 0 for all j N.
i i j
f C C C g N D 2 N D 2
(Creatingtheslackformrequiresnoexpliciteffort,asthevaluesofA,b,andc are
thesameinbothslackandstandard forms.) Ifline2findsthisbasicsolution tobe
feasible—that is, x 0 for all i N B—then line 3 returns the slack form.
i
N  2 [
Otherwise,inline4,weformtheauxiliarylinearprogramL asinLemma29.11.
aux
Since the initial basic solution to Lis not feasible, the initial basic solution to the
slack form for L cannot be feasible either. Tofinda basic feasible solution, we
aux
perform a single pivot operation. Line 6 selects l n k as the index of the
D C
basic variable that will be the leaving variable in the upcoming pivot operation.
Sincethebasic variables arex ;x ;:::;x ,theleaving variable x willbe
n 1 n 2 n m l
C C C
the one with the most negative value. Line 8 performs that call of PIVOT, with
x entering and x leaving. We shall see shortly that the basic solution resulting
0 l
from thiscallof PIVOT willbefeasible. Nowthatwehaveaslackformforwhich
the basic solution is feasible, we can, in line 10, repeatedly call PIVOT to fully
solve the auxiliary linear program. As the test in line 11 demonstrates, if we find
an optimal solution to L with objective value 0, then in lines 12–14, we create
aux
a slack form for L for which the basic solution is feasible. To do so, we first,
in lines 12–13, handle the degenerate case in which x may still be basic with
0
value x 0. In this case, we perform a pivot step to remove x from the basis,
0 0
N D
using any e N such that a 0 as the entering variable. The new basic
0e
2 ¤
solution remains feasible; the degenerate pivot does not change the value of any
variable. Next wedelete all x terms from the constraints and restore the original
0
objective function for L. The original objective function may contain both basic
andnonbasic variables. Therefore, intheobjective function wereplace eachbasic
variable by the right-hand side of its associated constraint. Line 15 then returns
this modified slack form. If, on the other hand, line 11 discovers that the original
linearprogram Lisinfeasible, thenline16returnsthisinformation.
We now demonstrate the operation of INITIALIZE-SIMPLEX on the linear pro-
gram (29.102)–(29.105). This linear program is feasible if we can find nonneg-
ative values for x and x that satisfy inequalities (29.103) and (29.104). Using
1 2
Lemma29.11,weformulate theauxiliary linearprogram
maximize x (29.109)
0
(cid:0)
subject to
2x x x 2 (29.110)
1 2 0
(cid:0) (cid:0) 
x 5x x 4 (29.111)
1 2 0
(cid:0) (cid:0)  (cid:0)
x ;x ;x 0 :
1 2 0

By Lemma 29.11, if the optimal objective value of this auxiliary linear program
is0,thentheoriginallinearprogramhasafeasiblesolution. Iftheoptimalobjective
29.5 Theinitialbasicfeasiblesolution 889
value of this auxiliary linear program is negative, then the original linear program
doesnothaveafeasible solution.
Wewritethislinearprograminslackform,obtaining
´ x
0
D (cid:0)
x 2 2x x x
3 1 2 0
D (cid:0) C C
x 4 x 5x x :
4 1 2 0
D (cid:0) (cid:0) C C
We are not out of the woods yet, because the basic solution, which would set
x 4, is not feasible for this auxiliary linear program. We can, however, with
4
D(cid:0)
one call to PIVOT, convert this slack form into one in which the basic solution is
feasible. Asline8indicates,wechoosex tobetheenteringvariable. Inline6,we
0
choose as the leaving variable x , which is the basic variable whose value in the
4
basicsolution ismostnegative. Afterpivoting, wehavetheslackform
´ 4 x 5x x
1 2 4
D (cid:0) (cid:0) C (cid:0)
x 4 x 5x x
0 1 2 4
D C (cid:0) C
x 6 x 4x x :
3 1 2 4
D (cid:0) (cid:0) C
Theassociatedbasicsolutionis.x ;x ;x ;x ;x / .4;0;0;6;0/, whichisfeasi-
0 1 2 3 4
N N N N N D
ble. Wenowrepeatedly call PIVOT until weobtain anoptimalsolution toL aux. In
thiscase,onecalltoPIVOT withx
2
enteringandx
0
leaving yields
´ x
0
D (cid:0)
4 x x x
x 0 1 4
2
D 5 (cid:0) 5 C 5 C 5
14 4x 9x x
x 0 1 4 :
3
D 5 C 5 (cid:0) 5 C 5
This slack form is the final solution to the auxiliary problem. Since this solution
has x 0, we know that our initial problem was feasible. Furthermore, since
0
D
x 0, we can just remove it from the set of constraints. We then restore the
0
D
original objective function, with appropriate substitutions made to include only
nonbasic variables. Inourexample,wegettheobjective function
4 x x x
0 1 4
2x x 2x :
1 2 1
(cid:0) D (cid:0) 5 (cid:0) 5 C 5 C 5
 
Settingx 0andsimplifying, wegettheobjectivefunction
0
D
4 9x x
1 4
;
(cid:0)5 C 5 (cid:0) 5
andtheslackform
890 Chapter29 LinearProgramming
4 9x x
1 4
´
D (cid:0)5 C 5 (cid:0) 5
4 x x
1 4
x
2
D 5 C 5 C 5
14 9x x
1 4
x :
3
D 5 (cid:0) 5 C 5
This slack form has a feasible basic solution, and we can return it to procedure
SIMPLEX.
Wenowformallyshowthecorrectness of INITIALIZE-SIMPLEX.
Lemma29.12
Ifalinear program Lhas nofeasible solution, then INITIALIZE-SIMPLEX returns
“infeasible.” Otherwise, it returns a valid slack form for which the basic solution
isfeasible.
Proof First suppose that the linear program Lhas no feasible solution. Then by
Lemma 29.11, the optimal objective value of L , defined in (29.106)–(29.108),
aux
is nonzero, and by the nonnegativity constraint on x , the optimal objective value
0
must be negative. Furthermore, this objective value must be finite, since setting
x 0, for i 1;2;:::;n, and x minm b is feasible, and this solution
hai sD objective vD alue (cid:0)jminm
i
1fb
i
gj0 . D Thj erefoi rDe1 ,f lini g ej 10 of INITIALIZE-SIMPLEX
finds a solution with a nonDpositive objective value. Let x be the basic solution
N
associated with the final slack form. We cannot have x 0, because then L
N0 D aux
wouldhaveobjectivevalue0,whichcontradictsthattheobjectivevalueisnegative.
Thusthetestinline11resultsinline16returning “infeasible.”
Suppose now that the linear program L does have a feasible solution. From
Exercise29.3-4,weknowthatifb 0fori 1;2;:::;m,thenthebasicsolution
i
 D
associated with the initial slack form is feasible. In this case, lines 2–3 return the
slack form associated with theinput. (Converting the standard form toslack form
iseasy,sinceA,b,andc arethesameinboth.)
Intheremainder oftheproof,wehandlethecaseinwhichthelinearprogramis
feasible butwedonotreturninline3. Wearguethatinthiscase,lines4–10finda
feasible solutiontoL withobjective value0. First,bylines1–2,wemusthave
aux
b <0;
k
and
b b foreachi B : (29.112)
k i
 2
In line 8, we perform one pivot operation in which the leaving variable x (recall
l
that l n k, so that b < 0) is the left-hand side of the equation with mini-
l
D C
mum b , and the entering variable is x , the extra added variable. We now show
i 0
29.5 Theinitialbasicfeasiblesolution 891
that after this pivot, all entries of b are nonnegative, and hence the basic solution
to L
aux
is feasible. Letting x
N
be the basic solution after the call to PIVOT, and
lettingb yandB
y
bevaluesreturnedby PIVOT,Lemma29.1impliesthat
b a b ifi B e ;
x i (cid:0) ieye 2 y(cid:0)f g (29.113)
i
N D
(
b l=a
le
ifi e :
D
The call to PIVOT in line 8 has e 0. If we rewrite inequalities (29.107), to
D
includecoefficients a ,
i0
n
a x b fori 1;2;:::;m; (29.114)
ij j i
 D
j 0
XD
then
a a 1 foreachi B : (29.115)
i0 ie
D D (cid:0) 2
(Note that a is the coefficient of x as it appears in inequalities (29.114), not
i0 0
thenegation ofthecoefficient, because L isinstandard rather thanslack form.)
aux
Since l B, we also have that a 1. Thus, b =a > 0, and so x > 0. For
le l le e
2 D (cid:0) N
theremainingbasicvariables, wehave
x b a b (byequation (29.113))
Ni
D
i
(cid:0)
ieye
b
i
a ie.b l=a le/ (byline3ofPIVOT)
D (cid:0)
b b (byequation (29.115) anda 1)
i l le
D (cid:0) D(cid:0)
0 (byinequality (29.112)) ,

which implies that each basic variable is now nonnegative. Hence the basic solu-
tion after the call to PIVOT in line 8 is feasible. We next execute line 10, which
solves L . Since we have assumed that L has a feasible solution, Lemma 29.11
aux
impliesthatL hasanoptimalsolutionwithobjectivevalue0. Sincealltheslack
aux
forms are equivalent, the final basic solution to L must have x 0, and after
aux N0 D
removingx fromthelinearprogram,weobtainaslackformthatisfeasibleforL.
0
Line15thenreturnsthisslackform.
Fundamentaltheoremoflinearprogramming
Weconclude this chapter by showing that the SIMPLEX procedure works. In par-
ticular, any linear program either is infeasible, is unbounded, or has an optimal
solutionwithafiniteobjectivevalue. Ineachcase, SIMPLEX actsappropriately.
892 Chapter29 LinearProgramming
Theorem29.13(Fundamentaltheoremoflinearprogramming)
Anylinearprogram L,giveninstandardform,either
1. hasanoptimalsolution withafiniteobjectivevalue,
2. isinfeasible, or
3. isunbounded.
If L is infeasible, SIMPLEX returns “infeasible.” If L is unbounded, SIMPLEX
returns“unbounded.” Otherwise,SIMPLEXreturnsanoptimalsolutionwithafinite
objective value.
Proof ByLemma29.12,iflinearprogramLisinfeasible, thenSIMPLEX returns
“infeasible.” NowsupposethatthelinearprogramLisfeasible. ByLemma29.12,
INITIALIZE-SIMPLEX returnsaslackformforwhichthebasicsolutionisfeasible.
By Lemma 29.7, therefore, SIMPLEX either returns “unbounded” or terminates
withafeasible solution. Ifitterminateswithafinitesolution, thenTheorem 29.10
tells us that this solution is optimal. On the other hand, if SIMPLEX returns “un-
bounded,” Lemma 29.2 tells us the linear program L is indeed unbounded. Since
SIMPLEX alwaysterminates inoneoftheseways,theproofiscomplete.
Exercises
29.5-1
Givedetailed pseudocode toimplementlines5and14ofINITIALIZE-SIMPLEX.
29.5-2
ShowthatwhenthemainloopofSIMPLEX isrunbyINITIALIZE-SIMPLEX, itcan
neverreturn“unbounded.”
29.5-3
Suppose that we are given a linear program L in standard form, and suppose that
for both L and the dual of L, the basic solutions associated with the initial slack
formsarefeasible. Showthattheoptimalobjective valueofLis0.
29.5-4
Suppose that we allow strict inequalities in a linear program. Show that in this
case,thefundamental theorem oflinearprogramming doesnothold.
29.5 Theinitialbasicfeasiblesolution 893
29.5-5
Solvethefollowinglinearprogramusing SIMPLEX:
maximize x 3x
1 2
C
subjectto
x x 8
1 2
(cid:0) 
x x 3
1 2
(cid:0) (cid:0)  (cid:0)
x 4x 2
1 2
(cid:0) C 
x ;x 0 :
1 2

29.5-6
Solvethefollowinglinearprogramusing SIMPLEX:
maximize x 2x
1 2
(cid:0)
subjectto
x 2x 4
1 2
C 
2x 6x 12
1 2
(cid:0) (cid:0)  (cid:0)
x 1
2

x ;x 0 :
1 2

29.5-7
Solvethefollowinglinearprogramusing SIMPLEX:
maximize x 3x
1 2
C
subjectto
x x 1
1 2
(cid:0) C  (cid:0)
x x 3
1 2
(cid:0) (cid:0)  (cid:0)
x 4x 2
1 2
(cid:0) C 
x ;x 0 :
1 2

29.5-8
Solvethelinearprogramgivenin(29.6)–(29.10).
29.5-9
Considerthefollowing1-variable linearprogram, whichwecallP:
maximize tx
subjectto
rx s

x 0 ;

wherer,s,andt arearbitrary realnumbers. LetD bethedualofP.
894 Chapter29 LinearProgramming
Stateforwhichvaluesofr,s,andt youcanassertthat
1. BothP andD haveoptimalsolutions withfiniteobjectivevalues.
2. P isfeasible, butD isinfeasible.
3. D isfeasible, butP isinfeasible.
4. NeitherP norD isfeasible.
Problems
29-1 Linear-inequalityfeasibility
Given a set of m linear inequalities on n variables x ;x ;:::;x , the linear-
1 2 n
inequality feasibility problem asks whether there is a setting of the variables that
simultaneously satisfieseachoftheinequalities.
a. Show that if we have an algorithm for linear programming, we can use it to
solvealinear-inequality feasibility problem. Thenumberofvariables andcon-
straints that you use in the linear-programming problem should be polynomial
innandm.
b. Showthatifwehaveanalgorithm forthelinear-inequality feasibility problem,
wecanuseittosolvealinear-programming problem. Thenumberofvariables
and linear inequalities that you use in the linear-inequality feasibility problem
should be polynomial in n and m, the number of variables and constraints in
thelinearprogram.
29-2 Complementaryslackness
Complementary slackness describes a relationship between the values of primal
variables and dual constraints and between the values of dual variables and pri-
mal constraints. Let x be a feasible solution to the primal linear program given
N
in(29.16)–(29.18), andlety beafeasiblesolutiontotheduallinearprogramgiven
N
in (29.83)–(29.85). Complementary slackness states that the following conditions
arenecessary andsufficientforx andy tobeoptimal:
N N
m
a y c orx 0 forj 1;2;:::;n
ij i j j
N D N D D
i 1
XD
and
n
a x b ory 0 fori 1;2;:::;m:
ij j i i
N D N D D
j 1
XD
ProblemsforChapter29 895
a. Verify that complementary slackness holds for the linear program in lines
(29.53)–(29.57).
b. Prove that complementary slackness holds for any primal linear program and
itscorresponding dual.
c. Prove that a feasible solution x to a primal linear program given in lines
N
(29.16)–(29.18)isoptimalifandonlyifthereexistvaluesy .y ;y ;:::;y /
1 2 m
N D N N N
suchthat
1. y isafeasible solutiontotheduallinearprogramgivenin(29.83)–(29.85),
N
2. m a y c forallj suchthatx > 0,and
i 1 ij Ni D j Nj
3. y D 0foralli suchthat n a x <b .
PNi D j 1 ij Nj i
D
P
29-3 Integerlinearprogramming
An integer linear-programming problem is a linear-programming problem with
the additional constraint that the variables x must take on integral values. Exer-
cise 34.5-3 shows that just determining whether an integer linear program has a
feasiblesolutionisNP-hard,whichmeansthatthereisnoknownpolynomial-time
algorithm forthisproblem.
a. Showthatweakduality (Lemma29.8)holdsforanintegerlinearprogram.
b. Show that duality (Theorem 29.10) does not always hold for an integer linear
program.
c. Givenaprimallinear program instandard form, letusdefine P tobethe opti-
malobjectivevaluefortheprimallinearprogram,Dtobetheoptimalobjective
valueforitsdual,IPtobetheoptimalobjectivevaluefortheintegerversionof
the primal (that is, the primal with the added constraint that the variables take
onintegervalues), andIDtobetheoptimal objective valuefortheintegerver-
sion of the dual. Assuming that both the primal integer program and the dual
integerprogram arefeasibleandbounded, showthat
IP P D ID:
 D 
29-4 Farkas’s lemma
Let A be an m n matrix and c be an n-vector. Then Farkas’s lemma states that

exactlyoneofthesystems
896 Chapter29 LinearProgramming
Ax 0;

cTx > 0
and
ATy c ;
D
y 0

issolvable, wherex isann-vectorandy isanm-vector. ProveFarkas’slemma.
29-5 Minimum-costcirculation
In this problem, we consider a variant of the minimum-cost-flow problem from
Section 29.2 in which we are not given a demand, a source, or a sink. Instead,
weare given, as before, aflow network and edge costs a.u;/. Aflow isfeasible
if it satisfies the capacity constraint on every edge and flow conservation at every
vertex. Thegoalistofind,amongallfeasible flows,theoneofminimumcost. We
callthisproblem theminimum-cost-circulation problem.
a. Formulatetheminimum-cost-circulation problem asalinearprogram.
b. Suppose that for all edges .u;/ E, we have a.u;/ > 0. Characterize an
2
optimalsolution totheminimum-cost-circulation problem.
c. Formulatethemaximum-flowproblemasaminimum-cost-circulation problem
linear program. That is given a maximum-flow problem instance G .V;E/
D
with source s, sink t and edge capacities c, create a minimum-cost-circulation
problem by giving a (possibly different) network G .V ;E / with edge
0 0 0
D
capacities c and edge costs a such that you can discern a solution to the
0 0
maximum-flowproblemfromasolutiontotheminimum-cost-circulation prob-
lem.
d. Formulate the single-source shortest-path problem as a minimum-cost-circu-
lationproblemlinearprogram.
Chapter notes
This chapter only begins to study the wide field of linear programming. A num-
ber of books are devoted exclusively to linear programming, including those by
Chva´tal [69], Gass [130], Karloff [197], Schrijver [303], and Vanderbei [344].
Many other books give a good coverage of linear programming, including those
by Papadimitriou and Steiglitz [271] and Ahuja, Magnanti, and Orlin [7]. The
coverageinthischapterdrawsontheapproach takenbyChva´tal.
NotesforChapter29 897
The simplex algorithm for linear programming was invented by G. Dantzig
in1947. Shortlyafter, researchers discovered howtoformulate anumberofprob-
lems in a variety of fields as linear programs and solve them with the simplex
algorithm. As a result, applications of linear programming flourished, along with
several algorithms. Variants of the simplex algorithm remain the most popular
methodsforsolvinglinear-programming problems. Thishistoryappearsinanum-
berofplaces, including thenotesin[69]and[197].
The ellipsoid algorithm was the first polynomial-time algorithm for linear pro-
gramming and is due to L. G. Khachian in 1979; it was based on earlier work by
N. Z. Shor, D. B. Judin, and A. S. Nemirovskii. Gro¨tschel, Lova´sz, and Schrijver
[154]describe howtousetheellipsoid algorithm tosolveavariety ofproblems in
combinatorial optimization. Todate, theellipsoid algorithm does notappear tobe
competitivewiththesimplexalgorithm inpractice.
Karmarkar’s paper [198] includes a description of the first interior-point algo-
rithm. Manysubsequent researchersdesignedinterior-point algorithms. Goodsur-
veysappearinthearticleofGoldfarbandTodd[141]andthebookbyYe[361].
Analysis of the simplex algorithm remains an active area of research. V. Klee
and G. J. Minty constructed an example on which the simplex algorithm runs
through 2n 1 iterations. The simplex algorithm usually performs very well in
(cid:0)
practice and many researchers have tried to give theoretical justification for this
empirical observation. A line of research begun by K. H. Borgwardt, and carried
on by many others, shows that under certain probabilistic assumptions on the in-
put, the simplex algorithm converges in expected polynomial time. Spielman and
Teng[322]madeprogressinthisarea,introducingthe“smoothedanalysisofalgo-
rithms”andapplying ittothesimplexalgorithm.
The simplex algorithm is known to run efficiently in certain special cases. Par-
ticularly noteworthy is the network-simplex algorithm, which is the simplex al-
gorithm, specialized to network-flow problems. For certain network problems,
including the shortest-paths, maximum-flow, and minimum-cost-flow problems,
variants ofthenetwork-simplex algorithm runinpolynomial time. See, forexam-
ple,thearticlebyOrlin[268]andthecitations therein.
30 Polynomials and the FFT
The straightforward method of adding two polynomials of degree n takes ‚.n/
time,butthestraightforward methodofmultiplyingthemtakes‚.n2/time. Inthis
chapter, weshallshowhowthefastFouriertransform,orFFT,canreducethetime
tomultiplypolynomials to‚.nlgn/.
The most common use for Fourier transforms, and hence the FFT, is in signal
processing. A signal is given in the time domain: as a function mapping time to
amplitude. Fourier analysis allows us to express the signal as a weighted sum of
phase-shifted sinusoidsofvaryingfrequencies. Theweightsandphasesassociated
with the frequencies characterize the signal in the frequency domain. Among the
many everyday applications of FFT’s are compression techniques used to encode
digitalvideoandaudioinformation, including MP3files. Severalfinebooksdelve
intotherichareaofsignalprocessing; thechapter notesreference afewofthem.
Polynomials
ApolynomialinthevariablexoveranalgebraicfieldF representsafunctionA.x/
asaformalsum:
n 1
(cid:0)
A.x/ a xj :
j
D
j 0
XD
We call the values a ;a ;:::;a the coefficients of the polynomial. The co-
0 1 n 1
efficients are drawn from a field
(cid:0)F,
typically the set
C
of complex numbers. A
polynomial A.x/ has degree k if its highest nonzero coefficient is a ; we write
k
that degree.A/ k. Any integer strictly greater than the degree of a polynomial
D
is a degree-bound of that polynomial. Therefore, the degree of a polynomial of
degree-bound nmaybeanyintegerbetween0andn 1,inclusive.
(cid:0)
We can define a variety of operations on polynomials. For polynomial addi-
tion, ifA.x/andB.x/are polynomials ofdegree-bound n,their sumisapolyno-
Chapter30 PolynomialsandtheFFT 899
mialC.x/,alsoofdegree-boundn,suchthatC.x/ A.x/ B.x/forallx inthe
D C
underlying field. Thatis,if
n 1
(cid:0)
A.x/ a xj
j
D
j 0
XD
and
n 1
(cid:0)
B.x/ b xj ;
j
D
j 0
XD
then
n 1
(cid:0)
C.x/ c xj ;
j
D
j 0
XD
where c a b for j 0;1;:::;n 1. For example, if we have the
j j j
D C D (cid:0)
polynomials A.x/ 6x3 7x2 10x 9 and B.x/ 2x3 4x 5, then
D C (cid:0) C D (cid:0) C (cid:0)
C.x/ 4x3 7x2 6x 4.
D C (cid:0) C
For polynomial multiplication, if A.x/ and B.x/ are polynomials of degree-
bound n, their product C.x/ is a polynomial of degree-bound 2n 1 such that
(cid:0)
C.x/ A.x/B.x/ for all x in the underlying field. You probably have multi-
D
pliedpolynomials before, bymultiplying eachterm inA.x/byeach terminB.x/
and then combining terms with equal powers. For example, we can multiply
A.x/ 6x3 7x2 10x 9andB.x/ 2x3 4x 5asfollows:
D C (cid:0) C D (cid:0) C (cid:0)
6x3 7x2 10x 9
C (cid:0) C
2x3 4x 5
(cid:0) C (cid:0)
30x3 35x2 50x 45
(cid:0) (cid:0) C (cid:0)
24x4 28x3 40x2 36x
C (cid:0) C
12x6 14x5 20x4 18x3
(cid:0) (cid:0) C (cid:0)
12x6 14x5 44x4 20x3 75x2 86x 45
(cid:0) (cid:0) C (cid:0) (cid:0) C (cid:0)
AnotherwaytoexpresstheproductC.x/is
2n 2
(cid:0)
C.x/ c xj ; (30.1)
j
D
j 0
XD
where
j
c a b : (30.2)
j k j k
D (cid:0)
k 0
XD
900 Chapter30 PolynomialsandtheFFT
Note that degree.C/ degree.A/ degree.B/, implying that if A is a polyno-
D C
mial of degree-bound n and B is a polynomial of degree-bound n , then C is a
a b
polynomial ofdegree-bound n n 1. Since apolynomial of degree-bound k
a b
C (cid:0)
isalsoapolynomialofdegree-bound k 1,wewillnormallysaythattheproduct
C
polynomial C isapolynomialofdegree-bound n n .
a b
C
Chapteroutline
Section30.1presentstwowaystorepresentpolynomials: thecoefficientrepresen-
tationandthepoint-valuerepresentation. Thestraightforwardmethodformultiply-
ing polynomials—equations (30.1) and (30.2)—takes ‚.n2/ timewhen werepre-
sent polynomials incoefficient form, butonly ‚.n/timewhenwerepresent them
in point-value form. Wecan, however, multiply polynomials using the coefficient
representation in only ‚.nlgn/ time by converting between the two representa-
tions. Toseewhythisapproach works,wemustfirststudycomplexrootsofunity,
whichwedoinSection30.2. Then,weusetheFFTanditsinverse, alsodescribed
inSection30.2,toperformtheconversions. Section30.3showshowtoimplement
theFFTquickly inbothserialandparallel models.
Thischapter uses complex numbers extensively, and within this chapter weuse
thesymboli exclusively todenotep 1.
(cid:0)
30.1 Representing polynomials
Thecoefficientandpoint-valuerepresentationsofpolynomialsareinasenseequiv-
alent; that is, a polynomial in point-value form has a unique counterpart in co-
efficient form. In this section, we introduce the two representations and show
how to combine them so that we can multiply two degree-bound n polynomials
in‚.nlgn/time.
Coefficientrepresentation
A coefficient representation of a polynomial A.x/
D
jn (cid:0)1 0a jxj of degree-
bound n is a vector of coefficients a .a ;a ;:::;a /. IDn matrix equations
0 1 n 1
D (cid:0) P
inthischapter, weshallgenerally treatvectorsascolumnvectors.
The coefficient representation is convenient for certain operations on polyno-
mials. For example, the operation of evaluating the polynomial A.x/ at a given
point x consists of computing the value of A.x /. Wecan evaluate a polynomial
0 0
in‚.n/timeusingHorner’srule:
A.x / a x .a x .a x .a x .a // //:
0 0 0 1 0 2 0 n 2 0 n 1
D C C CC (cid:0) C (cid:0) 
30.1 Representingpolynomials 901
Similarly, adding two polynomials represented by the coefficient vectors a
D
.a ;a ;:::;a / and b .b ;b ;:::;b / takes ‚.n/ time: we just produce
0 1 n 1 0 1 n 1
the coefficient(cid:0) vector c D .c ;c ;:::;c (cid:0) /, where c a b for j
0 1 n 1 j j j
0;1;:::;n 1. D (cid:0) D C D
(cid:0)
Now,considermultiplyingtwodegree-boundnpolynomialsA.x/andB.x/rep-
resented in coefficient form. If we use the method described by equations (30.1)
and (30.2), multiplying polynomials takes time ‚.n2/, since we must multiply
each coefficient in the vector a by each coefficient in the vector b. The operation
ofmultiplyingpolynomialsincoefficientformseemstobeconsiderablymorediffi-
cultthanthatofevaluatingapolynomialoraddingtwopolynomials. Theresulting
coefficient vectorc,givenbyequation (30.2), isalsocalledtheconvolution ofthe
input vectors a and b, denoted c a b. Since multiplying polynomials and
D ˝
computing convolutions are fundamental computational problems of considerable
practicalimportance, thischapterconcentrates onefficientalgorithms forthem.
Point-valuerepresentation
A point-value representation of a polynomial A.x/ of degree-bound n is a set of
npoint-valuepairs
.x ;y /;.x ;y /;:::;.x ;y /
0 0 1 1 n 1 n 1
f (cid:0) (cid:0) g
suchthatallofthex aredistinctand
k
y A.x / (30.3)
k k
D
fork 0;1;:::;n 1. Apolynomial has many different point-value representa-
D (cid:0)
tions, since we can use any set of n distinct points x ;x ;:::;x as a basis for
0 1 n 1
(cid:0)
therepresentation.
Computing a point-value representation for a polynomial given in coefficient
form is in principle straightforward, since all we have to do is select n distinct
points x ;x ;:::;x and then evaluate A.x / for k 0;1;:::;n 1. With
0 1 n 1 k
Horner’s method, eva(cid:0) luating a polynomial at n points takD es time ‚.n2/(cid:0) . We shall
seelaterthatifwechoosethepointsx cleverly,wecanacceleratethiscomputation
k
torunintime‚.nlgn/.
The inverse of evaluation—determining the coefficient form of a polynomial
fromapoint-valuerepresentation—is interpolation. Thefollowingtheoremshows
that interpolation is well defined when the desired interpolating polynomial must
haveadegree-bound equaltothegivennumberofpoint-value pairs.
Theorem30.1(Uniquenessofaninterpolating polynomial)
For any set .x ;y /;.x ;y /;:::;.x ;y / of n point-value pairs such that
0 0 1 1 n 1 n 1
allthex valf uesaredistinct,thereisau(cid:0) nique(cid:0) pog lynomialA.x/ofdegree-bound n
k
suchthaty A.x /fork 0;1;:::;n 1.
k k
D D (cid:0)
902 Chapter30 PolynomialsandtheFFT
Proof Theproof relies onthe existence ofthe inverse ofacertain matrix. Equa-
tion(30.3)isequivalent tothematrixequation
1 x x2 xn 1 a y
0 0  0(cid:0) 0 0
1 x x2 xn 1 a y
:
: :
:
:
:1 :
:
:1  :: : 1 :
:
:(cid:0) :
:
:1
D
:
:
:1 : (30.4)
˙ 1 x x2 xn 1 ˙ a  ˙ y 
n (cid:0)1 n (cid:0)1  n (cid:0)(cid:0)1 n (cid:0)1 n (cid:0)1
ThematrixontheleftisdenotedV.x ;x ;:::;x /andisknownasaVander-
0 1 n 1
(cid:0)
mondematrix. ByProblemD-1,thismatrixhasdeterminant
.x x /;
k j
(cid:0)
0 j<k n 1
 Y (cid:0)
and therefore, by Theorem D.5, it is invertible (that is, nonsingular) if the x are
k
distinct. Thus, wecan solve for the coefficients a uniquely given the point-value
j
representation:
a V.x ;x ;:::;x / 1y :
0 1 n 1 (cid:0)
D (cid:0)
The proof of Theorem 30.1 describes an algorithm for interpolation based on
solving theset(30.4)oflinearequations. UsingtheLUdecomposition algorithms
ofChapter28,wecansolvetheseequations intimeO.n3/.
Afasteralgorithm forn-pointinterpolation isbasedonLagrange’sformula:
.x x /
j
n 1 (cid:0)
(cid:0) j k
A.x/ y
k
Y¤ : (30.5)
D .x x /
Xk D0 k (cid:0) j
j k
Y¤
Youmaywishtoverify thattheright-hand sideofequation (30.5) isapolynomial
of degree-bound n that satisfies A.x / y for all k. Exercise 30.1-5 asks you
k k
D
howtocomputethecoefficients ofAusingLagrange’sformulaintime‚.n2/.
Thus, n-point evaluation and interpolation are well-defined inverse operations
that transform between the coefficient representation ofapolynomial and apoint-
value representation.1 The algorithms described above for these problems take
time‚.n2/.
Thepoint-value representation isquiteconvenient formanyoperations onpoly-
nomials. Foraddition, ifC.x/ A.x/ B.x/,thenC.x / A.x / B.x /for
k k k
D C D C
anypointx . Moreprecisely, ifwehaveapoint-value representation forA,
k
1Interpolationisanotoriouslytrickyproblemfromthepointofviewofnumericalstability.Although
theapproachesdescribedherearemathematicallycorrect,smalldifferencesintheinputsorround-off
errorsduringcomputationcancauselargedifferencesintheresult.
30.1 Representingpolynomials 903
.x ;y /;.x ;y /;:::;.x ;y / ;
0 0 1 1 n 1 n 1
f (cid:0) (cid:0) g
andforB,
.x ;y /;.x ;y /;:::;.x ;y /
f 0 00 1 10 n (cid:0)1 n0 (cid:0)1 g
(note that A and B are evaluated at the same n points), then a point-value repre-
sentation forC is
.x ;y y /;.x ;y y /;:::;.x ;y y / :
f 0 0 C 00 1 1 C 10 n (cid:0)1 n (cid:0)1 C n0 (cid:0)1 g
Thus, the time to add two polynomials of degree-bound n in point-value form
is‚.n/.
Similarly, the point-value representation is convenient for multiplying polyno-
mials. If C.x/ A.x/B.x/, then C.x / A.x /B.x / for any point x , and
k k k k
D D
wecanpointwisemultiplyapoint-value representation forAbyapoint-value rep-
resentation for B to obtain a point-value representation for C. We must face the
problem, however, that degree.C/ degree.A/ degree.B/; if A and B are of
D C
degree-bound n, then C is of degree-bound 2n. A standard point-value represen-
tation for A and B consists of n point-value pairs for each polynomial. When we
multiplythesetogether,wegetnpoint-valuepairs,butweneed2npairstointerpo-
late a unique polynomial C of degree-bound 2n. (See Exercise 30.1-4.) We must
therefore begin with “extended” point-value representations for A and for B con-
sistingof2npoint-value pairseach. Givenanextended point-value representation
forA,
.x ;y /;.x ;y /;:::;.x ;y / ;
0 0 1 1 2n 1 2n 1
f (cid:0) (cid:0) g
andacorresponding extendedpoint-value representation forB,
.x ;y /;.x ;y /;:::;.x ;y / ;
f 0 00 1 10 2n (cid:0)1 20n (cid:0)1 g
thenapoint-value representation forC is
.x ;y y /;.x ;y y /;:::;.x ;y y / :
f 0 0 00 1 1 10 2n (cid:0)1 2n (cid:0)1 20n (cid:0)1 g
Giventwoinputpolynomials inextendedpoint-value form,weseethatthetimeto
multiply them toobtain the point-value form of theresult is ‚.n/,muchless than
thetimerequired tomultiplypolynomials incoefficientform.
Finally,weconsiderhowtoevaluateapolynomialgiveninpoint-valueformata
newpoint. Forthisproblem, weknowofnosimplerapproach than converting the
polynomial tocoefficientformfirst,andthenevaluating itatthenewpoint.
Fastmultiplicationofpolynomialsincoefficientform
Can we use the linear-time multiplication method for polynomials in point-value
formtoexpeditepolynomialmultiplicationincoefficientform? Theanswerhinges
904 Chapter30 PolynomialsandtheFFT
a 0;a 1;:::;a n 1 Ordinarymultiplication Coefficient
b 0;b 1;:::;b n(cid:0)
1
Time‚.n2/ c 0;c 1;:::;c 2n (cid:0)2 representations
(cid:0)
Evaluation Interpolation
Time‚.nlgn/ Time‚.nlgn/
A.!0 /;B.!0 / C.!0 /
2n 2n 2n
A.! 21 n/;B.! 21 n/ Pointwisemultiplication C.! 21 n/ Point-value
: :
: : Time‚.n/ : : representations
A.!2n 1/;B.!2n 1/ C.!2n 1/
2n(cid:0) 2n(cid:0) 2n(cid:0)
Figure30.1 Agraphicaloutlineofanefficientpolynomial-multiplicationprocess.Representations
onthetopareincoefficient form, whilethoseonthebottomareinpoint-valueform. Thearrows
fromlefttorightcorrespondtothemultiplicationoperation.The!2ntermsarecomplex.2n/throots
ofunity.
on whether we can convert a polynomial quickly from coefficient form to point-
valueform(evaluate) andviceversa(interpolate).
We can use any points we want as evaluation points, but by choosing the eval-
uation points carefully, we can convert between representations in only ‚.nlgn/
time. As we shall see in Section 30.2, if we choose “complex roots of unity” as
the evaluation points, we can produce a point-value representation by taking the
discrete Fourier transform (or DFT) of a coefficient vector. We can perform the
inverse operation, interpolation, by taking the “inverse DFT”of point-value pairs,
yielding a coefficient vector. Section 30.2 will show how the FFT accomplishes
theDFTandinverseDFToperations in‚.nlgn/time.
Figure 30.1 shows this strategy graphically. One minor detail concerns degree-
bounds. The product of two polynomials of degree-bound n is a polynomial of
degree-bound 2n. Before evaluating the input polynomials A and B, therefore,
wefirstdoubletheirdegree-bounds to2nbyaddingnhigh-order coefficientsof0.
Because the vectors have 2n elements, we use “complex .2n/th roots of unity,”
whicharedenotedbythe! termsinFigure30.1.
2n
GiventheFFT,wehavethefollowing‚.nlgn/-timeprocedure formultiplying
two polynomials A.x/ and B.x/ of degree-bound n, where the input and output
representations are in coefficient form. Weassume that n is a power of 2; we can
alwaysmeetthisrequirement byaddinghigh-order zerocoefficients.
1. Double degree-bound: Create coefficient representations of A.x/and B.x/as
degree-bound2npolynomialsbyaddingnhigh-orderzerocoefficientstoeach.
30.1 Representingpolynomials 905
2. Evaluate: Computepoint-value representations ofA.x/andB.x/oflength2n
by applying the FFT of order 2n on each polynomial. These representations
contain thevaluesofthetwopolynomials atthe.2n/throotsofunity.
3. Pointwise multiply: Compute a point-value representation for the polynomial
C.x/ A.x/B.x/bymultiplyingthesevaluestogetherpointwise. Thisrepre-
D
sentation contains thevalueofC.x/ateach.2n/throotofunity.
4. Interpolate: Create the coefficient representation of the polynomial C.x/ by
applying theFFTon2npoint-value pairstocomputetheinverseDFT.
Steps (1)and (3) take time‚.n/, and steps (2)and (4)take time‚.nlgn/. Thus,
onceweshowhowtousetheFFT,wewillhaveproventhefollowing.
Theorem30.2
Wecan multiply twopolynomials of degree-bound n in time ‚.nlgn/, with both
theinputandoutputrepresentations incoefficientform.
Exercises
30.1-1
Multiply thepolynomials A.x/ 7x3 x2 x 10andB.x/ 8x3 6x 3
D (cid:0) C (cid:0) D (cid:0) C
usingequations (30.1)and(30.2).
30.1-2
Anotherwaytoevaluate apolynomial A.x/ofdegree-bound natagivenpoint x
0
istodivideA.x/bythepolynomial.x x /,obtainingaquotientpolynomialq.x/
0
(cid:0)
ofdegree-bound n 1andaremainderr,suchthat
(cid:0)
A.x/ q.x/.x x / r :
0
D (cid:0) C
Clearly, A.x / r. Show how to compute the remainder r and the coefficients
0
D
ofq.x/intime‚.n/fromx andthecoefficients ofA.
0
30.1-3
D vae lr ui eve rea prp eo si en nt t- av ta iolu ne fore rp Are .s xe /nt Dation jn (cid:0)fo 1 0r aA jxre jv ,.x as/ suD
mi
Pngjn D t(cid:0) h1 0 aa tn n(cid:0) o1 n(cid:0) ej ox fj thfr eo pm oia ntp so ii sn 0t-
.
D
30.1-4 P
Provethat n distinct point-value pairs are necessary touniquely specify apolyno-
mialofdegree-bound n,thatis,iffewerthanndistinctpoint-valuepairsaregiven,
they fail to specify a unique polynomial of degree-bound n. (Hint: Using Theo-
rem30.1,whatcanyousayaboutasetofn 1point-valuepairstowhichyouadd
(cid:0)
onemorearbitrarily chosenpoint-value pair?)
906 Chapter30 PolynomialsandtheFFT
30.1-5
Showhowtouseequation(30.5)tointerpolateintime‚.n2/. (Hint:Firstcompute
the coefficient representation of the polynomial .x x / and then divide by
j (cid:0) j
.x x /asnecessaryforthenumeratorofeachterm;seeExercise30.1-2. Youcan
k
(cid:0) Q
computeeachofthendenominators intimeO.n/.)
30.1-6
Explain what is wrong with the “obvious” approach to polynomial division using
a point-value representation, i.e., dividing the corresponding y values. Discuss
separately the case in which the division comes out exactly and the case in which
itdoesn’t.
30.1-7
ConsidertwosetsAandB,eachhavingnintegersintherangefrom0to10n. We
wishtocomputetheCartesiansumofAandB,definedby
C x y x Aandy B :
D f C W 2 2 g
Note that the integers in C are in the range from 0 to 20n. We want to find the
elements of C and the number of times each element of C is realized asa sum of
elements in A and B. Show how to solve the problem in O.nlgn/ time. (Hint:
Represent AandB aspolynomials ofdegreeatmost10n.)
30.2 The DFTand FFT
InSection30.1,weclaimedthatifweusecomplexrootsofunity, wecanevaluate
and interpolate polynomials in ‚.nlgn/ time. In this section, we define complex
roots of unity and study their properties, define the DFT, and then show how the
FFTcomputestheDFTanditsinversein‚.nlgn/time.
Complexrootsofunity
Acomplexnthrootofunityisacomplexnumber! suchthat
!n 1:
D
There are exactly n complex nth roots of unity: e2ik=n for k 0;1;:::;n 1.
D (cid:0)
To interpret this formula, we use the definition of the exponential of a complex
number:
eiu cos.u/ isin.u/:
D C
Figure30.2showsthatthencomplexrootsofunityareequally spacedaround the
circleofunitradiuscentered attheoriginofthecomplexplane. Thevalue
30.2 TheDFTandFFT 907
i !2
8
!3 !1
8 8
!4 !0 !8
8 8 D 8
1 1
(cid:0)
!5 !7
8 8
(cid:0)
i ! 86
Figure30.2 Thevaluesof! 80;! 81;:::;! 87inthecomplexplane,where!8 De2i=8istheprin-
cipal8throotofunity.
! e2i=n (30.6)
n
D
istheprincipalnthrootofunity;2 allothercomplexnthrootsofunityarepowers
of! .
n
Thencomplexnthrootsofunity,
!0;!1;:::;!n 1 ;
n n n(cid:0)
form a group under multiplication (see Section 31.3). This group has the same
structureastheadditivegroup.Z ; /modulon,since!n !0 1impliesthat
n C n D n D
!j!k !j k !.j k/modn. Similarly, ! 1 !n 1. The following lemmas
n n D nC D n C n(cid:0) D n(cid:0)
furnishsomeessentialproperties ofthecomplexnthrootsofunity.
Lemma30.3(Cancellation lemma)
Foranyintegersn 0,k 0,andd > 0,
 
!dk !k : (30.7)
dn D n
Proof Thelemmafollowsdirectly fromequation (30.6),since
!dk e2i=dn dk
dn D
(cid:0)e2i=n k 
D
!k :
D
(cid:0)n 
2Manyotherauthorsdefine!n differently: !n
D
e (cid:0)2i=n. Thisalternativedefinitiontendstobe
usedforsignal-processingapplications. Theunderlyingmathematicsissubstantiallythesamewith
eitherdefinitionof!n.
908 Chapter30 PolynomialsandtheFFT
Corollary 30.4
Foranyevenintegern > 0,
!n=2 ! 1:
n D 2 D(cid:0)
Proof TheproofisleftasExercise30.2-1.
Lemma30.5(Halvinglemma)
If n > 0is even, then the squares of the n complex nthroots of unity are the n=2
complex.n=2/throotsofunity.
Proof By the cancellation lemma, we have .!k/2 !k , for any nonnegative
n D n=2
integer k. Note that if we square all of the complex nth roots of unity, then we
obtaineach.n=2/throotofunityexactlytwice,since
.!k n=2/2 !2k n
nC D n C
!2k!n
D n n
!2k
D n
.!k/2 :
D n
Thus, !k and !k n=2 have the same square. We could also have used Corol-
n nC
lary 30.4 to prove this property, since !n=2 1 implies !k n=2 !k, and
n D (cid:0) nC D (cid:0) n
thus.!k n=2/2 .!k/2.
nC D n
As we shall see, the halving lemma is essential to our divide-and-conquer ap-
proach forconverting betweencoefficientandpoint-value representations ofpoly-
nomials, sinceitguarantees thattherecursivesubproblems areonlyhalfaslarge.
Lemma30.6(Summationlemma)
Foranyintegern 1andnonzerointegerk notdivisible byn,

n 1
(cid:0) !k j 0:
n D
j 0
XD (cid:0) 
Proof Equation (A.5) applies to complex values as well as to reals, and so we
have
30.2 TheDFTandFFT 909
n (cid:0)1
!k j
.! nk/n (cid:0)1
n D !k 1
j 0 n (cid:0)
XD (cid:0) 
.!n/k 1
n (cid:0)
D !k 1
n (cid:0)
.1/k 1
(cid:0)
D !k 1
n (cid:0)
0:
D
Because werequire that k isnotdivisible byn,and because !k 1only when k
n D
isdivisible byn,weensurethatthedenominator isnot0.
TheDFT
Recallthatwewishtoevaluate apolynomial
n 1
(cid:0)
A.x/ a xj
j
D
j 0
XD
of degree-bound n at !0;!1;!2;:::;!n 1 (that is, at the n complex nth roots of
n n n n(cid:0)
unity).3 WeassumethatAisgivenincoefficientform: a .a ;a ;:::;a /. Let
0 1 n 1
usdefinetheresultsy ,fork 0;1;:::;n 1,by D (cid:0)
k
D (cid:0)
y A.!k/
k D n
n 1
(cid:0)
a !kj : (30.8)
D j n
j 0
XD
Thevectory .y ;y ;:::;y /isthediscreteFouriertransform (DFT)ofthe
0 1 n 1
coefficientvecD tora .a ;a ;:(cid:0)::;a /. Wealsowritey DFT .a/.
0 1 n 1 n
D (cid:0) D
TheFFT
By using a method known as the fast Fourier transform (FFT), which takes ad-
vantage of the special properties of the complex roots of unity, we can compute
DFT .a/ in time ‚.nlgn/, as opposed to the ‚.n2/ time of the straightforward
n
method. Weassume throughout that nisanexact powerof2. Although strategies
3Thelengthnisactuallywhatwereferredtoas2ninSection30.1,sincewedoublethedegree-bound
ofthegivenpolynomialspriortoevaluation. Inthecontextofpolynomialmultiplication,therefore,
weareactuallyworkingwithcomplex.2n/throotsofunity.
910 Chapter30 PolynomialsandtheFFT
fordealingwithnon-power-of-2 sizesareknown,theyarebeyondthescopeofthis
book.
TheFFTmethodemploysadivide-and-conquerstrategy,usingtheeven-indexed
andodd-indexedcoefficientsofA.x/separatelytodefinethetwonewpolynomials
AŒ0.x/andAŒ1.x/ofdegree-bound n=2:
AŒ0.x/ a a x a x2 a xn=2 1 ;
0 2 4 n 2 (cid:0)
D C C CC (cid:0)
AŒ1.x/ a a x a x2 a xn=2 1 :
1 3 5 n 1 (cid:0)
D C C CC (cid:0)
NotethatAŒ0 contains alltheeven-indexed coefficients ofA(thebinaryrepresen-
tationoftheindexendsin0)andAŒ1containsalltheodd-indexedcoefficients(the
binaryrepresentation oftheindexendsin1). Itfollowsthat
A.x/ AŒ0.x2/ xAŒ1.x2/; (30.9)
D C
sothattheproblemofevaluating A.x/at!0;!1;:::;!n 1 reducesto
n n n(cid:0)
1. evaluating thedegree-bound n=2polynomials AŒ0.x/andAŒ1.x/atthepoints
.!0/2;.!1/2;:::;.!n 1/2 ; (30.10)
n n n(cid:0)
andthen
2. combiningtheresultsaccording toequation (30.9).
By the halving lemma, the list of values (30.10) consists not of n distinct val-
ues but only of the n=2 complex .n=2/th roots of unity, with each root occurring
exactly twice. Therefore, we recursively evaluate the polynomials AŒ0 and AŒ1
of degree-bound n=2 at the n=2 complex .n=2/th roots of unity. These subprob-
lems have exactly the same form as the original problem, but are half the size.
Wehavenowsuccessfully divided ann-element DFT computation intotwon=2-
n
element DFT computations. This decomposition is the basis for the follow-
n=2
ing recursive FFT algorithm, which computes the DFT of an n-element vector
a .a ;a ;:::;a /,wherenisapowerof2.
0 1 n 1
D (cid:0)
30.2 TheDFTandFFT 911
RECURSIVE-FFT.a/
1 n a:length //nisapowerof2
D
2 ifn == 1
3 returna
4 ! e2i=n
n
D
5 ! 1
D
6 aŒ0 .a ;a ;:::;a /
0 2 n 2
7 aŒ1 D .a ;a ;:::;a (cid:0) /
1 3 n 1
8 yŒ0 D RECURSIVE-F(cid:0) FT.aŒ0/
D
9 yŒ1 RECURSIVE-FFT.aŒ1/
D
10 fork 0ton=2 1
D (cid:0)
11 y yŒ0 !yŒ1
k D k C k
12 y yŒ0 !yŒ1
13
!k C.n= !2/ !D k (cid:0) k
n
D
14 returny //y isassumedtobeacolumnvector
The RECURSIVE-FFT procedure works as follows. Lines 2–3 represent the basis
oftherecursion; theDFTofoneelementistheelementitself,sinceinthiscase
y a !0
0 D 0 1
a 1
0
D 
a :
0
D
Lines 6–7 define the coefficient vectors for the polynomials AŒ0 and AŒ1. Lines
4, 5, and 13 guarantee that ! is updated properly so that whenever lines 11–12
are executed, we have ! !k. (Keeping a running value of ! from iteration
D n
to iteration saves time over computing !k from scratch each time through the for
n
loop.) Lines 8–9 perform the recursive DFT computations, setting, for k
n=2
D
0;1;:::;n=2 1,
(cid:0)
yŒ0 AŒ0.!k /;
k D n=2
yŒ1 AŒ1.!k /;
k D n=2
or,since!k !2k bythecancellation lemma,
n=2 D n
yŒ0 AŒ0.!2k/;
k D n
yŒ1 AŒ1.!2k/:
k D n
912 Chapter30 PolynomialsandtheFFT
Lines 11–12 combine the results oftherecursive DFT calculations. Fory ;y ;
n=2 0 1
:::;y ,line11yields
n=2 1
(cid:0)
y yŒ0 !kyŒ1
k D k C n k
AŒ0.!2k/ !kAŒ1.!2k/
D n C n n
A.!k/ (byequation (30.9)).
D n
Fory ;y ;:::;y ,lettingk 0;1;:::;n=2 1,line12yields
n=2 n=2 1 n 1
C (cid:0) D (cid:0)
y yŒ0 !kyŒ1
k C.n=2/ D k (cid:0) n k
yŒ0 !k .n=2/yŒ1 (since!k .n=2/ !k)
D k C nC k nC D(cid:0) n
AŒ0.!2k/ !k .n=2/AŒ1.!2k/
D n C nC n
AŒ0.!2k n/ !k .n=2/AŒ1.!2k n/ (since!2k n !2k)
D n C C nC n C n C D n
A.!k .n=2// (byequation (30.9)) .
D
nC
Thus, the vector y returned by RECURSIVE-FFT is indeed the DFT of the input
vectora.
Lines 11 and 12 multiply each value yŒ1 by !k, for k 0;1;:::;n=2 1.
k n D (cid:0)
Line 11 adds this product to
yŒ0
, and line 12 subtracts it. Because we use each
k
factor !k in both its positive and negative forms, we call the factors !k twiddle
n n
factors.
To determine the running time of procedure RECURSIVE-FFT, we note that
exclusive of the recursive calls, each invocation takes time ‚.n/, where n is the
lengthoftheinputvector. Therecurrence fortherunningtimeistherefore
T.n/ 2T.n=2/ ‚.n/
D C
‚.nlgn/:
D
Thus,wecanevaluateapolynomialofdegree-bound natthecomplexnthrootsof
unityintime‚.nlgn/usingthefastFouriertransform.
Interpolation atthecomplexrootsofunity
We now complete the polynomial multiplication scheme by showing how to in-
terpolate the complex roots ofunity byapolynomial, which enables ustoconvert
frompoint-valueformbacktocoefficientform. WeinterpolatebywritingtheDFT
asamatrixequation andthenlooking attheformofthematrixinverse.
From equation (30.4), we can write the DFT as the matrix product y V a,
n
D
whereV isaVandermondematrixcontaining theappropriate powersof! :
n n
30.2 TheDFTandFFT 913
y 1 1 1 1 1 a
0 0

y 1 ! !2 !3 !n 1 a
1 n n n  n(cid:0) 1
y 1 !2 !4 !6 !2.n 1/ a
2 n n n  n (cid:0) 2 :
y D 1 !3 !6 !9 !3.n 1/ a

:
:
:3
 
:
:
:
:
:
:n :
:
:n :
:
:n  :: : n :
:
:(cid:0)

:
:
:3

y 1 !n 1 !2.n 1/ !3.n 1/ !.n 1/.n 1/ a
n (cid:0)1 n(cid:0) n (cid:0) n (cid:0)  n (cid:0) (cid:0) n (cid:0)1
The .k;j/ entry of V is !kj, for j;k 0;1;:::;n 1. The exponents of the
n n D (cid:0)
entriesofV formamultiplication table.
n
For the inverse operation, which we write as a DFT 1.y/, we proceed by
D
(cid:0)n
multiplying y bythematrixV 1,theinverseofV .
n(cid:0) n
Theorem30.7
Forj;k 0;1;:::;n 1,the.j;k/entryofV 1 is! kj=n.
D (cid:0)
n(cid:0) n(cid:0)
Proof WeshowthatV 1V I ,then nidentitymatrix. Considerthe.j;j /
n(cid:0) n
D
n

0
entryofV 1V :
n(cid:0) n
n 1
ŒV 1V  (cid:0) .! kj=n/.!kj0 /
n(cid:0) njj0 D n(cid:0) n
k 0
XD
n 1
(cid:0) !k.j0 j/=n:
D n (cid:0)
k 0
XD
Thissummationequals1ifj j,anditis0otherwisebythesummationlemma
0
D
(Lemma30.6). Notethatwerelyon .n 1/ j j n 1,sothatj j is
0 0
(cid:0) (cid:0)  (cid:0)  (cid:0) (cid:0)
notdivisiblebyn,inorderforthesummationlemmatoapply.
GiventheinversematrixV 1,wehavethatDFT 1.y/isgivenby
n(cid:0) (cid:0)n
n 1
1 (cid:0)
a y ! kj (30.11)
j
D n
k n(cid:0)
k 0
XD
for j 0;1;:::;n 1. By comparing equations (30.8) and (30.11), we see that
D (cid:0)
bymodifyingtheFFTalgorithmtoswitchtherolesofaandy,replace! by! 1,
n n(cid:0)
and divide each element of the result by n, wecompute the inverse DFT(see Ex-
ercise30.2-4). Thus,wecancomputeDFT 1 in‚.nlgn/timeaswell.
(cid:0)n
We see that, by using the FFT and the inverse FFT, we can transform a poly-
nomial of degree-bound n back and forth between its coefficient representation
and a point-value representation in time ‚.nlgn/. In the context of polynomial
multiplication, wehaveshownthefollowing.
914 Chapter30 PolynomialsandtheFFT
Theorem30.8(Convolutiontheorem)
Foranytwovectorsaandb oflengthn,wherenisapowerof2,
a b DFT 1.DFT .a/ DFT .b//;
˝ D
(cid:0)2n 2n

2n
where the vectors a and b arepadded with0s tolength 2n and denotes the com-

ponentwise product oftwo2n-elementvectors.
Exercises
30.2-1
ProveCorollary 30.4.
30.2-2
ComputetheDFTofthevector.0;1;2;3/.
30.2-3
DoExercise30.1-1byusingthe‚.nlgn/-timescheme.
30.2-4
Writepseudocode tocomputeDFT 1 in‚.nlgn/time.
(cid:0)n
30.2-5
Describethegeneralization oftheFFTproceduretothecaseinwhichnisapower
of3. Givearecurrence fortherunningtime,andsolvetherecurrence.
30.2-6 ?
Suppose that instead of performing an n-element FFT over the field of complex
numbers (where n is even), we use the ring Z of integers modulo m, where
m
m 2tn=2 1 and t is an arbitrary positive integer. Use ! 2t instead of !
n
D C D
asaprincipalnthrootofunity,modulom. ProvethattheDFTandtheinverseDFT
arewelldefinedinthissystem.
30.2-7
Givenalistofvalues´ ;´ ;:::;´ (possiblywithrepetitions),showhowtofind
0 1 n 1
the coefficients of a polynomial P.(cid:0)x/ of degree-bound n 1 that has zeros only
C
at ´ ;´ ;:::;´ (possibly with repetitions). Your procedure should run in time
0 1 n 1
O.nlg2n/. (Hin(cid:0) t: The polynomial P.x/ has a zero at ´ if and only if P.x/ is a
j
multipleof.x ´ /.)
j
(cid:0)
30.2-8 ?
The chirp transform of a vector a .a ;a ;:::;a / is the vector y
0 1 n 1
.y 0;y 1;:::;y
n
(cid:0)1/, where y
k
D
jn D(cid:0)1 0D a j´kj and ´ is any(cid:0) complex number. ThD e
P
30.3 EfficientFFTimplementations 915
DFTistherefore aspecial caseofthechirptransform, obtained bytaking´ ! .
n
D
ShowhowtoevaluatethechirptransformintimeO.nlgn/foranycomplexnum-
ber´. (Hint:Usetheequation
n 1
y ´k2=2 (cid:0) a ´j2=2 ´ .k j/2=2
k j (cid:0) (cid:0)
D
j XD0  
toviewthechirptransform asaconvolution.)
30.3 Efficient FFTimplementations
Sincethepractical applications oftheDFT,suchassignalprocessing, demandthe
utmost speed, this section examines two efficient FFTimplementations. First, we
shallexamineaniterativeversionoftheFFTalgorithm thatrunsin‚.nlgn/time
but can have a lower constant hidden in the ‚-notation than the recursive version
in Section 30.2. (Depending on the exact implementation, the recursive version
mayusethehardware cache moreefficiently.) Then,weshall usetheinsights that
ledustotheiterativeimplementation todesignanefficientparallel FFTcircuit.
Aniterative FFTimplementation
We first note that the for loop of lines 10–13 of RECURSIVE-FFT involves com-
puting the value
!kyŒ1
twice. In compiler terminology, we call such a value a
n k
commonsubexpression. Wecanchange thelooptocompute itonlyonce, storing
itinatemporaryvariablet.
fork 0ton=2 1
D (cid:0)
t !yŒ1
D k
y yŒ0 t
k D k C
y yŒ0 t
!k C.n= !2/ !D k (cid:0)
n
D
Theoperation inthis loop, multiplying thetwiddle factor ! !k byyŒ1 , storing
D n k
the product into t, and adding and subtracting t from yŒ0 , is known as a butterfly
k
operation andisshownschematically inFigure30.3.
We now show how to make the FFT algorithm iterative rather than recursive
in structure. In Figure 30.4, we have arranged the input vectors to the recursive
calls in an invocation of RECURSIVE-FFT in a tree structure, where the initial
call is for n 8. The tree has one node for each call of the procedure, labeled
D
916 Chapter30 PolynomialsandtheFFT
yŒ0
+
yŒ0 !kyŒ1 yŒ0 yŒ0 !kyŒ1
k k C n k k k C n k
!k !k
n n
y kŒ1 • – y kŒ0 (cid:0)! nky kŒ1 y kŒ1 y kŒ0 (cid:0)! nky kŒ1
(a) (b)
Figure30.3 Abutterflyoperation. (a)Thetwoinput valuesenterfromtheleft, thetwiddlefac-
tor! nk ismultipliedbyy kŒ1 , andthesumanddifferenceareoutput ontheright. (b)Asimplified
drawingofabutterflyoperation.WewillusethisrepresentationinaparallelFFTcircuit.
(a ,a ,a ,a ,a ,a ,a ,a )
0 1 2 3 4 5 6 7
(a ,a ,a ,a ) (a ,a ,a ,a )
0 2 4 6 1 3 5 7
(a ,a ) (a ,a ) (a ,a ) (a ,a )
0 4 2 6 1 5 3 7
(a ) (a ) (a ) (a ) (a ) (a ) (a ) (a )
0 4 2 6 1 5 3 7
Figure30.4 ThetreeofinputvectorstotherecursivecallsoftheRECURSIVE-FFTprocedure.The
initialinvocationisforn 8.
D
by the corresponding input vector. Each RECURSIVE-FFT invocation makes two
recursive calls, unless it has received a 1-element vector. The first call appears in
theleftchild,andthesecondcallappears intherightchild.
Looking at the tree, we observe that if we could arrange the elements of the
initial vector a into the order in which they appear in the leaves, we could trace
the execution of the RECURSIVE-FFT procedure, but bottom up instead of top
down. First, we take the elements in pairs, compute the DFT of each pair using
one butterfly operation, and replace the pair with its DFT. The vector then holds
n=2 2-element DFTs. Next, we take these n=2 DFTs in pairs and compute the
DFTof the four vector elements they come from by executing two butterfly oper-
ations, replacing two 2-element DFTs with one 4-element DFT. The vector then
holds n=44-element DFTs. Wecontinue inthismanner untilthevectorholds two
.n=2/-element DFTs, which we combine using n=2 butterfly operations into the
finaln-elementDFT.
To turn this bottom-up approach into code, we use an array AŒ0::n 1 that
(cid:0)
initially holds theelements oftheinput vector a intheorder inwhich theyappear
30.3 EfficientFFTimplementations 917
intheleavesofthetreeofFigure30.4. (Weshallshowlaterhowtodeterminethis
order,whichisknownasabit-reversalpermutation.) Becausewehavetocombine
DFTsoneachlevelofthetree,weintroduceavariablestocountthelevels,ranging
from 1 (at the bottom, when we are combining pairs to form 2-element DFTs)
tolgn(atthetop,whenwearecombiningtwo.n=2/-elementDFTstoproducethe
finalresult). Thealgorithm thereforehasthefollowingstructure:
1 fors 1tolgn
D
2 fork 0ton 1by2s
D (cid:0)
3 combinethetwo2s 1-elementDFTsin
(cid:0)
AŒk::k 2s 1 1andAŒk 2s 1::k 2s 1
(cid:0) (cid:0)
C (cid:0) C C (cid:0)
intoone2s-elementDFTinAŒk::k 2s 1
C (cid:0)
We can express the body of the loop (line 3) as more precise pseudocode. We
copy the for loop from the RECURSIVE-FFT procedure, identifying yŒ0 with
AŒk::k 2s 1 1 and yŒ1 with AŒk 2s 1::k 2s 1. The twiddle fac-
(cid:0) (cid:0)
C (cid:0) C C (cid:0)
torusedineachbutterfly operation depends onthevalue ofs;itisapowerof! ,
m
where m 2s. (We introduce the variable m solely for the sake of readability.)
D
Weintroduce another temporary variable u that allows us to perform the butterfly
operation in place. When we replace line 3 of the overall structure by the loop
body, we get the following pseudocode, which forms the basis of the parallel im-
plementation we shall present later. The code first calls the auxiliary procedure
BIT-REVERSE-COPY.a;A/ to copy vector a into array A in the initial order in
whichweneedthevalues.
ITERATIVE-FFT.a/
1 BIT-REVERSE-COPY.a;A/
2 n a:length //nisapowerof2
D
3 fors 1tolgn
D
4 m 2s
D
5 ! e2i=m
m
D
6 fork 0ton 1bym
D (cid:0)
7 ! 1
D
8 forj 0tom=2 1
D (cid:0)
9 t !AŒk j m=2
D C C
10 u AŒk j
D C
11 AŒk j u t
C D C
12 AŒk j m=2 u t
C C D (cid:0)
13 ! !!
m
D
14 returnA
How does BIT-REVERSE-COPY get the elements of the input vector a into the
desired order in the array A? Theorder in which the leaves appear in Figure 30.4
918 Chapter30 PolynomialsandtheFFT
is a bit-reversal permutation. That is, if we let rev.k/ be the lgn-bit integer
formed by reversing the bits of the binary representation of k, then we want to
place vector element a in array position AŒrev.k/. In Figure 30.4, for exam-
k
ple, the leaves appear in the order 0;4;2;6;1;5;3;7; this sequence in binary is
000;100;010;110;001;101;011;111, andwhenwereverse thebits ofeachvalue
weget the sequence 000;001;010;011;100;101;110;111. Tosee that wewant a
bit-reversal permutationingeneral, wenotethatatthetoplevelofthetree,indices
whose low-order bit is 0 go into the left subtree and indices whose low-order bit
is1gointotherightsubtree. Stripping offthelow-orderbitateachlevel,wecon-
tinue this process down the tree, until we get the order given by the bit-reversal
permutation attheleaves.
Sincewecaneasilycomputethefunctionrev.k/,theBIT-REVERSE-COPY pro-
cedureissimple:
BIT-REVERSE-COPY.a;A/
1 n a:length
D
2 fork 0ton 1
D (cid:0)
3 AŒrev.k/ a
k
D
The iterative FFT implementation runs in time ‚.nlgn/. The call to BIT-
REVERSE-COPY.a;A/ certainly runs in O.nlgn/ time, since we iterate n times
and can reverse an integer between 0 and n 1, with lgn bits, in O.lgn/ time.
(cid:0)
(In practice, because weusually know the initial value ofn in advance, wewould
probably code a table mapping k to rev.k/, making BIT-REVERSE-COPY run in
‚.n/timewithalowhiddenconstant. Alternatively,wecouldusethecleveramor-
tized reverse binary counter scheme described in Problem 17-1.) Tocomplete the
proofthatITERATIVE-FFT runsintime‚.nlgn/,weshowthatL.n/,thenumber
of times the body of the innermost loop (lines 8–13) executes, is ‚.nlgn/. The
for loop of lines 6–13 iterates n=m n=2s times for each value of s, and the
D
innermost loopoflines8–13iterates m=2 2s 1 times. Thus,
(cid:0)
D
lgn
n
L.n/ 2s 1
(cid:0)
D 2s 
s 1
XD
lgn
n
D 2
s 1
XD
‚.nlgn/:
D
30.3 EfficientFFTimplementations 919
a y
0 0
!0
2
a y
1 1
!0
4
a y
2 2
!0 !1
2 4
a y
3 3
!0
8
a y
4 4
!0 !1
2 8
a y
5 5
!0 !2
4 8
a y
6 6
!0 !1 !3
2 4 8
a y
7 7
stages 1 stages 2 stages 3
D D D
Figure 30.5 A circuit that computes the FFT in parallel, here shown on n 8 inputs. Each
D
butterflyoperationtakesasinputthevaluesontwowires,alongwithatwiddlefactor,anditproduces
asoutputsthevaluesontwowires. Thestagesofbutterfliesarelabeledtocorrespondtoiterations
of the outermost loop of the ITERATIVE-FFTprocedure. Only the top and bottom wirespassing
through a butterflyinteract withit; wiresthat pass through the middle of a butterflydo not affect
thatbutterfly,noraretheirvalueschangedbythatbutterfly.Forexample,thetopbutterflyinstage2
hasnothingtodowithwire1(thewirewhoseoutputislabeledy1);itsinputsandoutputsareonly
on wires 0 and 2 (labeled y0 and y2, respectively). This circuit has depth ‚.lgn/ and performs
‚.nlgn/butterflyoperationsaltogether.
AparallelFFTcircuit
We can exploit many of the properties that allowed us to implement an efficient
iterative FFTalgorithm toproduce anefficient parallel algorithm for theFFT. We
will express the parallel FFT algorithm as a circuit. Figure 30.5 shows a parallel
FFT circuit, which computes the FFT on n inputs, for n 8. The circuit begins
D
with a bit-reverse permutation of the inputs, followed by lgn stages, each stage
consisting of n=2 butterflies executed in parallel. The depth of the circuit—the
maximum number of computational elements between any output and any input
thatcanreachit—istherefore ‚.lgn/.
The leftmost part of the parallel FFT circuit performs the bit-reverse permuta-
tion, andthe remainder mimicstheiterative ITERATIVE-FFT procedure. Because
eachiterationoftheoutermostforloopperformsn=2independent butterflyopera-
tions, thecircuit performs them inparallel. Thevalueofs ineachiteration within
920 Chapter30 PolynomialsandtheFFT
ITERATIVE-FFT corresponds to a stage of butterflies shown in Figure 30.5. For
s 1;2;:::;lgn,stage s consists ofn=2s groups ofbutterflies (corresponding to
D
eachvalueofkinITERATIVE-FFT),with2s (cid:0)1butterfliespergroup(corresponding
toeachvalueofj inITERATIVE-FFT). ThebutterfliesshowninFigure30.5corre-
spond tothebutterfly operations oftheinnermost loop(lines9–12of ITERATIVE-
FFT).Notealsothatthetwiddlefactorsusedinthebutterflies correspond tothose
usedinITERATIVE-FFT: instages,weuse! m0;! m1;:::;! mm=2 (cid:0)1,wherem
D
2s.
Exercises
30.3-1
Show how ITERATIVE-FFT computes the DFTof the input vector .0;2;3; 1;4;
(cid:0)
5;7;9/.
30.3-2
ShowhowtoimplementanFFTalgorithmwiththebit-reversalpermutationoccur-
ring at the end, rather than at the beginning, of the computation. (Hint: Consider
theinverseDFT.)
30.3-3
How many times does ITERATIVE-FFT compute twiddle factors in each stage?
Rewrite ITERATIVE-FFT tocomputetwiddlefactorsonly2s (cid:0)1 timesinstages.
30.3-4 ?
Suppose that the adders within the butterfly operations of the FFT circuit some-
times fail in such a manner that they always produce a zero output, independent
oftheir inputs. Supposethatexactly oneadderhasfailed, butthatyoudon’tknow
which one. Describe how you can identify the failed adder bysupplying inputs to
theoverallFFTcircuitandobserving theoutputs. Howefficientisyourmethod?
Problems
30-1 Divide-and-conquer multiplication
a. Show how to multiply two linear polynomials ax b and cx d using only
C C
threemultiplications. (Hint:Oneofthemultiplications is.a b/ .c d/.)
C  C
b. Give two divide-and-conquer algorithms for multiplying two polynomials of
degree-bound n in ‚.nlg3/ time. The first algorithm should divide the input
polynomialcoefficientsintoahighhalfandalowhalf,andthesecondalgorithm
shoulddividethemaccording towhethertheirindexisoddoreven.
ProblemsforChapter30 921
c. Show how to multiply two n-bit integers in O.nlg3/ steps, where each step
operates onatmostaconstantnumberof1-bitvalues.
30-2 Toeplitzmatrices
A Toeplitz matrix is an n n matrix A .a / such that a a for
ij ij i 1;j 1
i 2;3;:::;nandj 2;3 ;:::;n. D D (cid:0) (cid:0)
D D
a. IsthesumoftwoToeplitzmatricesnecessarily Toeplitz? Whatabouttheprod-
uct?
b. Describe how to represent a Toeplitz matrix so that you can add two n n

ToeplitzmatricesinO.n/time.
c. GiveanO.nlgn/-timealgorithmformultiplyingann nToeplitzmatrixbya

vectoroflengthn. Useyourrepresentation frompart(b).
d. Giveanefficientalgorithmformultiplyingtwon nToeplitzmatrices. Analyze

itsrunningtime.
30-3 MultidimensionalfastFouriertransform
We can generalize the 1-dimensional discrete Fourier transform defined by equa-
tion (30.8) tod dimensions. Theinput isa d-dimensional array A .a /
D
j1;j2;:::;jd
whose dimensions are n ;n ;:::;n , where n n n n. We define the
1 2 d 1 2 d
 D
d-dimensional discrete Fouriertransform bytheequation
n1 1n2 1 nd 1
(cid:0) (cid:0) (cid:0)
y a !j1k1!j2k2 !jdkd
k1;k2;:::;kd D  j1;j2;:::;jd n1 n2  nd
j X1 D0j X2 D0 j Xd D0
for0 k < n ,0 k < n ,...,0 k < n .
1 1 2 2 d d
  
a. Showthatwecancomputead-dimensional DFTbycomputing 1-dimensional
DFTs on each dimension in turn. That is, we first compute n=n separate
1
1-dimensional DFTs along dimension 1. Then, using the result of the DFTs
alongdimension1astheinput,wecomputen=n separate1-dimensionalDFTs
2
along dimension 2. Using this result as the input, we compute n=n separate
3
1-dimensional DFTsalongdimension3,andsoon,through dimensiond.
b. Showthat the ordering ofdimensions does notmatter, sothatwecancompute
a d-dimensional DFT by computing the 1-dimensional DFTs in any order of
thed dimensions.
922 Chapter30 PolynomialsandtheFFT
c. Showthatifwecomputeeach1-dimensional DFTbycomputingthefastFour-
ier transform, the total time to compute a d-dimensional DFT is O.nlgn/,
independent ofd.
30-4 Evaluatingallderivatives ofapolynomialatapoint
Givenapolynomial A.x/ofdegree-bound n,wedefineitstthderivativeby
A.x/ ift 0;
D
A.t/.x/
D
dd xA.t (cid:0)1/.x/ if1

t n (cid:0)1;
„ 0 ift n:

Fromthecoefficientrepresentation.a ;a ;:::;a /ofA.x/andagivenpointx ,
0 1 n 1 0
wewishtodetermineA.t/.x /fort 0;1;:::;n(cid:0) 1.
0
D (cid:0)
a. Givencoefficients b ;b ;:::;b suchthat
0 1 n 1
(cid:0)
n 1
(cid:0)
A.x/ b .x x /j ;
j 0
D (cid:0)
j 0
XD
showhowtocomputeA.t/.x /,fort 0;1;:::;n 1,inO.n/time.
0
D (cid:0)
b. Explain how to find b ;b ;:::;b in O.nlgn/ time, given A.x !k/ for
k 0;1;:::;n 1.
0 1 n (cid:0)1 0 C n
D (cid:0)
c. Provethat
n (cid:0)1 !kr n (cid:0)1
A.x !k/ n f.j/g.r j/ ;
0 C n D rŠ (cid:0)
!
r 0 j 0
XD XD
wheref.j/ a jŠ and
j
D 
x l=. l/Š if .n 1/ l 0;
g.l/ 0(cid:0) (cid:0) (cid:0) (cid:0)  
D
(
0 if1 l n 1:
  (cid:0)
d. Explain how to evaluate A.x !k/ for k 0;1;:::;n 1 in O.nlgn/
0 C n D (cid:0)
time. Conclude that we can evaluate all nontrivial derivatives of A.x/ at x in
0
O.nlgn/time.
ProblemsforChapter30 923
30-5 Polynomialevaluation atmultiplepoints
Wehave seenhowtoevaluate apolynomial ofdegree-bound natasingle point in
O.n/ time using Horner’s rule. We have also discovered how to evaluate such a
polynomial at all n complex roots of unity in O.nlgn/ time using the FFT. We
shall now show how to evaluate a polynomial of degree-bound n at n arbitrary
pointsinO.nlg2n/time.
Todoso,weshallassumethatwecancomputethepolynomial remainderwhen
onesuchpolynomial isdivided byanother inO.nlgn/time,aresult thatwestate
withoutproof. Forexample,theremainderof3x3 x2 3x 1whendividedby
C (cid:0) C
x2 x 2is
C C
.3x3 x2 3x 1/ mod .x2 x 2/ 7x 5:
C (cid:0) C C C D (cid:0) C
Given the coefficient representation of a polynomial A.x/
D
n k(cid:0)1 0a kxk and
n points x ;x ;:::;x , we wish to compute the n values A.x /D;A.x /;:::;
0 1 n 1 0 1
A.x /. For0 i j(cid:0) n 1,definethepolynomials P .x/ P j .x x /
andn Q(cid:0)1
.x/
A .x/ mod
P
.(cid:0)
x/. NotethatQ
.x/hasdegrij eeatD mostk jDi i.(cid:0) k
ij ij ij
D Q (cid:0)
a. ProvethatA.x/mod .x ´/ A.´/foranypoint´.
(cid:0) D
b. ProvethatQ .x/ A.x /andthatQ .x/ A.x/.
kk k 0;n 1
D (cid:0) D
c. Prove that for i k j, we have Q .x/ Q .x/ modP .x/ and
ik ij ik
  D
Q .x/ Q .x/ modP .x/.
kj ij kj
D
d. GiveanO.nlg2n/-timealgorithm toevaluate A.x /;A.x /;:::;A.x /.
0 1 n 1
(cid:0)
30-6 FFTusingmodulararithmetic
As defined, the discrete Fourier transform requires us to compute with complex
numbers, whichcanresultinalossofprecision duetoround-off errors. Forsome
problems, the answer is known to contain only integers, and by using a variant of
the FFT based on modular arithmetic, we can guarantee that the answer is calcu-
latedexactly. Anexampleofsuchaproblemisthatofmultiplyingtwopolynomials
with integer coefficients. Exercise 30.2-6 gives one approach, using amodulus of
length .n/ bits to handle a DFT on n points. This problem gives another ap-
proach, which uses a modulus of the more reasonable length O.lgn/; it requires
thatyouunderstand thematerialofChapter31. Letnbeapowerof2.
a. Supposethatwesearchforthesmallestk suchthatp kn 1isprime. Give
D C
a simple heuristic argument why we might expect k to be approximately lnn.
(Thevalueofk mightbemuchlarger orsmaller, butwecanreasonably expect
toexamine O.lgn/candidate values ofk onaverage.) Howdoes the expected
lengthofp comparetothelengthofn?
924 Chapter30 PolynomialsandtheFFT
Letg beagenerator ofZ ,andletw gk mod p.
p
D
b. Argue that the DFT and the inverse DFT are well-defined inverse operations
modulop,wherew isusedasaprincipalnthrootofunity.
c. Show how to make the FFT and its inverse work modulo p in time O.nlgn/,
where operations on words of O.lgn/ bits take unit time. Assume that the
algorithm isgivenp andw.
d. ComputetheDFTmodulop 17ofthevector.0;5;3;7;7;2;1;6/. Notethat
g 3isagenerator ofZ . D
D
17
Chapter notes
VanLoan’sbook[343]providesanoutstanding treatmentofthefastFouriertrans-
form. Press, Teukolsky, Vetterling, and Flannery [283, 284] have a good descrip-
tionofthefastFouriertransformanditsapplications. Foranexcellentintroduction
to signal processing, a popular FFT application area, see the texts by Oppenheim
andSchafer[266]andOppenheimandWillsky[267]. TheOppenheimandSchafer
bookalsoshowshowtohandlecasesinwhichnisnotanintegerpowerof2.
Fourieranalysis isnotlimitedto1-dimensional data. Itiswidelyusedinimage
processing to analyze data in 2 or more dimensions. The books by Gonzalez and
Woods[146]andPratt[281]discussmultidimensionalFouriertransformsandtheir
use inimageprocessing, andbooks byTolimieri, An, andLu[338]and VanLoan
[343]discussthemathematicsofmultidimensional fastFouriertransforms.
Cooley and Tukey[76]arewidely credited withdevising the FFTinthe 1960s.
TheFFThadinfactbeendiscoveredmanytimespreviously,butitsimportancewas
not fully realized before the advent ofmodern digital computers. Although Press,
Teukolsky, Vetterling, and Flannery attribute the origins of the method to Runge
and Ko¨nig in 1924, an article by Heideman, Johnson, and Burrus [163] traces the
history oftheFFTasfarbackasC.F.Gaussin1805.
Frigo and Johnson [117] developed a fast and flexible implementation of the
FFT,calledFFTW(“fastestFouriertransformintheWest”). FFTWisdesignedfor
situations requiring multipleDFTcomputations onthesameproblem size. Before
actually computing the DFTs, FFTW executes a “planner,” which, by a series of
trial runs, determines how best to decompose the FFT computation for the given
problem size on the host machine. FFTW adapts to use the hardware cache ef-
ficiently, and once subproblems are small enough, FFTW solves them with opti-
mized,straight-linecode. Furthermore,FFTWhastheunusualadvantageoftaking
‚.nlgn/timeforanyproblem sizen,evenwhennisalargeprime.
NotesforChapter30 925
AlthoughthestandardFouriertransformassumesthattheinputrepresentspoints
thatareuniformlyspacedinthetimedomain,othertechniquescanapproximatethe
FFTon“nonequispaced” data. ThearticlebyWare[348]provides anoverview.
31 Number-Theoretic Algorithms
Number theory was once viewed as a beautiful but largely useless subject in pure
mathematics. Todaynumber-theoreticalgorithmsareusedwidely,dueinlargepart
to the invention of cryptographic schemes based on large prime numbers. These
schemes are feasible because we can find large primes easily, and they are secure
becausewedonotknowhowtofactortheproductoflargeprimes(orsolverelated
problems,suchascomputingdiscretelogarithms)efficiently. Thischapterpresents
someofthenumbertheoryandrelatedalgorithmsthatunderlie suchapplications.
Section 31.1 introduces basic concepts of number theory, such as divisibility,
modular equivalence, and unique factorization. Section 31.2 studies one of the
world’s oldest algorithms: Euclid’s algorithm for computing the greatest common
divisoroftwointegers. Section31.3reviewsconceptsofmodulararithmetic. Sec-
tion31.4thenstudiesthesetofmultiplesofagivennumbera,modulon,andshows
how tofindallsolutions totheequation ax b .mod n/byusing Euclid’s algo-

rithm. The Chinese remainder theorem is presented in Section 31.5. Section 31.6
considerspowersofagivennumbera,modulon,andpresentsarepeated-squaring
algorithm forefficientlycomputing ab mod n,givena,b,andn. Thisoperation is
at the heart of efficient primality testing and of much modern cryptography. Sec-
tion31.7thendescribes theRSApublic-key cryptosystem. Section31.8examines
a randomized primality test. We can use this test to find large primes efficiently,
which we need to do in order to create keys for the RSA cryptosystem. Finally,
Section31.9reviewsasimplebuteffectiveheuristicforfactoringsmallintegers. It
is a curious fact that factoring is one problem people may wish to be intractable,
sincethesecurityofRSAdepends onthedifficultyoffactoring largeintegers.
Sizeofinputsandcostofarithmeticcomputations
Because weshall be working with large integers, weneed to adjust how wethink
aboutthesizeofaninputandaboutthecostofelementaryarithmetic operations.
In this chapter, a “large input” typically means an input containing “large in-
tegers” rather than an input containing “many integers” (as for sorting). Thus,
31.1 Elementarynumber-theoreticnotions 927
we shall measure the size of an input in terms of the number of bits required to
represent that input, not just the number of integers in the input. An algorithm
withintegerinputsa ;a ;:::;a isapolynomial-timealgorithm ifitrunsintime
1 2 k
polynomialinlga ;lga ;:::;lga ,thatis,polynomialinthelengthsofitsbinary-
1 2 k
encodedinputs.
In most of this book, we have found it convenient to think of the elemen-
tary arithmetic operations (multiplications, divisions, or computing remainders)
asprimitiveoperations thattakeoneunitoftime. Bycounting thenumberofsuch
arithmetic operations that an algorithm performs, we have a basis for making a
reasonableestimateofthealgorithm’sactualrunningtimeonacomputer. Elemen-
tary operations can be time-consuming, however, when their inputs are large. It
thusbecomes convenient tomeasure howmanybitoperations anumber-theoretic
algorithm requires. In this model, multiplying two ˇ-bit integers by the ordinary
method uses ‚.ˇ2/ bit operations. Similarly, we can divide a ˇ-bit integer by a
shorterintegerortaketheremainderofaˇ-bitintegerwhendividedbyashorterin-
tegerintime‚.ˇ2/bysimplealgorithms. (SeeExercise31.1-12.) Fastermethods
areknown. Forexample,asimpledivide-and-conquer methodformultiplyingtwo
ˇ-bit integers has a running time of ‚.ˇlg3/, and the fastest known method has
a running time of ‚.ˇlgˇlglgˇ/. For practical purposes, however, the ‚.ˇ2/
algorithm isoftenbest,andweshallusethisboundasabasisforouranalyses.
Weshallgenerallyanalyzealgorithmsinthischapterintermsofboththenumber
ofarithmetic operations andthenumberofbitoperations theyrequire.
31.1 Elementary number-theoretic notions
This section provides a brief review of notions from elementary number theory
concerning the set Z :::; 2; 1;0;1;2;::: of integers and the set N
D f (cid:0) (cid:0) g D
0;1;2;::: ofnatural numbers.
f g
Divisibilityanddivisors
Thenotionofoneintegerbeingdivisiblebyanotheriskeytothetheoryofnumbers.
The notation d a (read “d divides a”) means that a kd for some integer k.
j D
Everyinteger divides 0. Ifa > 0and d a,then d a . Ifd a,then wealso
j j j  j j j
saythataisamultipleofd. Ifd doesnotdividea,wewrited − a.
Ifd a and d 0, wesay that d is a divisor ofa. Note that d a ifand only
j  j
if d a, so that no generality is lost by defining the divisors to be nonnegative,
(cid:0) j
with the understanding that the negative of any divisor of a also divides a. A
928 Chapter31 Number-TheoreticAlgorithms
divisorofanonzerointegeraisatleast1butnotgreaterthan a . Forexample,the
j j
divisors of24are1,2,3,4,6,8,12,and24.
Everypositiveintegeraisdivisiblebythetrivialdivisors1anda. Thenontrivial
divisorsofaarethefactorsofa. Forexample,thefactorsof20are2,4,5,and10.
Primeandcompositenumbers
An integer a > 1 whose only divisors are the trivial divisors 1 and a is a prime
numberor,moresimply,aprime. Primeshavemanyspecial properties andplaya
criticalroleinnumbertheory. Thefirst20primes,inorder, are
2; 3; 5; 7; 11; 13; 17; 19; 23; 29; 31; 37; 41; 43; 47; 53; 59; 61; 67; 71:
Exercise31.1-2asksyoutoprovethatthereareinfinitelymanyprimes. Aninteger
a > 1 that is not prime isa composite numberor, more simply, acomposite. For
example, 39 is composite because 3 39. We call the integer 1 a unit, and it is
j
neither prime nor composite. Similarly, the integer 0 and all negative integers are
neitherprimenorcomposite.
Thedivisiontheorem,remainders, andmodularequivalence
Given an integer n, wecan partition the integers into those that are multiples of n
and those that are not multiples of n. Much number theory isbased upon refining
this partition by classifying the nonmultiples of n according to their remainders
when divided by n. The following theorem provides the basis for this refinement.
Weomittheproof(butsee,forexample, NivenandZuckerman[265]).
Theorem31.1(Divisiontheorem)
For any integer a and any positive integer n, there exist unique integers q and r
suchthat0 r < nanda qn r.
 D C
The value q a=n is the quotient of the division. The value r a mod n
D b c D
is the remainder (or residue) of the division. We have that n a if and only if
j
a mod n 0.
D
We can partition the integers into n equivalence classes according to their re-
maindersmodulon. Theequivalenceclassmoduloncontaining anintegerais
Œa a kn k Z :
n
D f C W 2 g
For example, Œ3 :::; 11; 4;3;10;17;::: ; we can also denote this set by
7
D f (cid:0) (cid:0) g
Œ 4 and Œ10 . Using the notation defined on page 54, we can say that writing
7 7
(cid:0)
a Œb is the same as writing a b .mod n/. The set of all such equivalence
n
2 
classesis
31.1 Elementarynumber-theoreticnotions 929
Z Œa 0 a n 1 : (31.1)
n n
D f W   (cid:0) g
Whenyouseethedefinition
Z 0;1;:::;n 1 ; (31.2)
n
D f (cid:0) g
you should read it as equivalent to equation (31.1) with the understanding that 0
representsŒ0 ,1representsŒ1 ,andsoon;eachclassisrepresentedbyitssmallest
n n
nonnegativeelement. Youshouldkeeptheunderlyingequivalenceclassesinmind,
however. Forexample,ifwereferto
1asamemberofZ
,wearereallyreferring
n
(cid:0)
toŒn 1 ,since 1 n 1 .mod n/.
n
(cid:0) (cid:0)  (cid:0)
Commondivisorsandgreatest commondivisors
Ifd isadivisor ofa andd isalsoadivisor ofb,then d isacommondivisor ofa
and b. Forexample, the divisors of 30 are 1, 2, 3, 5, 6, 10, 15, and 30, and so the
commondivisors of24 and30 are1, 2, 3, and 6. Notethat 1is acommon divisor
ofanytwointegers.
Animportant propertyofcommondivisorsisthat
d aandd b implies d .a b/andd .a b/: (31.3)
j j j C j (cid:0)
Moregenerally, wehavethat
d aandd b implies d .ax by/ (31.4)
j j j C
for any integers x and y. Also, if a b, then either a b or b 0, which
j j j  j j D
impliesthat
a b andb a implies a b : (31.5)
j j D ˙
The greatest common divisor of two integers a and b, not both zero, is the
largestofthecommondivisorsofaandb;wedenoteitbygcd.a;b/. Forexample,
gcd.24;30/ 6, gcd.5;7/ 1, and gcd.0;9/ 9. If a and b are both nonzero,
D D D
then gcd.a;b/ is an integer between 1 and min. a ; b /. We define gcd.0;0/ to
j j j j
be 0; this definition is necessary to make standard properties of the gcd function
(suchasequation (31.9)below)universally valid.
Thefollowingareelementary properties ofthegcdfunction:
gcd.a;b/ gcd.b;a/; (31.6)
D
gcd.a;b/ gcd. a;b/; (31.7)
D (cid:0)
gcd.a;b/ gcd. a ; b /; (31.8)
D j j j j
gcd.a;0/ a ; (31.9)
D j j
gcd.a;ka/ a foranyk Z: (31.10)
D j j 2
The following theorem provides an alternative and useful characterization of
gcd.a;b/.
930 Chapter31 Number-TheoreticAlgorithms
Theorem31.2
If a and b are any integers, not both zero, then gcd.a;b/ is the smallest positive
elementoftheset ax by x;y Z oflinearcombinations ofaandb.
f C W 2 g
Proof Let s be the smallest positive such linear combination of a and b, and let
s ax by forsomex;y Z . Letq a=s . Equation(3.8)thenimplies
D C 2 D b c
a mod s a qs
D (cid:0)
a q.ax by/
D (cid:0) C
a.1 qx/ b. qy/ ;
D (cid:0) C (cid:0)
and so a mod s is a linear combination of a and b as well. But, since 0

a mod s < s,wehavethata mod s 0,becausesisthesmallestpositivesuchlin-
D
earcombination. Therefore, wehavethats aand,byanalogous reasoning, s b.
j j
Thus, s is a common divisor of a and b, and so gcd.a;b/ s. Equation (31.4)

implies that gcd.a;b/ s, since gcd.a;b/ divides both a and b and s is a linear
j
combination of a and b. But gcd.a;b/ s and s > 0 imply that gcd.a;b/ s.
j 
Combining gcd.a;b/ s and gcd.a;b/ s yields gcd.a;b/ s. We conclude
  D
thats isthegreatestcommondivisorofaandb.
Corollary 31.3
Foranyintegersaandb,ifd aandd b,thend gcd.a;b/.
j j j
Proof This corollary follows from equation (31.4), because gcd.a;b/ is a linear
combination ofaandb byTheorem31.2.
Corollary 31.4
Forallintegersaandb andanynonnegative integern,
gcd.an;bn/ ngcd.a;b/:
D
Proof Ifn 0,thecorollaryistrivial. Ifn > 0,thengcd.an;bn/isthesmallest
positive elemD ent oftheset anx bny x;y Z ,whichisntimesthesmallest
positiveelementoftheset
f
ax
C
by
x;W
y
Z2
.
g
f C W 2 g
Corollary 31.5
Forallpositiveintegersn,a,andb,ifn ab andgcd.a;n/ 1,thenn b.
j D j
Proof WeleavetheproofasExercise31.1-5.
31.1 Elementarynumber-theoreticnotions 931
Relativelyprimeintegers
Two integers a and b are relatively prime if their only common divisor is 1, that
is,ifgcd.a;b/ 1. Forexample, 8and 15arerelatively prime, since thedivisors
D
of 8 are 1, 2, 4, and 8, and the divisors of 15 are 1, 3, 5, and 15. The following
theorem states that if two integers are each relatively prime to an integer p, then
theirproduct isrelativelyprimetop.
Theorem31.6
For any integers a, b, and p, if both gcd.a;p/ 1 and gcd.b;p/ 1, then
D D
gcd.ab;p/ 1.
D
Proof ItfollowsfromTheorem31.2thatthereexistintegersx,y,x ,andy such
0 0
that
ax py 1;
C D
bx py 1:
0 0
C D
Multiplying theseequations andrearranging, wehave
ab.xx/ p.ybx y ax pyy / 1:
0 0 0 0
C C C D
Since 1 is thus a positive linear combination of ab and p, an appeal to Theo-
rem31.2completestheproof.
Integers n , n , ..., n are pairwise relatively prime if, whenever i j, we
1 2 k
¤
havegcd.n ;n / 1.
i j
D
Uniquefactorization
Anelementarybutimportant factaboutdivisibility byprimesisthefollowing.
Theorem31.7
Forallprimesp andallintegersaandb,ifp ab,thenp aorp b (orboth).
j j j
Proof Assume for the purpose of contradiction that p ab, but that p − a and
j
p − b. Thus, gcd.a;p/ 1andgcd.b;p/ 1, since theonly divisors ofp are1
D D
and p, and we assume that p divides neither a nor b. Theorem 31.6 then implies
that gcd.ab;p/ 1, contradicting our assumption that p ab, since p ab
D j j
impliesgcd.ab;p/ p. Thiscontradiction completestheproof.
D
A consequence of Theorem 31.7 is that we can uniquely factor any composite
integerintoaproductofprimes.
932 Chapter31 Number-TheoreticAlgorithms
Theorem31.8(Uniquefactorization)
Thereisexactlyonewaytowriteanycompositeintegeraasaproductoftheform
a pe1pe2 per ;
D 1 2  r
wherethep areprime,p <p < < p ,andthee arepositiveintegers.
i 1 2 r i

Proof WeleavetheproofasExercise31.1-11.
Asanexample, thenumber6000isuniquely factoredintoprimesas24 3 53.
 
Exercises
31.1-1
Provethatifa >b > 0andc a b,thenc mod a b.
D C D
31.1-2
Prove that there are infinitely many primes. (Hint: Show that none of the primes
p ;p ;:::;p divide.p p p / 1.)
1 2 k 1 2 k
 C
31.1-3
Provethatifa b andb c,thena c.
j j j
31.1-4
Provethatifp isprimeand0 < k < p,thengcd.k;p/ 1.
D
31.1-5
ProveCorollary 31.5.
31.1-6
Provethatifpisprimeand0 <k <p,thenp p . Concludethatforallintegers
j k
a andb andallprimesp,
(cid:0) 
.a b/p ap bp .mod p/:
C  C
31.1-7
Provethatifaandb areanypositiveintegerssuchthata b,then
j
.x mod b/ moda x mod a
D
foranyx. Prove,underthesameassumptions, that
x y .mod b/ implies x y .mod a/
 
foranyintegersx andy.
31.2 Greatestcommondivisor 933
31.1-8
Foranyinteger k > 0,aninteger nisakthpowerifthereexistsaninteger a such
that ak n. Furthermore, n > 1 is a nontrivial power if it is a kth power for
D
some integer k > 1. Show how to determine whether a given ˇ-bit integer n is a
nontrivial powerintimepolynomial inˇ.
31.1-9
Proveequations (31.6)–(31.10).
31.1-10
Show that the gcd operator is associative. That is, prove that for all integers a, b,
andc,
gcd.a;gcd.b;c// gcd.gcd.a;b/;c/:
D
31.1-11 ?
ProveTheorem31.8.
31.1-12
Giveefficientalgorithms fortheoperations ofdividing aˇ-bitintegerbyashorter
integer and of taking the remainder of a ˇ-bit integer when divided by a shorter
integer. Youralgorithms shouldrunintime‚.ˇ2/.
31.1-13
Give an efficient algorithm to convert a given ˇ-bit (binary) integer to a decimal
representation. Argue that if multiplication or division of integers whose length
is at most ˇ takes time M.ˇ/, then we can convert binary to decimal in time
‚.M.ˇ/lgˇ/. (Hint: Use a divide-and-conquer approach, obtaining the top and
bottomhalvesoftheresultwithseparate recursions.)
31.2 Greatestcommondivisor
Inthissection, wedescribe Euclid’salgorithm forefficiently computing thegreat-
est common divisor of two integers. When weanalyze the running time, we shall
seeasurprising connection withthe Fibonacci numbers, whichyield aworst-case
inputforEuclid’salgorithm.
We restrict ourselves in this section to nonnegative integers. This restriction is
justifiedbyequation (31.8),whichstatesthatgcd.a;b/ gcd. a ; b /.
D j j j j
934 Chapter31 Number-TheoreticAlgorithms
In principle, we can compute gcd.a;b/ for positive integers a and b from the
primefactorizations ofaandb. Indeed, if
a pe1pe2 per ; (31.11)
D 1 2  r
b pf1pf2 pfr ; (31.12)
D 1 2  r
withzeroexponents being used tomakethesetofprimes p ;p ;:::;p thesame
1 2 r
forbotha andb,then,asExercise31.2-1asksyoutoshow,
gcd.a;b/ pmin.e1;f1/pmin.e2;f2/ pmin.er;fr/ : (31.13)
D 1 2  r
AsweshallshowinSection31.9,however,thebestalgorithmstodateforfactoring
donotruninpolynomialtime. Thus,thisapproachtocomputinggreatestcommon
divisors seemsunlikelytoyieldanefficientalgorithm.
Euclid’salgorithmforcomputinggreatestcommondivisorsreliesonthefollow-
ingtheorem.
Theorem31.9(GCDrecursiontheorem)
Foranynonnegativeintegeraandanypositiveintegerb,
gcd.a;b/ gcd.b;a mod b/:
D
Proof We shall show that gcd.a;b/ and gcd.b;a mod b/ divide each other, so
thatbyequation (31.5)theymustbeequal(sincetheyarebothnonnegative).
We first show that gcd.a;b/ gcd.b;a mod b/. If we let d gcd.a;b/, then
j D
d a and d b. By equation (3.8), a mod b a qb, where q a=b .
j j D (cid:0) D b c
Sincea mod b isthusalinearcombinationofaandb,equation(31.4)impliesthat
d .a modb/. Therefore, since d b and d .a mod b/, Corollary 31.3 implies
j j j
thatd gcd.b;a mod b/or,equivalently, that
j
gcd.a;b/ gcd.b;a mod b/: (31.14)
j
Showing that gcd.b;a mod b/ gcd.a;b/ is almost the same. If we now let
j
d gcd.b;a mod b/,thend b andd .a mod b/. Sincea qb .a mod b/,
D j j D C
whereq a=b ,wehave thata isalinear combination ofb and .a mod b/. By
D b c
equation (31.4), we conclude that d a. Since d b and d a, we have that
j j j
d gcd.a;b/byCorollary31.3or,equivalently, that
j
gcd.b;a mod b/ gcd.a;b/: (31.15)
j
Using equation (31.5) to combine equations (31.14) and (31.15) completes the
proof.
31.2 Greatestcommondivisor 935
Euclid’salgorithm
The Elements of Euclid (circa 300 B.C.) describes the following gcd algorithm,
although it may be of even earlier origin. We express Euclid’s algorithm as a
recursiveprogrambaseddirectlyonTheorem31.9. Theinputsaandbarearbitrary
nonnegativeintegers.
EUCLID.a;b/
1 ifb ==0
2 returna
3 elsereturn EUCLID.b;a mod b/
AsanexampleoftherunningofEUCLID,considerthecomputationofgcd.30;21/:
EUCLID.30;21/ EUCLID.21;9/
D
EUCLID.9;3/
D
EUCLID.3;0/
D
3:
D
Thiscomputation calls EUCLID recursively threetimes.
Thecorrectness of EUCLID follows from Theorem 31.9and theproperty that if
the algorithm returns a in line 2, then b 0, so that equation (31.9) implies that
D
gcd.a;b/ gcd.a;0/ a. The algorithm cannot recurse indefinitely, since the
D D
secondargumentstrictlydecreasesineachrecursivecallandisalwaysnonnegative.
Therefore, EUCLID alwaysterminates withthecorrectanswer.
TherunningtimeofEuclid’salgorithm
We analyze the worst-case running time of EUCLID as a function of the size of
a and b. We assume with no loss of generality that a > b 0. To justify this

assumption, observethatifb > a 0,then EUCLID.a;b/immediatelymakesthe

recursive call EUCLID.b;a/. That is, if the first argument is less than the second
argument, EUCLID spendsonerecursivecallswappingitsargumentsandthenpro-
ceeds. Similarly, if b a > 0, the procedure terminates after one recursive call,
D
sincea mod b 0.
D
The overall running time of EUCLID is proportional to the number of recursive
calls it makes. Our analysis makes use of the Fibonacci numbers F , defined by
k
therecurrence (3.22).
Lemma31.10
If a > b 1 and the call EUCLID.a;b/ performs k 1 recursive calls, then
 
a F andb F .
k 2 k 1
 C  C
936 Chapter31 Number-TheoreticAlgorithms
Proof The proof proceeds by induction on k. For the basis of the induction, let
k 1. Then, b 1 F , and since a > b, we must have a 2 F . Since
2 3
D  D  D
b > .a mod b/, in each recursive call the first argument is strictly larger than the
second; theassumptionthata > b therefore holdsforeachrecursivecall.
Assume inductively that the lemma holds if k 1 recursive calls are made; we
(cid:0)
shall then prove that the lemmaholds for k recursive calls. Since k > 0, wehave
b > 0, and EUCLID.a;b/ calls EUCLID.b;a mod b/ recursively, which in turn
makesk 1recursive calls. Theinductive hypothesis thenimpliesthat b F
k 1
(thusprov(cid:0) ingpartofthelemma),anda mod b F . Wehave  C
k

b .a mod b/ b .a b a=b /
C D C (cid:0) b c
a;

sincea >b > 0implies a=b 1. Thus,
b c 
a b .a mod b/
 C
F F
k 1 k
 C C
F :
k 2
D C
Thefollowingtheorem isanimmediatecorollary ofthislemma.
Theorem31.11(Lame´’stheorem)
For any integer k 1, if a > b 1 and b < F
k
1, then the call EUCLID.a;b/
makesfewerthank recursivecalls
.
C
We can show that the upper bound of Theorem 31.11 is the best possible by
showing that the call EUCLID.F
k
1;F k/ makes exactly k 1 recursive calls
when k 2. We use induction oC n k. For the base case, k(cid:0) 2, and the call
 D
EUCLID.F 3;F 2/ makes exactly one recursive call, to EUCLID.1;0/. (We have to
start at k 2, because when k 1 we do not have F > F .) For the induc-
2 1
D D
tivestep, assumethat EUCLID.F k;F
k
1/makesexactly k 2recursive calls. For
k > 2,wehaveF > F > 0andF(cid:0) F F ,and(cid:0) sobyExercise31.1-1,
k k 1 k 1 k k 1
wehaveF mod F (cid:0)F . Thus,C weD have C (cid:0)
k 1 k k 1
C D (cid:0)
gcd.F ;F / gcd.F ;F modF /
k 1 k k k 1 k
C D C
gcd.F ;F /:
k k 1
D (cid:0)
Therefore, the call EUCLID.F
k
1;F k/ recurses one time more than the call
EUCLID.F k;F
k
1/, or exactly kC 1 times, meeting the upper bound of Theo-
(cid:0) (cid:0)
rem31.11.
SinceF isapproximately k=p5,where isthegoldenratio.1 p5/=2de-
k
C
finedbyequation (3.24),thenumberofrecursivecallsinEUCLID isO.lgb/. (See
31.2 Greatestcommondivisor 937
a b a=b d x y
b c
99 78 1 3 11 14
(cid:0)
78 21 3 3 3 11
(cid:0)
21 15 1 3 2 3
(cid:0)
15 6 2 3 1 2
(cid:0)
6 3 2 3 0 1
3 0 — 3 1 0
Figure31.1 How EXTENDED-EUCLIDcomputes gcd.99;78/. Eachlineshows one level of the
recursion: thevaluesoftheinputsaandb,thecomputedvalue a=b ,andthevaluesd,x,andy
b c
returned. Thetriple.d;x;y/returnedbecomes thetriple.d ;x ;y /usedatthenext higher level
0 0 0
ofrecursion. ThecallEXTENDED-EUCLID.99;78/returns.3; 11;14/,sothatgcd.99;78/ 3
(cid:0) D D
99 . 11/ 78 14.
 (cid:0) C 
Exercise 31.2-5 for a tighter bound.) Therefore, if we call EUCLID on two ˇ-bit
numbers, then it performs O.ˇ/ arithmetic operations and O.ˇ3/ bit operations
(assuming that multiplication and division of ˇ-bit numbers take O.ˇ2/ bit oper-
ations). Problem 31-2 asks you to show an O.ˇ2/ bound on the number of bit
operations.
TheextendedformofEuclid’salgorithm
We now rewrite Euclid’s algorithm to compute additional useful information.
Specifically, we extend the algorithm to compute the integer coefficients x and y
suchthat
d gcd.a;b/ ax by : (31.16)
D D C
Notethat x andy maybezero ornegative. Weshall findthese coefficients useful
later for computing modular multiplicative inverses. The procedure EXTENDED-
EUCLID takes as input a pair of nonnegative integers and returns a triple of the
form.d;x;y/thatsatisfiesequation (31.16).
EXTENDED-EUCLID.a;b/
1 ifb ==0
2 return.a;1;0/
3 else.d 0;x 0;y 0/ EXTENDED-EUCLID.b;a modb/
D
4 .d;x;y/ .d ;y ;x a=b y /
0 0 0 0
D (cid:0)b c
5 return.d;x;y/
Figure31.1illustrates how EXTENDED-EUCLID computesgcd.99;78/.
The EXTENDED-EUCLID procedure is a variation of the EUCLID procedure.
Line 1 is equivalent to the test “b == 0” in line 1 of EUCLID. If b 0, then
D
938 Chapter31 Number-TheoreticAlgorithms
EXTENDED-EUCLID returns not only d a in line 2, but also the coefficients
D
x 1 and y 0, so that a ax by. If b 0, EXTENDED-EUCLID first
D D D C ¤
computes.d ;x ;y /suchthatd gcd.b;a mod b/and
0 0 0 0
D
d bx .a mod b/y : (31.17)
0 0 0
D C
As for EUCLID, we have in this case d gcd.a;b/ d
0
gcd.b;a mod b/.
D D D
Toobtain x and y such that d ax by, westart byrewriting equation (31.17)
D C
usingtheequation d d andequation (3.8):
0
D
d bx .a b a=b /y
0 0
D C (cid:0) b c
ay b.x a=b y /:
0 0 0
D C (cid:0)b c
Thus,choosingx y andy x a=b y satisfiestheequationd ax by,
0 0 0
D D (cid:0)b c D C
provingthecorrectness ofEXTENDED-EUCLID.
Since the number of recursive calls made in EUCLID is equal to the number
of recursive calls made in EXTENDED-EUCLID, the running times of EUCLID
and EXTENDED-EUCLID are the same, to within a constant factor. That is, for
a > b > 0,thenumberofrecursive callsisO.lgb/.
Exercises
31.2-1
Provethatequations (31.11)and(31.12)implyequation (31.13).
31.2-2
Computethevalues.d;x;y/thatthecall EXTENDED-EUCLID.899;493/ returns.
31.2-3
Provethatforallintegersa,k,andn,
gcd.a;n/ gcd.a kn;n/:
D C
31.2-4
Rewrite EUCLID in aniterative form that uses only aconstant amount ofmemory
(thatis,storesonlyaconstant numberofintegervalues).
31.2-5
Ifa > b 0,showthatthecall EUCLID.a;b/makesatmost1 log b recursive
 C
calls. Improvethisboundto1 log .b=gcd.a;b//.
C
31.2-6
Whatdoes EXTENDED-EUCLID.F
k
1;F k/return? Proveyouranswercorrect.
C
31.3 Modulararithmetic 939
31.2-7
Define the gcd function for more than two arguments by the recursive equation
gcd.a ;a ;:::;a / gcd.a ;gcd.a ;a ;:::;a //. Show that the gcd function
0 1 n 0 1 2 n
D
returnsthesameanswerindependentoftheorderinwhichitsargumentsarespeci-
fied. Alsoshowhowtofindintegersx ;x ;:::;x suchthatgcd.a ;a ;:::;a /
0 1 n 0 1 n
D
a x a x a x . Showthatthenumberofdivisions performed byyour
0 0 1 1 n n
C CC
algorithm isO.n lg.max a ;a ;:::;a //.
0 1 n
C f g
31.2-8
Define lcm.a ;a ;:::;a / to be the least common multiple of the n integers
1 2 n
a ;a ;:::;a ,thatis,thesmallestnonnegativeintegerthatisamultipleofeacha .
1 2 n i
Showhowtocomputelcm.a ;a ;:::;a /efficientlyusingthe(two-argument)gcd
1 2 n
operation asasubroutine.
31.2-9
Provethatn ,n ,n ,andn arepairwiserelativelyprimeifandonlyif
1 2 3 4
gcd.n n ;n n / gcd.n n ;n n / 1:
1 2 3 4 1 3 2 4
D D
More generally, show that n ;n ;:::;n are pairwise relatively prime if and only
1 2 k
ifasetof lgk pairsofnumbersderivedfromthen arerelatively prime.
i
d e
31.3 Modulararithmetic
Informally, we can think of modular arithmetic as arithmetic as usual over the
integers, except that if we are working modulo n, then every result x is replaced
by the element of 0;1;:::;n 1 that is equivalent to x, modulo n (that is, x is
f (cid:0) g
replaced by x modn). This informal model suffices if we stick to the operations
of addition, subtraction, and multiplication. A more formal model for modular
arithmetic, which we now give, is best described within the framework of group
theory.
Finitegroups
A group .S; / is a set S together with a binary operation defined on S for
˚ ˚
whichthefollowingproperties hold:
1. Closure: Foralla,b S,wehavea b S.
2 ˚ 2
2. Identity: Thereexistsanelement e S,called theidentity ofthe group, such
2
thate a a e aforalla S.
˚ D ˚ D 2
3. Associativity: Foralla,b,c S,wehave.a b/ c a .b c/.
2 ˚ ˚ D ˚ ˚
940 Chapter31 Number-TheoreticAlgorithms
4. Inverses: For each a S, there exists a unique element b S, called the
2 2
inverseofa,suchthata b b a e.
˚ D ˚ D
As an example, consider the familiar group .Z; / of the integers Z under the
C
operationofaddition: 0istheidentity,andtheinverseofais a. Ifagroup.S; /
(cid:0) ˚
satisfiesthecommutativelawa b b aforalla;b S,thenitisanabelian
˚ D ˚ 2
group. Ifagroup.S; /satisfies S < ,thenitisafinitegroup.
˚ j j 1
Thegroupsdefinedbymodularadditionandmultiplication
We can form two finite abelian groups by using addition and multiplication mod-
ulo n, where n is a positive integer. These groups are based on the equivalence
classesoftheintegersmodulon,definedinSection31.1.
Z
To define a group on , we need to have suitable binary operations, which
n
we obtain by redefining the ordinary operations of addition and multiplication.
Z
We can easily define addition and multiplication operations for , because the
n
equivalenceclassoftwointegersuniquelydeterminestheequivalenceclassoftheir
sumorproduct. Thatis,ifa a .mod n/andb b .mod n/,then
0 0
 
a b a b .mod n/;
0 0
C  C
ab ab .mod n/:
0 0

Thus,wedefineaddition andmultiplication modulon,denoted and ,by
n n
C 
Œa Œb Œa b ; (31.18)
n n n n
C D C
Œa Œb Œab :
n n n n
 D
(We can define subtraction similarly on Z by Œa Œb Œa b , but divi-
n n n n n
(cid:0) D (cid:0)
sion is more complicated, as we shall see.) These facts justify the common and
convenient practiceofusingthesmallestnonnegativeelementofeachequivalence
Z
classasitsrepresentative whenperforming computations in . Weadd, subtract,
n
and multiply as usual on the representatives, but we replace each result x by the
representative ofitsclass, thatis,byx mod n.
Using this definition of addition modulo n, we define the additive group
modulo n as .Z ; /. The size of the additive group modulo n is Z n.
n n n
Figure31.2(a)giveC stheoperation tableforthegroup.Z ; /. j j D
6 6
C
Theorem31.12
Thesystem .Z ; /isafiniteabelian group.
n n
C
Proof Equation (31.18) shows that .Z ; / is closed. Associativity and com-
n n
C
mutativity of followfromtheassociativity andcommutativity of :
n
C C
31.3 Modulararithmetic 941
+ 0 1 2 3 4 5 · 1 2 4 7 8 11 13 14
6 15
0 0 1 2 3 4 5 1 1 2 4 7 8 11 13 14
1 1 2 3 4 5 0 2 2 4 8 14 1 7 11 13
2 2 3 4 5 0 1 4 4 8 1 13 2 14 7 11
3 3 4 5 0 1 2 7 7 14 13 4 11 2 1 8
4 4 5 0 1 2 3 8 8 1 2 11 4 13 14 7
5 5 0 1 2 3 4 11 11 7 14 2 13 1 8 4
13 13 11 7 1 14 8 4 2
14 14 13 11 8 7 4 2 1
(a) (b)
Figure31.2 Twofinitegroups. Equivalenceclassesaredenotedbytheirrepresentativeelements.
(a)Thegroup.Z 6; C6/.(b)Thegroup.Z 15; 15/.
.Œa Œb / Œc Œa b Œc
n n n n n n n n
C C D C C
Œ.a b/ c
n
D C C
Œa .b c/
n
D C C
Œa Œb c
n n n
D C C
Œa .Œb Œc /;
n n n n n
D C C
Œa Œb Œa b
n n n n
C D C
Œb a
n
D C
Œb Œa :
n n n
D C
The identity element of .Z ; / is 0 (that is, Œ0 ). The (additive) inverse of
n n n
C
an element a (that is, of Œa )is the element a (that is, Œ a or Œn a ), since
n n n
(cid:0) (cid:0) (cid:0)
Œa Œ a Œa a Œ0 .
n n n n n
C (cid:0) D (cid:0) D
Using the definition of multiplication modulo n, we define the multiplicative
groupmodulonas.Z ; /. TheelementsofthisgrouparethesetZ ofelements
inZ
thatarerelativelyn prn imeton,sothateachonehasauniqueinversn
e,modulon:
n
Z Œa Z gcd.a;n/ 1 :
n
D f
n
2
n
W D g
To see that Z is well defined, note that for 0 a < n, we have a .a kn/
n
  C
.mod n/ for all integers k. By Exercise 31.2-3, therefore, gcd.a;n/ 1 implies
gcd.a kn;n/ 1forallintegers k. SinceŒa a kn k ZD ,thesetZ
C D
n
D f C W 2 g
n
iswelldefined. Anexampleofsuchagroupis
Z 1;2;4;7;8;11;13;14 ;
15
D f g
942 Chapter31 Number-TheoreticAlgorithms
where the group operation is multiplication modulo 15. (Here we denote an el-
ement Œa as a; for example, we denote Œ7 as 7.) Figure 31.2(b) shows the
15 15
group .Z ; /. Forexample, 8 11 13 .mod 15/, working in Z . Theiden-
15 15
 
15
tityforthisgroupis1.
Theorem31.13
Thesystem .Z ; /isafiniteabeliangroup.
n n
Proof Theorem 31.6 implies that .Z ; / is closed. Associativity and commu-
n n
tativity can be proved for as they were for in the proof of Theorem 31.12.
n n
 C
TheidentityelementisŒ1 . Toshowtheexistenceofinverses, letabeanelement
n
o sif nZ
cen
aand Zlet ,. ad nd;x;y/ be returned by EXTENDED-EUCLID.a;n/. Then, d
D
1,
2
n
ax ny 1 (31.19)
C D
or,equivalently,
ax 1 .mod n/:

Thus, Œx is a multiplicative inverse of Œa , modulo n. Furthermore, we claim
n n
that Œx Z . To see why, equation (31.19) demonstrates that the smallest pos-
n
2
n
itive linear combination of x and n must be 1. Therefore, Theorem 31.2 implies
that gcd.x;n/ 1. We defer the proof that inverses are uniquely defined until
D
Corollary31.26.
As an example of computing multiplicative inverses, suppose that a 5 and
D
n 11. Then EXTENDED-EUCLID.a;n/ returns .d;x;y/ .1; 2;1/, so that
D D (cid:0)
1 5 . 2/ 11 1. Thus,Œ 2 (i.e.,Œ9 )isthemultiplicativeinverseofŒ5 .
11 11 11
D Whe n(cid:0) worC king with the g(cid:0) roups .Z ; / and .Z ; / in the remainder of this
n Cn n n
chapter, wefollowtheconvenientpracticeofdenotingequivalenceclassesbytheir
representative elements and denoting the operations and by the usual arith-
n n
C 
metic notations and (or juxtaposition, so that ab a b) respectively. Also,
equivalences
moC
dulo
n
may also be interpreted as
equD ation
s in
Z
. For example,
n
thefollowingtwostatements areequivalent:
ax b .mod n/;

Œa Œx Œb :
n n n n
 D
As a further convenience, we sometimes refer to a group .S; / merely as S
˚
whentheoperation isunderstoodfromcontext. Wemaythusrefertothegroups
.Z ; /and.Z ; ˚ /asZ andZ ,respectively.
n Cn n n n n
Wedenotethe(multiplicative)inverseofanelementaby.a 1 mod n/. Division
(cid:0)
in Z is defined by the equation a=b ab 1 .mod n/. For example, in Z
n

(cid:0) 15
31.3 Modulararithmetic 943
we have that 7 1 13 .mod 15/, since 7 13 91 1 .mod 15/, so that
(cid:0)
  D 
4=7 4 13 7 .mod 15/.
Th esiz
e
ofZ
is denoted .n/. This function, known as Euler’s phi function,
n
satisfiestheequation
1
.n/ n 1 ; (31.20)
D (cid:0) p
 
p pisprYimeandp n
W j
so that p runs over all the primes dividing n (including n itself, if n is prime).
We shall not prove this formula here. Intuitively, we begin with a list of the n
remainders 0;1;:::;n 1 and then, for each prime p that divides n, cross out
f (cid:0) g
every multiple of p in the list. For example, since the prime divisors of 45 are 3
and5,
1 1
.45/ 45 1 1
D (cid:0) 3 (cid:0) 5
  
2 4
45
D 3 5
  
24:
D
Ifp isprime,thenZ 1;2;:::;p 1 ,and
p
D f (cid:0) g
1
.p/ p 1
D (cid:0) p
 
p 1: (31.21)
D (cid:0)
Ifniscomposite, then .n/ < n 1,although itcanbeshownthat
(cid:0)
n
.n/ > (31.22)
e lnlnn 3
C lnlnn
forn 3,where 0:5772156649::: isEuler’sconstant. Asomewhatsimpler
 D
(butlooser)lowerboundforn >5is
n
.n/ > : (31.23)
6lnlnn
Thelowerbound(31.22)isessentially thebestpossible, since
.n/
liminf e : (31.24)
(cid:0)
n n=lnlnn D
!1
Subgroups
If.S; /isagroup,S S,and.S ; /isalsoagroup,then.S ; /isasubgroup
0 0 0
˚  ˚ ˚
of.S; /. Forexample,theevenintegersformasubgroupoftheintegersunderthe
˚
operationofaddition. Thefollowingtheoremprovidesausefultoolforrecognizing
subgroups.
944 Chapter31 Number-TheoreticAlgorithms
Theorem31.14(Anonemptyclosedsubsetofafinitegroupisasubgroup)
If.S; /isafinitegroupandS isanynonemptysubsetofS suchthata b S
0 0
˚ ˚ 2
foralla;b S ,then.S ; /isasubgroup of.S; /.
0 0
2 ˚ ˚
Proof WeleavetheproofasExercise31.3-3.
For example, the set 0;2;4;6 forms a subgroup of Z , since it is nonempty
8
f g
andclosedundertheoperation (thatis,itisclosedunder ).
8
C C
The following theorem provides an extremely useful constraint on the size of a
subgroup; weomittheproof.
Theorem31.15(Lagrange’stheorem)
If.S; /isafinitegroupand.S ; /isasubgroupof.S; /,then S isadivisor
0 0
˚ ˚ ˚ j j
of S .
j j
A subgroup S of a group S is a proper subgroup if S S. We shall use the
0 0
¤
following corollary in our analysis in Section 31.8 of the Miller-Rabin primality
testprocedure.
Corollary 31.16
IfS isapropersubgroup ofafinitegroupS,then S S =2.
0 0
j j  j j
Subgroupsgeneratedbyanelement
Theorem31.14givesusaneasywaytoproduceasubgroupofafinitegroup.S; /:
˚
choose an element a and take all elements that can be generated from a using the
groupoperation. Specifically, definea.k/ fork 1by

k
a.k/ a a a a :
D D ˚ ˚˚
i 1
MD k
Forexample,ifwetakea 2inthegroupZ ,thesequence a.1/;a.2/;a.3/;:::is
6
D
2;4;0;2;4;0;2;œ4;0;::: :
In thegroup Z , wehave a.k/ ka mod n, and inthe group Z , wehave a.k/
n
D
n
D
ak mod n. Wedefinethesubgroupgeneratedbya,denoted a or. a ; /,by
h i h i ˚
a a.k/ k 1 :
h iDf W  g
Wesaythatageneratesthesubgroup a orthataisageneratorof a . SinceS is
h i h i
finite, a isafinitesubsetofS,possiblyincludingallofS. Sincetheassociativity
h i
of implies
˚
31.3 Modulararithmetic 945
a.i/ a.j/ a.i j/ ;
C
˚ D
a isclosedandtherefore,byTheorem31.14, a isasubgroupofS. Forexample,
h iZ h i
in ,wehave
6
0 0 ;
h i D f g
1 0;1;2;3;4;5 ;
h i D f g
2 0;2;4 :
h i D f g
Z
Similarly,in ,wehave
7
1 1 ;
h i D f g
2 1;2;4 ;
h i D f g
3 1;2;3;4;5;6 :
h i D f g
Theorderofa (inthegroupS),denotedord.a/,isdefinedasthesmallestposi-
tiveintegert suchthata.t/ e.
D
Theorem31.17
Foranyfinitegroup.S; /andanya S,theorderofaisequaltothesizeofthe
˚ 2
subgroup itgenerates, orord.a/ a .
D jh ij
Proof Let t ord.a/. Since a.t/ e and a.t k/ a.t/ a.k/ a.k/ for
C
D D D ˚ D
k 1, if i > t, then a.i/ a.j/ for some j < i. Thus, as we generate ele-
 D
ments by a, we see no new elements after a.t/. Thus, a a.1/;a.2/;:::;a.t/ ,
h i D f g
and so a t. To show that a t, we show that each element of the se-
jh ij  jh ij 
quencea.1/;a.2/;:::;a.t/ isdistinct. Supposeforthepurposeofcontradiction that
a.i/ a.j/ for some i and j satisfying 1 i < j t. Then, a.i k/ a.j k/
C C
D   D
for k 0. Butthis equality implies that a.i .t j// a.j .t j// e, a contradic-
C (cid:0) C (cid:0)
 D D
tion,sincei .t j/ < t butt istheleastpositivevaluesuchthata.t/ e. There-
C (cid:0) D
fore,eachelementofthesequence a.1/;a.2/;:::;a.t/ isdistinct, and a t. We
jh ij
conclude thatord.a/ a .
D jh ij
Corollary31.18
Thesequence a.1/;a.2/;::: isperiodic with period t ord.a/;that is, a.i/ a.j/
D D
ifandonlyifi j .mod t/.

Consistent with the above corollary, we define a.0/ as e and a.i/ as a.imodt/,
wheret ord.a/,forallintegersi.
D
Corollary31.19
If.S; /isafinitegroupwithidentity e,thenforalla S,
˚ 2
a.S/ e :
j j
D
946 Chapter31 Number-TheoreticAlgorithms
Proof Lagrange’s theorem (Theorem 31.15) implies that ord.a/ S , and so
j j j
S 0 .mod t/,wheret ord.a/. Therefore, a.S/ a.0/ e.
j j
j j  D D D
Exercises
31.3-1
Draw the group operation tables for the groups .Z ; / and .Z ; /. Show that
4 C4 5 5
thesegroupsareisomorphicbyexhibitingaone-to-one correspondence ˛ between
their elements such that a b c .mod 4/ if and only if ˛.a/ ˛.b/ ˛.c/
C   
.mod 5/.
31.3-2
Z Z
Listallsubgroups of andof .
9 13
31.3-3
ProveTheorem31.14.
31.3-4
Showthatifp isprimeande isapositiveinteger,then
.pe/ pe 1.p 1/:
(cid:0)
D (cid:0)
31.3-5
Show that for any integer n > 1 and for any a Z , the function f Z Z
definedbyf .x/ ax mod
nisapermutation2
ofZn
.
a
W
n
!
n
a
D
n
31.4 Solving modularlinearequations
Wenowconsider theproblem offindingsolutions totheequation
ax b .mod n/; (31.25)

where a > 0 and n > 0. This problem has several applications; for example,
we shall use it as part of the procedure for finding keys in the RSA public-key
cryptosystem in Section 31.7. Weassume that a, b, and nare given, and wewish
to find all values of x,modulo n, that satisfy equation (31.25). Theequation may
havezero,one,ormorethanonesuchsolution.
Let a denotethesubgroupofZ generatedbya. Since a a.x/ x > 0
n
h i h iDf W g D
ax mod n x >0 , equation (31.25) has a solution if and only if Œb a . La-
f W g 2 h i
grange’s theorem (Theorem 31.15) tells us that a must be a divisor of n. The
jh ij
followingtheorem givesusaprecisecharacterization of a .
h i
31.4 Solvingmodularlinearequations 947
Theorem31.20
Foranypositiveintegersaandn,ifd gcd.a;n/,then
D
a d 0;d;2d;:::;..n=d/ 1/d (31.26)
h iDh iD f (cid:0) g
Z
in ,andthus
n
a n=d :
jh ijD
Proof Webeginbyshowingthatd a . RecallthatEXTENDED-EUCLID.a;n/
2h i
producesintegersx andy suchthatax ny d. Thus,ax d .mod n/,so
0 0 0 0 0
thatd a . Inotherwords,d isamultipC leofaD inZ . 
n
2h i
Since d a , it follows that every multiple of d belongs to a , because any
2 h i h i
multipleofamultipleofaisitselfamultipleofa. Thus, a containseveryelement
h i
in 0;d;2d;:::;..n=d/ 1/d . Thatis, d a .
f (cid:0) g h i h i
We now show that a d . If m a , then m ax mod n for some
h i  h i 2 h i D
integerx,andsom ax ny forsomeintegery. However,d aandd n,and
D C j j
sod mbyequation (31.4). Therefore, m d .
j 2 h i
Combining these results, we have that a d . To see that a n=d,
h i D h i jh ij D
observethatthereareexactlyn=d multiplesofd between0andn 1,inclusive.
(cid:0)
Corollary31.21
Theequation ax b .mod n/issolvable fortheunknown x ifandonlyifd b,
 j
whered gcd.a;n/.
D
Proof Theequation ax b .mod n/issolvable ifand onlyifŒb a ,which
 2 h i
isthesameassaying
.b mod n/ 0;d;2d;:::;..n=d/ 1/d ;
2f (cid:0) g
by Theorem 31.20. If 0 b < n, then b a if and only if d b, since the
 2 h i j
members of a are precisely the multiples of d. Ifb < 0or b n, the corollary
h i 
then follows from the observation that d b if and only if d .b mod n/, since b
j j
andb mod ndifferbyamultipleofn,whichisitselfamultipleofd.
Corollary31.22
The equation ax b .mod n/ either has d distinct solutions modulo n, where

d gcd.a;n/,orithasnosolutions.
D
Proof If ax b .mod n/ has a solution, then b a . By Theorem 31.17,
 2 h i
ord.a/ a ,andsoCorollary31.18andTheorem31.20implythatthesequence
D jh ij
ai mod n,fori 0;1;:::,isperiodicwithperiod a n=d. Ifb a ,thenb
D jh ijD 2 h i
appears exactly d times in the sequence ai mod n, for i 0;1;:::;n 1, since
D (cid:0)
948 Chapter31 Number-TheoreticAlgorithms
thelength-.n=d/ blockofvalues a repeatsexactly d timesasi increases from0
h i
ton 1. Theindicesxofthed positionsforwhichax mod n barethesolutions
(cid:0) D
oftheequationax b .mod n/.

Theorem31.23
Let d gcd.a;n/, and suppose that d ax ny for some integers x and y
0 0 0 0
D D C
(for example, as computed by EXTENDED-EUCLID). If d b, then the equation
j
ax b .mod n/hasasoneofitssolutions thevaluex ,where
0

x x .b=d/ mod n:
0 0
D
Proof Wehave
ax ax .b=d/ .mod n/
0 0

d.b=d/ .mod n/ (because ax d .mod n/)
0
 
b .mod n/;

andthusx isasolutiontoax b .mod n/.
0

Theorem31.24
Suppose that the equation ax b .mod n/ is solvable (that is, d b, where
 j
d gcd.a;n/) and that x is any solution to this equation. Then, this equa-
0
D
tion has exactly d distinct solutions, modulo n, given by x x i.n=d/ for
i 0
D C
i 0;1;:::;d 1.
D (cid:0)
Proof Because n=d > 0 and 0 i.n=d/ < n for i 0;1;:::;d 1, the
 D (cid:0)
valuesx ;x ;:::;x arealldistinct,modulon. Sincex isasolutionofax b
0 1 d 1 0
.mod n/, we have ax(cid:0) mod n b .mod n/. Thus, for i 0;1;:::;d 1, we
0
 D (cid:0)
have
ax mod n a.x in=d/ mod n
i 0
D C
.ax ain=d/ mod n
0
D C
ax mod n (becaused aimpliesthatain=d isamultipleofn)
0
D j
b .mod n/;

and hence ax b .mod n/, making x a solution, too. By Corollary 31.22, the
i i

equation ax b .mod n/ has exactly d solutions, so that x ;x ;:::;x must
0 1 d 1
 (cid:0)
beallofthem.
We have now developed the mathematics needed to solve the equation ax b

.mod n/; the following algorithm prints all solutions to this equation. The inputs
a andnarearbitrary positiveintegers, andb isanarbitrary integer.
31.4 Solvingmodularlinearequations 949
MODULAR-LINEAR-EQUATION-SOLVER.a;b;n/
1 .d;x 0;y 0/ EXTENDED-EUCLID.a;n/
D
2 ifd b
j
3 x x .b=d/ mod n
0 0
D
4 fori 0tod 1
D (cid:0)
5 print.x i.n=d// modn
0
C
6 elseprint“nosolutions”
Asan example of theoperation of this procedure, consider the equation 14x

30 .mod 100/ (here, a 14, b 30, and n 100). Calling EXTENDED-
D D D
EUCLID in line 1, we obtain .d;x 0;y 0/ .2; 7;1/. Since 2 30, lines 3–5
D (cid:0) j
execute. Line 3 computes x . 7/.15/ mod 100 95. The loop on lines 4–5
0
D (cid:0) D
printsthetwosolutions 95and45.
The procedure MODULAR-LINEAR-EQUATION-SOLVER works as follows.
Line 1 computes d gcd.a;n/, along with two values x and y such that d
0 0
D D
ax ny , demonstrating that x is a solution to the equation ax d .mod n/.
0 0 0 0
C 
If d does not divide b, then the equation ax b .mod n/ has no solution, by

Corollary31.21. Line2checkstoseewhetherd b;ifnot,line6reportsthatthere
j
are no solutions. Otherwise, line 3 computes a solution x to ax b .mod n/,
0

inaccordance withTheorem31.23. Givenonesolution, Theorem31.24statesthat
adding multiples of .n=d/, modulo n, yields the other d 1 solutions. The for
(cid:0)
loop of lines 4–5 prints out all d solutions, beginning with x and spaced n=d
0
apart,modulon.
MODULAR-LINEAR-EQUATION-SOLVER performs O.lgn gcd.a;n// arith-
C
metic operations, since EXTENDED-EUCLID performs O.lgn/ arithmetic opera-
tions,andeachiterationoftheforloopoflines4–5performsaconstantnumberof
arithmeticoperations.
The following corollaries of Theorem 31.24 give specializations of particular
interest.
Corollary31.25
Foranyn > 1,ifgcd.a;n/ 1,then theequation ax b .mod n/hasaunique
D 
solution, modulon.
Ifb 1, acommon case of considerable interest, the x weare looking for is a
D
multiplicativeinverseofa,modulon.
Corollary31.26
Foranyn > 1,ifgcd.a;n/ 1,thentheequation ax 1 .mod n/hasaunique
D 
solution, modulon. Otherwise,ithasnosolution.
950 Chapter31 Number-TheoreticAlgorithms
Thanks to Corollary 31.26, we can use the notation a 1 mod n to refer to the
(cid:0)
multiplicative inverse of a, modulo n, when a and n are relatively prime. If
gcd.a;n/ 1, then the unique solution to the equation ax 1 .mod n/ is the
D 
integerx returned byEXTENDED-EUCLID, sincetheequation
gcd.a;n/ 1 ax ny
D D C
implies ax 1 .mod n/. Thus, we can compute a 1 mod n efficiently using
(cid:0)

EXTENDED-EUCLID.
Exercises
31.4-1
Findallsolutions totheequation 35x 10 .mod 50/.

31.4-2
Prove that the equation ax ay .mod n/ implies x y .mod n/ whenever
 
gcd.a;n/ 1. Showthatthecondition gcd.a;n/ 1isnecessary bysupplying a
D D
counterexample withgcd.a;n/ > 1.
31.4-3
Consider the following change to line 3 of the procedure MODULAR-LINEAR-
EQUATION-SOLVER:
3 x x .b=d/ mod .n=d/
0 0
D
Willthiswork? Explainwhyorwhynot.
31.4-4 ?
Let p be prime and f.x/ f f x f xt .mod p/ be a polyno-
0 1 t
mial of degree t, with coef ficientsC f draC wn f roC m Z . We say that a Z
i p p
2
is a zero of f if f.a/ 0 .mod p/. Prove that if a is a zero of f, then

f.x/ .x a/g.x/ .mod p/for some polynomial g.x/of degree t 1. Prove
 (cid:0) (cid:0)
by induction on t that if p is prime, then a polynomial f.x/of degree t can have
atmostt distinct zerosmodulop.
31.5 The Chinese remainder theorem
Around A.D.100,theChinesemathematicianSun-Tsu˘ solvedtheproblemoffind-
ing those integers x that leave remainders 2, 3, and 2 when divided by 3, 5, and 7
respectively. Onesuchsolution isx 23;allsolutions areoftheform23 105k
D C
31.5 TheChineseremaindertheorem 951
for arbitrary integers k. The “Chinese remainder theorem” provides a correspon-
dence between a system of equations modulo a set of pairwise relatively prime
moduli(forexample,3,5,and7)andanequationmodulotheirproduct(forexam-
ple,105).
The Chinese remainder theorem has two major applications. Let the inte-
ger n be factored as n n n n , where the factors n are pairwise relatively
1 2 k i
D 
prime. First, the Chinese remainder theorem is a descriptive “structure theorem”
Z
that describes the structure of as identical to that of the Cartesian product
n
Z Z Z withcomponentwiseadditionandmultiplicationmodulon
n1

n2

nk i
in the ith component. Second, this description helps us to design efficient algo-
Z
rithms,sinceworkingineachofthesystems canbemoreefficient(intermsof
ni
bitoperations) thanworkingmodulon.
Theorem31.27(Chineseremaindertheorem)
Let n n n n , where the n are pairwise relatively prime. Consider the
1 2 k i
D 
correspondence
a .a ;a ;:::;a /; (31.27)
1 2 k
$
wherea Z ,a Z ,and
2
n i
2
ni
a a mod n
i i
D
fori 1;2;:::;k. Then,mapping(31.27)isaone-to-one correspondence (bijec-
D Z Z Z Z
tion)between andtheCartesianproduct . Operationsper-
n
Z
n1 n2 nk
formedontheelementsof canbeequivalently performedonthecorresponding
n
k-tuplesbyperformingtheoperationsindependentlyineachcoordinatepositionin
theappropriate system. Thatis,if
a .a ;a ;:::;a /;
1 2 k
$
b .b ;b ;:::;b /;
1 2 k
$
then
.a b/ modn ..a b / mod n ;:::;.a b / mod n /; (31.28)
1 1 1 k k k
C $ C C
.a b/ mod n ..a b / mod n ;:::;.a b / mod n /; (31.29)
1 1 1 k k k
(cid:0) $ (cid:0) (cid:0)
.ab/mod n .a b mod n ;:::;a b mod n /: (31.30)
1 1 1 k k k
$
Proof Transforming between the two representations is fairly straightforward.
Going from a to .a ;a ;:::;a / is quite easy and requires only k “mod” opera-
1 2 k
tions.
Computing a from inputs .a ;a ;:::;a / is a bit more complicated. We begin
1 2 k
bydefiningm n=n fori 1;2;:::;k;thusm istheproductofallofthen ’s
i i i j
D D
otherthann : m n n n n n . Wenextdefine
i i 1 2 i 1 i 1 k
D  (cid:0) C 
952 Chapter31 Number-TheoreticAlgorithms
c m .m 1 mod n / (31.31)
i
D
i (cid:0)i i
for i 1;2;:::;k. Equation (31.31) is always well defined: since m and n are
i i
D
relatively prime (by Theorem 31.6), Corollary 31.26 guarantees that m 1 mod n
(cid:0)i i
exists. Finally,wecancomputeaasafunction ofa ,a ,...,a asfollows:
1 2 k
a .a c a c a c / .mod n/: (31.32)
1 1 2 2 k k
 C CC
We now show that equation (31.32) ensures that a a .mod n / for i
i i
 D
1;2;:::;k. Note that if j i, then m 0 .mod n /, which implies that c
j i j
¤  
m 0 .mod n /. Note also that c 1 .mod n /, from equation (31.31). We
j i i i
 
thushavetheappealing andusefulcorrespondence
c .0;0;:::;0;1;0;:::;0/;
i
$
avectorthathas0severywhereexceptintheithcoordinate,whereithasa1;thec
i
thusforma“basis”fortherepresentation, inacertainsense. Foreachi,therefore,
wehave
a a c .mod n /
i i i

a m .m 1 mod n / .mod n /

i i (cid:0)i i i
a .mod n /;
i i

which is what wewished to show: our method of computing a from the a ’s pro-
i
ducesaresultathatsatisfiestheconstraints a a .mod n /fori 1;2;:::;k.
i i
 D
The correspondence is one-to-one, since we can transform in both directions.
Finally, equations (31.28)–(31.30) follow directly from Exercise 31.1-7, since
x mod n .x mod n/ mod n foranyx andi 1;2;:::;k.
i i
D D
Weshallusethefollowingcorollaries laterinthischapter.
Corollary 31.28
If n ;n ;:::;n are pairwise relatively prime and n n n n , then for any
1 2 k 1 2 k
D 
integersa ;a ;:::;a ,thesetofsimultaneous equations
1 2 k
x a .mod n /;
i i

fori 1;2;:::;k,hasauniquesolution modulonfortheunknownx.
D
Corollary 31.29
If n ;n ;:::;n are pairwise relatively prime and n n n n , then for all
1 2 k 1 2 k
D 
integersx anda,
x a .mod n /
i

fori 1;2;:::;k ifandonlyif
D
x a .mod n/:

31.5 TheChineseremaindertheorem 953
0 1 2 3 4 5 6 7 8 9 10 11 12
0 0 40 15 55 30 5 45 20 60 35 10 50 25
1 26 1 41 16 56 31 6 46 21 61 36 11 51
2 52 27 2 42 17 57 32 7 47 22 62 37 12
3 13 53 28 3 43 18 58 33 8 48 23 63 38
4 39 14 54 29 4 44 19 59 34 9 49 24 64
Figure31.3 AnillustrationoftheChineseremaindertheoremforn1 5andn2 13. Forthis
D D
example, c1 26 and c2 40. In row i, column j isshown the value of a, modulo 65, such
D D
thatamod5 i andamod13 j. Notethatrow0, column0containsa0. Similarly,row4,
D D
column12containsa64(equivalentto 1). Sincec1 26,movingdownarowincreasesaby26.
(cid:0) D
Similarly, c2 40 means that moving right by a column increases a by 40. Increasing a by 1
D
correspondstomovingdiagonallydownwardandtotheright,wrappingaroundfromthebottomto
thetopandfromtherighttotheleft.
AsanexampleoftheapplicationoftheChineseremaindertheorem,supposewe
aregiventhetwoequations
a 2 .mod 5/;

a 3 .mod 13/;

so that a 2, n m 5, a 3, and n m 13, and we wish
1 1 2 2 2 1
D D D D D D
to compute a mod 65, since n n n 65. Because 13 1 2 .mod 5/ and
1 2 (cid:0)
D D 
5 1 8 .mod 13/,wehave
(cid:0)

c 13.2 mod 5/ 26;
1
D D
c 5.8 mod 13/ 40;
2
D D
and
a 2 26 3 40 .mod 65/
  C 
52 120 .mod 65/
 C
42 .mod 65/:

SeeFigure31.3foranillustration oftheChineseremaindertheorem,modulo65.
Thus,wecanworkmodulonbyworkingmodulondirectlyorbyworkinginthe
transformedrepresentation usingseparatemodulon computations, asconvenient.
i
Thecomputations areentirely equivalent.
Exercises
31.5-1
Findallsolutions totheequations x 4 .mod 5/andx 5 .mod 11/.
 
954 Chapter31 Number-TheoreticAlgorithms
31.5-2
Findallintegersxthatleaveremainders1,2,3whendividedby9,8,7respectively.
31.5-3
Arguethat,underthedefinitions ofTheorem31.27,ifgcd.a;n/ 1,then
D
.a 1 mod n/ ..a 1 modn /;.a 1 mod n /;:::;.a 1 mod n //:
(cid:0)
$
1(cid:0) 1 2(cid:0) 2 k(cid:0) k
31.5-4
UnderthedefinitionsofTheorem31.27,provethatforanypolynomialf,thenum-
ber of roots of the equation f.x/ 0 .mod n/ equals the product of the number

of roots of each of the equations f.x/ 0 .mod n /, f.x/ 0 .mod n /, ...,
1 2
 
f.x/ 0 .mod n /.
k

31.6 Powers ofanelement
Justasweoftenconsiderthemultiplesofagivenelementa,modulon,weconsider
thesequence ofpowersofa,modulon,wherea Z :
2
n
a0;a1;a2;a3;:::; (31.33)
modulo n. Indexing from 0, the 0th value in this sequence is a0 mod n 1, and
D
theithvalueisai mod n. Forexample,thepowersof3modulo7are
i 0 1 2 3 4 5 6 7 8 9 10 11

3i mod 7 1 3 2 6 4 5 1 3 2 6 4 5

whereasthepowersof2modulo7are
i 0 1 2 3 4 5 6 7 8 9 10 11

2i mod 7 1 2 4 1 2 4 1 2 4 1 2 4

In this section, let a denote the subgroup of Z generated by a by repeated
h i
n
multiplication, and let ord .a/ (the “order of a, modulo n”) denote the order of a
n
inZ . Forexample, 2 1;2;4 inZ ,andord .2/ 3. Usingthedefinitionof
the
En
uler phi
functioh ni D .n/f astheg
size
o7
f
Z
(see7
SectD
ion 31.3), wenow translate
n
Z
Corollary 31.19 intothenotation of toobtain Euler’stheorem andspecialize it
n
toZ ,wherep isprime,toobtainFermat’stheorem.
p
Theorem31.30(Euler’stheorem)
Foranyintegern >1,
a .n/ 1 .mod n/foralla Z :
 2
n
31.6 Powersofanelement 955
Theorem31.31(Fermat’stheorem)
Ifp isprime,then
ap 1 1 .mod p/foralla Z :
(cid:0)
 2
p
Proof Byequation(31.21), .p/ p 1ifp isprime.
D (cid:0)
Fermat’stheorem appliestoeveryelementinZ except0,since0 Z . Forall
a Z ,however,wehaveap a .mod p/ifp
ip
sprime. 62
p
p
2 If ord .g/ Z , then ev ery element in Z is a power of g, modulo n, and
g is a
prn
imitivD e rj
oon
tj or a generator of Z .
Fon
r example, 3 is a primitive root,
n
modulo 7, but 2 is not a primitive root, modulo 7. If Z possesses a primitive
n
Z
root,thegroup iscyclic. Weomittheproof ofthefollowing theorem, whichis
n
provenbyNivenandZuckerman[265].
Theorem31.32
The values of n > 1 for which Z is cyclic are 2, 4, pe, and 2pe, for all primes
n
p > 2andallpositiveintegerse.
IfgisaprimitiverootofZ andaisanyelementofZ ,thenthereexistsa´such
n n
that g´ a .mod n/. This´isadiscrete logarithm oranindex ofa,modulo n,

tothebaseg;wedenotethisvalueasind .a/.
n;g
Theorem31.33(Discretelogarithm theorem)
If g is a primitive root of Z , then the equation gx gy .mod n/ holds if and
n

onlyiftheequation x y .mod .n//holds.

Proof Suppose first that x y .mod .n//. Then, x y k .n/ for some
 D C
integerk. Therefore,
gx gy k .n/ .mod n/
C

gy .g .n//k .mod n/
 
gy 1k .mod n/ (byEuler’stheorem)
 
gy .mod n/:

Conversely,supposethatgx gy .mod n/. Becausethesequenceofpowersofg

generates every element of g and g .n/, Corollary 31.18 implies that
h i jh ij D
the sequence of powers of g is periodic with period .n/. Therefore, if gx gy

.mod n/,thenwemusthavex y .mod .n//.

Wenow turn our attention tothe square roots of1, modulo aprime power. The
following theorem will be useful in our development of a primality-testing algo-
rithminSection31.8.
956 Chapter31 Number-TheoreticAlgorithms
Theorem31.34
Ifp isanoddprimeande 1,thentheequation

x2 1 .mod pe/ (31.34)

hasonlytwosolutions, namelyx 1andx 1.
D D (cid:0)
Proof Equation(31.34)isequivalent to
pe .x 1/.x 1/:
j (cid:0) C
Since p > 2, we can have p .x 1/ or p .x 1/, but not both. (Otherwise,
j (cid:0) j C
by property (31.3), p would also divide their difference .x 1/ .x 1/ 2.)
C (cid:0) (cid:0) D
If p − .x 1/, then gcd.pe;x 1/ 1, and by Corollary 31.5, we would have
(cid:0) (cid:0) D
pe .x 1/. That is, x 1 .mod pe/. Symmetrically, if p − .x 1/,
j C  (cid:0) C
then gcd.pe;x 1/ 1, and Corollary 31.5 implies that pe .x 1/, so that
C D j (cid:0)
x 1 .mod pe/. Therefore,eitherx 1 .mod pe/orx 1 .mod pe/.
 (cid:0) 
Anumberxisanontrivialsquarerootof1,modulon,ifitsatisfiestheequation
x2 1 .mod n/ but x is equivalent to neither of the two “trivial” square roots:

1 or 1, modulo n. For example, 6 is a nontrivial square root of 1, modulo 35.
(cid:0)
Weshall use the following corollary to Theorem 31.34 in the correctness proof in
Section31.8fortheMiller-Rabin primality-testing procedure.
Corollary 31.35
Ifthereexistsanontrivial squarerootof1,modulon,thenniscomposite.
Proof Bythecontrapositive ofTheorem31.34, ifthereexistsanontrivial square
root of 1, modulo n, then n cannot be an odd prime or a power of an odd prime.
If x2 1 .mod 2/, then x 1 .mod 2/, and so allsquare roots of1, modulo 2,
 
are trivial. Thus, n cannot be prime. Finally, wemust have n > 1for a nontrivial
squarerootof1toexist. Therefore, nmustbecomposite.
Raisingtopowerswithrepeated squaring
A frequently occurring operation in number-theoretic computations is raising one
number to a power modulo another number, also known as modular exponentia-
tion. Moreprecisely, wewould likeanefficientwaytocompute ab mod n,where
a and b are nonnegative integers and n is a positive integer. Modular exponenti-
ation is an essential operation in many primality-testing routines and in the RSA
public-key cryptosystem. The method of repeated squaring solves this problem
efficientlyusingthebinaryrepresentation ofb.
Let b ;b ;:::;b ;b bethe binary representation ofb. (Thatis, the binary
k k 1 1 0
represeh ntation(cid:0) is k 1 bitsi long, b is the most significant bit, and b is the least
k 0
C
31.6 Powersofanelement 957
i 9 8 7 6 5 4 3 2 1 0
b i 1 0 0 0 1 1 0 0 0 0
c 1 2 4 8 17 35 70 140 280 560
d 7 49 157 526 160 241 298 166 67 1
Figure31.4 Theresultsof MODULAR-EXPONENTIATIONwhencomputing ab .modn/, where
a 7,b 560 1000110000 , andn 561. Thevaluesareshownaftereachexecutionofthe
D D Dh i D
forloop.Thefinalresultis1.
significant bit.) The following procedure computes ac mod n as c is increased by
doublings andincrementations from0tob.
MODULAR-EXPONENTIATION.a;b;n/
1 c 0
D
2 d 1
D
3 let b ;b ;:::;b bethebinaryrepresentation ofb
k k 1 0
4 forh i k(cid:0) downto0i
D
5 c 2c
D
6 d .d d/ modn
D 
7 ifb ==1
i
8 c c 1
D C
9 d .d a/ mod n
D 
10 returnd
Theessentialuseofsquaringinline6ofeachiterationexplainsthename“repeated
squaring.” As an example, for a 7, b 560, and n 561, the algorithm
D D D
computes the sequence of values modulo 561 shown in Figure 31.4; the sequence
ofexponents usedappearsintherowofthetablelabeledbyc.
The variable c is not really needed by the algorithm but is included for the fol-
lowingtwo-partloopinvariant:
Justpriortoeachiterationoftheforloopoflines4–9,
1. Thevalueofc isthesameastheprefix b ;b ;:::;b ofthebinary
k k 1 i 1
representation ofb,and h (cid:0) C i
2. d ac mod n.
D
Weusethisloopinvariant asfollows:
Initialization: Initially, i k, so that the prefix b ;b ;:::;b is empty,
k k 1 i 1
whichcorresponds tocD 0. Moreover, d 1 h a0 m(cid:0) od n. C i
D D D
958 Chapter31 Number-TheoreticAlgorithms
Maintenance: Letc andd denotethevaluesofc andd attheendofaniteration
0 0
of the for loop, and thus the values prior to the next iteration. Each iteration
updatesc 2c (ifb 0)orc 2c 1(ifb 1),sothatc willbecorrect
0 i 0 i
D D D C D
prior to the next iteration. If b 0, then d d2 mod n .ac/2 mod n
i 0
a2c mod n ac0 mod n. Ifb D 1,thend D d2a mod n D .ac/2a modn D
i 0
a2c 1 modD n ac0 mod n. InD either case,D d ac modD n prior to the neD xt
C
D D
iteration.
Termination: Attermination, i 1. Thus, c b, since c has the value of the
D (cid:0) D
prefix b ;b ;:::;b ofb’sbinaryrepresentation. Henced ac mod n
k k 1 0
ab modh n. (cid:0) i D D
If the inputs a, b, and n are ˇ-bit numbers, then the total number of arith-
metic operations required is O.ˇ/ and the total number of bit operations required
isO.ˇ3/.
Exercises
31.6-1
Z
Drawatableshowingtheorderofeveryelementin . Pickthesmallestprimitive
11
rootg andcomputeatablegivingind .x/forallx Z .
11;g
2
11
31.6-2
Giveamodular exponentiation algorithm that examines thebits ofb fromright to
leftinsteadoflefttoright.
31.6-3
Assumingthatyouknow .n/,explainhowtocomputea 1 mod nforanya Z
(cid:0)
2
n
usingtheprocedure MODULAR-EXPONENTIATION.
31.7 The RSA public-key cryptosystem
Withapublic-key cryptosystem, wecanencrypt messages sentbetween twocom-
municating partiessothataneavesdropper whooverhears theencrypted messages
will not be able to decode them. A public-key cryptosystem also enables a party
to append an unforgeable “digital signature” to the end of an electronic message.
Suchasignatureistheelectronicversionofahandwrittensignatureonapaperdoc-
ument. Itcanbeeasily checked byanyone, forged bynoone, yetlosesitsvalidity
ifanybitofthemessageisaltered. Ittherefore provides authentication ofboththe
identity ofthe signer and the contents ofthe signed message. Itis the perfect tool
31.7 TheRSApublic-keycryptosystem 959
forelectronically signed business contracts, electronic checks, electronic purchase
orders,andotherelectronic communications thatpartieswishtoauthenticate.
TheRSApublic-keycryptosystem reliesonthedramaticdifferencebetweenthe
ease of finding large prime numbers and the difficulty of factoring the product of
twolargeprimenumbers. Section31.8describesanefficientprocedureforfinding
large prime numbers, and Section 31.9 discusses the problem of factoring large
integers.
Public-keycryptosystems
In a public-key cryptosystem, each participant has both a public key and a secret
key. Each key is a piece of information. For example, in the RSA cryptosystem,
each key consists of a pair of integers. The participants “Alice” and “Bob” are
traditionally used in cryptography examples; we denote their public and secret
keysasP ,S forAliceandP ,S forBob.
A A B B
Each participant creates his or her own public and secret keys. Secret keys are
kept secret, but public keys can be revealed to anyone or even published. In fact,
it is often convenient to assume that everyone’s public key is available in a pub-
lic directory, so that any participant can easily obtain the public key of any other
participant.
Thepublicandsecretkeysspecifyfunctionsthatcanbeappliedtoanymessage.
D D
Let denotethesetofpermissiblemessages. Forexample, mightbethesetof
allfinite-length bitsequences. Inthesimplest, andoriginal, formulation ofpublic-
key cryptography, we require that the public and secret keys specify one-to-one
D
functionsfrom toitself. Wedenotethefunctioncorresponding toAlice’spublic
keyP byP ./andthe function corresponding tohersecret keyS by S ./. The
A A A A
functionsP ./andS ./arethuspermutationsofD . Weassumethatthefunctions
A A
P ./andS ./areefficientlycomputable giventhecorresponding keyP orS .
A A A A
Thepublic and secret keys for any participant are a“matched pair” in that they
specifyfunctions thatareinverses ofeachother. Thatis,
M S .P .M//; (31.35)
A A
D
M P .S .M// (31.36)
A A
D
for any message M D . Transforming M with the two keys P and S succes-
A A
2
sively,ineitherorder, yieldsthemessageM back.
In a public-key cryptosystem, we require that no one but Alice be able to com-
putethefunction S ./inanypractical amount oftime. Thisassumption iscrucial
A
to keeping encrypted mail sent to Alice private and to knowing that Alice’s digi-
tal signatures are authentic. Alice must keep S secret; if she does not, she loses
A
her uniqueness and the cryptosystem cannot provide her with unique capabilities.
TheassumptionthatonlyAlicecancomputeS ./mustholdeventhougheveryone
A
960 Chapter31 Number-TheoreticAlgorithms
Bob Alice
communication channel
encrypt decrypt
C P .M/
M P A D A S A M
eavesdropper
C
Figure31.5 Encryptioninapublickeysystem.BobencryptsthemessageM usingAlice’spublic
key P A andtransmitstheresultingciphertext C P A.M/over acommunication channel toAl-
D
ice. AneavesdropperwhocapturesthetransmittedciphertextgainsnoinformationaboutM. Alice
receivesC anddecryptsitusinghersecretkeytoobtaintheoriginalmessageM S A.C/.
D
knowsP andcancomputeP ./,theinversefunction toS ./,efficiently. Inorder
A A A
to design a workable public-key cryptosystem, we must figure out how to create
a system in which we can reveal a transformation P ./ without thereby revealing
A
how tocompute the corresponding inverse transformation S ./. Thistaskappears
A
formidable, butweshallseehowtoaccomplish it.
In a public-key cryptosystem, encryption works as shown in Figure 31.5. Sup-
pose Bob wishes to send Alice a message M encrypted so that it will look like
unintelligible gibberish to aneavesdropper. Thescenario for sending the message
goesasfollows.
 Bob obtains Alice’s public key P (from a public directory or directly from
A
Alice).
 Bob computes the ciphertext C P .M/ corresponding to the message M
A
D
andsendsC toAlice.
 WhenAlicereceivestheciphertext C,sheapplies hersecret keyS toretrieve
A
theoriginalmessage: S .C/ S .P .M// M.
A A A
D D
Because S ./ and P ./ are inverse functions, Alice can compute M from C. Be-
A A
causeonlyAliceisabletocomputeS ./,AliceistheonlyonewhocancomputeM
A
fromC. BecauseBobencryptsM usingP ./,onlyAlicecanunderstandthetrans-
A
mittedmessage.
We can just as easily implement digital signatures within our formulation of a
public-key cryptosystem. (There are other ways of approaching the problem of
constructing digital signatures, but weshall not go into them here.) Suppose now
that Alice wishes to send Bob a digitally signed response M . Figure 31.6 shows
0
howthedigital-signature scenarioproceeds.
 Alice computes her digital signature  for the message M using her secret
0
keyS andtheequation  S .M /.
A A 0
D
31.7 TheRSApublic-keycryptosystem 961
Alice Bob
sign verify
 S .M / 
S A D A 0 P A
=? accept
.M ;/ M
M 0 0
0
communication channel
Figure31.6 Digitalsignaturesinapublic-keysystem. AlicesignsthemessageM 0 byappending
herdigitalsignature
D
S A.M 0/toit. Shetransmitsthemessage/signaturepair.M 0;/toBob,
whoverifiesitbycheckingtheequationM
0
DP A./. Iftheequationholds,heaccepts.M 0;/as
amessagethatAlicehassigned.
 Alicesendsthemessage/signature pair.M ;/toBob.
0
 When Bob receives .M ;/, he can verify that it originated from Alice by us-
0
ing Alice’s public key to verify the equation M P ./. (Presumably, M
0 A 0
D
contains Alice’sname,soBobknowswhosepublickeytouse.) Iftheequation
holds, then Bob concludes that the message M was actually signed by Alice.
0
If the equation fails to hold, Bob concludes either that the message M or the
0
digitalsignature wascorruptedbytransmissionerrorsorthatthepair.M ;/
0
isanattemptedforgery.
Becauseadigitalsignatureprovidesbothauthenticationofthesigner’sidentityand
authentication ofthecontentsofthesignedmessage,itisanalogoustoahandwrit-
tensignature attheendofawrittendocument.
A digital signature must be verifiable by anyone who has access to the signer’s
public key. A signed message can be verified by one party and then passed on to
other parties who can also verify the signature. For example, the message might
be an electronic check from Alice to Bob. After Bobverifies Alice’s signature on
thecheck,hecangivethechecktohisbank,whocanthenalsoverifythesignature
andeffecttheappropriate fundstransfer.
Asignedmessageisnotnecessarilyencrypted;themessagecanbe“intheclear”
and not protected from disclosure. Bycomposing the above protocols for encryp-
tionandforsignatures,wecancreatemessagesthatarebothsignedandencrypted.
The signer first appends his or her digital signature to the message and then en-
crypts the resulting message/signature pair with the public key ofthe intended re-
cipient. The recipient decrypts the received message with his or her secret key to
obtain both the original message and its digital signature. The recipient can then
verify the signature using the public key of the signer. The corresponding com-
binedprocessusingpaper-based systemswouldbetosignthepaperdocumentand
962 Chapter31 Number-TheoreticAlgorithms
thensealthedocumentinsideapaperenvelopethatisopenedonlybytheintended
recipient.
TheRSAcryptosystem
In the RSA public-key cryptosystem, a participant creates his or her public and
secretkeyswiththefollowingprocedure:
1. Selectatrandomtwolargeprimenumberspandqsuchthatp q. Theprimes
¤
p andq mightbe,say,1024bitseach.
2. Computen pq.
D
3. Select a small odd integer e that is relatively prime to .n/, which, by equa-
tion(31.20), equals.p 1/.q 1/.
(cid:0) (cid:0)
4. Compute d as the multiplicative inverse of e, modulo .n/. (Corollary 31.26
guarantees that d exists and is uniquely defined. We can use the technique of
Section31.4tocomputed,givene and .n/.)
5. PublishthepairP .e;n/astheparticipant’s RSApublickey.
D
6. KeepsecretthepairS .d;n/astheparticipant’s RSAsecretkey.
D
Forthis scheme, the domain D is the set Z . Totransform a message M asso-
n
ciatedwithapublickeyP .e;n/,compute
D
P.M/ Me mod n: (31.37)
D
Totransform aciphertext C associated withasecretkeyS .d;n/,compute
D
S.C/ Cd mod n: (31.38)
D
Theseequationsapplytobothencryptionandsignatures. Tocreateasignature,the
signer applies his or her secret key to the message to be signed, rather than to a
ciphertext. Toverifyasignature, thepublickeyofthesignerisappliedtoit,rather
thantoamessagetobeencrypted.
Wecanimplementthepublic-keyandsecret-keyoperationsusingtheprocedure
MODULAR-EXPONENTIATION described inSection 31.6. Toanalyze therunning
time of these operations, assume that the public key .e;n/ and secret key .d;n/
satisfy lge O.1/,lgd ˇ,andlgn ˇ. Then,applying apublic keyrequires
D  
O.1/ modular multiplications and uses O.ˇ2/ bit operations. Applying a secret
keyrequires O.ˇ/modularmultiplications, usingO.ˇ3/bitoperations.
Theorem31.36(CorrectnessofRSA)
Z
TheRSAequations(31.37)and(31.38)defineinversetransformations of satis-
n
fyingequations (31.35)and(31.36).
31.7 TheRSApublic-keycryptosystem 963
Proof Fromequations (31.37)and(31.38), wehavethatforanyM Z ,
n
2
P.S.M// S.P.M// Med .mod n/:
D D
Sincee andd aremultiplicative inversesmodulo .n/ .p 1/.q 1/,
D (cid:0) (cid:0)
ed 1 k.p 1/.q 1/
D C (cid:0) (cid:0)
forsomeintegerk. Butthen,ifM 0 .mod p/,wehave
6
Med M.Mp 1/k.q 1/ .mod p/
(cid:0) (cid:0)

M..M mod p/p 1/k.q 1/ .mod p/
(cid:0) (cid:0)

M.1/k.q 1/ .mod p/ (byTheorem31.31)
(cid:0)

M .mod p/:

Also,Med M .mod p/ifM 0 .mod p/. Thus,
 
Med M .mod p/

forallM. Similarly,
Med M .mod q/

forallM. Thus,byCorollary31.29totheChineseremaindertheorem,
Med M .mod n/

forallM.
ThesecurityoftheRSAcryptosystem restsinlargepartonthedifficultyoffac-
toringlargeintegers. Ifanadversarycanfactorthemodulusninapublickey,then
the adversary can derive the secret key from the public key, using the knowledge
ofthefactorsp andq inthesamewaythatthecreatorofthepublickeyusedthem.
Therefore, iffactoring large integers iseasy, then breaking theRSAcryptosystem
iseasy. Theconverse statement, thatiffactoring largeintegersishard,thenbreak-
ing RSA is hard, is unproven. After two decades of research, however, no easier
method has been found to break the RSA public-key cryptosystem than to factor
themodulusn. AndasweshallseeinSection31.9,factoring largeintegersissur-
prisingly difficult. By randomly selecting and multiplying together two 1024-bit
primes,wecancreateapublickeythatcannotbe“broken” inanyfeasible amount
of timewith current technology. In the absence of afundamental breakthrough in
the design of number-theoretic algorithms, and when implemented with care fol-
lowing recommended standards, the RSA cryptosystem is capable of providing a
highdegreeofsecurity inapplications.
In order to achieve security with the RSA cryptosystem, however, we should
use integers that are quite long—hundreds or even more than one thousand bits
964 Chapter31 Number-TheoreticAlgorithms
long—to resist possible advances in the art of factoring. At the time of this
writing(2009), RSA moduli were commonly in the range of 768 to 2048 bits.
To create moduli of such sizes, we must be able to find large primes efficiently.
Section31.8addresses thisproblem.
For efficiency, RSA is often used in a “hybrid” or “key-management” mode
with fast non-public-key cryptosystems. With such a system, the encryption and
decryption keys are identical. If Alice wishes to send a long message M to Bob
privately,sheselectsarandomkeyK forthefastnon-public-keycryptosystem and
encrypts M using K, obtaining ciphertext C. Here, C is as long as M, but K
is quite short. Then, she encrypts K using Bob’s public RSA key. Since K is
short, computing P .K/ is fast (much faster than computing P .M/). She then
B B
transmits .C;P .K// to Bob, who decrypts P .K/ to obtain K and then uses K
B B
todecryptC,obtaining M.
We can use a similar hybrid approach to make digital signatures efficiently.
ThisapproachcombinesRSAwithapubliccollision-resistanthashfunctionh—a
function that is easy to compute but for which it is computationally infeasible to
find two messages M and M such that h.M/ h.M /. The value h.M/ is
0 0
D
a short (say, 256-bit) “fingerprint” of the message M. If Alice wishes to sign a
message M, she first applies h to M to obtain the fingerprint h.M/, which she
then encrypts withhersecret key. Shesends.M;S .h.M///toBobashersigned
A
version of M. Bob can verify the signature by computing h.M/ and verifying
thatP appliedtoS .h.M//asreceived equalsh.M/. Becausenoonecancreate
A A
two messages with the same fingerprint, it is computationally infeasible to alter a
signedmessageandpreservethevalidityofthesignature.
Finally,wenotethattheuseofcertificates makesdistributing publickeysmuch
easier. For example, assume there is a “trusted authority” T whose public key
isknownby everyone. Alice canobtain from T asigned message (hercertificate)
statingthat“Alice’spublickeyisP .” Thiscertificateis“self-authenticating”since
A
everyone knows P . Alice can include her certificate with her signed messages,
T
sothattherecipienthasAlice’spublickeyimmediatelyavailableinordertoverify
her signature. Because her key was signed by T, the recipient knows that Alice’s
keyisreallyAlice’s.
Exercises
31.7-1
Consider an RSA key set with p 11, q 29, n 319, and e 3. What
D D D D
valueofd shouldbeusedinthesecretkey? Whatistheencryptionofthemessage
M 100?
D
31.8 Primalitytesting 965
31.7-2
ProvethatifAlice’spublicexponente is3andanadversary obtainsAlice’ssecret
exponentd,where0 < d < .n/,thentheadversarycanfactorAlice’smodulusn
intimepolynomialinthenumberofbitsinn. (Althoughyouarenotaskedtoprove
it,youmaybeinterested toknowthatthisresultremainstrueevenifthecondition
e 3isremoved. SeeMiller[255].)
D
31.7-3 ?
ProvethatRSAismultiplicative inthesensethat
P .M /P .M / P .M M / .mod n/:
A 1 A 2 A 1 2

Use this fact to prove that if an adversary had a procedure that could efficiently
decrypt 1 percent of messages from Z encrypted with P , then he could employ
n A
a probabilistic algorithm to decrypt every message encrypted with P with high
A
probability.
? 31.8 Primalitytesting
In this section, we consider the problem of finding large primes. Webegin with a
discussionofthedensityofprimes,proceedtoexamineaplausible,butincomplete,
approach to primality testing, and then present an effective randomized primality
testduetoMillerandRabin.
Thedensityofprimenumbers
For many applications, such as cryptography, we need to find large “random”
primes. Fortunately, large primes are not too rare, so that it is feasible to test
random integers oftheappropriate sizeuntil wefindaprime. Theprimedistribu-
tionfunction .n/specifies thenumber ofprimes thatareless thanorequal ton.
Forexample, .10/ 4,sincethereare4primenumberslessthanorequalto10,
D
namely, 2, 3, 5, and 7. The prime number theorem gives a useful approximation
to.n/.
Theorem31.37(Primenumbertheorem)
.n/
lim 1:
n n=lnn D
!1
The approximation n=lnn gives reasonably accurate estimates of .n/ even
for small n. For example, it is off by less than 6% at n 109, where .n/
D D
966 Chapter31 Number-TheoreticAlgorithms
50,847,534 and n=lnn 48,254,942. (Toanumber theorist, 109 isasmallnum-

ber.)
We can view the process of randomly selecting an integer n and determining
whether it is prime as a Bernoulli trial (see Section C.4). By the prime number
theorem, the probability of a success—that is, the probability that n is prime—is
approximately1=lnn. Thegeometricdistributiontellsushowmanytrialsweneed
to obtain a success, and by equation (C.32), the expected number of trials is ap-
proximately lnn. Thus, we would expect to examine approximately lnn integers
chosen randomly near n in order to find a prime that is of the same length as n.
For example, we expect that finding a 1024-bit prime would require testing ap-
proximately ln21024 710randomly chosen 1024-bit numbers forprimality. (Of

course, wecancutthisfigureinhalfbychoosing onlyoddintegers.)
Intheremainderofthissection,weconsidertheproblemofdeterminingwhether
ornotalargeoddintegernisprime. Fornotationalconvenience, weassumethatn
hastheprimefactorization
n pe1pe2 per ; (31.39)
D 1 2  r
wherer 1,p ;p ;:::;p aretheprimefactors ofn,ande ;e ;:::;e areposi-
1 2 r 1 2 r

tiveintegers. Theintegernisprimeifandonlyifr 1ande 1.
1
D D
Onesimpleapproachtotheproblemoftestingforprimalityistrialdivision. We
try dividing n by each integer 2;3;:::; pn . (Again, wemay skip even integers
b c
greater than2.) Itiseasytoseethatnisprimeifandonlyifnoneofthetrialdivi-
sorsdividesn. Assumingthateachtrialdivisiontakesconstanttime,theworst-case
running time is ‚.pn/, which is exponential in the length of n. (Recall that if n
is encoded in binary using ˇ bits, then ˇ lg.n 1/ , and so pn ‚.2ˇ=2/.)
D d C e D
Thus, trial division works well only if n is very small or happens to have a small
prime factor. When it works, trial division has the advantage that it not only de-
termines whether n is prime or composite, but also determines one of n’s prime
factorsifniscomposite.
In this section, we are interested only in finding out whether a given number n
is prime; if n is composite, we are not concerned with finding its prime factor-
ization. As we shall see in Section 31.9, computing the prime factorization of a
numberiscomputationallyexpensive. Itisperhapssurprisingthatitismucheasier
to tell whether or not a given number is prime than it is to determine the prime
factorization ofthenumberifitisnotprime.
Pseudoprimalitytesting
We now consider a method for primality testing that “almost works” and in fact
is good enough for many practical applications. Later on, we shall present a re-
31.8 Primalitytesting 967
Z
finementofthismethodthatremovesthesmalldefect. Let denote thenonzero
Cn
Z
elementsof :
n
Z 1;2;:::;n 1 :
Cn
D f (cid:0) g
Ifnisprime,thenZ Z
.
Cn
D
n
Wesaythatnisabase-a pseudoprime ifniscompositeand
an 1 1 .mod n/: (31.40)
(cid:0)

Fermat’stheorem(Theorem31.31)impliesthatifnisprime,thennsatisfiesequa-
tion (31.40) for every a in Z . Thus, if wecan find any a Z such that n does
Cn
2
Cn
not satisfy equation (31.40), then n is certainly composite. Surprisingly, the con-
versealmost holds, sothatthiscriterion formsanalmostperfecttestforprimality.
Wetesttoseewhether n satisfies equation (31.40) fora 2. Ifnot, wedeclare n
D
tobecomposite by returning COMPOSITE. Otherwise, wereturn PRIME, guessing
that n is prime (when, in fact, all we know is that n is either prime or a base-2
pseudoprime).
The following procedure pretends in this manner to be checking the primality
ofn. Ituses theprocedure MODULAR-EXPONENTIATION fromSection 31.6. We
assumethattheinputnisanoddintegergreaterthan2.
PSEUDOPRIME.n/
1 ifMODULAR-EXPONENTIATION.2;n 1;n/ 1 .mod n/
(cid:0) 6
2 return COMPOSITE //definitely
3 elsereturn PRIME //wehope!
This procedure can make errors, but only of one type. That is, if it says that n
is composite, then it is always correct. If it says that n is prime, however, then it
makesanerroronlyifnisabase-2pseudoprime.
Howoftendoesthisprocedureerr? Surprisinglyrarely. Thereareonly22values
of n less than 10,000 for which it errs; the first four such values are 341, 561,
645, and 1105. We won’t prove it, but the probability that this program makes an
error on a randomly chosen ˇ-bit number goes to zero as ˇ . Using more
! 1
preciseestimatesduetoPomerance[279]ofthenumberofbase-2pseudoprimesof
agivensize,wemayestimatethatarandomlychosen512-bitnumberthatiscalled
prime by the above procedure has less than one chance in 1020 of being a base-2
pseudoprime, andarandomlychosen1024-bitnumberthatiscalledprimehasless
than one chance in 1041 of being a base-2 pseudoprime. So if you are merely
trying to find a large prime for some application, for all practical purposes you
almost never go wrong by choosing large numbers at random until one of them
causes PSEUDOPRIME to return PRIME. But when the numbers being tested for
primalityarenotrandomlychosen,weneedabetterapproachfortestingprimality.
968 Chapter31 Number-TheoreticAlgorithms
As we shall see, a little more cleverness, and some randomization, will yield a
primality-testing routinethatworkswellonallinputs.
Unfortunately, we cannot entirely eliminate all the errors by simply checking
equation (31.40) for a second base number, say a 3, because there exist com-
D
positeintegersn,knownasCarmichaelnumbers,thatsatisfyequation(31.40)for
all a Z . (We note that equation (31.40) does fail when gcd.a;n/ > 1—that
is, wh2 en
an
Z —but hoping to demonstrate that n is composite by finding such
62
n
an a can be difficult if n has only large prime factors.) Thefirst three Carmichael
numbers are 561, 1105, and 1729. Carmichael numbers are extremely rare; there
are, for example, only 255 of them less than 100,000,000. Exercise 31.8-2 helps
explainwhytheyaresorare.
We next show how to improve our primality test so that it won’t be fooled by
Carmichaelnumbers.
TheMiller-Rabinrandomizedprimalitytest
The Miller-Rabin primality test overcomes the problems of the simple test PSEU-
DOPRIME withtwomodifications:
 Ittriesseveralrandomlychosenbasevaluesa insteadofjustonebasevalue.
 Whilecomputing eachmodular exponentiation, itlooksforanontrivial square
root of 1, modulo n, during the final set of squarings. If it finds one, it stops
andreturns COMPOSITE. Corollary 31.35 from Section 31.6 justifies detecting
composites inthismanner.
Thepseudocode forthe Miller-Rabin primality test follows. Theinput n > 2is
the odd number to be tested for primality, and s is the number of randomly cho-
Z
sen basevalues from tobetried. Thecodeuses therandom-number generator
Cn
RANDOM described on page 117: RANDOM.1;n 1/ returns a randomly chosen
(cid:0)
integerasatisfying1 a n 1. ThecodeusesanauxiliaryprocedureWITNESS
  (cid:0)
suchthat WITNESS.a;n/is TRUE ifandonly ifa isa“witness” tothecomposite-
ness ofn—thatis, ifitispossible using a toprove(inamannerthat weshall see)
thatniscomposite. Thetest WITNESS.a;n/isanextension of,butmoreeffective
than,thetest
an 1 1 .mod n/
(cid:0)
6
that formed the basis (using a 2) for PSEUDOPRIME. We first present and
D
justify theconstruction of WITNESS, and then weshall show howweuseitinthe
Miller-Rabin primality test. Let n 1 2tu where t 1 and u is odd; i.e.,
(cid:0) D 
thebinary representation ofn 1isthebinary representation oftheoddinteger u
followed by exactly t zeros. T(cid:0) herefore, an 1 .au/2t .mod n/, so that we can
(cid:0)

31.8 Primalitytesting 969
compute an 1 mod n by first computing au mod n and then squaring the result t
(cid:0)
timessuccessively.
WITNESS.a;n/
1 lett andubesuchthatt 1,uisodd,andn 1 2tu
 (cid:0) D
2 x
0
MODULAR-EXPONENTIATION.a;u;n/
D
3 fori 1tot
D
4 x x2 mod n
i D i 1
5 ifx == (cid:0)1andx 1andx n 1
i i 1 i 1
(cid:0) ¤ (cid:0) ¤ (cid:0)
6 return TRUE
7 ifx 1
t
¤
8 return TRUE
9 return FALSE
This pseudocode for WITNESS computes an (cid:0)1 mod n by first computing the
value x au mod ninline 2andthen squaring theresult t times inarowinthe
0
D
for loop of lines 3–6. By induction on i, the sequence x , x , ..., x of values
0 1 t
computed satisfies the equation x a2iu .mod n/ for i 0;1;:::;t, so that in
i
 D
particular x an 1 .mod n/. After line 4 performs a squaring step, however,
t (cid:0)

the loop may terminate early if lines 5–6 detect that a nontrivial square root of 1
has just been discovered. (We shall explain these tests shortly.) If so, the algo-
rithm stops and returns TRUE. Lines 7–8 return TRUE if the value computed for
x
t
an (cid:0)1 .mod n/isnotequalto1,justasthePSEUDOPRIME procedurereturns

COMPOSITE in this case. Line 9 returns FALSE if we haven’t returned TRUE in
lines6or8.
We now argue that if WITNESS.a;n/ returns TRUE, then we can construct a
proofthatniscomposite usingaasawitness.
If WITNESS returns TRUE from line 8, then it has discovered that x t
D
an 1 mod n 1. If n is prime, however, we have by Fermat’s theorem (Theo-
(cid:0)
rem31.31)th¤ atan 1 1 .mod n/foralla Z . Therefore, ncannot beprime,
(cid:0)
 2
Cn
andtheequationan 1 mod n 1provesthisfact.
(cid:0)
¤
If WITNESS returns TRUE fromline6,thenithasdiscovered thatx i 1 isanon-
trivial square root of 1, modulo n, since we have that x 1 .(cid:0) mod n/ yet
i 1
x x2 1 .mod n/. Corollary 31.35 states that onl(cid:0) y if6 n˙ is composite can
i  i 1 
there exi(cid:0)st a nontrivial square root of 1 modulo n, so that demonstrating that x
i 1
isanontrivial squarerootof1modulonprovesthatniscomposite. (cid:0)
ThiscompletesourproofofthecorrectnessofWITNESS. Ifwefindthatthecall
WITNESS.a;n/returns TRUE,thennissurelycomposite,andthewitnessa,along
with the reason that the procedure returns TRUE (did it return from line 6 or from
line8?),provides aproofthatniscomposite.
970 Chapter31 Number-TheoreticAlgorithms
At this point, we briefly present an alternative description of the behavior of
WITNESS asafunction ofthesequence X x 0;x 1;:::;x
t
,whichweshallfind
Dh i
useful later on, when weanalyze the efficiency of the Miller-Rabin primality test.
Note that if x
i
1 for some 0 i < t, WITNESS might not compute the rest
D 
ofthesequence. Ifitweretodoso, however, each value x ;x ;:::;x would
i 1 i 2 t
be 1, and we consider these positions in the sequence X asC beingC all 1s. We have
fourcases:
1. X :::;d , where d 1: the sequence X does not end in 1. Return TRUE
D h i ¤
inline8;aisawitnesstothecompositeness ofn(byFermat’sTheorem).
2. X 1;1;:::;1 : the sequence X is all1s. Return FALSE; a is not awitness
D h i
tothecompositeness ofn.
3. X :::; 1;1;:::;1 : thesequence X ends in1,andthelastnon-1 isequal
D h (cid:0) i
to 1. Return FALSE; aisnotawitnesstothecompositeness ofn.
(cid:0)
4. X :::;d;1;:::;1 ,whered 1: thesequence X ends in1, butthelast
D h i ¤ ˙
non-1 is not 1. Return TRUE in line 6; a is a witness to the compositeness
(cid:0)
ofn,sinced isanontrivial squarerootof1.
Wenowexamine theMiller-Rabin primalitytestbased ontheuseof WITNESS.
Again,weassumethatnisanoddintegergreaterthan2.
MILLER-RABIN.n;s/
1 forj 1tos
D
2 a RANDOM.1;n 1/
D (cid:0)
3 ifWITNESS.a;n/
4 return COMPOSITE //definitely
5 return PRIME //almostsurely
The procedure MILLER-RABIN is a probabilistic search for a proof that n is
composite. Themain loop (beginning online 1)picks up to s random values of a
from Z (line 2). If one of the a’s picked is a witness to the compositeness of n,
Cn
then MILLER-RABIN returns COMPOSITE on line 4. Such a result is always cor-
rect, by the correctness of WITNESS. If MILLER-RABIN finds no witness in s
trials,thentheprocedureassumesthatthisisbecausenowitnessesexist,andthere-
fore it assumes that n is prime. We shall see that this result is likely to be correct
if s is large enough, but that there is still a tiny chance that the procedure may be
unlucky initschoice ofa’sandthatwitnesses doexisteventhough nonehasbeen
found.
To illustrate the operation of MILLER-RABIN, let n be the Carmichael num-
ber 561, so that n 1 560 24 35, t 4, and u 35. If the pro-
(cid:0) D D  D D
cedure chooses a 7 as a base, Figure 31.4 in Section 31.6 shows that WIT-
D
NESS computes x 0 a35 241 .mod 561/ and thus computes the sequence
 
31.8 Primalitytesting 971
X 241;298; 166; 67; 1 . Thus, WITNESS discovers a nontrivial square root
D h i
of 1 in the last squaring step, since a280 67 .mod n/ and a560 1 .mod n/.
 
Therefore, a 7 is a witness to the compositeness of n, WITNESS.7;n/ returns
D
TRUE,and MILLER-RABIN returns COMPOSITE.
If n is a ˇ-bit number, MILLER-RABIN requires O.sˇ/ arithmetic operations
and O.sˇ3/ bit operations, since it requires asymptotically no more work than s
modularexponentiations.
ErrorrateoftheMiller-Rabinprimalitytest
IfMILLER-RABIN returnsPRIME,thenthereisaveryslimchancethatithasmade
an error. Unlike PSEUDOPRIME, however, the chance of error does not depend
onn;therearenobadinputsforthisprocedure. Rather,itdepends onthesizeofs
andthe“luck ofthedraw”inchoosing basevalues a. Moreover, since eachtestis
more stringent than a simple check of equation (31.40), wecan expect on general
principles that the error rate should besmall for randomly chosen integers n. The
followingtheorempresents amorepreciseargument.
Theorem31.38
If n is an odd composite number, then the number of witnesses to the composite-
nessofnisatleast.n 1/=2.
(cid:0)
Proof The proof shows that the number of nonwitnesses is at most .n 1/=2,
(cid:0)
whichimpliesthetheorem.
Z
We start by claiming that any nonwitness must be a member of . Why?
n
Consider any nonwitness a. It must satisfy an 1 1 .mod n/ or, equivalently,
(cid:0)

a an 2 1 .mod n/. Thus, the equation ax 1 .mod n/ has a solution,
(cid:0)
  
namely an 2. By Corollary 31.21, gcd.a;n/ 1, which in turn implies that
(cid:0)
gcd.a;n/ 1. Therefore, aisamemberofZ ;aj llnonwitnesses belongtoZ .
D
n n
To complete the proof, we show that not only are all nonwitnesses contained
in Z , they are all contained in a proper subgroup B of Z (recall that we say B
n n
is a proper subgroup of Z when B is subgroup of Z but B is not equal to Z ).
n n n
By Corollary 31.16, we then have B Z =2. Since Z n 1, we obtain
j j  j
nj
j
nj
 (cid:0)
B .n 1/=2. Therefore, thenumber ofnonwitnesses isatmost.n 1/=2, so
j j  (cid:0) (cid:0)
thatthenumberofwitnessesmustbeatleast.n 1/=2.
We now show how to find a proper subgrou(cid:0) p B of Z containing all of the
n
nonwitnesses. Webreaktheproofintotwocases.
Case1: Thereexistsanx Z suchthat
2
n
xn 1 1 .mod n/:
(cid:0)
6
972 Chapter31 Number-TheoreticAlgorithms
In other words, n is not a Carmichael number. Because, as we noted earlier,
Carmichael numbers are extremely rare, case 1 is the main case that arises “in
practice” (e.g., when n has been chosen randomly and is being tested for primal-
ity).
LetB b Z bn 1 1 .mod n/ . Clearly, B is nonempty, since 1 B.
D f 2
n
W
(cid:0)
 g 2
Since B is closed under multiplication modulo n, we have that B is a subgroup
of Z by Theorem 31.14. Note that every nonwitness belongs to B, since a non-
n
witness a satisfies an 1 1 .mod n/. Since x Z B, we have that B is a
Z(cid:0)
 2
n
(cid:0)
propersubgroup of .
n
Case2: Forallx Z ,
2
n
xn 1 1 .mod n/: (31.41)
(cid:0)

In other words, n is a Carmichael number. This case is extremely rare in prac-
tice. However,theMiller-Rabintest(unlikeapseudo-primalitytest)canefficiently
determine thatCarmichaelnumbersarecomposite, aswenowshow.
In this case, n cannot be a prime power. To see why, let us suppose to the
contrary that n pe, where p is a prime and e > 1. We derive a contradiction
D
as follows. Since we assume that n is odd, p must also be odd. Theorem 31.32
implies that Z is a cyclic group: it contains a generator g such that ord .g/
Z .n/
n
pe.1 1=p/ .p 1/pe 1. (Theformula for
.n/comn
esfroD m
j
nj
D D (cid:0) D (cid:0)
(cid:0)
equation (31.20).) By equation (31.41), we have gn 1 1 .mod n/. Then the
(cid:0)

discrete logarithm theorem(Theorem31.33, takingy 0)impliesthatn 1 0
D (cid:0) 
.mod .n//,or
.p 1/pe 1 pe 1:
(cid:0)
(cid:0) j (cid:0)
This is a contradiction for e > 1, since .p 1/pe 1 is divisible by the prime p
(cid:0)
(cid:0)
butpe 1isnot. Thus,nisnotaprimepower.
(cid:0)
Since the odd composite number n is not a prime power, we decompose it into
aproductn n ,wheren andn areoddnumbersgreaterthan1thatarerelatively
1 2 1 2
prime to each other. (There may be several ways to decompose n, and it does not
matter which one we choose. For example, if n pe1pe2 per, then we can
choosen pe1 andn pe2pe3 per.) D 1 2  r
1 D 1 2 D 2 3  r
Recallthatwedefinet andusothatn 1 2tu,wheret 1anduisodd,and
(cid:0) D 
thatforaninputa,theprocedure WITNESS computesthesequence
X
au;a2u;a22u;:::;a2tu
Dh i
(allcomputations areperformedmodulon).
Letuscallapair.;j/ofintegersacceptable if Z ,j 0;1;:::;t ,and
2
n
2f g
2ju
1 .mod n/:
(cid:0)
31.8 Primalitytesting 973
Acceptable pairs certainly exist since u is odd; we can choose  n 1 and
D (cid:0)
j 0,sothat.n 1;0/isanacceptablepair. Nowpickthelargestpossiblej such
D (cid:0)
that there exists an acceptable pair .;j/, and fix so that .;j/ is an acceptable
pair. Let
B x Z x2ju 1 .mod n/ :
Df 2
n
W ˙ g
SinceB isclosedundermultiplication modulon,itisasubgroupofZ . ByTheo-
n
rem31.15, therefore, B divides Z . Everynonwitness mustbeamemberofB,
j j j
nj
sincethesequenceX producedbyanonwitnessmusteitherbeall1sorelsecontain
a 1nolaterthanthejthposition,bythemaximalityofj. (If.a;j /isacceptable,
0
(cid:0)
whereaisanonwitness, wemusthavej j byhowwechosej.)
0
We now use the existence of  to demo nstrate that there exists a w Z B,
2
n
(cid:0)
andhencethatB isapropersubgroupofZ . Since2ju 1 .mod n/,wehave
n
(cid:0)
2ju 1 .mod n / by Corollary 31.29 to the Chinese remainder theorem. By
1
 (cid:0)
Corollary31.28,thereexistsaw simultaneously satisfying theequations
w  .mod n /;
1

w 1 .mod n /:
2

Therefore,
w2ju
1 .mod n /;
1
 (cid:0)
w2ju
1 .mod n /:
2

By Corollary 31.29, w2ju 1 .mod n / implies w2ju 1 .mod n/, and
1
6 6
w2ju 1 .mod n / implies w2ju 1 .mod n/. Hence, we conclude that
2
6 (cid:0) 6 (cid:0)
w2ju 1 .mod n/,andsow B.
Itre6 ma˙
ins to show that w
Z62
, which wedo by first working separately mod-
ulo n and modulo n . Work2
ingn
modulo n , we observe that since  Z , we
1 2 1
2
n
have that gcd.;n/ 1, and so also gcd.;n / 1; if  does not have any com-
1
D D
mondivisors withn,thenitcertainly doesnothaveanycommondivisors withn .
1
Since w  .mod n /, we see that gcd.w;n / 1. Working modulo n , we
1 1 2
 D
observe that w 1 .mod n / implies gcd.w;n / 1. Tocombine these results,
2 2
 D
weuseTheorem31.6,whichimpliesthatgcd.w;n n / gcd.w;n/ 1. Thatis,
1 2
w Z . D D
T2
hern
efore w Z B, and we finish case 2 with the conclusion that B is a
2
Zn
(cid:0)
propersubgroup of .
n
Ineither case, wesee thatthe number ofwitnesses tothecompositeness ofnis
atleast.n 1/=2.
(cid:0)
Theorem31.39
For any odd integer n > 2 and positive integer s, the probability that MILLER-
RABIN.n;s/errsisatmost2 (cid:0)s.
974 Chapter31 Number-TheoreticAlgorithms
Proof UsingTheorem31.38,weseethatifniscomposite,theneachexecutionof
theforloopoflines1–4hasaprobabilityofatleast1=2ofdiscoveringawitnessx
tothecompositeness ofn. MILLER-RABIN makesanerroronlyifitissounlucky
astomissdiscoveringawitnesstothecompositenessofnoneachofthesiterations
ofthemainloop. Theprobability ofsuchasequence ofmissesisatmost2 s.
(cid:0)
Ifnisprime, MILLER-RABIN alwaysreports PRIME, andifniscomposite, the
chancethat MILLER-RABIN reports PRIME isatmost2 (cid:0)s.
Whenapplying MILLER-RABIN toalargerandomlychosenintegern,however,
we need to consider as well the prior probability that n is prime, in order to cor-
rectly interpret MILLER-RABIN’s result. Suppose that we fix a bit length ˇ and
choose at random an integer n of length ˇ bits to be tested for primality. Let A
denote the event that n is prime. Bythe prime number theorem (Theorem 31.37),
theprobability thatnisprimeisapproximately
Pr A 1=lnn
f g 
1:443=ˇ :

Now let B denote the event that MILLER-RABIN returns PRIME. We have that
Pr B A 0 (or equivalently, that Pr B A 1) and Pr B A 2 s (or
(cid:0)
j D f j g D j 
equivalently, thatPr B A > 1 2 s).
˚ (cid:9) j (cid:0) (cid:0) ˚ (cid:9)
But what is Pr A B , the probability that n is prime, given that MILLER-
f ˚j g (cid:9)
RABIN has returned PRIME? By the alternate form of Bayes’s theorem (equa-
tion(C.18))wehave
Pr A Pr B A
Pr A B f g f j g
f j g D Pr A Pr B A Pr A Pr B A
f g f j gC j
1
: ˚ (cid:9) ˚ (cid:9)
 1 2 s.lnn 1/
C (cid:0) (cid:0)
This probability does notexceed 1=2 until s exceeds lg.lnn 1/. Intuitively, that
(cid:0)
many initial trials are needed just for the confidence derived from failing tofind a
witness to the compositeness of n to overcome the prior bias in favor of n being
composite. Foranumberwithˇ 1024bits,thisinitialtestingrequires about
D
lg.lnn 1/ lg.ˇ=1:443/
(cid:0) 
9

trials. In any case, choosing s 50 should suffice for almost any imaginable
D
application.
In fact, the situation is much better. If we are trying to find large primes by
applying MILLER-RABIN to large randomly chosen odd integers, then choosing
a small value of s (say 3) is very unlikely to lead to erroneous results, though
31.9 Integerfactorization 975
we won’t prove it here. The reason is that for a randomly chosen odd composite
integern,theexpectednumberofnonwitnesses tothecompositeness ofnislikely
tobeverymuchsmallerthan.n 1/=2.
(cid:0)
If the integer n is not chosen randomly, however, the best that can be proven is
that the number of nonwitnesses is at most .n 1/=4, using an improved version
(cid:0)
ofTheorem31.38. Furthermore, theredoexistintegersnforwhichthenumberof
nonwitnesses is.n 1/=4.
(cid:0)
Exercises
31.8-1
Provethatifanoddintegern > 1isnotaprimeoraprimepower,thenthereexists
anontrivial squarerootof1modulon.
31.8-2 ?
Itispossible tostrengthen Euler’stheorem slightlytotheform
a.n/ 1 .mod n/foralla Z ;
 2
n
wheren pe1 per and.n/isdefinedby
D 1  r
.n/ lcm. .pe1/;:::; .per//: (31.42)
D 1 r
Prove that .n/ .n/. A composite number n is a Carmichael number if
j
.n/ n 1. The smallest Carmichael number is 561 3 11 17; here,
j (cid:0) D  
.n/ lcm.2;10;16/ 80, which divides 560. Prove that Carmichael num-
D D
bersmustbeboth “square-free” (not divisible bythesquare ofanyprime)andthe
productofatleastthreeprimes. (Forthisreason, theyarenotverycommon.)
31.8-3
Prove that if x is a nontrivial square root of 1, modulo n, then gcd.x 1;n/ and
(cid:0)
gcd.x 1;n/arebothnontrivial divisors ofn.
C
? 31.9 Integer factorization
Suppose we have an integer n that wewish to factor, that is, to decompose into a
product ofprimes. Theprimality testofthepreceding section maytellusthatnis
composite,butitdoesnottellustheprimefactorsofn. Factoringalargeintegern
seems to be much more difficult than simply determining whether n is prime or
composite. Evenwith today’s supercomputers andthe best algorithms todate, we
cannotfeasiblyfactoranarbitrary1024-bit number.
976 Chapter31 Number-TheoreticAlgorithms
Pollard’srhoheuristic
TrialdivisionbyallintegersuptoRisguaranteedtofactorcompletelyanynumber
uptoR2. Forthesameamountofwork,thefollowingprocedure, POLLARD-RHO,
factors anynumber uptoR4 (unless weareunlucky). Since theprocedure isonly
a heuristic, neither its running time nor its success is guaranteed, although the
procedure is highly effective in practice. Another advantage of the POLLARD-
RHOprocedureisthatitusesonlyaconstantnumberofmemorylocations. (Ifyou
wantedto,youcouldeasilyimplementPOLLARD-RHO onaprogrammablepocket
calculator tofindfactorsofsmallnumbers.)
POLLARD-RHO.n/
1 i 1
D
2 x
1
RANDOM.0;n 1/
D (cid:0)
3 y x
1
D
4 k 2
D
5 whileTRUE
6 i i 1
D C
7 x .x2 1/ mod n
i D i 1(cid:0)
8 d gcd(cid:0).y x ;n/
i
D (cid:0)
9 ifd 1andd n
¤ ¤
10 printd
11 ifi ==k
12 y x
i
D
13 k 2k
D
Theprocedureworksasfollows. Lines1–2initializei to1andx toarandomly
1
Z
chosen valuein . Thewhileloopbeginning online5iterates forever, searching
n
forfactorsofn. Duringeachiteration ofthewhileloop,line7usestherecurrence
x .x2 1/ mod n (31.43)
i D i 1(cid:0)
(cid:0)
toproducethenextvalueofx intheinfinitesequence
i
x ;x ;x ;x ;::: ; (31.44)
1 2 3 4
withline6correspondingly incrementing i. Thepseudocode iswrittenusing sub-
scripted variables x for clarity, but the program works the same if all of the sub-
i
scripts aredropped, sinceonlythemostrecentvalueofx needstobemaintained.
i
With this modification, the procedure uses only a constant number of memory lo-
cations.
Every so often, the program saves the most recently generated x value in the
i
variabley. Specifically,thevaluesthataresavedaretheoneswhosesubscriptsare
powersof2:
31.9 Integerfactorization 977
x ;x ;x ;x ;x ;::: :
1 2 4 8 16
Line 3 saves the value x , and line 12 saves x whenever i is equal to k. The
1 k
variable k is initialized to 2 in line 4, and line 13 doubles it whenever line 12
updates y. Therefore, k follows the sequence 1;2;4;8;::: and always gives the
subscript ofthenextvaluex tobesavediny.
k
Lines 8–10 try to find a factor of n, using the saved value of y and the cur-
rent value of x . Specifically, line 8 computes the greatest common divisor
i
d gcd.y x ;n/. If line 9 finds d to be a nontrivial divisor of n, then line 10
i
D (cid:0)
printsd.
This procedure for finding a factor may seem somewhat mysterious at first.
Note, however, that POLLARD-RHO never prints an incorrect answer; any num-
ber it prints is a nontrivial divisor of n. POLLARD-RHO might not print anything
at all, though; it comes with no guarantee that it will print any divisors. We shall
see, however, that we have good reason to expect POLLARD-RHO to print a fac-
tor p of n after ‚.pp/ iterations of the while loop. Thus, if n is composite, we
can expect this procedure to discover enough divisors to factor n completely after
approximately n1=4 updates, since every prime factor p of n except possibly the
largestoneislessthanpn.
We begin our analysis of how this procedure behaves by studying how long
it takes a random sequence modulo n to repeat a value. Since Z is finite, and
n
since each value in the sequence (31.44) depends only on the previous value, the
sequence (31.44) eventually repeats itself. Oncewereach anx such thatx x
i i j
D
for some j < i, we are in a cycle, since x x , x x , and so on.
i 1 j 1 i 2 j 2
C D C C D C
Thereasonforthename“rhoheuristic” isthat,asFigure31.7shows,wecandraw
thesequencex ;x ;:::;x asthe“tail”oftherhoandthecyclex ;x ;:::;x
1 2 j 1 j j 1 i
(cid:0) C
asthe“body”oftherho.
Letusconsiderthequestionofhowlongittakesforthesequenceofx torepeat.
i
Thisinformationisnotexactlywhatweneed,butweshallseelaterhowtomodify
theargument. Forthepurposeofthisestimation, letusassumethatthefunction
f .x/ .x2 1/ mod n
n
D (cid:0)
behaves like a “random” function. Of course, it is not really random, but this as-
sumption yields results consistent with the observed behavior of POLLARD-RHO.
Wecanthenconsidereachx tohavebeenindependentlydrawnfromZ according
i n
Z
toauniformdistribution on . Bythebirthday-paradox analysisofSection5.4.1,
n
weexpect‚.pn/stepstobetakenbeforethesequence cycles.
Now for the required modification. Let p be a nontrivial factor of n such that
gcd.p;n=p/ 1. Forexample, ifnhasthefactorization n pe1pe2 per,then
wemaytakeD p tobepe1. (Ife 1, then p isjustthe smalD lest1 prim2 e f a ctor r ofn,
1 1 D
agoodexampletokeepinmind.)
978 Chapter31 Number-TheoreticAlgorithms
996 310
814 396
x
x 7 177 84 700
31
x 6 1186 120 x 600 18 11
x 5 1194 339 529 x 500 26 47
595 1053
x 4 63
6
x 40 x 400 63
x
70
x 3 8 x 30 8 x 60 x 300 8
16
x 2 3 x 20 3 x x 200 3
50
x 1 2 x 10 2 x 100 2
mod 1387 mod 19 mod 73
(a) (b) (c)
Figure 31.7 Pollard’s rho heuristic. (a) The values produced by the recurrence x i 1
.x i2 (cid:0)1/mod1387, startingwithx1
D
2. Theprimefactorizationof1387 is19 73. TheCheavD y
arrows indicate the iteration steps that are executed before the factor 19 is discovered. The light
arrowspointtounreachedvaluesintheiteration,toillustratethe“rho”shape.Theshadedvaluesare
theyvaluesstoredbyPOLLARD-RHO.Thefactor19isdiscovereduponreachingx7 177,when
D
gcd.63 177;1387/ 19iscomputed. Thefirstx valuethatwouldberepeatedis1186, butthe
(cid:0) D
factor19isdiscoveredbeforethisvalueisrepeated.(b)Thevaluesproducedbythesamerecurrence,
modulo19. Everyvaluex i giveninpart(a)isequivalent, modulo19,tothevaluex i0 shownhere.
Forexample,bothx4 63andx7 177areequivalentto6,modulo19. (c)Thevaluesproduced
D D
bythesamerecurrence,modulo73.Everyvaluex i giveninpart(a)isequivalent,modulo73,tothe
valuex shownhere.BytheChineseremaindertheorem,eachnodeinpart(a)correspondstoapair
i00
ofnodes,onefrompart(b)andonefrompart(c).
Thesequence x induces acorresponding sequence x modulop,where
h
i
i h
i0
i
x x modp
i0
D
i
foralli.
Furthermore, because f is defined using only arithmetic operations (squaring
n
andsubtraction)modulon,wecancomputex fromx ;the“modulop”viewof
i0 1 i0
C
31.9 Integerfactorization 979
thesequenceisasmallerversionofwhatishappening modulon:
x x mod p
i0 C1 D i C1
f .x / mod p
n i
D
..x2 1/ mod n/ mod p
D i (cid:0)
.x2 1/ mod p (byExercise31.1-7)
D i (cid:0)
..x mod p/2 1/ mod p
i
D (cid:0)
..x /2 1/ mod p
D
i0
(cid:0)
f .x /:
D
p i0
Thus,althoughwearenotexplicitlycomputingthesequence x ,thissequenceis
h
i0
i
welldefinedandobeysthesamerecurrence asthesequence x .
i
h i
Reasoning as before, we find that the expected number of steps before the se-
quence x repeatsis‚.pp/. Ifpissmallcomparedton,thesequence x might
h
i0
i h
i0
i
repeat much more quickly than the sequence x . Indeed, as parts (b) and (c) of
i
h i
Figure 31.7 show, the x sequence repeats as soon as two elements of the se-
h
i0
i
quence x aremerelyequivalent modulop,ratherthanequivalent modulon.
i
h i
Let t denote the index of the first repeated value in the x sequence, and let
h
i0
i
u > 0 denote the length of the cycle that has been thereby produced. That is, t
and u > 0 are the smallest values such that x x for all i 0. By the
t0 i D t0 u i 
above arguments, the expected values of t andCu are bCotCh ‚.pp/. Note that if
x x ,thenp .x x /. Thus,gcd.x x ;n/ > 1.
t0 CTi hD erefot0 Creu ,Coi nce POLj LARt C Du -C Ri H(cid:0) O ht aC si saved as y anyt C vu aC lui e(cid:0) x kt C sui ch that k t,

then y mod p is always on the cycle modulo p. (If a new value is saved as y,
that value is also on the cycle modulo p.) Eventually, k is set to a value that
is greater than u, and the procedure then makes an entire loop around the cycle
modulo p without changing thevalue ofy. Theprocedure then discovers afactor
ofnwhenx “runsinto”thepreviously storedvalueofy,modulop,thatis,when
i
x y .mod p/.
i

Presumably, the factor found is the factor p, although it may occasionally hap-
penthatamultipleofpisdiscovered. Sincetheexpectedvaluesofbotht anduare
‚.pp/,theexpected numberofstepsrequired toproduce thefactorp is‚.pp/.
This algorithm might not perform quite as expected, for two reasons. First, the
heuristicanalysisoftherunningtimeisnotrigorous,anditispossiblethatthecycle
of values, modulo p, could be much larger than pp. In this case, the algorithm
performscorrectlybutmuchmoreslowlythandesired. Inpractice,thisissueseems
tobemoot. Second, thedivisors ofnproduced by thisalgorithm mightalways be
one of the trivial factors 1 or n. For example, suppose that n pq, where p
D
and q are prime. It can happen that the values of t and u for p are identical with
the values of t and u for q, and thus the factor p is always revealed in the same
gcdoperation thatrevealsthefactor q. Sinceboth factors arerevealed atthesame
980 Chapter31 Number-TheoreticAlgorithms
time, the trivial factor pq n is revealed, which is useless. Again, this problem
D
seemstobeinsignificant inpractice. Ifnecessary, wecanrestarttheheuristic with
a different recurrence of the form x .x2 c/ mod n. (We should avoid the
values c 0and c 2for reasonsi C w1 eD willni o(cid:0) t go into here, but other values are
D D
fine.)
Of course, this analysis is heuristic and not rigorous, since the recurrence is
not really “random.” Nonetheless, the procedure performs well in practice, and
it seems to be as efficient as this heuristic analysis indicates. It is the method of
choiceforfindingsmallprimefactorsofalargenumber. Tofactoraˇ-bitcompos-
ite number n completely, we only need to find all prime factors less than n1=2 ,
b c
andsoweexpect POLLARD-RHO torequireatmostn1=4 2ˇ=4 arithmeticopera-
D
tionsandatmostn1=4ˇ2 2ˇ=4ˇ2bitoperations. POLLARD-RHO’sabilitytofind
D
asmallfactor p ofn withanexpected number ‚.pp/ofarithmetic operations is
oftenitsmostappealing feature.
Exercises
31.9-1
Referring totheexecution historyshowninFigure31.7(a), whendoes POLLARD-
RHO printthefactor73of1387?
31.9-2
Suppose that weare given afunction f Z Z and an initial value x Z .
n n 0 n
W ! 2
Definex f.x /fori 1;2;:::. Lett andu >0bethesmallestvaluessuch
i i 1
thatx D x (cid:0) fori D 0;1;:::. IntheterminologyofPollard’srhoalgorithm,
t i t u i
t istheC lenD gthoC ftC hetailaD nduisthelengthofthecycleoftherho. Giveanefficient
algorithm todeterminet anduexactly, andanalyzeitsrunning time.
31.9-3
Howmanysteps wouldyouexpect POLLARD-RHO torequire todiscover afactor
oftheformpe,wherep isprimeande >1?
31.9-4 ?
Onedisadvantage of POLLARD-RHO aswrittenisthatitrequires onegcdcompu-
tation for each step of the recurrence. Instead, we could batch the gcd computa-
tionsbyaccumulating theproductofseveralx valuesinarowandthenusingthis
i
product instead of x in the gcd computation. Describe carefully how you would
i
implementthisidea,whyitworks,andwhatbatchsizeyouwouldpickasthemost
effectivewhenworkingonaˇ-bitnumbern.
ProblemsforChapter31 981
Problems
31-1 Binarygcdalgorithm
Most computers can perform the operations of subtraction, testing the parity (odd
oreven)ofabinaryinteger,andhalvingmorequicklythancomputingremainders.
This problem investigates the binary gcd algorithm, which avoids the remainder
computations usedinEuclid’salgorithm.
a. Provethatifaandb arebotheven,thengcd.a;b/ 2 gcd.a=2;b=2/.
D 
b. Provethatifaisoddandb iseven,thengcd.a;b/ gcd.a;b=2/.
D
c. Provethatifaandb arebothodd,thengcd.a;b/ gcd..a b/=2;b/.
D (cid:0)
d. Design an efficient binary gcd algorithm for input integers a and b, where
a b, that runs in O.lga/ time. Assume that each subtraction, parity test,

andhalving takesunittime.
31-2 Analysisofbitoperations inEuclid’salgorithm
a. Consider theordinary “paperandpencil” algorithm forlong division: dividing
a by b, which yields a quotient q and remainder r. Show that this method
requires O..1 lgq/lgb/bitoperations.
C
b. Define.a;b/ .1 lga/.1 lgb/. Showthatthenumberofbitoperations
D C C
performed by EUCLID inreducing the problem ofcomputing gcd.a;b/to that
ofcomputinggcd.b;a mod b/isatmostc..a;b/ .b;a modb//forsome
(cid:0)
sufficiently largeconstantc > 0.
c. Show that EUCLID.a;b/ requires O..a;b// bit operations in general and
O.ˇ2/bitoperations whenappliedtotwoˇ-bitinputs.
31-3 ThreealgorithmsforFibonaccinumbers
This problem compares the efficiency of three methods for computing the nth Fi-
bonacci number F , givenn. Assumethat thecost ofadding, subtracting, ormul-
n
tiplyingtwonumbersisO.1/,independent ofthesizeofthenumbers.
a. Show that the running time of the straightforward recursive method for com-
putingF basedonrecurrence(3.22)isexponentialinn. (See,forexample,the
n
FIB procedure onpage775.)
b. ShowhowtocomputeF inO.n/timeusingmemoization.
n
982 Chapter31 Number-TheoreticAlgorithms
c. ShowhowtocomputeF inO.lgn/timeusingonlyintegeraddition andmul-
n
tiplication. (Hint:Considerthematrix
0 1
1 1
 
anditspowers.)
d. Assume now that adding two ˇ-bit numbers takes ‚.ˇ/ time and that multi-
plying twoˇ-bitnumbers takes ‚.ˇ2/time. Whatistherunning timeofthese
threemethodsunderthismorereasonablecostmeasurefortheelementaryarith-
meticoperations?
31-4 Quadraticresidues
Let p be an odd prime. A number a Z is a quadratic residue if the equation
2
p
x2 a .mod p/hasasolution fortheunknownx.
D
a. Showthatthereareexactly.p 1/=2quadratic residues, modulop.
(cid:0)
b. Ifp isprime, wedefinetheLegendresymbol .a/,fora Z ,tobe1ifa isa
quadratic residuemodulop and 1otherwise. Pp rovethat2 ifap Z ,then
(cid:0) 2
p
a
a.p 1/=2 .mod p/:
p  (cid:0)
 
Giveanefficientalgorithm thatdetermines whetheragivennumbera isaqua-
draticresiduemodulop. Analyzetheefficiencyofyouralgorithm.
c. Provethatifpisaprimeoftheform4k 3andaisaquadraticresidueinZ ,
C
p
then ak 1 mod p is asquare root ofa, modulo p. Howmuch time isrequired
C
tofindthesquarerootofaquadratic residuea modulop?
d. Describe anefficientrandomized algorithm forfindinganonquadratic residue,
modulo an arbitrary prime p, that is, a member of Z that is not a quadratic
p
residue. Howmanyarithmetic operations doesyouralgorithm require onaver-
age?
Chapter notes
NivenandZuckerman[265]provideanexcellentintroduction toelementary num-
ber theory. Knuth [210] contains a good discussion of algorithms for finding the
NotesforChapter31 983
greatestcommondivisor,aswellasotherbasicnumber-theoretic algorithms. Bach
[30] and Riesel [295] provide more recent surveys of computational number the-
ory. Dixon [91] gives an overview of factorization and primality testing. The
conference proceedings edited byPomerance[280]contains severalexcellent sur-
vey articles. More recently, Bach and Shallit [31] have provided an exceptional
overviewofthebasicsofcomputational numbertheory.
Knuth [210] discusses the origin of Euclid’s algorithm. It appears in Book 7,
Propositions 1 and 2, of the Greek mathematician Euclid’s Elements, which was
written around 300 B.C. Euclid’s description may have been derived from an al-
gorithm due to Eudoxus around 375 B.C. Euclid’s algorithm may hold the honor
ofbeingtheoldestnontrivial algorithm; itisrivaledonlybyanalgorithm formul-
tiplication known to the ancient Egyptians. Shallit [312] chronicles the history of
theanalysisofEuclid’salgorithm.
Knuth attributes a special case of the Chinese remainder theorem (Theo-
rem 31.27) to the Chinese mathematician Sun-Tsu˘, who lived sometime between
200 B.C. and A.D. 200—the date is quite uncertain. The same special case was
given by the Greek mathematician Nichomachus around A.D. 100. It was gener-
alized by Chhin Chiu-Shao in 1247. The Chinese remainder theorem was finally
statedandprovedinitsfullgenerality byL.Eulerin1734.
Therandomizedprimality-testingalgorithmpresentedhereisduetoMiller[255]
and Rabin [289]; it is the fastest randomized primality-testing algorithm known,
to within constant factors. The proof of Theorem 31.39 is a slight adaptation of
one suggested by Bach [29]. A proof of a stronger result for MILLER-RABIN
wasgivenbyMonier [258,259]. Formanyyears primality-testing wastheclassic
example of a problem where randomization appeared to be necessary to obtain
an efficient (polynomial-time) algorithm. In 2002, however, Agrawal, Kayal, and
Saxema[4]surprisedeveryonewiththeirdeterministicpolynomial-timeprimality-
testing algorithm. Until then, the fastest deterministic primality testing algorithm
known,duetoCohenandLenstra[73],ranintime.lgn/O.lglglgn/oninputn,which
is just slightly superpolynomial. Nonetheless, for practical purposes randomized
primality-testing algorithmsremainmoreefficientandarepreferred.
The problem of finding large “random” primes is nicely discussed in an article
byBeauchemin, Brassard, Cre´peau,Goutier, andPomerance[36].
The concept of a public-key cryptosystem is due to Diffie and Hellman [87].
The RSA cryptosystem was proposed in 1977 by Rivest, Shamir, and Adleman
[296]. Since then, the field of cryptography has blossomed. Our understanding
of the RSA cryptosystem has deepened, and modern implementations use signif-
icant refinements of the basic techniques presented here. In addition, many new
techniques have been developed for proving cryptosystems to be secure. For ex-
ample, Goldwasser and Micali [142] show that randomization can be an effective
toolinthedesignofsecurepublic-keyencryptionschemes. Forsignatureschemes,
984 Chapter31 Number-TheoreticAlgorithms
Goldwasser, Micali,andRivest[143]presentadigital-signature schemeforwhich
every conceivable type of forgery is provably as difficult as factoring. Menezes,
vanOorschot, andVanstone[254]provideanoverviewofapplied cryptography.
The rho heuristic for integer factorization was invented by Pollard [277]. The
versionpresented hereisavariantproposed byBrent[56].
Thebest algorithms forfactoring large numbers havearunning timethatgrows
roughly exponentially with the cube root of the length of the number n to be fac-
tored. The general number-field sieve factoring algorithm (as developed by Buh-
ler, Lenstra, and Pomerance [57] as an extension of the ideas in the number-field
sieve factoring algorithm by Pollard [278] and Lenstra et al. [232] and refined by
Coppersmith [77]and others) is perhaps the most efficient such algorithm in gen-
eral for large inputs. Although it is difficult to give a rigorous analysis of this
algorithm, underreasonable assumptionswecanderivearunning-time estimateof
L.1=3;n/1:902 o.1/,whereL.˛;n/ e.lnn/˛.lnlnn/1(cid:0)˛ .
C
D
Theelliptic-curve method duetoLenstra [233]maybemoreeffective forsome
inputs thanthe number-field sievemethod, since, like Pollard’s rhomethod, itcan
find a small prime factor p quite quickly. With this method, the time to find p is
estimatedtobeL.1=2;p/p2 o.1/.
C
32 String Matching
Text-editing programs frequently need to find all occurrences of a pattern in the
text. Typically,thetextisadocumentbeingedited,andthepatternsearchedforisa
particularwordsuppliedbytheuser. Efficientalgorithmsforthisproblem—called
“stringmatching”—cangreatlyaidtheresponsiveness ofthetext-editing program.
Among their many other applications, string-matching algorithms search for par-
ticular patterns in DNA sequences. Internet search engines also use them to find
Webpagesrelevanttoqueries.
We formalize the string-matching problem as follows. We assume that the
text is an array TŒ1::n of length n and that the pattern is an array PŒ1::m
of length m n. We further assume that the elements of P and T are char-

acters drawn from a finite alphabet †. For example, we may have † 0,1
D f g
or † a;b;:::;z . The character arrays P and T are often called strings of
D f g
characters.
Referring to Figure 32.1, we say that pattern P occurs with shift s in text T
(or, equivalently, that pattern P occurs beginning at position s C1 in text T) if
0 s n mandTŒs 1::s m PŒ1::m(thatis,ifTŒs j PŒj,for
  (cid:0) C C D C D
1 j m). IfP occurs withshifts inT,then wecalls avalid shift;otherwise,
 
wecalls an invalid shift. Thestring-matching problem isthe problem offinding
allvalidshiftswithwhichagivenpatternP occursinagiventextT.
text T a b c a b a a b c a b a c
s = 3
pattern P a b a a
Figure32.1 Anexampleofthestring-matchingproblem,wherewewanttofindalloccurrencesof
thepatternP abaainthetextT abcabaabcabac.Thepatternoccursonlyonceinthetext,
D D
atshifts 3,whichwecallavalidshift.Averticallineconnectseachcharacterofthepatterntoits
D
matchingcharacterinthetext,andallmatchedcharactersareshaded.
986 Chapter32 StringMatching
Algorithm Preprocessingtime Matchingtime
Naive 0 O..n m 1/m/
(cid:0) C
Rabin-Karp ‚.m/ O..n m 1/m/
(cid:0) C
Finiteautomaton O.m †/ ‚.n/
j j
Knuth-Morris-Pratt ‚.m/ ‚.n/
Figure32.2 Thestring-matchingalgorithmsinthischapterandtheirpreprocessingandmatching
times.
Except for the naive brute-force algorithm, which we review in Section 32.1,
eachstring-matching algorithminthischapterperformssomepreprocessing based
on the pattern and then finds all valid shifts; we call this latter phase “matching.”
Figure32.2showsthepreprocessingandmatchingtimesforeachofthealgorithms
in this chapter. Thetotal running time ofeach algorithm isthe sum of the prepro-
cessing and matching times. Section 32.2 presents an interesting string-matching
algorithm, due to Rabin and Karp. Although the ‚..n m 1/m/ worst-case
(cid:0) C
running time of this algorithm is no better than that of the naive method, it works
much better onaverage andinpractice. Italso generalizes nicely toother pattern-
matching problems. Section 32.3 then describes a string-matching algorithm that
beginsbyconstructingafiniteautomatonspecificallydesignedtosearchforoccur-
rencesofthegivenpatternP inatext. Thisalgorithm takesO.m † /preprocess-
j j
ingtime,butonly‚.n/matchingtime. Section32.4presentsthesimilar,butmuch
cleverer, Knuth-Morris-Pratt (or KMP)algorithm; it has the same ‚.n/ matching
time,anditreducesthepreprocessing timetoonly‚.m/.
Notationandterminology
We denote by † (read “sigma-star”) the set of all finite-length strings formed

using characters from the alphabet †. In this chapter, we consider only finite-
length strings. The zero-length empty string, denoted ", also belongs to † . The

length of a string x is denoted x . The concatenation of two strings x and y,
j j
denotedxy,haslength x y andconsistsofthecharactersfromx followedby
j jCj j
thecharacters fromy.
We say that a string w is a prefix of a string x, denoted w < x, if x wy for
some string y † . Note that if w < x,then w x . Similarly, weD say that a

string w isasu2 ffixofastringx,denoted w = xj ,ij f x j j yw forsomey † . As

with aprefix, w = x implies w x . Forexample,D wehave ab < ab2 ccaand
cca= abcca. Theemptystrj ingj  "isj bj othasuffixandaprefixofeverystring. For
any strings x and y and any character a, we have x = y if and only if xa = ya.
Chapter32 StringMatching 987
x x x
z z z
y y y
x x x
y y y
(a) (b) (c)
Figure32.3 AgraphicalproofofLemma32.1.Wesupposethatx=´andy =´.Thethreeparts
ofthefigureillustratethethreecasesofthelemma. Verticallinesconnectmatchingregions(shown
shaded)ofthestrings. (a)If x y ,thenx = y. (b)If x y ,theny = x. (c)If x y ,
j jj j j j j j j jD j j
thenx y.
D
Alsonotethat<and=aretransitiverelations. Thefollowinglemmawillbeuseful
later.
Lemma32.1(Overlapping-suffix lemma)
Suppose that x, y, and ´ are strings such that x = ´ and y = ´. If x y ,
thenx =y. If x y ,theny = x. If x y ,thenx y. j j  j j
j j j j j jD j j D
Proof SeeFigure32.3foragraphical proof.
Forbrevityofnotation, wedenotethek-character prefixPŒ1::kofthepattern
PŒ1::m by P . Thus, P " and P P PŒ1::m. Similarly, we denote
k 0 m
D D D
the k-character prefix of the text T by T . Using this notation, we can state the
k
string-matching problem asthatoffinding allshifts s intherange0 s n m
suchthatP = T .   (cid:0)
s m
C
Inourpseudocode, weallowtwoequal-length stringstobecomparedforequal-
ity as a primitive operation. If the strings are compared from left to right and the
comparison stops when a mismatch is discovered, we assume that the time taken
bysuchatestisalinearfunctionofthenumberofmatchingcharactersdiscovered.
To be precise, the test “x == y” is assumed to take time ‚.t 1/, where t is the
length of the longest string ´ such that ´ < x and ´ < y. (C We write ‚.t 1/
C
rather than ‚.t/ to handle the case in which t 0; the first characters compared
D
donotmatch,butittakesapositiveamountoftimetoperformthiscomparison.)
988 Chapter32 StringMatching
32.1 The naivestring-matching algorithm
The naive algorithm finds all valid shifts using a loop that checks the condition
PŒ1::m TŒs 1::s mforeachofthen m 1possible valuesofs.
D C C (cid:0) C
NAIVE-STRING-MATCHER.T;P/
1 n T:length
D
2 m P:length
D
3 fors 0ton m
D (cid:0)
4 ifPŒ1::m == TŒs 1::s m
C C
5 print“Patternoccurswithshift”s
Figure32.4portraysthenaivestring-matchingprocedureasslidinga“template”
containing the pattern over the text, noting for which shifts all of the characters
on the template equal the corresponding characters in the text. The for loop of
lines 3–5 considers each possible shift explicitly. The test in line 4 determines
whether thecurrent shiftisvalid; thistestimplicitly loops tocheck corresponding
character positions until all positions match successfully or a mismatch is found.
Line5printsouteachvalidshifts.
Procedure NAIVE-STRING-MATCHER takes time O..n m 1/m/, and this
(cid:0) C
bound is tight inthe worst case. Forexample, consider the text string an (a string
ofna’s)andthepatternam. Foreachofthen m 1possiblevaluesoftheshifts,
(cid:0) C
the implicit loop on line 4 to compare corresponding characters must execute m
timestovalidatetheshift. Theworst-caserunning timeisthus‚..n m 1/m/,
(cid:0) C
which is ‚.n2/ if m n=2 . Because it requires no preprocessing, NAIVE-
D b c
STRING-MATCHER’s running timeequalsitsmatchingtime.
a c a a b c a c a a b c a c a a b c a c a a b c
s = 0 s = 1 s = 2 s = 3
a a b a a b a a b a a b
(a) (b) (c) (d)
Figure 32.4 The operation of the naive string matcher for the pattern P aab and the text
D
T acaabc.WecanimaginethepatternP asatemplatethatweslidenexttothetext.(a)–(d)The
D
foursuccessivealignmentstriedbythenaivestringmatcher.Ineachpart,verticallinesconnectcor-
respondingregionsfoundtomatch(shownshaded),andajaggedlineconnectsthefirstmismatched
characterfound,ifany. Thealgorithmfindsoneoccurrenceofthepattern,atshifts 2,shownin
D
part(c).
32.1 Thenaivestring-matchingalgorithm 989
Asweshallsee,NAIVE-STRING-MATCHER isnotanoptimalprocedureforthis
problem. Indeed,inthischapterweshallseethattheKnuth-Morris-Prattalgorithm
is much better in the worst case. The naive string-matcher is inefficient because
it entirely ignores information gained about the text for one value of s when it
considers other values ofs. Suchinformation can bequitevaluable, however. For
example,ifP aaabandwefindthats 0isvalid,thennoneoftheshifts1,2,
D D
or3arevalid,sinceTŒ4 b. Inthefollowingsections,weexamineseveralways
D
tomakeeffectiveuseofthissortofinformation.
Exercises
32.1-1
Show the comparisons the naive string matcher makes for the pattern P 0001
D
inthetextT 000010001010001.
D
32.1-2
Suppose that all characters in the pattern P are different. Show how to accelerate
NAIVE-STRING-MATCHER torunintimeO.n/onann-character textT.
32.1-3
Supposethatpattern P andtextT arerandomlychosen stringsoflength mandn,
respectively,fromthed-aryalphabet† 0;1;:::;d 1 ,whered 2. Show
d
D f (cid:0) g 
that the expected number of character-to-character comparisons made by the im-
plicitloopinline4ofthenaivealgorithm is
1 d m
.n m 1/ (cid:0) (cid:0) 2.n m 1/
(cid:0) C 1 d 1  (cid:0) C
(cid:0) (cid:0)
overallexecutionsofthisloop. (Assumethatthenaivealgorithmstopscomparing
characters foragivenshiftonceitfindsamismatchormatchestheentirepattern.)
Thus,forrandomlychosenstrings, thenaivealgorithm isquiteefficient.
32.1-4
Suppose weallow the pattern P to contain occurrences of agap character that
}
canmatchanarbitrarystringofcharacters(evenoneofzerolength). Forexample,
thepatternab ba coccursinthetextcabccbacbacabas
} }
c ab cc ba cba c ab
ab ba c
} }
andas
c’ab ’ccba’c b“a ’c ab:
ab ba c
} }
’—’’’
990 Chapter32 StringMatching
Note that the gap character may occur an arbitrary number of times in the pattern
but not at all in the text. Give a polynomial-time algorithm to determine whether
such a pattern P occurs in a given text T, and analyze the running time of your
algorithm.
32.2 The Rabin-Karp algorithm
Rabin and Karp proposed astring-matching algorithm that performs wellin prac-
tice and that also generalizes to other algorithms for related problems, such as
two-dimensional pattern matching. TheRabin-Karp algorithm uses ‚.m/prepro-
cessingtime,anditsworst-caserunningtimeis‚..n m 1/m/.Basedoncertain
(cid:0) C
assumptions, however,itsaverage-case runningtimeisbetter.
This algorithm makes use of elementary number-theoretic notions such as the
equivalence of two numbers modulo a third number. You might want to refer to
Section31.1fortherelevantdefinitions.
For expository purposes, let us assume that † 0;1;2;:::;9 , so that each
D f g
character isadecimal digit. (Inthegeneral case, wecanassume thateach charac-
ter is adigit inradix-d notation, where d † .) Wecan then view astring ofk
D j j
consecutive characters as representing a length-k decimal number. The character
string 31415thus corresponds to the decimal number 31,415. Because we inter-
prettheinputcharactersasbothgraphicalsymbolsanddigits,wefinditconvenient
inthissectiontodenotethemaswewoulddigits, inourstandard textfont.
GivenapatternPŒ1::m,letpdenoteitscorrespondingdecimalvalue. Inasim-
ilar manner, given a text TŒ1::n, let t denote the decimal value of the length-m
s
substring TŒs 1::s m,fors 0;1;:::;n m. Certainly, t p ifandonly
s
C C D (cid:0) D
ifTŒs 1::s m PŒ1::m;thus,s isavalidshiftifandonlyift p. Ifwe
s
C C D D
couldcomputepintime‚.m/andallthet valuesinatotalof‚.n m 1/time,1
s
(cid:0) C
then wecould determine allvalid shifts s intime‚.m/ ‚.n m 1/ ‚.n/
C (cid:0) C D
bycomparing p witheachofthet values. (Forthemoment,let’snotworryabout
s
thepossibility thatp andthet valuesmightbeverylargenumbers.)
s
Wecancomputep intime‚.m/usingHorner’srule(seeSection30.1):
p PŒm 10.PŒm 1 10.PŒm 2 10.PŒ2 10PŒ1/ //:
D C (cid:0) C (cid:0) CC C 
Similarly,wecancomputet fromTŒ1::mintime‚.m/.
0
1Wewrite‚.n m 1/insteadof‚.n m/becausestakesonn m 1differentvalues. The
(cid:0) C (cid:0) (cid:0) C
“ 1”issignificantinanasymptoticsensebecausewhenm n,computingthelonets valuetakes
C D
‚.1/time,not‚.0/time.
32.2 TheRabin-Karpalgorithm 991
Tocompute theremaining valuest ;t ;:::;t intime‚.n m/,weobserve
1 2 n m
thatwecancomputet fromt inconstanttim(cid:0) e,since (cid:0)
s 1 s
C
t 10.t 10m 1TŒs 1/ TŒs m 1: (32.1)
s 1 s (cid:0)
C D (cid:0) C C C C
Subtracting 10m 1TŒs 1 removes the high-order digit from t , multiplying the
(cid:0) s
C
resultby10shiftsthenumberleftbyonedigitposition, andadding TŒs m 1
C C
brings inthe appropriate low-order digit. Forexample, ifm 5and t 31415,
s
D D
then we wish to remove the high-order digit TŒs 1 3 and bring in the new
C D
low-orderdigit(suppose itisTŒs 5 1 2)toobtain
C C D
t 10.31415 10000 3/ 2
s 1
C D (cid:0)  C
14152:
D
Ifweprecompute theconstant 10m 1 (whichwecandointimeO.lgm/usingthe
(cid:0)
techniques of Section 31.6, although for this application a straightforward O.m/-
timemethodsuffices),theneachexecutionofequation(32.1)takesaconstantnum-
ber of arithmetic operations. Thus, we can compute p in time ‚.m/, and we can
compute all of t ;t ;:::;t in time ‚.n m 1/. Therefore, we can find all
0 1 n m
occurrences ofthepattern
P(cid:0)Œ1::minthete(cid:0) xtTC
Œ1::nwith‚.m/preprocessing
timeand‚.n m 1/matchingtime.
(cid:0) C
Until now, we have intentionally overlooked one problem: p and t may be
s
too large to work with conveniently. If P contains m characters, then we cannot
reasonably assume that each arithmetic operation on p (which is m digits long)
takes“constanttime.” Fortunately,wecansolvethisproblemeasily,asFigure32.5
shows: computepandthet valuesmoduloasuitablemodulusq. Wecancompute
s
p modulo q in ‚.m/ time and all the t values modulo q in ‚.n m 1/ time.
s
(cid:0) C
Ifwechoose themodulusq asaprimesuchthat10q justfitswithinonecomputer
word, then we can perform all the necessary computations with single-precision
arithmetic. In general, with a d-ary alphabet 0;1;:::;d 1 , we choose q so
f (cid:0) g
that dq fits within a computer word and adjust the recurrence equation (32.1) to
workmoduloq,sothatitbecomes
t .d.t TŒs 1h/ TŒs m 1/ mod q ; (32.2)
s 1 s
C D (cid:0) C C C C
where h dm 1 .mod q/ is the value of the digit “1” in the high-order position
(cid:0)

ofanm-digittextwindow.
The solution of working modulo q is not perfect, however: t p .mod q/
s

does not imply that t p. On the other hand, if t p .mod q/, then we
s s
D 6
definitely have that t p, so that shift s is invalid. We can thus use the test
s
¤
t p .mod q/ asa fast heuristic test to rule out invalid shifts s. Anyshift s for
s

which t p .mod q/ must be tested further to see whether s is really valid or
s

we just have a spurious hit. This additional test explicitly checks the condition
992 Chapter32 StringMatching
2 3 5 9 0 2 3 1 4 1 5 2 6 7 3 9 9 2 1
mod 13
7
(a)
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19
2 3 5 9 0 2 3 1 4 1 5 2 6 7 3 9 9 2 1
… … …
mod 13
8 9 3 11 0 1 7 8 4 5 10 11 7 9 11
valid spurious
match hit
(b)
old new old new
high-order low-order high-order low-order
digit digit digit shift digit
”
14152 (31415 – 3·10000)·10 + 2 (mod 13)
3 1 4 1 5 2
”
(7 – 3·3)·10 + 2 (mod 13)
”
8 (mod 13)
7 8
(c)
Figure32.5 TheRabin-Karpalgorithm. Eachcharacterisadecimaldigit,andwecomputevalues
modulo13. (a)Atextstring. Awindowoflength5isshownshaded. Thenumericalvalueofthe
shadednumber,computedmodulo13,yieldsthevalue7. (b)Thesametextstringwithvaluescom-
putedmodulo13foreachpossiblepositionofalength-5window.AssumingthepatternP 31415,
D
welookforwindowswhosevaluemodulo13is7,since31415 7 .mod 13/.Thealgorithmfinds

twosuchwindows,shownshadedinthefigure. Thefirst,beginningattextposition7,isindeedan
occurrenceofthepattern,whilethesecond,beginningattextposition13,isaspurioushit. (c)How
tocomputethevalueforawindowinconstanttime,giventhevalueforthepreviouswindow. The
firstwindowhasvalue31415. Droppingthehigh-orderdigit3,shiftingleft(multiplyingby10),and
then adding inthe low-order digit 2 gives us thenew value 14152. Because all computations are
performedmodulo13,thevalueforthefirstwindowis7,andthevalueforthenewwindowis8.
32.2 TheRabin-Karpalgorithm 993
PŒ1::m TŒs 1::s m. If q is large enough, then we hope that spurious
D C C
hitsoccurinfrequently enoughthatthecostoftheextrachecking islow.
Thefollowingprocedure makestheseideasprecise. Theinputstotheprocedure
arethetextT,thepatternP,theradixd touse(whichistypicallytakentobe † ),
j j
andtheprimeq touse.
RABIN-KARP-MATCHER.T;P;d;q/
1 n T:length
D
2 m P:length
D
3 h dm 1 mod q
(cid:0)
D
4 p 0
D
5 t 0
0
D
6 fori 1tom //preprocessing
D
7 p .dp PŒi/mod q
D C
8 t .dt TŒi/ modq
0 0
D C
9 fors 0ton m //matching
D (cid:0)
10 ifp ==t
s
11 ifPŒ1::m == TŒs 1::s m
C C
12 print“Patternoccurswithshift”s
13 ifs < n m
(cid:0)
14 t .d.t TŒs 1h/ TŒs m 1/ mod q
s 1 s
C D (cid:0) C C C C
The procedure RABIN-KARP-MATCHER works as follows. All characters are
interpreted asradix-d digits. Thesubscripts ont areprovided onlyforclarity; the
programworkscorrectlyifallthesubscriptsaredropped. Line3initializeshtothe
valueofthehigh-order digitposition ofanm-digitwindow. Lines4–8compute p
as the value of PŒ1::m modq and t as the value of TŒ1::m mod q. The for
0
loop of lines 9–14 iterates through all possible shifts s, maintaining the following
invariant:
Wheneverline10isexecuted, t TŒs 1::s mmod q.
s
D C C
If p t in line 10 (a “hit”), then line 11 checks to see whether PŒ1::m
s
D D
TŒs 1::s minordertoruleoutthepossibilityofaspurioushit. Line12prints
C C
outanyvalidshifts that arefound. Ifs < n m(checked inline13), then thefor
(cid:0)
loopwillexecuteatleastonemoretime,andsoline14firstexecutestoensurethat
the loop invariant holds when weget back toline 10. Line 14computes the value
of t mod q from the value of t mod q in constant time using equation (32.2)
s 1 s
C
directly.
RABIN-KARP-MATCHER takes‚.m/preprocessingtime,anditsmatchingtime
is‚..n m 1/m/inthe worstcase, since (like the naive string-matching algo-
(cid:0) C
rithm) the Rabin-Karp algorithm explicitly verifies every valid shift. If P am
D
994 Chapter32 StringMatching
andT an,thenverifyingtakestime‚..n m 1/m/,sinceeachofthen m 1
D (cid:0) C (cid:0) C
possible shiftsisvalid.
In many applications, we expect few valid shifts—perhaps some constant c of
them. In such applications, the expected matching time of the algorithm is only
O..n m 1/ cm/ O.n m/, plus the time required to process spurious
(cid:0) C C D C
hits. Wecanbaseaheuristicanalysisontheassumptionthatreducingvaluesmod-
uloq actslikearandommappingfrom† toZ . (Seethediscussionontheuseof
 q
divisionforhashinginSection11.3.1. Itisdifficulttoformalizeandprovesuchan
assumption, although one viable approach is toassume that q ischosen randomly
fromintegersoftheappropriate size. Weshallnotpursue thisformalization here.)
We can then expect that the number of spurious hits is O.n=q/, since we can es-
timate the chance that an arbitrary t will be equivalent to p, modulo q, as 1=q.
s
SincethereareO.n/positionsatwhichthetestofline10failsandwespendO.m/
time for each hit, the expected matching time taken by the Rabin-Karp algorithm
is
O.n/ O.m. n=q//;
C C
where  isthe number ofvalidshifts. Thisrunning timeisO.n/if O.1/ and
D
we choose q m. That is, if the expected number of valid shifts is small (O.1/)

and we choose the prime q to be larger than the length of the pattern, then we
canexpecttheRabin-Karpprocedure touseonlyO.n m/matching time. Since
C
m n,thisexpectedmatchingtimeisO.n/.

Exercises
32.2-1
Workingmoduloq 11,howmanyspurioushitsdoestheRabin-Karpmatcheren-
D
counterinthetextT 3141592653589793 whenlookingforthepatternP 26?
D D
32.2-2
Howwouldyou extend theRabin-Karp method totheproblem ofsearching atext
stringforanoccurrence ofanyoneofagivensetofk patterns? Startbyassuming
thatallk patternshavethesamelength. Thengeneralizeyoursolutiontoallowthe
patterns tohavedifferent lengths.
32.2-3
Show howto extend the Rabin-Karp method to handle theproblem oflooking for
agivenm mpatterninann narrayofcharacters. (Thepatternmaybeshifted
 
vertically andhorizontally, butitmaynotberotated.)
32.3 Stringmatchingwithfiniteautomata 995
32.2-4
Alice has acopy of a long n-bit file A a ;a ;:::;a , and Bob similarly
n 1 n 2 0
has an n-bit file B b ;b ;:::;D b h . A(cid:0) lice (cid:0) and Bob wi ish to know if their
n 1 n 2 0
filesare identical. TD oah voi(cid:0) d tran(cid:0) smitting ali l of A or B, they use the following fast
probabilistic check. Together, theyselect aprimeq > 1000n andrandomly select
anintegerx from 0;1;:::;q 1 . Then,Aliceevaluates
f (cid:0) g
n 1
(cid:0)
A.x/ a xi mod q
i
D
!
i 0
XD
and Bob similarly evaluates B.x/. Prove that if A B, there is at most one
¤
chance in 1000 that A.x/ B.x/, whereas if the two files are the same, A.x/ is
D
necessarily thesameasB.x/. (Hint:SeeExercise31.4-4.)
32.3 String matching withfinite automata
Many string-matching algorithms build a finite automaton—a simple machine for
processing information—that scansthetextstringT foralloccurrences ofthepat-
tern P. This section presents a method for building such an automaton. These
string-matching automata are very efficient: they examine each text character ex-
actlyonce, takingconstant timepertextcharacter. Thematchingtimeused—after
preprocessing the pattern to build the automaton—is therefore ‚.n/. The time to
build the automaton, however, can be large if †is large. Section 32.4 describes a
cleverwayaroundthisproblem.
Webeginthissectionwiththedefinitionofafiniteautomaton. Wethenexamine
a special string-matching automaton and show how to use it to find occurrences
of apattern in a text. Finally, we shall show how to construct the string-matching
automatonforagiveninputpattern.
Finiteautomata
A finite automaton M, illustrated in Figure 32.6, is a 5-tuple .Q;q ;A;†;ı/,
0
where
 Qisafinitesetofstates,
 q Qisthestartstate,
0
2
 A Qisadistinguished setofacceptingstates,

 †isafiniteinputalphabet,
 ı isafunction fromQ †intoQ,calledthetransition functionofM.

996 Chapter32 StringMatching
a
input b
state a b 0 1
a
0 1 0
1 0 0 b
(a) (b)
Figure32.6 A simple two-state finiteautomaton withstate set Q 0;1 , start stateq0 0,
D f g D
and input alphabet † a;b . (a) A tabular representation of thetransition function ı. (b) An
D f g
equivalentstate-transitiondiagram. State1,shownblackened,istheonlyacceptingstate. Directed
edges represent transitions. For example, the edge fromstate1tostate0 labeled bindicates that
ı.1;b/ 0.Thisautomatonacceptsthosestringsthatendinanoddnumberofa’s.Moreprecisely,
itaccepD tsastringxifandonlyifx y´,wherey "oryendswithab,and´ ak,wherekis
D D D
odd. Forexample,oninputabaaa,includingthestartstate,thisautomatonentersthesequenceof
states 0;1;0;1;0;1 ,andsoitacceptsthisinput. Forinputabbaa,itentersthesequenceofstates
h i
0;1;0;0;1;0 ,andsoitrejectsthisinput.
h i
Thefiniteautomatonbeginsinstateq andreadsthecharactersofitsinputstring
0
one at a time. If the automaton is in state q and reads input character a, it moves
(“makes atransition”) fromstate q tostateı.q;a/. Wheneveritscurrent stateq is
a member of A, the machine M has accepted the string read so far. An input that
isnotacceptedisrejected.
A finite automaton M induces a function , called the final-state function,
from† toQsuchthat .w/isthestateM endsupinafterscanningthestringw.

Thus, M accepts a string w if and only if .w/ A. We define the function
2
recursively, usingthetransition function:
."/ q ;
0
D
.wa/ ı. .w/;a/ forw † ;a † .

D 2 2
String-matchingautomata
ForagivenpatternP,weconstructastring-matchingautomatoninapreprocessing
step before using it tosearch the text string. Figure 32.7 illustrates the automaton
for the pattern P ababaca. From now on, we shall assume that P is a given
D
fixed pattern string; for brevity, we shall not indicate the dependence upon P in
ournotation.
Inordertospecify thestring-matching automaton corresponding toagivenpat-
tern PŒ1::m, we first define an auxiliary function , called the suffix function
corresponding toP. Thefunction maps† to 0;1;:::;m suchthat.x/isthe

f g
lengthofthelongestprefixofP thatisalsoasuffixofx:
.x/ max k P = x : (32.3)
k
D f W g
32.3 Stringmatchingwithfiniteautomata 997
a
a
a a
a b a b a c a
0 1 2 3 4 5 6 7
b
b
(a)
input
state a b c P
0 1 0 0 a
1 1 2 0 b
2 3 0 0 a
3 1 4 0 b
4 5 0 0 a
5 1 4 6 c i — 1 2 3 4 5 6 7 8 9 10 11
6 7 0 0 a TŒi — a b a b a b a c a b a
7 1 2 0 state .T i/ 0 1 2 3 4 5 4 5 6 7 2 3
(b) (c)
Figure 32.7 (a) A state-transition diagram for the string-matching automaton that accepts all
stringsending in thestringababaca. State0 isthestart state, and state7(shown blackened) is
theonlyacceptingstate. Thetransitionfunctionıisdefinedbyequation(32.4),andadirectededge
fromstatei tostatej labeledarepresentsı.i;a/ j. Theright-goingedgesformingthe“spine”
D
oftheautomaton,shownheavyinthefigure,correspondtosuccessfulmatchesbetweenpatternand
inputcharacters.Exceptfortheedgesfromstate7tostates1and2,theleft-goingedgescorrespond
tomismatches. Someedges corresponding tomismatches areomitted; by convention, if astatei
hasnooutgoingedgelabeledaforsomea †,thenı.i;a/ 0. (b)Thecorrespondingtransition
2 D
functionı,andthepatternstringP ababaca. Theentriescorrespondingtosuccessfulmatches
D
betweenpatternandinputcharactersareshownshaded. (c)Theoperationoftheautomatononthe
textT abababacaba. UndereachtextcharacterTŒiappearsthestate .T i/thattheautoma-
D
tonisinafterprocessingtheprefixT i.Theautomatonfindsoneoccurrenceofthepattern,endingin
position9.
The suffix function  is well defined since the empty string P " is a suf-
0
D
fix of every string. As examples, for the pattern P ab, we have ."/ 0,
D D
.ccaca/ 1, and .ccab/ 2. For a pattern P of length m, we have
.x/ mD if and only if P = D x. From the definition of the suffix function,
x = yD implies.x/ .y/.

We define the string-matching automaton that corresponds to a given pattern
PŒ1::masfollows:
998 Chapter32 StringMatching
 Thestate setQ is 0;1;:::;m . Thestart state q isstate 0, andstate misthe
0
f g
onlyaccepting state.
 The transition function ı is defined by the following equation, for any state q
andcharacter a:
ı.q;a/ .P a/: (32.4)
q
D
We define ı.q;a/ .P a/ because wewant to keep track of the longest pre-
q
D
fix of the pattern P that has matched the text string T so far. We consider the
most recently read characters of T. In order for a substring of T—let’s say the
substring ending at TŒi—tomatch some prefix P of P, this prefix P must be a
j j
suffix of T . Suppose that q .T /, so that after reading T , the automaton isin
i i i
D
stateq. Wedesignthetransition functionı sothatthisstatenumber,q,tellsusthe
length of the longest prefix of P that matches a suffix of T . That is, in state q,
i
P = T and q .T /. (Whenever q m, allmcharacters ofP matchasuffix
q i i
D D
of T , and so we have found a match.) Thus, since .T / and .T / both equal q,
i i i
weshall see(inTheorem 32.4, below)thattheautomaton maintains thefollowing
invariant:
.T / .T /: (32.5)
i i
D
If the automaton is in state q and reads the next character TŒi 1 a, then we
C D
wantthetransitiontoleadtothestatecorresponding tothelongestprefixofP that
is a suffix of T a, and that state is .T a/. Because P is the longest prefix of P
i i q
thatisasuffixofT ,thelongestprefixofP thatisasuffixofT aisnotonly.T a/,
i i i
but also .P a/. (Lemma 32.3, on page 1000, proves that .T a/ .P a/.)
q i q
D
Thus,whentheautomaton isinstateq,wewantthetransition function oncharac-
teratotaketheautomatontostate.P a/.
q
There are two cases to consider. In the first case, a PŒq 1, so that the
D C
characteracontinuestomatchthepattern;inthiscase,becauseı.q;a/ q 1,the
D C
transition continues to go along the “spine” of the automaton (the heavy edges in
Figure32.7). Inthesecondcase,a PŒq 1,sothatadoesnotcontinuetomatch
¤ C
the pattern. Here, we must find a smaller prefix of P that is also a suffix of T .
i
Becausethepreprocessingstepmatchesthepatternagainstitselfwhencreatingthe
string-matching automaton, the transition function quickly identifies the longest
suchsmallerprefixofP.
Let’s look at an example. The string-matching automaton of Figure 32.7 has
ı.5;c/ 6, illustrating the first case, in which the match continues. To illus-
D
trate the second case, observe that the automaton of Figure 32.7 has ı.5;b/ 4.
D
We make this transition because if the automaton reads a b in state q 5, then
D
P b ababab, and the longest prefix of P that is also a suffix of ababab is
q
D
P abab.
4
D
32.3 Stringmatchingwithfiniteautomata 999
x
P a
r–1
P
r
Figure32.8 Anillustrationfortheproof of Lemma32.2. Thefigureshowsthatr .x/ 1,
 C
wherer .xa/.
D
Toclarify theoperation of astring-matching automaton, wenow giveasimple,
efficient program for simulating the behavior of such an automaton (represented
byitstransitionfunctionı)infindingoccurrences ofapatternP oflengthminan
inputtextTŒ1::n. Asforanystring-matchingautomatonforapatternoflengthm,
the state set Q is 0;1;:::;m , the start state is 0, and the only accepting state is
f g
statem.
FINITE-AUTOMATON-MATCHER.T;ı;m/
1 n T:length
D
2 q 0
D
3 fori 1ton
D
4 q ı.q;TŒi/
D
5 ifq == m
6 print“Patternoccurswithshift”i m
(cid:0)
FromthesimpleloopstructureofFINITE-AUTOMATON-MATCHER, wecaneasily
see that its matching time on a text string of length n is ‚.n/. This matching
time, however, does not include the preprocessing time required to compute the
transition function ı. We address this problem later, after first proving that the
procedure FINITE-AUTOMATON-MATCHER operatescorrectly.
Considerhowtheautomaton operates onaninputtextTŒ1::n. Weshallprove
thattheautomatonisinstate.T /afterscanningcharacterTŒi. Since.T / m
i i
ifand only ifP = T , the machine is inthe accepting state m if and only if iD thas
i
justscanned thepatternP. Toprovethisresult, wemakeuseofthefollowingtwo
lemmasaboutthesuffixfunction .
Lemma32.2(Suffix-functioninequality)
Foranystringx andcharacter a,wehave.xa/ .x/ 1.
 C
Proof Referring to Figure 32.8, let r .xa/. If r 0, then the conclusion
D D
.xa/ r .x/ 1 is trivially satisfied, by the nonnegativity of .x/. Now
assumeD that r > 0. TC hen, P = xa, by the definition of . Thus, P = x, by
r r 1
(cid:0)
1000 Chapter32 StringMatching
x
a
P a
q
P
r
Figure 32.9 An illustration for the proof of Lemma 32.3. The figure shows that r .Pqa/,
D
whereq .x/andr .xa/.
D D
droppingtheafromtheendofP andfromtheendofxa. Therefore,r 1 .x/,
r
since.x/isthelargest k suchthatP = x,andthus.xa/ r .(cid:0) x/ 1.
k
D  C
Lemma32.3(Suffix-functionrecursionlemma)
Foranystringx andcharacter a,ifq .x/,then.xa/ .P a/.
q
D D
Proof From the definition of , we have P = x. As Figure 32.9 shows, we
q
also have P a = xa. If we let r .xa/, then P = xa and, by Lemma 32.2,
q r
r q 1. Thus,wehave P rD q 1 P a . SinceP a = xa,P = xa,
r q q r
an
d
PC
P a ,
Lemmaj 32j .1D imp liesC thatD
P
j= Pj
a. Therefore, r .P a/,
r q r q q
thatj is, j . xaj / j .P a/. But we also have .P a/ .xa/, since P a = xa.
q q q
 
Thus,.xa/ .P a/.
q
D
We are now ready to prove our main theorem characterizing the behavior of a
string-matching automaton on a given input text. As noted above, this theorem
shows that the automaton is merely keeping track, at each step, of the longest
prefix of the pattern that is a suffix of what has been read so far. In other words,
theautomaton maintainstheinvariant (32.5).
Theorem32.4
If isthefinal-statefunctionofastring-matchingautomatonforagivenpatternP
andTŒ1::nisaninputtextfortheautomaton, then
.T / .T /
i i
D
fori 0;1;:::;n.
D
Proof The proof is by induction on i. For i 0, the theorem is trivially true,
D
sinceT ". Thus, .T / 0 .T /.
0 0 0
D D D
32.3 Stringmatchingwithfiniteautomata 1001
Now,weassume that .T / .T / and prove that .T / .T /. Letq
i i i 1 i 1
denote .T /,andleta denoteTD Œi 1. Then, C D C
i
C
.T / .T a/ (bythedefinitions ofT anda)
i 1 i i 1
C D C
ı. .T /;a/ (bythedefinitionof )
i
D
ı.q;a/ (bythedefinitionofq)
D
.P a/ (bythedefinition(32.4)ofı)
q
D
.T a/ (byLemma32.3andinduction)
i
D
.T / (bythedefinitionofT ) .
i 1 i 1
D C C
By Theorem 32.4, if the machine enters state q on line 4, then q is the largest
value such that P = T . Thus, we have q m on line 5 if and only if the ma-
q i
D
chine has just scanned an occurrence of the pattern P. Weconclude that FINITE-
AUTOMATON-MATCHER operates correctly.
Computingthetransitionfunction
The following procedure computes the transition function ı from a given pattern
PŒ1::m.
COMPUTE-TRANSITION-FUNCTION.P;†/
1 m P:length
D
2 forq 0tom
D
3 foreachcharacter a †
2
4 k min.m 1;q 2/
D C C
5 repeat
6 k k 1
7
untilPD= P(cid:0)
a
k q
8 ı.q;a/ k
D
9 returnı
Thisprocedure computes ı.q;a/inastraightforward manner according toitsdef-
inition in equation (32.4). The nested loops beginning on lines 2 and 3 consider
all states q and all characters a, and lines 4–8 set ı.q;a/ to be the largest k such
that P = P a. The code starts with the largest conceivable value of k, which is
k q
min.m;q 1/. Itthen decreases k untilP = P a,whichmusteventually occur,
k q
C
sinceP "isasuffixofeverystring.
0
D
The running time of COMPUTE-TRANSITION-FUNCTION is O.m3 † /, be-
j j
cause the outer loops contribute a factor of m † , the inner repeat loop can run
at most m 1 times, and the test P = P a oj n lj ine 7 can require comparing up
k q
C
1002 Chapter32 StringMatching
to m characters. Much faster procedures exist; by utilizing some cleverly com-
puted information about the pattern P (see Exercise 32.4-8), we can improve the
timerequiredtocomputeıfromP toO.m † /. Withthisimprovedprocedurefor
j j
computing ı, we can find all occurrences of a length-m pattern in a length-n text
overanalphabet †withO.m † /preprocessing timeand‚.n/matchingtime.
j j
Exercises
32.3-1
Constructthestring-matchingautomatonforthepatternP aababandillustrate
D
itsoperation onthetextstringT aaababaabaababaab.
D
32.3-2
Draw a state-transition diagram for a string-matching automaton for the pattern
ababbabbababbababbabboverthealphabet † a;b .
D f g
32.3-3
We call a pattern P nonoverlappable if P = P implies k 0 or k q. De-
k q
D D
scribethestate-transition diagramofthestring-matchingautomatonforanonover-
lappable pattern.
32.3-4 ?
Given two patterns P and P , describe how to construct a finite automaton that
0
determines alloccurrences ofeither pattern. Trytominimizethenumber ofstates
inyourautomaton.
32.3-5
Given a pattern P containing gap characters (see Exercise 32.1-4), show how to
build a finite automaton that can find an occurrence of P in a text T in O.n/
matchingtime,wheren T .
D j j
? 32.4 The Knuth-Morris-Pratt algorithm
Wenowpresentalinear-timestring-matching algorithmduetoKnuth,Morris,and
Pratt. Thisalgorithm avoidscomputing thetransition function ı altogether, andits
matching time is ‚.n/ using just an auxiliary function , which we precompute
from the pattern intime‚.m/and store inanarray Œ1::m. Thearray  allows
us to compute the transition function ı efficiently (in an amortized sense) “on the
fly”asneeded. Loosely speaking, foranystateq 0;1;:::;m andanycharacter
D
32.4 TheKnuth-Morris-Prattalgorithm 1003
a †, the value Œq contains the information we need to compute ı.q;a/ but
2
that does not depend on a. Since the array  has only m entries, whereas ı has
‚.m † /entries,wesaveafactorof † inthepreprocessingtimebycomputing
j j j j
ratherthanı.
Theprefixfunctionforapattern
The prefix function  for a pattern encapsulates knowledge about how the pat-
tern matches against shifts of itself. We can take advantage of this information to
avoid testing useless shifts in the naive pattern-matching algorithm and to avoid
precomputing thefulltransition function ı forastring-matching automaton.
Consider the operation of the naive string matcher. Figure 32.10(a) shows a
particular shift s of a template containing the pattern P ababaca against a
D
text T. For this example, q 5 of the characters have matched successfully, but
D
the6thpatterncharacterfailstomatchthecorresponding textcharacter. Theinfor-
mation that q characters have matched successfully determines the corresponding
text characters. Knowing these q text characters allows us to determine immedi-
ately that certain shifts are invalid. In the example of the figure, the shift s 1 is
C
necessarilyinvalid,sincethefirstpatterncharacter(a)wouldbealignedwithatext
character that we know does not match the first pattern character, but does match
thesecond pattern character (b). Theshift s s 2showninpart(b)ofthefig-
0
D C
ure,however,alignsthefirstthreepatterncharacters withthreetextcharacters that
mustnecessarilymatch. Ingeneral,itisusefultoknowtheanswertothefollowing
question:
GiventhatpatterncharactersPŒ1::qmatchtextcharactersTŒs 1::s q,
C C
whatistheleastshifts > s suchthatforsomek < q,
0
PŒ1::k TŒs 1::s k; (32.6)
0 0
D C C
wheres k s q?
0
C D C
In other words, knowing that P = T , we want the longest proper prefix P
q s q k
of P that is also a suffix of T . (SC ince s k s q, if we are given s
q s q 0
and q, then finding the smallest sC hift s is tantamC ountD tofinC ding the longest prefix
0
length k.) Weaddthedifference q k inthelengths ofthese prefixesofP tothe
(cid:0)
shiftstoarriveatournewshifts ,sothats s .q k/. Inthebestcase,k 0,
0 0
D C (cid:0) D
sothats s q,andweimmediatelyruleoutshiftss 1;s 2;:::;s q 1.
0
D C C C C (cid:0)
Inanycase,atthenewshifts wedon’tneedtocomparethefirstk charactersofP
0
withthecorresponding characters ofT,sinceequation (32.6)guarantees thatthey
match.
Wecanprecompute thenecessary information bycomparing thepatternagainst
itself, as Figure 32.10(c) demonstrates. Since TŒs 1::s k is part of the
0 0
C C
1004 Chapter32 StringMatching
b a c b a b a b a a b c b a b T
s a b a b a c a P
q
(a)
b a c b a b a b a a b c b a b T
s¢ = s + 2
a b a b a c a P
k
(b)
a b a b a P
q
a b a P
k
(c)
Figure32.10 Theprefixfunction. (a)ThepatternP ababacaalignswithatextT sothat
D
thefirstq 5charactersmatch.Matchingcharacters,shownshaded,areconnectedbyverticallines.
D
(b)Usingonlyourknowledge ofthe5matchedcharacters, wecandeducethatashiftofs 1is
C
invalid,butthatashiftofs s 2isconsistentwitheverythingweknowaboutthetextandtherefore
0D C
ispotentiallyvalid.(c)Wecanprecomputeusefulinformationforsuchdeductionsbycomparingthe
patternwithitself. Here,weseethatthelongestprefixofP thatisalsoapropersuffixofP5isP3.
Werepresentthisprecomputedinformationinthearray,sothatŒ5 3.Giventhatqcharacters
D
havematchedsuccessfullyatshifts,thenextpotentiallyvalidshiftisats s .q Œq/asshown
0D C (cid:0)
inpart(b).
knownportionofthetext,itisasuffixofthestringP . Therefore,wecaninterpret
q
equation (32.6)asaskingforthegreatestk < q suchthatP = P . Then,thenew
k q
shifts s .q k/isthenextpotentiallyvalidshift. Wewillfinditconvenientto
0
D C (cid:0)
store, foreachvalueofq,thenumberk ofmatching characters atthenewshifts ,
0
ratherthanstoring, say,s s.
0
(cid:0)
We formalize the information that we precompute as follows. Given a pattern
PŒ1::m,theprefixfunctionforthepatternP isthefunction 1;2;:::;m
W f g !
0;1;:::;m 1 suchthat
f (cid:0) g
Œq max k k < q andP =P :
k q
D f W g
That is, Œq is the length of the longest prefix of P that is a proper suffix of P .
q
Figure32.11(a) givesthecompleteprefixfunction  forthepatternababaca.
32.4 TheKnuth-Morris-Prattalgorithm 1005
P a b a b a c a
5
P a b a b a c a Œ5 3
3 D
P a b a b a c a Œ3 1
i 1 2 3 4 5 6 7 1 D
PŒi a b a b a c a
P " a b a b a c a Œ1 0
Œi 0 0 1 2 3 0 1 0 D
(a) (b)
Figure32.11 AnillustrationofLemma32.5forthepatternP ababacaandq 5.(a)The
D D
functionforthegivenpattern. SinceŒ5 3,Œ3 1,andŒ1 0,byiterating weobtain
D D D
 Œ5 3;1;0 .(b)WeslidethetemplatecontainingthepatternP totherightandnotewhensome
 Df g
prefixP
k
ofP matchesupwithsomepropersuffixofP5;wegetmatcheswhenk 3,1,and0.In
D
thefigure,thefirstrowgivesP,andthedottedverticallineisdrawnjustafterP5. Successiverows
showalltheshiftsofP thatcausesomeprefixP k ofP tomatchsomesuffixofP5. Successfully
matched characters are shown shaded. Vertical lines connect aligned matching characters. Thus,
fk Wk<5andP k =P5 gDf3;1;0 g. Lemma32.5claimsthat Œq Dfk Wk<qandP k =Pq
g
forallq.
The pseudocode below gives the Knuth-Morris-Pratt matching algorithm as
the procedure KMP-MATCHER. For the most part, the procedure follows from
FINITE-AUTOMATON-MATCHER, asweshallsee. KMP-MATCHERcallstheaux-
iliaryprocedure COMPUTE-PREFIX-FUNCTION tocompute.
KMP-MATCHER.T;P/
1 n T:length
D
2 m P:length
D
3  COMPUTE-PREFIX-FUNCTION.P/
D
4 q 0 //numberofcharacters matched
D
5 fori 1ton //scanthetextfromlefttoright
D
6 whileq > 0andPŒq 1 TŒi
C ¤
7 q Œq //nextcharacter doesnotmatch
D
8 ifPŒq 1 == TŒi
C
9 q q 1 //nextcharacter matches
D C
10 ifq ==m //isallofP matched?
11 print“Patternoccurswithshift”i m
(cid:0)
12 q Œq //lookforthenextmatch
D
1006 Chapter32 StringMatching
COMPUTE-PREFIX-FUNCTION.P/
1 m P:length
D
2 letŒ1::mbeanewarray
3 Œ1 0
D
4 k 0
D
5 forq 2tom
D
6 whilek > 0andPŒk 1 PŒq
C ¤
7 k Œk
D
8 ifPŒk 1 ==PŒq
C
9 k k 1
D C
10 Œq k
D
11 return
Thesetwoprocedures havemuchincommon, because bothmatchastringagainst
the pattern P: KMP-MATCHER matches the text T against P, and COMPUTE-
PREFIX-FUNCTION matchesP againstitself.
We begin with an analysis of the running times of these procedures. Proving
theseprocedures correctwillbemorecomplicated.
Running-timeanalysis
The running time of COMPUTE-PREFIX-FUNCTION is ‚.m/, which we show by
using the aggregate method of amortized analysis (see Section 17.1). The only
tricky part is showing that the while loop of lines 6–7 executes O.m/ times alto-
gether. Weshall show that itmakes at most m 1iterations. Westart by making
(cid:0)
some observations about k. First, line 4 starts k at 0, and the only way that k
increases is by theincrement operation in line 9, which executes at most once per
iterationoftheforloopoflines5–10. Thus,thetotalincreaseinkisatmostm 1.
(cid:0)
Second, since k < q upon entering the for loop and each iteration of the loop in-
crements q, we always have k < q. Therefore, the assignments in lines 3 and 10
ensure that Œq < q for all q 1;2;:::;m, which means that each iteration of
D
the while loop decreases k. Third, k never becomes negative. Putting these facts
together, we see that the total decrease in k from the while loop is bounded from
above bythetotalincrease ink overalliterations oftheforloop, whichism 1.
(cid:0)
Thus, the while loop iterates at most m 1 times in all, and COMPUTE-PREFIX-
(cid:0)
FUNCTION runsintime‚.m/.
Exercise32.4-4asksyoutoshow,byasimilaraggregateanalysis,thatthematch-
ingtimeofKMP-MATCHER is‚.n/.
ComparedwithFINITE-AUTOMATON-MATCHER, byusing ratherthanı,we
havereducedthetimeforpreprocessingthepatternfromO.m † /to‚.m/,while
j j
keeping theactualmatchingtimebounded by‚.n/.
32.4 TheKnuth-Morris-Prattalgorithm 1007
Correctnessoftheprefix-functioncomputation
Weshallseealittlelaterthattheprefixfunction  helps ussimulate thetransition
function ı in a string-matching automaton. But first, we need to prove that the
procedure COMPUTE-PREFIX-FUNCTION does indeed compute the prefix func-
tioncorrectly. Inordertodoso,wewillneedtofindallprefixesP thatareproper
k
suffixesofagivenprefixP . ThevalueofŒqgivesusthelongestsuchprefix,but
q
thefollowinglemma,illustrated inFigure32.11, showsthatbyiterating theprefix
function , we can indeed enumerate all the prefixes P that are proper suffixes
k
ofP . Let
q
 Œq Œq;.2/Œq;.3/Œq;:::;.t/Œq ;

Df g
where .i/Œq is defined in terms of functional iteration, so that .0/Œq q and
D
.i/Œq Œ.i 1/Œq for i 1, and where the sequence in  Œq stops upon
(cid:0) 
D 
reaching.t/Œq 0.
D
Lemma32.5(Prefix-functioniteration lemma)
LetP beapattern oflength mwithprefixfunction . Then,for q 1;2;:::;m,
wehave Œq k k < q andP = P . D
 k q
D f W g
Proof Wefirstprovethat Œq k k < q andP = P or,equivalently,
 k q
 f W g
i  ŒqimpliesP = P : (32.7)
 i q
2
If i  Œq, then i .u/Œq for some u > 0. We prove equation (32.7) by

2 D
induction on u. Foru 1, wehave i Œq, and the claim follows since i < q
andP =P bythedD efinitionof. UD singtherelationsŒi < i andP = P
Œq q Œi i
and the transitivity of <and =establishes the claim for all i in Œq. Therefore,

 Œq k k < q andP = P .
 k q
We nowf pW rove that k k < qg andP = P  Œq by contradiction. Sup-
k q 
pose to the contrary thf atW the set k k < q ang d P = P  Œq is nonempty,
k q 
f W g(cid:0)
and let j be the largest number in the set. Because Œq is the largest value in
k k <q andP = P and Œq  Œq, we must have j < Œq, and so we
k q 
f W g 2
let j denote the smallest integer in  Œq that is greater than j. (We can choose
0 
j Œqifnoothernumberin Œqisgreaterthanj.) WehaveP = P because
0  j q
j D k k < q andP = P , and from j  Œqand equation (32.7), wehave
k q 0 
P j2 0 =f PW q. Thus, P j = P j0 byg Lemma32.1,2 and j isthe largest value less than j 0
withthis property. Therefore, wemusthave Œj  j and, since j  Œq, we
0 0 
D 2
musthavej  Œqaswell. Thiscontradiction provesthelemma.

2
Thealgorithm COMPUTE-PREFIX-FUNCTION computesŒq,inorder,forq
D
1;2;:::;m. Setting Œ1 to 0 in line 3 of COMPUTE-PREFIX-FUNCTION is cer-
tainly correct, since Œq < q for all q. We shall use the following lemma and
1008 Chapter32 StringMatching
itscorollarytoprovethatCOMPUTE-PREFIX-FUNCTION computesŒqcorrectly
forq >1.
Lemma32.6
Let P be a pattern of length m, and let  be the prefix function for P. For q
D
1;2;:::;m,ifŒq > 0,thenŒq 1  Œq 1.

(cid:0) 2 (cid:0)
Proof Let r Œq > 0, so that r < q and P = P ; thus, r 1 < q 1 and
r q
P = P D (by dropping the last character from P and P , w(cid:0) hich we(cid:0) can do
r 1 q 1 r q
bec(cid:0) ause r >(cid:0) 0). By Lemma 32.5, therefore, r 1  Œq 1. Thus, we have

(cid:0) 2 (cid:0)
Œq 1 r 1  Œq 1.

(cid:0) D (cid:0) 2 (cid:0)
Forq 2;3;:::;m,definethesubsetE  Œq 1by
q 1 
D (cid:0)  (cid:0)
E k  Œq 1 PŒk 1 PŒq
q 1 
(cid:0) Df 2 (cid:0) W C D g
k k < q 1andP = P andPŒk 1 PŒq (byLemma32.5)
k q 1
Df W (cid:0) (cid:0) C D g
k k < q 1andP = P :
k 1 q
Df W (cid:0) C g
ThesetE consistsofthevaluesk < q 1forwhichP = P andforwhich,
q 1 k q 1
because P(cid:0)Œk 1 PŒq, we have P (cid:0) = P . Thus, E (cid:0) consists of those
k 1 q q 1
values k C Œq D 1 such that wecan exC tend P to P and(cid:0) get a proper suffix
 k k 1
ofP . 2 (cid:0) C
q
Corollary 32.7
Let P be a pattern of length m, and let  be the prefix function for P. For q
D
2;3;:::;m,
0 ifE ;
Œq q (cid:0)1 D;
D ( 1 max k E q 1 ifE q 1 :
C f 2 (cid:0) g (cid:0) ¤;
Proof If E is empty, there is no k  Œq 1 (including k 0) for which
q 1 
wecanextend(cid:0)P toP andgetaprop2 ersuffix(cid:0) ofP . ThereforeD Œq 0.
k k 1 q
IfE isnonempty,tC henforeachk E wehavek 1 <qandPD = P .
q 1 q 1 k 1 q
Therefo(cid:0)
re, fromthedefinition
ofŒq,w2 eha(cid:0)
ve
C C
Œq 1 max k E : (32.8)
q 1
 C f 2 (cid:0) g
Note that Œq > 0. Let r Œq 1, so that r 1 Œq and there-
fore P = P . Since r 1D > 0, we(cid:0) have PŒr 1C PD Œq. Furthermore,
r 1 q
by LemC ma 32.6, we haveC r  Œq 1. TherefC ore, rD E , and so r
 q 1
max k E or,equivalent2 ly, (cid:0) 2 (cid:0) 
q 1
f 2 (cid:0) g
Œq 1 max k E : (32.9)
q 1
 C f 2 (cid:0) g
Combiningequations (32.8)and(32.9)completestheproof.
32.4 TheKnuth-Morris-Prattalgorithm 1009
We now finish the proof that COMPUTE-PREFIX-FUNCTION computes  cor-
rectly. In the procedure COMPUTE-PREFIX-FUNCTION, at the start of each iter-
ation of the for loop of lines 5–10, we have that k Œq 1. This condition
D (cid:0)
is enforced by lines 3 and 4 when the loop is first entered, and it remains true in
eachsuccessive iteration because ofline10. Lines6–9adjustk sothatitbecomes
the correct value of Œq. Thewhileloop of lines 6–7 searches through all values
k  Œq 1untilitfindsavalueofk forwhichPŒk 1 PŒq;atthatpoint,

2 (cid:0) C D
k is the largest value in the set E , so that, by Corollary 32.7, we can set Œq
q 1
tok 1. Ifthewhileloopcannotfi(cid:0) ndak  Œq 1suchthatPŒk 1 PŒq,

C 2 (cid:0) C D
thenk equals0atline8. IfPŒ1 PŒq,thenweshouldsetbothk andŒqto1;
D
otherwise we should leave k alone and set Œq to 0. Lines 8–10 set k and Œq
correctlyineithercase. ThiscompletesourproofofthecorrectnessofCOMPUTE-
PREFIX-FUNCTION.
CorrectnessoftheKnuth-Morris-Prattalgorithm
We can think of the procedure KMP-MATCHER as a reimplemented version of
the procedure FINITE-AUTOMATON-MATCHER, but using the prefix function 
tocomputestatetransitions. Specifically, weshallprovethatintheithiteration of
theforloopsofboth KMP-MATCHER and FINITE-AUTOMATON-MATCHER, the
state q has the same value when we test for equality with m (at line 10 in KMP-
MATCHER and at line 5 in FINITE-AUTOMATON-MATCHER). Once we have
argued that KMP-MATCHER simulates the behavior of FINITE-AUTOMATON-
MATCHER, the correctness of KMP-MATCHER follows from the correctness of
FINITE-AUTOMATON-MATCHER (thoughweshallseealittlelaterwhyline12in
KMP-MATCHER isnecessary).
Before we formally prove that KMP-MATCHER correctly simulates FINITE-
AUTOMATON-MATCHER, let’s take amoment to understand how the prefix func-
tion  replaces the ı transition function. Recall that when a string-matching
automaton is in state q and it scans a character a TŒi, it moves to a new
D
state ı.q;a/. If a PŒq 1, so that a continues to match the pattern, then
D C
ı.q;a/ q 1. Otherwise, a PŒq 1, so that a does not continue tomatch
D C ¤ C
the pattern, and 0 ı.q;a/ q. In the first case, when a continues to match,
 
KMP-MATCHER moves to state q 1 without referring to the  function: the
C
while loop test in line 6 comes up false the first time, the test in line 8 comes up
true,andline9incrementsq.
The functioncomesintoplaywhenthecharacteradoesnotcontinuetomatch
thepattern,sothatthenewstateı.q;a/iseitherqortotheleftofqalongthespine
oftheautomaton. Thewhileloopoflines6–7inKMP-MATCHER iteratesthrough
the states in  Œq, stopping either when it arrives in a state, say q , such that a
 0
matches PŒq 1 or q has gone all the way down to 0. If a matches PŒq 1,
0 0 0
C C
1010 Chapter32 StringMatching
thenline9setsthenewstatetoq 1,whichshouldequalı.q;a/forthesimulation
0
C
to work correctly. In other words, the new state ı.q;a/should be either state 0or
onegreaterthansomestatein Œq.

Let’s look at the example in Figures 32.7 and 32.11, which are for the pattern
P ababaca. Suppose that the automaton is in state q 5; the states in
D D
 Œ5are,indescending order, 3,1,and0. Ifthenextcharacter scanned isc,then

we can easily see that the automaton moves to state ı.5;c/ 6 in both FINITE-
D
AUTOMATON-MATCHER and KMP-MATCHER. Nowsuppose thatthe nextchar-
acterscannedisinsteadb,sothattheautomatonshouldmovetostateı.5;b/ 4.
D
The while loop in KMP-MATCHER exits having executed line 7 once, and it ar-
rives in state q Œ5 3. Since PŒq 1 PŒ4 b, the test in line 8
0 0
D D C D D
comesuptrue,andKMP-MATCHER movestothenewstateq
0
1 4 ı.5;b/.
C D D
Finally, suppose that the next character scanned is instead a, so that the automa-
ton should move to state ı.5;a/ 1. The first three times that the test in line 6
D
executes, the test comes up true. The first time, we find that PŒ6 c a, and
D ¤
KMP-MATCHER moves to state Œ5 3 (the first state in  Œ5). The second
D
time, we find that PŒ4 b a and move to state Œ3 1 (the second state
D ¤ D
in Œ5). Thethird time, wefindthat PŒ2 b aand movetostate Œ1 0

D ¤ D
(the laststate in Œ5). Thewhileloop exits once itarrives instate q 0. Now,
 0
D
line8findsthatPŒq 1 PŒ1 a,andline9movestheautomatontothenew
0
C D D
stateq 1 1 ı.5;a/.
0
C D D
Thus,ourintuitionisthatKMP-MATCHERiteratesthroughthestatesin Œqin
decreasingorder,stoppingatsomestateq andthenpossiblymovingtostateq 1.
0 0
C
Although that might seem like a lot of work just to simulate computing ı.q;a/,
bear in mind that asymptotically, KMP-MATCHER is no slower than FINITE-
AUTOMATON-MATCHER.
We are now ready to formally prove the correctness of the Knuth-Morris-Pratt
algorithm. ByTheorem 32.4, wehave that q .T / after each time weexecute
i
D
line 4of FINITE-AUTOMATON-MATCHER. Therefore, itsuffices toshow that the
same property holds with regard to the for loop in KMP-MATCHER. The proof
proceeds by induction on the number ofloop iterations. Initially, both procedures
set q to0as they enter their respective forloops forthe firsttime. Consider itera-
tioni oftheforloopinKMP-MATCHER,andletq 0bestateatthestartofthisloop
iteration. By the inductive hypothesis, we have q .T /. We need to show
0 i 1
thatq .T /atline10. (Again,weshallhandleliD ne12se(cid:0) parately.)
i
D
WhenweconsiderthecharacterTŒi,thelongestprefixofP thatisasuffixofT
i
is either P q0 1 (if PŒq 0 1 TŒi) or some prefix (not necessarily proper, and
possiblyempC ty)ofP q0. WC ecoD nsiderseparatelythethreecasesinwhich.T i/ 0,
D
.T / q 1,and0 < .T / q .
i 0 i 0
D C 
32.4 TheKnuth-Morris-Prattalgorithm 1011
 If.T / 0,thenP "istheonlyprefixofP thatisasuffixofT . Thewhile
i 0 i
loop ofliD nes6–7 iteraD tes through thevalues in Œq ,butalthough P = T
 0 q i 1
for every q  Œq , the loop never finds a q such that PŒq 1 TŒi(cid:0).
 0
2 C D
The loop terminates when q reaches 0, and of course line 9 does not execute.
Therefore, q 0atline10,sothatq .T /.
i
D D
 If .T / q 1, then PŒq 1 TŒi, and the while loop test in line 6
i 0 0
D C C D
fails the first time through. Line 9 executes, incrementing q so that afterward
wehaveq q 1 .T /.
0 i
D C D
 If 0 < .T / q , then the while loop of lines 6–7 iterates at least once,
i 0

checkingindecreasingordereachvalueq  Œq untilitstopsatsomeq < q .
 0 0
2
Thus,P qisthelongestprefixofP
q0
forwhichPŒq 1 TŒi,sothatwhenthe
C D
whileloopterminates, q 1 .P q0TŒi/. Sinceq
0
.T
i
1/,Lemma32.3
impliesthat.T
i
1TŒi/C .D P q0TŒi/. Thus,wehaveD (cid:0)
(cid:0) D
q 1 .P TŒi/
q0
C D
.T TŒi/
i 1
D (cid:0)
.T /
i
D
whenthewhileloopterminates. Afterline9incrementsq,wehaveq .T /.
i
D
Line 12 is necessary in KMP-MATCHER, because otherwise, we might refer-
ence PŒm 1 on line 6 after finding an occurrence of P. (The argument that
C
q .T / upon the next execution of line 6 remains valid by the hint given in
i 1
ExD ercise3(cid:0) 2.4-8: ı.m;a/ ı.Œm;a/ or, equivalently, .Pa/ .P a/ for
Œm
D D
any a †.) The remaining argument for the correctness of the Knuth-Morris-
2
Prattalgorithm followsfromthecorrectness of FINITE-AUTOMATON-MATCHER,
since we have shown that KMP-MATCHER simulates the behavior of FINITE-
AUTOMATON-MATCHER.
Exercises
32.4-1
Computetheprefixfunction  forthepatternababbabbabbababbabb.
32.4-2
Give an upper bound on the size of  Œq as a function of q. Give an example to

showthatyourboundistight.
32.4-3
Explainhowtodeterminetheoccurrences ofpatternP inthetextT byexamining
the functionforthestringPT (thestringoflengthm nthatistheconcatenation
C
ofP andT).
1012 Chapter32 StringMatching
32.4-4
Use an aggregate analysis to show that the running time of KMP-MATCHER
is‚.n/.
32.4-5
UseapotentialfunctiontoshowthattherunningtimeofKMP-MATCHERis‚.n/.
32.4-6
Showhowtoimprove KMP-MATCHER byreplacing theoccurrenceof inline7
(butnotline12)by ,where isdefinedrecursively forq 1;2;:::;m 1by
0 0
D (cid:0)
theequation
0 ifŒq 0;
D
 0Œq
D
 0ŒŒq ifŒq
¤
0andPŒŒq C1
D
PŒq C1;
Œq ifŒq 0andPŒŒq 1 PŒq 1:
 ¤ C ¤ C
Explain why the modified algorithm is correct, and explain in what sense this
changeconstitutes animprovement.
32.4-7
Give a linear-time algorithm to determine whether a text T is a cyclic rotation of
another stringT . Forexample,arcandcararecyclicrotationsofeachother.
0
32.4-8 ?
Give an O.m † /-time algorithm for computing the transition function ı for the
j j
string-matching automaton corresponding to a given pattern P. (Hint: Prove that
ı.q;a/ ı.Œq;a/ifq morPŒq 1 a.)
D D C ¤
Problems
32-1 Stringmatchingbasedonrepetition factors
Let yi denote the concatenation of string y with itself i times. For example,
.ab/3 ababab. Wesaythat astring x † has repetition factor r ifx yr

D 2 D
forsomestring y † andsomer > 0. Let.x/denote thelargest r suchthatx

2
hasrepetition factorr.
a. GiveanefficientalgorithmthattakesasinputapatternPŒ1::mandcomputes
the value .P / for i 1;2;:::;m. What is the running time of your algo-
i
D
rithm?
NotesforChapter32 1013
b. ForanypatternPŒ1::m,let .P/bedefinedasmax .P /. Provethatif
 1 i m i
thepatternP ischosenrandomlyfromthesetofallbin a rystringsoflengthm,
thentheexpected valueof .P/isO.1/.

c. Argue that the following string-matching algorithm correctly finds all occur-
rencesofpatternP inatextTŒ1::nintimeO. .P/n m/:

C
REPETITION-MATCHER.P;T/
1 m P:length
D
2 n T:length
D
3 k 1  .P/

D C
4 q 0
D
5 s 0
D
6 whiles n m
 (cid:0)
7 ifTŒs q 1 ==PŒq 1
C C C
8 q q 1
D C
9 ifq == m
10 print“Patternoccurswithshift”s
11 ifq == morTŒs q 1 PŒq 1
C C ¤ C
12 s s max.1; q=k /
D C d e
13 q 0
D
This algorithm is due to Galil and Seiferas. By extending these ideas greatly,
theyobtained alinear-timestring-matching algorithmthatusesonlyO.1/stor-
agebeyondwhatisrequired forP andT.
Chapter notes
The relation of string matching to the theory of finite automata is discussed by
Aho, Hopcroft, and Ullman [5]. The Knuth-Morris-Pratt algorithm [214] was
invented independently by Knuth and Pratt and by Morris; they published their
workjointly. Reingold,Urban,andGries[294]giveanalternativetreatmentofthe
Knuth-Morris-Pratt algorithm. The Rabin-Karp algorithm was proposed by Karp
and Rabin [201]. Galil and Seiferas [126] give aninteresting deterministic linear-
timestring-matching algorithm that uses only O.1/ space beyond that required to
storethepatternandtext.
33 Computational Geometry
Computational geometryisthebranchofcomputersciencethatstudiesalgorithms
for solving geometric problems. In modern engineering and mathematics, com-
putational geometry has applications in such diverse fields as computer graphics,
robotics, VLSI design, computer-aided design, molecular modeling, metallurgy,
manufacturing,textilelayout,forestry,andstatistics. Theinputtoacomputational-
geometry problem is typically a description of a set of geometric objects, such as
asetofpoints, asetoflinesegments,ortheverticesofapolygon incounterclock-
wise order. The output is often a response to a query about the objects, such as
whether any of the lines intersect, or perhaps a new geometric object, such as the
convexhull(smallestenclosing convexpolygon) ofthesetofpoints.
In this chapter, we look at a few computational-geometry algorithms in two
dimensions, that is, in the plane. We represent each input object by a set of
points p ;p ;p ;::: , where each p .x ;y / and x ;y R . For exam-
1 2 3 i i i i i
f g D 2
ple, we represent an n-vertex polygon P by a sequence p ; p ; p ; :::; p
0 1 2 n 1
of its vertices in order of their appearance on the boundaryh of P. Computatio(cid:0) nai l
geometrycanalsoapplytothreedimensions, andevenhigher-dimensional spaces,
but such problems and their solutions can be very difficult to visualize. Even in
two dimensions, however, we can see a good sample of computational-geometry
techniques.
Section 33.1 shows how to answer basic questions about line segments effi-
ciently and accurately: whether one segment is clockwise or counterclockwise
from another that shares an endpoint, which way we turn when traversing two
adjoining line segments, and whether two line segments intersect. Section 33.2
presents a technique called “sweeping” that we use to develop an O.nlgn/-time
algorithm for determining whether a set of n line segments contains any inter-
sections. Section 33.3 gives two “rotational-sweep” algorithms that compute the
convex hull (smallest enclosing convex polygon) of a set of n points: Graham’s
scan, which runs in time O.nlgn/, and Jarvis’s march, which takes O.nh/ time,
where h is the number of vertices of the convex hull. Finally, Section 33.4 gives
33.1 Line-segmentproperties 1015
an O.nlgn/-time divide-and-conquer algorithm for finding the closest pair of
pointsinasetofnpointsintheplane.
33.1 Line-segment properties
Several of the computational-geometry algorithms in this chapter require answers
to questions about the properties of line segments. A convex combination of two
distinct points p .x ;y / and p .x ;y / is any point p .x ;y / such
1 1 1 2 2 2 3 3 3
D D D
that for some ˛ in the range 0 ˛ 1, we have x ˛x .1 ˛/x and
3 1 2
  D C (cid:0)
y ˛y .1 ˛/y . Wealsowritethatp ˛p .1 ˛/p . Intuitively, p
3 1 2 3 1 2 3
D C (cid:0) D C (cid:0)
isanypoint that ison theline passing through p andp and ison orbetween p
1 2 1
and p on the line. Given two distinct points p and p , the line segment p p
2 1 2 1 2
is the set of convex combinations of p and p . We call p and p the endpoints
1 2 1 2
of segment p p . Sometimes the ordering of p and p matters, and wespeak of
1 2 1 2
thedirectedsegmentp p . Ifp istheorigin.0;0/,thenwecantreatthedirected
(cid:0)(cid:0)1(cid:0)!2 1
segment(cid:0)p (cid:0)1(cid:0)p !2 asthevectorp 2.
Inthissection, weshallexplorethefollowingquestions:
1. Given two directed segments p p and p p , is p p clockwise from p p
(cid:0)(cid:0)0(cid:0)!1 (cid:0)(cid:0)0(cid:0)!2 (cid:0)(cid:0)0(cid:0)!1 (cid:0)(cid:0)0(cid:0)!2
withrespecttotheircommonendpointp ?
0
2. Given two line segments p p and p p , if we traverse p p and then p p ,
0 1 1 2 0 1 1 2
dowemakealeftturnatpointp ?
1
3. Dolinesegmentsp p andp p intersect?
1 2 3 4
Therearenorestrictions onthegivenpoints.
We can answer each question in O.1/ time, which should come as no surprise
sincetheinputsizeofeachquestion isO.1/. Moreover, ourmethods useonlyad-
ditions, subtractions, multiplications, and comparisons. We need neither division
nor trigonometric functions, both of which can be computationally expensive and
pronetoproblemswithround-offerror. Forexample,the“straightforward”method
ofdetermining whether two segments intersect—compute the line equation ofthe
form y mx b for each segment (m is the slope and b is the y-intercept),
D C
find the point of intersection of the lines, and check whether this point is on both
segments—uses division to find the point of intersection. When the segments are
nearlyparallel, thismethodisverysensitivetotheprecision ofthedivision opera-
tiononrealcomputers. Themethodinthissection,whichavoidsdivision,ismuch
moreaccurate.
1016 Chapter33 ComputationalGeometry
y y
p 1 + p 2
p
p
2
(0,0)
x
p
1
(0,0) x
(a) (b)
Figure33.1 (a)Thecrossproduct ofvectorsp1 andp2 isthesignedareaoftheparallelogram.
(b)Thelightlyshadedregioncontainsvectorsthatareclockwisefromp. Thedarklyshadedregion
containsvectorsthatarecounterclockwisefromp.
Crossproducts
Computing cross products liesattheheartofourline-segment methods. Consider
vectors p and p , shown in Figure 33.1(a). We can interpret the cross product
1 2
p p asthesignedareaoftheparallelogram formedbythepoints.0;0/,p ,p ,
1 2 1 2

andp p .x x ;y y /. Anequivalent,butmoreuseful,definitiongives
1 2 1 2 1 2
C D C C
thecrossproductasthedeterminant ofamatrix:1
x x
p p det 1 2
1  2 D y 1 y 2
 
x y x y
1 2 2 1
D (cid:0)
p p :
2 1
D (cid:0) 
Ifp p ispositive,thenp isclockwisefromp withrespecttotheorigin.0;0/;
1 2 1 2

if this cross product is negative, then p is counterclockwise from p . (See Exer-
1 2
cise 33.1-1.) Figure 33.1(b) shows the clockwise and counterclockwise regions
relativetoavectorp. Aboundary condition arisesifthecrossproduct is0;inthis
case,thevectorsarecolinear,pointing ineitherthesameoropposite directions.
To determine whether a directed segment p p is closer to a directed seg-
(cid:0)(cid:0)0(cid:0)!1
mentp p inaclockwisedirectionorinacounterclockwise directionwithrespect
(cid:0)(cid:0)0(cid:0)!2
to their common endpoint p , we simply translate to use p as the origin. That
0 0
is, we let p p denote the vector p .x ;y /, where x x x and
1
(cid:0)
0 10
D
10 10 10
D
1
(cid:0)
0
y y y ,andwedefinep p similarly. Wethencomputethecrossproduct
10
D
1
(cid:0)
0 2
(cid:0)
0
1Actually, thecrossproduct isathree-dimensional concept. It isavector that isperpendicular to
bothp1 andp2accordingtothe“right-handrule”andwhosemagnitudeis x1y2 x2y1. Inthis
j (cid:0) j
chapter,however,wefinditconvenienttotreatthecrossproductsimplyasthevaluex1y2 x2y1.
(cid:0)
33.1 Line-segmentproperties 1017
p p
2 2
p p
1 1
counterclockwise clockwise
p p
0 0
(a) (b)
Figure33.2 Usingthecrossproducttodeterminehowconsecutivelinesegmentsp0p1andp1p2
turnatpoint p1. Wecheck whether thedirectedsegment (cid:0)p (cid:0)0(cid:0)p !2 isclockwise or counterclockwise
relativetothedirectedsegment (cid:0)p (cid:0)0(cid:0)p !1. (a)Ifcounterclockwise, thepointsmakealeftturn. (b)If
clockwise,theymakearightturn.
.p p / .p p / .x x /.y y / .x x /.y y /:
1 0 2 0 1 0 2 0 2 0 1 0
(cid:0)  (cid:0) D (cid:0) (cid:0) (cid:0) (cid:0) (cid:0)
If this cross product is positive, then p p is clockwise from p p ; if negative, it
(cid:0)(cid:0)0(cid:0)!1 (cid:0)(cid:0)0(cid:0)!2
iscounterclockwise.
Determiningwhetherconsecutivesegmentsturnleftorright
Our next question is whether two consecutive line segments p p and p p turn
0 1 1 2
leftorrightatpointp . Equivalently,wewantamethodtodeterminewhichwaya
1
givenangle p p p turns. Crossproducts allowustoanswerthisquestion with-
0 1 2
†
outcomputingtheangle. AsFigure33.2shows,wesimplycheckwhetherdirected
segmentp p isclockwiseorcounterclockwise relativetodirectedsegmentp p .
(cid:0)(cid:0)0(cid:0)!2 (cid:0)(cid:0)0(cid:0)!1
To do so, we compute the cross product .p p / .p p /. If the sign of
2 0 1 0
(cid:0)  (cid:0)
thiscrossproductisnegative,thenp p iscounterclockwise withrespecttop p ,
(cid:0)(cid:0)0(cid:0)!2 (cid:0)(cid:0)0(cid:0)!1
andthus wemakealeftturn atp . Apositive cross product indicates aclockwise
1
orientation andarightturn. Acrossproduct of0meansthatpoints p ,p ,andp
0 1 2
arecolinear.
Determiningwhethertwolinesegmentsintersect
Todeterminewhethertwolinesegmentsintersect,wecheckwhethereachsegment
straddlesthelinecontaining theother. Asegmentp p straddlesalineifpointp
1 2 1
lies on one side of the line and point p lies on the other side. A boundary case
2
arises ifp orp lies directly on the line. Twoline segments intersect if and only
1 2
ifeither(orboth)ofthefollowingconditions holds:
1. Eachsegmentstraddles thelinecontaining theother.
2. An endpoint of one segment lies on the other segment. (This condition comes
fromtheboundary case.)
1018 Chapter33 ComputationalGeometry
Thefollowingproceduresimplementthisidea. SEGMENTS-INTERSECT returns
TRUE if segments p 1p 2 and p 3p 4 intersect and FALSE if they do not. It calls
thesubroutines DIRECTION, whichcomputesrelativeorientations usingthecross-
product method above, and ON-SEGMENT, which determines whether a point
knowntobecolinear withasegmentliesonthatsegment.
SEGMENTS-INTERSECT.p 1;p 2;p 3;p 4/
1 d
1
DIRECTION.p 3;p 4;p 1/
D
2 d
2
DIRECTION.p 3;p 4;p 2/
D
3 d
3
DIRECTION.p 1;p 2;p 3/
D
4 d
4
DIRECTION.p 1;p 2;p 4/
D
5 if..d > 0andd < 0/or.d < 0andd >0//and
1 2 1 2
..d >0andd < 0/or.d < 0andd > 0//
3 4 3 4
6 return TRUE
7 elseifd
1
==0and ON-SEGMENT.p 3;p 4;p 1/
8 return TRUE
9 elseifd
2
==0and ON-SEGMENT.p 3;p 4;p 2/
10 return TRUE
11 elseifd
3
==0and ON-SEGMENT.p 1;p 2;p 3/
12 return TRUE
13 elseifd
4
==0and ON-SEGMENT.p 1;p 2;p 4/
14 return TRUE
15 elsereturn FALSE
DIRECTION.p i;p j;p k/
1 return.p p / .p p /
k i j i
(cid:0)  (cid:0)
ON-SEGMENT.p i;p j;p k/
1 ifmin.x ;x / x max.x ;x /andmin.y ;y / y max.y ;y /
i j k i j i j k i j
   
2 return TRUE
3 elsereturn FALSE
SEGMENTS-INTERSECT works as follows. Lines 1–4 compute the relative ori-
entationd ofeachendpointp withrespecttotheothersegment. Ifalltherelative
i i
orientations are nonzero, then we can easily determine whether segments p p
1 2
and p p intersect, as follows. Segment p p straddles the line containing seg-
3 4 1 2
mentp p ifdirectedsegmentsp p andp p haveopposite orientations relative
3 4 (cid:0)(cid:0)3(cid:0)!1 (cid:0)(cid:0)3(cid:0)!2
to (cid:0)p (cid:0)3(cid:0)!p 4. In this case, the signs of d
1
and d
2
differ. Similarly, p 3p
4
straddles
the line containing p p if the signs of d and d differ. If the test of line 5 is
1 2 3 4
true, then the segments straddle each other, and SEGMENTS-INTERSECT returns
TRUE. Figure 33.3(a) shows this case. Otherwise, the segments do not straddle
33.1 Line-segmentproperties 1019
(p –p ) · (p –p ) < 0 p (p –p ) · (p –p ) < 0 p
1 3 4 3 4 1 3 4 3 4
p 1 (p 4–p 1) · (p 2–p 1) < 0 p 1 p (p 4–p 1) · (p 2–p 1) < 0
2
(p –p ) · (p –p ) < 0
2 3 4 3
p
(p 3–p 1) · (p 2–p 1) > 0 2 (p –p ) · (p –p ) > 0
p (p –p ) · (p –p ) > 0 p 3 1 2 1
3 2 3 4 3 3
(a) (b)
p p
p 4 p 4
1 1
p
3
p
2 p
2 p
3
(c) (d)
Figure33.3 Casesintheprocedure SEGMENTS-INTERSECT. (a)Thesegmentsp1p2 andp3p4
straddleeachother’slines. Becausep3p4straddlesthelinecontainingp1p2,thesignsofthecross
products.p3 p1/ .p2 p1/and.p4 p1/ .p2 p1/differ.Becausep1p2straddlestheline
(cid:0)  (cid:0) (cid:0)  (cid:0)
containingp3p4,thesignsofthecrossproducts.p1 p3/ .p4 p3/and.p2 p3/ .p4 p3/
(cid:0)  (cid:0) (cid:0)  (cid:0)
differ. (b)Segment p3p4 straddlesthelinecontainingp1p2, butp1p2 does notstraddletheline
containingp3p4.Thesignsofthecrossproducts.p1 p3/ .p4 p3/and.p2 p3/ .p4 p3/
(cid:0)  (cid:0) (cid:0)  (cid:0)
arethesame.(c)Pointp3iscolinearwithp1p2andisbetweenp1andp2.(d)Pointp3iscolinear
withp1p2,butitisnotbetweenp1andp2.Thesegmentsdonotintersect.
each other’s lines, although aboundary case mayapply. Ifall the relative orienta-
tions are nonzero, no boundary case applies. All the tests against 0 in lines 7–13
then fail, and SEGMENTS-INTERSECT returns FALSE in line 15. Figure 33.3(b)
showsthiscase.
Aboundarycaseoccursifanyrelativeorientationd is0. Here,weknowthatp
k k
is colinear with the other segment. It is directly on the other segment if and only
ifitisbetween the endpoints ofthe other segment. Theprocedure ON-SEGMENT
returns whether p is between the endpoints of segment p p , which will be the
k i j
othersegmentwhencalledinlines7–13;theprocedureassumesthatp iscolinear
k
with segment p p . Figures 33.3(c) and (d) show cases with colinear points. In
i j
Figure 33.3(c), p 3 is on p 1p 2, and so SEGMENTS-INTERSECT returns TRUE in
line12. NoendpointsareonothersegmentsinFigure33.3(d),andsoSEGMENTS-
INTERSECT returns FALSE inline15.
1020 Chapter33 ComputationalGeometry
Otherapplicationsofcrossproducts
Latersections ofthischapter introduce additional usesfor cross products. InSec-
tion 33.3, weshall need tosort aset ofpoints according totheir polar angles with
respect to a given origin. As Exercise 33.1-3 asks you to show, we can use cross
products toperform thecomparisons inthesortingprocedure. InSection33.2,we
shallusered-blacktreestomaintaintheverticalorderingofasetoflinesegments.
Rather than keeping explicit key values which we compare to each other in the
red-black tree code, we shall compute a cross-product to determine which of two
segmentsthatintersect agivenverticallineisabovetheother.
Exercises
33.1-1
Prove that if p p is positive, then vector p is clockwise from vector p with
1 2 1 2

respect to the origin .0;0/ and that if this cross product is negative, then p is
1
counterclockwise fromp .
2
33.1-2
Professor van Peltproposes that only the x-dimension needs to be tested in line 1
of ON-SEGMENT. Showwhytheprofessor iswrong.
33.1-3
Thepolarangleofapointp withrespecttoanoriginpointp istheangleofthe
1 0
vector p p intheusual polar coordinate system. Forexample, thepolar angle
1 0
(cid:0)
of.3;5/withrespectto.2;4/istheangleofthevector.1;1/,whichis45degrees
or =4 radians. The polar angle of .3;3/ with respect to .2;4/ is the angle of the
vector .1; 1/, which is 315 degrees or 7=4 radians. Write pseudocode to sort a
(cid:0)
sequence p ;p ;:::;p ofn points according to their polar angles withrespect
1 2 n
h i
toagivenoriginpointp . YourprocedureshouldtakeO.nlgn/timeandusecross
0
products tocompareangles.
33.1-4
Show how to determine in O.n2lgn/ time whether any three points in a set of n
pointsarecolinear.
33.1-5
A polygon is a piecewise-linear, closed curve in the plane. That is, it is a curve
ending on itself that is formed by a sequence of straight-line segments, called the
sidesofthepolygon. Apointjoiningtwoconsecutive sidesisavertexofthepoly-
gon. Ifthepolygonissimple,asweshallgenerallyassume,itdoesnotcrossitself.
The set of points in the plane enclosed by a simple polygon forms the interior of
33.2 Determiningwhetheranypairofsegmentsintersects 1021
thepolygon, thesetofpoints onthepolygon itselfformsitsboundary,andtheset
ofpoints surrounding the polygon forms itsexterior. Asimple polygon isconvex
if, given any two points on its boundary or in its interior, all points on the line
segment drawnbetween them arecontained inthe polygon’s boundary orinterior.
Avertexofaconvexpolygoncannotbeexpressedasaconvexcombination ofany
twodistinctpointsontheboundary orintheinteriorofthepolygon.
ProfessorAmundsenproposes thefollowingmethodtodetermine whetherase-
quence p ;p ;:::;p of n points forms the consecutive vertices of a convex
0 1 n 1
polygonh . Output “yes”i(cid:0) fti heset p p p i 0;1;:::;n 1 ,wheresub-
i i 1 i 2
script addition is performed modf u† lo n, C doesC notW coD ntain both lef(cid:0) t tug rns and right
turns; otherwise, output “no.” Showthat although thismethod runs inlinear time,
it does not always produce the correct answer. Modify the professor’s method so
thatitalwaysproduces thecorrectanswerinlineartime.
33.1-6
Given a point p .x ;y /, the right horizontal ray from p is the set of points
0 0 0 0
D
p .x ;y / x x andy y ,thatis,itisthesetofpointsduerightofp
i i i i 0 i 0 0
f D W  D g
along with p itself. Show how to determine whether a given right horizontal ray
0
from p intersects a line segment p p in O.1/ time by reducing the problem to
0 1 2
thatofdetermining whethertwolinesegmentsintersect.
33.1-7
One way to determine whether a point p is in the interior of a simple, but not
0
necessarilyconvex,polygonP istolookatanyrayfromp andcheckthattheray
0
intersects the boundary of P an odd number of times but that p itself is not on
0
the boundary of P. Show how to compute in ‚.n/ time whether a point p is in
0
theinteriorofann-vertexpolygonP. (Hint:UseExercise33.1-6. Makesureyour
algorithm is correct when the ray intersects the polygon boundary at a vertex and
whentherayoverlapsasideofthepolygon.)
33.1-8
Show how to compute the area of an n-vertex simple, but not necessarily convex,
polygonin‚.n/time. (SeeExercise33.1-5fordefinitionspertainingtopolygons.)
33.2 Determining whether any pairofsegments intersects
Thissectionpresents analgorithm fordetermining whetheranytwolinesegments
in a set of segments intersect. The algorithm uses a technique known as “sweep-
ing,”whichiscommontomanycomputational-geometry algorithms. Moreover,as
1022 Chapter33 ComputationalGeometry
theexercisesattheendofthissectionshow,thisalgorithm,orsimplevariationsof
it,canhelpsolveothercomputational-geometry problems.
ThealgorithmrunsinO.nlgn/time,wherenisthenumberofsegmentsweare
given. It determines only whether or not any intersection exists; it does not print
all the intersections. (ByExercise 33.2-1, it takes .n2/ time in the worst case to
findalltheintersections inasetofnlinesegments.)
In sweeping, an imaginary vertical sweep line passes through the given set of
geometric objects, usually from left to right. We treat the spatial dimension that
the sweep line moves across, in this case the x-dimension, as a dimension of
time. Sweepingprovidesamethodfororderinggeometricobjects,usuallybyplac-
ing them into a dynamic data structure, and for taking advantage of relationships
among them. Theline-segment-intersection algorithm inthissection considers all
theline-segmentendpointsinleft-to-rightorderandchecksforanintersectioneach
timeitencounters anendpoint.
To describe and prove correct our algorithm for determining whether any two
ofnlinesegmentsintersect, weshallmaketwosimplifyingassumptions. First,we
assume that no input segment is vertical. Second, we assume that no three input
segments intersect at a single point. Exercises 33.2-8 and 33.2-9 ask you to show
thatthealgorithm isrobust enoughthatitneedsonlyaslightmodification towork
even when these assumptions do not hold. Indeed, removing such simplifying
assumptions anddealing withboundary conditions oftenpresent themostdifficult
challenges when programming computational-geometry algorithms and proving
theircorrectness.
Orderingsegments
Because we assume that there are no vertical segments, we know that any input
segmentintersectingagivenverticalsweeplineintersectsitatasinglepoint. Thus,
we can order the segments that intersect a vertical sweep line according to the y-
coordinates ofthepointsofintersection.
Tobemoreprecise,considertwosegmentss ands . Wesaythatthesesegments
1 2
arecomparableatxiftheverticalsweeplinewithx-coordinatexintersectsbothof
them. Wesaythats isaboves atx,writtens < s ,ifs ands arecomparable
1 2 1 x 2 1 2
atxandtheintersectionofs withthesweeplineatxishigherthantheintersection
1
of s with the same sweep line, or if s and s intersect at the sweep line. In
2 1 2
Figure 33.4(a), for example, we have the relationships a < c, a < b, b < c,
r t t
a < c,andb < c. Segmentd isnotcomparable withanyothersegment.
t u
For any given x, the relation “< ” is a total preorder (see Section B.2) for all
x
segments that intersect the sweep line at x. That is, the relation is transitive, and
if segments s and s each intersect the sweep line at x, then either s < s
1 2 1 x 2
or s < s , or both (if s and s intersect at the sweep line). (The relation < is
2 x 1 1 2 x
33.2 Determiningwhetheranypairofsegmentsintersects 1023
e
d
a b g
i
h
c
f
r t u v z w
(a) (b)
Figure33.4 Theorderingamonglinesegmentsatvariousverticalsweeplines.(a)Wehavea<r c,
a <t b, b <t c, a <t c, andb <u c. Segment d iscomparable withnoother segment shown.
(b)Whensegmentseandf intersect,theyreversetheirorders: wehavee < f butf <w e. Any
sweepline(suchas´)thatpassesthroughtheshadedregionhaseandf consecutiveintheordering
givenbytherelation<´.
also reflexive, but neither symmetric nor antisymmetric.) The total preorder may
differfordiffering valuesofx,however, assegmentsenterandleavetheordering.
Asegmententers theordering whenitsleftendpoint isencountered bythesweep,
anditleavestheorderingwhenitsrightendpoint isencountered.
What happens when the sweep line passes through the intersection of two seg-
ments? As Figure 33.4(b) shows, the segments reverse their positions in the total
preorder. Sweep lines  and w are to the left and right, respectively, of the point
of intersection of segments e and f, and we have e < f and f < e. Note
 w
that because we assume that no three segments intersect at the same point, there
must be some vertical sweep line x for which intersecting segments e and f are
consecutiveinthetotalpreorder< . Anysweeplinethatpassesthroughtheshaded
x
regionofFigure33.4(b),suchas´,hase andf consecutive initstotalpreorder.
Movingthesweepline
Sweepingalgorithms typically managetwosetsofdata:
1. Thesweep-linestatusgivestherelationships amongtheobjects thatthesweep
lineintersects.
2. The event-point schedule is a sequence of points, called event points, which
we order from left to right according to their x-coordinates. As the sweep
progresses fromlefttoright,wheneverthesweeplinereachesthex-coordinate
ofaneventpoint, thesweephalts,processes theeventpoint,andthenresumes.
Changestothesweep-line statusoccuronlyateventpoints.
Forsome algorithms (the algorithm asked for in Exercise 33.2-7, for example),
theevent-pointscheduledevelopsdynamicallyasthealgorithmprogresses. Theal-
gorithm athand, however, determines allthe eventpoints before the sweep, based
1024 Chapter33 ComputationalGeometry
solely onsimpleproperties ofthe input data. Inparticular, eachsegment endpoint
is an event point. We sort the segment endpoints by increasing x-coordinate and
proceed from lefttoright. (Iftwoormoreendpoints arecovertical, i.e., theyhave
thesamex-coordinate, webreak thetiebyputting allthecovertical leftendpoints
before the covertical right endpoints. Within aset of covertical left endpoints, we
putthosewithlowery-coordinates first,andwedothesamewithinasetofcover-
ticalrightendpoints.) Whenweencounter asegment’s leftendpoint, weinsertthe
segmentintothesweep-linestatus,andwedeletethesegmentfromthesweep-line
status upon encountering its right endpoint. Whenever two segments firstbecome
consecutive inthetotalpreorder, wecheckwhethertheyintersect.
The sweep-line status is a total preorder T, for which we require the following
operations:
 INSERT.T;s/: insertsegments intoT.
 DELETE.T;s/: deletesegments fromT.
 ABOVE.T;s/: returnthesegmentimmediately abovesegments inT.
 BELOW.T;s/: returnthesegmentimmediatelybelowsegments inT.
It is possible for segments s and s to be mutually above each other in the total
1 2
preorder T; this situation can occur if s and s intersect at the sweep line whose
1 2
total preorder is given by T. In this case, the two segments may appear in either
orderinT.
Iftheinputcontainsnsegments,wecanperformeachoftheoperations INSERT,
DELETE, ABOVE, and BELOW in O.lgn/ time using red-black trees. Recall that
the red-black-tree operations in Chapter 13 involve comparing keys. We can re-
placethekeycomparisonsbycomparisonsthatusecrossproductstodeterminethe
relativeordering oftwosegments(seeExercise33.2-2).
Segment-intersection pseudocode
The following algorithm takes as input a set S of n line segments, returning the
boolean value TRUE ifanypair ofsegments inS intersects, and FALSE otherwise.
Ared-black treemaintainsthetotalpreorder T.
33.2 Determiningwhetheranypairofsegmentsintersects 1025
ANY-SEGMENTS-INTERSECT.S/
1 T
D ;
2 sorttheendpoints ofthesegmentsinS fromlefttoright,
breakingtiesbyputtingleftendpoints beforerightendpoints
andbreakingfurther tiesbyputting pointswithlower
y-coordinates first
3 foreachpointp inthesortedlistofendpoints
4 ifp istheleftendpoint ofasegments
5 INSERT.T;s/
6 if(ABOVE.T;s/existsandintersects s)
or(BELOW.T;s/existsandintersects s)
7 return TRUE
8 ifp istherightendpoint ofasegments
9 ifboth ABOVE.T;s/and BELOW.T;s/exist
and ABOVE.T;s/intersects BELOW.T;s/
10 return TRUE
11 DELETE.T;s/
12 return FALSE
Figure33.5illustrateshowthealgorithmworks. Line1initializesthetotalpreorder
tobeempty. Line2determinestheevent-pointschedulebysortingthe2nsegment
endpoints fromlefttoright,breaking tiesasdescribed above. Onewaytoperform
line2isbylexicographically sortingtheendpoints on.x;e;y/,wherex andy are
theusualcoordinates, e 0foraleftendpoint, ande 1forarightendpoint.
D D
Eachiteration oftheforloop oflines 3–11 processes one eventpoint p. Ifp is
the left endpoint of a segment s, line 5 adds s to the total preorder, and lines 6–7
return TRUE ifs intersects eitherofthesegments itisconsecutive withinthetotal
preorder defined by the sweep line passing through p. (A boundary condition
occurs if p lies on another segment s . In this case, we require only that s and s
0 0
be placed consecutively into T.) If p is the right endpoint of a segment s, then
we need to delete s from the total preorder. But first, lines 9–10 return TRUE if
there is an intersection between the segments surrounding s in the total preorder
defined by the sweep line passing through p. If these segments do not intersect,
line 11 deletes segment s from the total preorder. If the segments surrounding
segment s intersect, they would have become consecutive after deleting s had the
returnstatement inline10notprevented line11fromexecuting. Thecorrectness
argument, which follows, will make it clear why it suffices to check the segments
surrounding s. Finally, if we never find any intersections after having processed
all2neventpoints, line12returns FALSE.
1026 Chapter33 ComputationalGeometry
e
a d
c
f
b
a a a d d e
b c a c d
b c b c
b b
time
Figure33.5 TheexecutionofANY-SEGMENTS-INTERSECT.Eachdashedlineisthesweeplineat
aneventpoint.Exceptfortherightmostsweepline,theorderingofsegmentnamesbeloweachsweep
linecorrespondstothetotalpreorderT attheendoftheforloopprocessingthecorrespondingevent
point. Therightmostsweeplineoccurswhenprocessingtherightendpoint ofsegmentc; because
segmentsd andbsurroundcandintersecteachother,theprocedurereturnsTRUE.
Correctness
To show that ANY-SEGMENTS-INTERSECT is correct, we will prove that the call
ANY-SEGMENTS-INTERSECT.S/ returns TRUE ifand only ifthere isanintersec-
tionamongthesegmentsinS.
It is easy to see that ANY-SEGMENTS-INTERSECT returns TRUE (on lines 7
and 10) only if itfindsan intersection between two ofthe input segments. Hence,
ifitreturns TRUE,thereisanintersection.
We also need to show the converse: that if there is an intersection, then ANY-
SEGMENTS-INTERSECT returns TRUE. Let us suppose that there is at least one
intersection. Letpbetheleftmostintersectionpoint,breakingtiesbychoosingthe
point with the lowest y-coordinate, and let a and b be the segments that intersect
atp. Sincenointersections occurtotheleftofp,theordergivenbyT iscorrectat
allpointstotheleftofp. Becausenothreesegmentsintersectatthesamepoint, a
and b become consecutive in the total preorder at some sweep line ´.2 Moreover,
´istothe left ofp orgoes through p. Somesegment endpoint q onsweep line ´
2Ifweallowthreesegmentstointersectatthesamepoint,theremaybeaninterveningsegmentcthat
intersectsbothaandbatpointp.Thatis,wemayhavea<w candc<w bforallsweeplineswto
theleftofpforwhicha<w b.Exercise33.2-8asksyoutoshowthatANY-SEGMENTS-INTERSECT
iscorrectevenifthreesegmentsdointersectatthesamepoint.
33.2 Determiningwhetheranypairofsegmentsintersects 1027
is the event point at which a and b become consecutive in the total preorder. If p
is on sweep line ´, then q p. If p is not on sweep line ´, then q is to the left
D
of p. In either case, the order given by T is correct just before encountering q.
(Here is where we use the lexicographic order in which the algorithm processes
eventpoints. Because p isthelowestoftheleftmost intersection points, evenifp
isonsweepline´andsomeother intersection point p ison´,eventpoint q p
0
D
isprocessedbeforetheotherintersectionp caninterferewiththetotalpreorderT.
0
Moreover, even if p is the left endpoint of one segment, say a, and the right end-
point of the other segment, say b, because left endpoint events occur before right
endpointevents,segmentbisinT uponfirstencounteringsegmenta.) Eitherevent
pointq isprocessed by ANY-SEGMENTS-INTERSECT oritisnotprocessed.
If q is processed by ANY-SEGMENTS-INTERSECT, only two possible actions
mayoccur:
1. Either a or b is inserted into T, and the other segment is above or below it in
thetotalpreorder. Lines4–7detectthiscase.
2. Segments a and b are already in T, and a segment between them in the total
preorderisdeleted,makingaandbbecomeconsecutive. Lines8–11detectthis
case.
Ineithercase,wefindtheintersectionpandANY-SEGMENTS-INTERSECT returns
TRUE.
If event point q is not processed by ANY-SEGMENTS-INTERSECT, the proce-
dure must have returned before processing all event points. This situation could
have occurred only if ANY-SEGMENTS-INTERSECT had already found an inter-
sectionandreturned TRUE.
Thus, if there is an intersection, ANY-SEGMENTS-INTERSECT returns TRUE.
As we have already seen, if ANY-SEGMENTS-INTERSECT returns TRUE, there is
anintersection. Therefore, ANY-SEGMENTS-INTERSECT alwaysreturnsacorrect
answer.
Runningtime
If set S contains n segments, then ANY-SEGMENTS-INTERSECT runs in time
O.nlgn/. Line 1 takes O.1/ time. Line 2 takes O.nlgn/ time, using merge
sort or heapsort. The for loop of lines 3–11 iterates at most once per event point,
andsowith2neventpoints,theloopiteratesatmost2ntimes. Eachiterationtakes
O.lgn/time,sinceeachred-black-treeoperationtakesO.lgn/timeand,usingthe
method of Section 33.1, each intersection test takes O.1/ time. The total time is
thusO.nlgn/.
1028 Chapter33 ComputationalGeometry
Exercises
33.2-1
Showthatasetofnlinesegmentsmaycontain‚.n2/intersections.
33.2-2
Given two segments a and b that are comparable at x, show how to determine
in O.1/ time which of a < b or b < a holds. Assume that neither segment
x x
is vertical. (Hint: If a and b do not intersect, you can just use cross products.
If a and b intersect—which you can of course determine using only cross prod-
ucts—you can still use only addition, subtraction, and multiplication, avoiding
division. Of course, in the application of the < relation used here, if a and b
x
intersect, wecanjuststopanddeclarethatwehavefoundanintersection.)
33.2-3
Professor Mason suggests that we modify ANY-SEGMENTS-INTERSECT so that
instead of returning upon finding an intersection, it prints the segments that inter-
sect and continues on to the next iteration of the for loop. The professor calls the
resulting procedure PRINT-INTERSECTING-SEGMENTS and claims that it prints
all intersections, from left to right, as they occur in the set of line segments. Pro-
fessor Dixon disagrees, claiming that Professor Mason’s idea is incorrect. Which
professor is right? Will PRINT-INTERSECTING-SEGMENTS always find the left-
mostintersection first? Willitalwaysfindalltheintersections?
33.2-4
Give an O.nlgn/-time algorithm to determine whether an n-vertex polygon is
simple.
33.2-5
GiveanO.nlgn/-timealgorithm todetermine whethertwosimplepolygons with
atotalofnverticesintersect.
33.2-6
Adiskconsistsofacircleplusitsinteriorandisrepresentedbyitscenterpointand
radius. Twodisks intersect if they have any point in common. Give an O.nlgn/-
timealgorithm todetermine whetheranytwodisksinasetofnintersect.
33.2-7
Given a set of n line segments containing a total of k intersections, show how to
outputallk intersections inO..n k/lgn/time.
C
33.3 Findingtheconvexhull 1029
33.2-8
Argue that ANY-SEGMENTS-INTERSECT works correctly even if three or more
segmentsintersect atthesamepoint.
33.2-9
Showthat ANY-SEGMENTS-INTERSECT works correctly inthe presence ofverti-
cal segments if we treat the bottom endpoint of a vertical segment as if it were a
left endpoint and the top endpoint as if it were a right endpoint. How does your
answertoExercise33.2-2changeifweallowverticalsegments?
33.3 Finding the convex hull
The convex hull of a set Q of points, denoted by CH.Q/, is the smallest convex
polygon P for which each point in Q is either on the boundary of P or in its
interior. (See Exercise33.1-5 for a precise definition of a convex polygon.) We
implicitly assume that all points in the set Q are unique and that Q contains at
least three points which are not colinear. Intuitively, we can think of each point
in Q as being a nail sticking out from a board. The convex hull is then the shape
formedbyatightrubber bandthatsurrounds allthenails. Figure33.6showsaset
ofpointsanditsconvexhull.
In this section, we shall present two algorithms that compute the convex hull
of a set of n points. Both algorithms output the vertices of the convex hull in
counterclockwiseorder. Thefirst,knownasGraham’sscan,runsinO.nlgn/time.
The second, called Jarvis’s march, runs in O.nh/ time, where h is the number of
vertices oftheconvex hull. AsFigure 33.6illustrates, every vertex ofCH.Q/isa
p
10
p
p 11 p 9 p 7 6 p 5
p p
8 3
p 12 p 4
p
2
p
1
p
0
Figure33.6 AsetofpointsQ p0;p1;:::;p12 withitsconvexhullCH.Q/ingray.
Df g
1030 Chapter33 ComputationalGeometry
point inQ. Bothalgorithms exploit thisproperty, deciding whichvertices inQ to
keepasverticesoftheconvexhullandwhichverticesinQtoreject.
Wecan compute convex hulls inO.nlgn/timebyanyoneofseveral methods.
BothGraham’sscanandJarvis’s marchuseatechnique called “rotational sweep,”
processing vertices in the order of the polar angles they form with a reference
vertex. Othermethodsinclude thefollowing:
 Intheincrementalmethod,wefirstsortthepointsfromlefttoright,yieldinga
sequence p ;p ;:::;p . At the ith stage, we update the convex hull of the
1 2 n
h i
i 1 leftmost points, CH. p ;p ;:::;p /, according to the ith point from
1 2 i 1
th(cid:0)
e left, thus forming
CH.f
p ;p ;:::;p
(cid:0)/.g
Exercise 33.3-6 asks you how to
1 2 i
f g
implementthismethodtotakeatotalofO.nlgn/time.
 In the divide-and-conquer method, wedivide the set of n points in ‚.n/time
into two subsets, one containing the leftmost n=2 points and one containing
d e
therightmost n=2 points,recursivelycomputetheconvexhullsofthesubsets,
b c
and then, by means of a clever method, combine the hulls in O.n/ time. The
runningtimeisdescribedbythefamiliarrecurrence T.n/ 2T.n=2/ O.n/,
D C
andsothedivide-and-conquer methodrunsinO.nlgn/time.
 Theprune-and-search method is similar tothe worst-case linear-time median
algorithmofSection9.3. Withthismethod,wefindtheupperportion(or“upper
chain”)oftheconvexhullbyrepeatedly throwingoutaconstantfractionofthe
remainingpointsuntilonlytheupperchainoftheconvexhullremains. Wethen
do the same for the lower chain. This method is asymptotically the fastest: if
theconvexhullcontainshvertices,itrunsinonlyO.nlgh/time.
Computingtheconvexhullofasetofpointsisaninterestingprobleminitsown
right. Moreover,algorithmsforsomeothercomputational-geometryproblemsstart
bycomputingaconvexhull. Consider,forexample,thetwo-dimensional farthest-
pair problem: we are given a set of n points in the plane and wish to find the
two points whose distance from each other is maximum. AsExercise 33.3-3 asks
you to prove, these two points must be vertices of the convex hull. Although we
won’t prove ithere, wecan findthefarthest pairof vertices of ann-vertex convex
polygon in O.n/ time. Thus, by computing the convex hull of the n input points
inO.nlgn/timeandthenfindingthefarthestpairoftheresultingconvex-polygon
vertices, we can find the farthest pair of points in any set of n points in O.nlgn/
time.
Graham’sscan
Graham’s scan solves the convex-hull problem by maintaining a stack S of can-
didate points. It pushes each point of the input set Q onto the stack one time,
33.3 Findingtheconvexhull 1031
and it eventually pops from the stack each point that is not a vertex of CH.Q/.
Whenthealgorithmterminates,stackS containsexactlytheverticesofCH.Q/,in
counterclockwise orderoftheirappearance ontheboundary.
TheprocedureGRAHAM-SCANtakesasinputasetQofpoints,where Q 3.
j j 
It calls the functions TOP.S/, which returns the point on top of stack S without
changing S, and NEXT-TO-TOP.S/, which returns the point one entry below the
top of stack S without changing S. As we shall prove in a moment, the stack S
returned by GRAHAM-SCAN contains, from bottom to top, exactly the vertices
ofCH.Q/incounterclockwise order.
GRAHAM-SCAN.Q/
1 letp bethepointinQwiththeminimumy-coordinate,
0
ortheleftmostsuchpointincaseofatie
2 let p ;p ;:::;p betheremainingpointsinQ,
1 2 m
h i
sortedbypolarangleincounterclockwise orderaroundp
0
(ifmorethanonepointhasthesameangle,removeallbut
theonethatisfarthestfromp )
0
3 ifm < 2
4 return“convexhullisempty”
5 elseletS beanemptystack
6 PUSH.p 0;S/
7 PUSH.p 1;S/
8 PUSH.p 2;S/
9 fori 3tom
D
10 whiletheangleformedbypoints NEXT-TO-TOP.S/, TOP.S/,
andp makesanonleftturn
i
11 POP.S/
12 PUSH.p i;S/
13 returnS
Figure 33.7 illustrates the progress of GRAHAM-SCAN. Line 1 chooses point p
0
as the point with the lowest y-coordinate, picking the leftmost such point in case
of a tie. Since there is no point in Q that is below p and any other points with
0
the same y-coordinate are to its right, p must be a vertex of CH.Q/. Line 2
0
sorts the remaining points of Q by polar angle relative to p , using the same
0
method—comparing cross products—as inExercise 33.1-3. Iftwoormorepoints
have the samepolar angle relative top , all but the farthest such point areconvex
0
combinations of p and the farthest point, and so we remove them entirely from
0
consideration. We let m denote the number of points other than p that remain.
0
The polar angle, measured in radians, of each point in Q relative to p is in the
0
half-open intervalŒ0;/. Since the points are sorted according to polar angles,
they are sorted in counterclockwise order relative to p . We designate this sorted
0
1032 Chapter33 ComputationalGeometry
p p
10 10
p p
p 11 p 9 p 7 6 p 11 p 9 p 7 6
p 8 p 5 p 8 p 5
p p p p
4 3 4 3
p p
12 12
p p
2 2
p p
1 1
p p
0 (a) 0 (b)
p p
10 10
p p
p 11 p 9 p 7 6 p 11 p 9 p 7 6 p
p 8 p 5 p 8 5
p p
p 12 p 4 3 p 12 p 4 3
p p
2 2
p p
1 1
p p
0 (c) 0 (d)
p p
10 10
p p
p 11 p 9 p 7 6 p 11 p 9 p 7 6
p 8 p 5 p 8 p 5
p p p p
4 3 4 3
p p
12 12
p p
2 2
p p
1 1
p p
0 (e) 0 (f)
Figure33.7 Theexecutionof GRAHAM-SCANonthesetQofFigure33.6. Thecurrentconvex
hullcontainedinstackSisshowningrayateachstep.(a)Thesequence p1;p2;:::;p12 ofpoints
h i
numberedinorderofincreasingpolaranglerelativetop0,andtheinitialstackS containingp0,p1,
andp2.(b)–(k)StackS aftereachiterationoftheforloopoflines9–12.Dashedlinesshownonleft
turns, which cause points tobe popped from thestack. In part (h), for example, the right turn at
angle p7p8p9causesp8tobepopped,andthentherightturnatangle p6p7p9causesp7tobe
† †
popped.
33.3 Findingtheconvexhull 1033
p p
10 10
p p
p 11 p 9 p 7 6 p 11 p 9 6
p p p
5 7 5
p p p p p p
8 4 3 8 4 3
p p
12 12
p p
2 2
p p
1 1
p p
0 (g) 0 (h)
p p
10 10
p
p 11 p 9 p 7 p 6 p 5 p 11 p 9 p 8 p 7 6 p 5
p p p p p
8 4 3 4 3
p p
12 12
p p
2 2
p p
1 1
p p
0 (i) 0 (j)
p p
10 10
p p
p p 6 p p 6
9 7 9 7
p 11 p 8 p 5 p 11 p 8 p 5
p p p p
4 3 4 3
p p
12 12
p p
2 2
p p
1 1
p p
0 (k) 0 (l)
Figure 33.7, continued (l) The convex hull returned by the procedure, which matches that of
Figure33.6.
1034 Chapter33 ComputationalGeometry
sequence of points by p ;p ;:::;p . Note that points p and p are vertices
1 2 m 1 m
h i
of CH.Q/ (see Exercise 33.3-1). Figure 33.7(a) shows the points of Figure 33.6
sequentially numbered inorderofincreasing polaranglerelativetop .
0
The remainder of the procedure uses the stack S. Lines 5–8 initialize the stack
tocontain, frombottomtotop,thefirstthreepointsp ,p ,andp . Figure33.7(a)
0 1 2
shows the initial stack S. The for loop of lines 9–12 iterates once for each point
in the subsequence p ;p ;:::;p . We shall see that after processing point p ,
3 4 m i
h i
stackS contains,frombottomtotop,theverticesofCH. p ;p ;:::;p /incoun-
0 1 i
f g
terclockwise order. The while loop of lines 10–11 removes points from the stack
ifwefindthemnottobeverticesoftheconvexhull. Whenwetraversetheconvex
hull counterclockwise, weshould make aleftturn ateach vertex. Thus, eachtime
the while loop finds a vertex at which we make a nonleft turn, we pop the vertex
from the stack. (By checking for a nonleft turn, rather than just a right turn, this
test precludes the possibility of astraight angle atavertex ofthe resulting convex
hull. We want no straight angles, since no vertex of a convex polygon may be a
convex combination of other vertices of the polygon.) After we pop all vertices
that have nonleft turns when heading toward point p , we push p onto the stack.
i i
Figures 33.7(b)–(k) show the state of the stack S after each iteration of the for
loop. Finally, GRAHAM-SCAN returnsthestackS inline13. Figure33.7(l)shows
thecorresponding convexhull.
Thefollowingtheorem formallyprovesthecorrectness ofGRAHAM-SCAN.
Theorem33.1(Correctness ofGraham’sscan)
IfGRAHAM-SCAN executesonasetQofpoints, where Q 3,thenattermina-
j j 
tion, thestack S consists of,from bottom totop, exactly thevertices ofCH.Q/in
counterclockwise order.
Proof After line 2, we have the sequence of points p ; p ; :::; p . Let us
1 2 m
h i
define, for i 2;3;:::;m, the subset of points Q p ;p ;:::;p . The
i 0 1 i
D D f g
points in Q Q are those that were removed because they had the same polar
m
(cid:0)
angle relative to p as some point in Q ; these points are not in CH.Q/, and
0 m
so CH.Q m/ CH.Q/. Thus, it suffices to show that when GRAHAM-SCAN
D
terminates, the stack S consists of the vertices of CH.Q / in counterclockwise
m
order,whenlistedfrombottomtotop. Notethatjustasp ,p ,andp arevertices
0 1 m
ofCH.Q/,thepointsp ,p ,andp areallverticesofCH.Q /.
0 1 i i
Theproofusesthefollowingloopinvariant:
Atthestartofeachiterationoftheforloopoflines9–12,stackS consistsof,
from bottom to top, exactly the vertices of CH.Q / in counterclockwise
i 1
(cid:0)
order.
Initialization: The invariant holds the first time we execute line 9, since at that
time,stackS consistsofexactlytheverticesofQ Q ,andthissetofthree
2 i 1
D (cid:0)
33.3 Findingtheconvexhull 1035
p p
j p j
k
p p p
i i r
p
t
Q
j
p
2
p
1
p
1
p p
0 0
(a) (b)
Figure33.8 Theproof of correctnessof GRAHAM-SCAN. (a) Becausep i’spolar anglerelative
top0isgreaterthanp j’spolarangle,andbecausetheangle p kp jp i makesaleftturn,addingp i
†
toCH.Q j/givesexactlytheverticesofCH.Q j p i /.(b)Iftheangle prptp i makesanonleft
[f g †
turn, thenpt iseither intheinterior of thetriangleformed byp0, pr, andp i or on asideof the
triangle,whichmeansthatitcannotbeavertexofCH.Q i/.
verticesformsitsownconvexhull. Moreover, theyappearincounterclockwise
orderfrombottom totop.
Maintenance: Entering an iteration of the for loop, the top point on stack S
is p , which was pushed at the end of the previous iteration (or before the
i 1
first i(cid:0) teration, when i 3). Let p be the top point on S after executing the
j
D
while loop oflines 10–11 but before line12 pushes p ,and letp bethe point
i k
just below p on S. At the moment that p is the top point on S and we have
j j
not yet pushed p , stack S contains exactly the same points it contained after
i
iteration j oftheforloop. Bytheloop invariant, therefore, S contains exactly
the vertices of CH.Q / at that moment, and they appear in counterclockwise
j
orderfrombottom totop.
Let us continue to focus on this moment just before pushing p . We know
i
that p ’s polar angle relative to p is greater than p ’s polar angle and that
i 0 j
the angle p p p makes a left turn (otherwise we would have popped p ).
k j i j
†
Therefore, because S contains exactly the vertices of CH.Q /, we see from
j
Figure 33.8(a) that once we push p , stack S will contain exactly the vertices
i
ofCH.Q p /,stillincounterclockwise orderfrombottomtotop.
j i
[f g
WenowshowthatCH.Q p /isthesamesetofpointsasCH.Q /. Consider
j i i
[f g
any point p that was popped during iteration i of the for loop, and let p be
t r
thepointjustbelowp onstackS atthetimep waspopped(p mightbep ).
t t r j
The angle p p p makes a nonleft turn, and the polar angle of p relative
r t i t
†
to p is greater than the polar angle of p . As Figure 33.8(b) shows, p must
0 r t
1036 Chapter33 ComputationalGeometry
beeither intheinterior ofthetriangle formedbyp ,p ,andp oronasideof
0 r i
this triangle (but itis not avertex ofthe triangle). Clearly, since p is within a
t
triangle formed by three other points of Q , it cannot be a vertex of CH.Q /.
i i
Sincep isnotavertexofCH.Q /,wehavethat
t i
CH.Q p / CH.Q /: (33.1)
i t i
(cid:0)f g D
Let P be the set of points that were popped during iteration i of the for loop.
i
Sincetheequality(33.1)appliesforallpointsinP ,wecanapplyitrepeatedly
i
toshow that CH.Q P / CH.Q /. ButQ P Q p , and sowe
i i i i i j i
(cid:0) D (cid:0) D [f g
conclude thatCH.Q p / CH.Q P / CH.Q /.
j i i i i
[f g D (cid:0) D
We have shown that once we push p , stack S contains exactly the vertices
i
ofCH.Q / incounterclockwise order from bottom totop. Incrementing i will
i
thencausetheloopinvariant toholdforthenextiteration.
Termination: When the loop terminates, we have i m 1, and so the loop
D C
invariantimpliesthatstackS consistsofexactlytheverticesofCH.Q /,which
m
is CH.Q/, in counterclockwise order from bottom to top. This completes the
proof.
We now show that the running time of GRAHAM-SCAN is O.nlgn/, where
n Q . Line 1 takes ‚.n/ time. Line 2 takes O.nlgn/ time, using merge sort
D j j
or heapsort to sort the polar angles and the cross-product method of Section 33.1
to compare angles. (We can remove all but the farthest point with the same polar
angle intotal ofO.n/ timeoverall npoints.) Lines5–8 take O.1/ time. Because
m n 1, the for loop of lines 9–12 executes at most n 3 times. Since PUSH
 (cid:0) (cid:0)
takes O.1/ time, each iteration takes O.1/ time exclusive of the time spent in the
whileloopoflines10–11, andthusoveralltheforlooptakesO.n/timeexclusive
ofthenestedwhileloop.
Weuse aggregate analysis toshow that thewhileloop takes O.n/ time overall.
For i 0;1;:::;m, we push each point p onto stack S exactly once. As in the
i
D
analysisoftheMULTIPOPprocedureofSection17.1,weobservethatwecanpopat
mostthenumberofitemsthatwepush. Atleastthreepoints—p ,p ,andp —are
0 1 m
never popped from the stack, so that in fact at most m 2 POP operations are
(cid:0)
performed in total. Each iteration of the while loop performs one POP, and so
there are at most m 2 iterations of the while loop altogether. Since the test in
(cid:0)
line10takesO.1/time,eachcallofPOPtakesO.1/time,andm n 1,thetotal
 (cid:0)
timetaken bythewhileloop isO.n/. Thus, the running timeof GRAHAM-SCAN
isO.nlgn/.
33.3 Findingtheconvexhull 1037
left chain right chain
p
3
p p
4 2
p
1
p
0
left chain right chain
Figure33.9 Theoperation of Jarvis’smarch. Wechoose thefirstvertexasthelowest point p0.
Thenextvertex,p1,hasthesmallestpolarangleofanypointwithrespecttop0. Then,p2hasthe
smallestpolaranglewithrespecttop1. Therightchaingoesashighasthehighestpointp3. Then,
weconstructtheleftchainbyfindingsmallestpolarangleswithrespecttothenegativex-axis.
Jarvis’smarch
Jarvis’smarchcomputestheconvexhullofasetQofpointsbyatechniqueknown
as package wrapping (or gift wrapping). The algorithm runs in time O.nh/,
wherehisthe number ofvertices ofCH.Q/. Whenhiso.lgn/,Jarvis’s marchis
asymptotically fasterthanGraham’sscan.
Intuitively, Jarvis’s march simulates wrapping a taut piece of paper around the
setQ. Westartbytapingtheendofthepapertothelowestpointintheset,thatis,
tothe same point p withwhich westart Graham’s scan. Weknow that this point
0
mustbeavertex of theconvex hull. Wepull the paper to theright tomakeittaut,
andthenwepullithigher untilittouches apoint. Thispointmustalsobeavertex
oftheconvex hull. Keepingthepaper taut, wecontinue inthis wayaround the set
ofverticesuntilwecomebacktoouroriginalpointp .
0
Moreformally,Jarvis’smarchbuildsasequenceH p ;p ;:::;p ofthe
0 1 h 1
vertices of CH.Q/. We start with p . As Figure 33.9D shh ows, the next (cid:0) vei rtex p
0 1
intheconvex hull has thesmallest polar angle with respect to p . (Incase of ties,
0
we choose the point farthest from p .) Similarly, p has the smallest polar angle
0 2
1038 Chapter33 ComputationalGeometry
withrespecttop ,andsoon. Whenwereachthehighestvertex,sayp (breaking
1 k
ties by choosing the farthest such vertex), we have constructed, as Figure 33.9
shows, the right chain of CH.Q/. To construct the left chain, we start at p and
k
choosep asthepointwiththesmallestpolaranglewithrespecttop ,butfrom
k 1 k
thenegativC e x-axis. Wecontinue on,forming theleftchain bytaking polar angles
fromthenegativex-axis,untilwecomebacktoouroriginalvertexp .
0
WecouldimplementJarvis’smarchinoneconceptual sweeparoundtheconvex
hull, that is, without separately constructing theright and leftchains. Suchimple-
mentationstypicallykeeptrackoftheangleofthelastconvex-hullsidechosenand
require the sequence of angles of hull sides to be strictly increasing (in the range
of0to2 radians). Theadvantage ofconstructing separate chainsisthatweneed
not explicitly compute angles; the techniques of Section 33.1 suffice to compare
angles.
Ifimplemented properly, Jarvis’s marchhasarunning timeofO.nh/. Foreach
ofthehverticesofCH.Q/,wefindthevertexwiththeminimumpolarangle. Each
comparison between polar angles takes O.1/ time, using the techniques of Sec-
tion33.1. AsSection9.1shows,wecancomputetheminimumofnvaluesinO.n/
timeifeachcomparison takesO.1/time. Thus,Jarvis’s marchtakesO.nh/time.
Exercises
33.3-1
Prove that in the procedure GRAHAM-SCAN, points p
1
and p
m
must be vertices
ofCH.Q/.
33.3-2
Consideramodelofcomputationthatsupportsaddition,comparison,andmultipli-
cation andforwhichthere isalowerbound of.nlgn/tosortnnumbers. Prove
that .nlgn/ isalowerbound forcomputing, inorder, the vertices ofthe convex
hullofasetofnpointsinsuchamodel.
33.3-3
GivenasetofpointsQ,provethatthepairofpointsfarthestfromeachothermust
beverticesofCH.Q/.
33.3-4
For a given polygon P and a point q on its boundary, the shadow of q is the set
of points r such that the segment qr is entirely on the boundary or in the interior
of P. As Figure 33.10 illustrates, a polygon P is star-shaped if there exists a
point p in the interior of P that is in the shadow of every point on the boundary
of P. The set of all such points p is called the kernel of P. Given an n-vertex,
33.4 Findingtheclosestpairofpoints 1039
q¢
p
q
(a) (b)
Figure33.10 Thedefinitionofastar-shapedpolygon,foruseinExercise33.3-4.(a)Astar-shaped
polygon.Thesegmentfrompointptoanypointqontheboundaryintersectstheboundaryonlyatq.
(b)Anon-star-shaped polygon. Theshaded regionontheleftistheshadow of q,andtheshaded
regionontherightistheshadowofq 0.Sincetheseregionsaredisjoint,thekernelisempty.
star-shaped polygon P specified by its vertices in counterclockwise order, show
howtocomputeCH.P/inO.n/time.
33.3-5
Intheon-lineconvex-hullproblem,wearegiventhesetQofnpointsonepointat
atime. After receiving each point, wecompute the convex hull ofthe points seen
so far. Obviously, we could run Graham’s scan once for each point, with a total
runningtimeofO.n2lgn/. Showhowtosolvetheon-lineconvex-hull problemin
atotalofO.n2/time.
33.3-6 ?
Show how to implement the incremental method for computing the convex hull
ofnpointssothatitrunsinO.nlgn/time.
33.4 Finding the closestpairofpoints
We now consider the problem of finding the closest pair of points in a set Q of
n 2points. “Closest”referstotheusualeuclideandistance: thedistancebetween

points p .x ;y /andp .x ;y /is .x x /2 .y y /2. Twopoints
1 1 1 2 2 2 1 2 1 2
D D (cid:0) C (cid:0)
insetQmaybecoincident, inwhichcasethedistance betweenthemiszero. This
p
problem has applications in, for example, traffic-control systems. A system for
controllingairorseatrafficmightneedtoidentifythetwoclosestvehiclesinorder
todetectpotential collisions.
A brute-force closest-pair algorithm simply looks at all the n ‚.n2/ pairs
2 D
of points. In this section, we shall describe a divide-and-conquer algorithm for
(cid:0) 
1040 Chapter33 ComputationalGeometry
this problem, whose running time is described by the familiar recurrence T.n/
D
2T.n=2/ O.n/. Thus,thisalgorithm usesonlyO.nlgn/time.
C
Thedivide-and-conqueralgorithm
Each recursive invocation of the algorithm takes as input a subset P Q and

arrays X and Y, each of which contains all the points of the input subset P.
The points in array X are sorted so that their x-coordinates are monotonically
increasing. Similarly, arrayY issortedbymonotonically increasing y-coordinate.
Note that in order to attain the O.nlgn/ time bound, we cannot afford to sort
in each recursive call; if we did, the recurrence for the running time would be
T.n/ 2T.n=2/ O.nlgn/, whose solution is T.n/ O.nlg2n/. (Use the
D C D
version of the master method given in Exercise 4.6-2.) We shall see a little later
howtouse“presorting”tomaintainthissortedpropertywithoutactuallysortingin
eachrecursivecall.
A given recursive invocation with inputs P, X, and Y first checks whether
P 3. If so, the invocation simply performs the brute-force method described
j aboj v e: try all jP 2j pairs of points and return the closest pair. If jP
j
> 3, the
recursive invocation carriesoutthedivide-and-conquer paradigm asfollows.
(cid:0) 
Divide: Findavertical linel thatbisects thepoint setP intotwosets P and P
L R
such that P P =2 , P P =2 , all points in P are on or to the
L R L
j j D dj j e j j D bj j c
leftoflinel,andallpointsinP areonortotherightofl. DividethearrayX
R
into arrays X and X , which contain the points of P and P respectively,
L R L R
sortedbymonotonically increasing x-coordinate. Similarly,dividethearrayY
into arrays Y and Y , which contain the points of P and P respectively,
L R L R
sortedbymonotonically increasing y-coordinate.
Conquer: HavingdividedP intoP andP ,maketworecursivecalls,onetofind
L R
the closest pair of points in P and the other to find the closest pair of points
L
in P . Theinputs to the firstcall are the subset P and arrays X and Y ; the
R L L L
second call receives the inputs P , X , and Y . Let the closest-pair distances
R R R
returnedforP andP beı andı ,respectively, andletı min.ı ;ı /.
L R L R L R
D
Combine: The closest pair is either the pair with distance ı found by one of the
recursivecalls,oritisapairofpointswithonepointinP andtheotherinP .
L R
Thealgorithm determines whether there is a pair with one point in P and the
L
other point in P and whose distance is less than ı. Observe that if a pair of
R
points has distance less than ı, both points of the pair must be within ı units
oflinel. Thus,asFigure33.11(a) shows,theybothmustresideinthe2ı-wide
vertical strip centered at line l. To find such a pair, if one exists, we do the
following:
33.4 Findingtheclosestpairofpoints 1041
1. Create an array Y , which is the array Y with all points not in the 2ı-wide
0
verticalstripremoved. ThearrayY issortedbyy-coordinate, justasY is.
0
2. For each point p in the array Y , try to find points in Y that are within ı
0 0
unitsofp. Asweshallseeshortly,onlythe7pointsinY thatfollowpneed
0
be considered. Compute the distance from p to each of these 7 points, and
keeptrackoftheclosest-pair distanceı foundoverallpairsofpointsinY .
0 0
3. If ı < ı, then the vertical strip does indeed contain a closer pair than the
0
recursive calls found. Return this pair and its distance ı. Otherwise, return
0
theclosestpairanditsdistanceı foundbytherecursivecalls.
The above description omits some implementation details that are necessary to
achievetheO.nlgn/runningtime. Afterprovingthecorrectnessofthealgorithm,
weshallshowhowtoimplementthealgorithm toachievethedesired timebound.
Correctness
The correctness of this closest-pair algorithm is obvious, except for two aspects.
First,bybottomingouttherecursionwhen P 3,weensurethatwenevertryto
j j
solveasubproblemconsistingofonlyonepoint. Thesecondaspectisthatweneed
onlycheckthe7pointsfollowingeachpointpinarrayY ;weshallnowprovethis
0
property.
Supposethatatsomeleveloftherecursion, theclosestpairofpointsisp P
L L
2
and p P . Thus, the distance ı between p and p is strictly less than ı.
R R 0 L R
2
Pointp mustbeonortotheleftoflinel andlessthanıunitsaway. Similarly,p
L R
is on or to the right of l and less than ı units away. Moreover, p and p are
L R
withinı unitsofeachothervertically. Thus,asFigure33.11(a)shows,p andp
L R
arewithinaı 2ı rectangle centered atlinel. (Theremaybeother points within

thisrectangle aswell.)
Wenextshowthatatmost8pointsofP canresidewithinthisı 2ı rectangle.

Consider the ı ı square forming the left half of this rectangle. Since all points

within P are atleast ı units apart, at most 4points can reside within this square;
L
Figure 33.11(b) shows how. Similarly, at most 4 points in P can reside within
R
theı ısquareformingtherighthalfoftherectangle. Thus,atmost8pointsofP

can reside within the ı 2ı rectangle. (Note that since points on line l maybe in

eitherP orP ,theremaybeupto4pointsonl. Thislimitisachievedifthereare
L R
twopairsofcoincidentpointssuchthateachpairconsistsofonepointfromP and
L
onepoint from P ,onepair isattheintersection ofl and thetopoftherectangle,
R
andtheotherpairiswherel intersects thebottom oftherectangle.)
Having shown that at most 8 points of P can reside within the rectangle, we
caneasily seewhyweneed to check only the 7points following each point inthe
array Y . Still assuming that the closest pair is p and p , let us assume without
0 L R
1042 Chapter33 ComputationalGeometry
P
R
P
L
P
2d R
P
L
d d
d p p R coincident points,
L
one in P ,
L
d one in P R
coincident points,
one in P ,
L
l l one in P
R
(a) (b)
Figure33.11 Keyconceptsintheproofthattheclosest-pairalgorithmneedstocheckonly7points
followingeachpointinthearrayY 0. (a)Ifp L 2P Landp R 2P R arelessthanıunitsapart,they
mustresidewithinaı 2ırectanglecenteredatlinel.(b)How4pointsthatarepairwiseatleastı

unitsapartcanallresidewithinaı ısquare. Ontheleftare4pointsinP L,andontherightare4

pointsinP R. Theı 2ı rectanglecancontain8pointsifthepointsshownonlinel areactually

pairsofcoincidentpointswithonepointinP LandoneinP R.
lossofgeneralitythatp precedesp inarrayY . Then,evenifp occursasearly
L R 0 L
aspossible inY and p occurs aslate aspossible, p isinone ofthe7positions
0 R R
followingp . Thus,wehaveshownthecorrectness oftheclosest-pair algorithm.
L
Implementationandrunningtime
Aswehavenoted,ourgoalistohavetherecurrencefortherunningtimebeT.n/
D
2T.n=2/ O.n/, where T.n/ isthe running time for a set of n points. Themain
C
difficulty comes from ensuring that the arrays X , X , Y , and Y , which are
L R L R
passed to recursive calls, are sorted by the proper coordinate and also that the
array Y is sorted by y-coordinate. (Note that if the array X that is received by a
0
recursive callisalready sorted, then wecan easily dividesetP intoP andP in
L R
lineartime.)
The key observation is that in each call, we wish to form a sorted subset of a
sorted array. For example, a particular invocation receives the subset P and the
arrayY,sortedbyy-coordinate. Havingpartitioned P intoP andP ,itneedsto
L R
form the arrays Y and Y , which are sorted by y-coordinate, in linear time. We
L R
can view the method as the opposite of the MERGE procedure from merge sort in
33.4 Findingtheclosestpairofpoints 1043
Section2.3.1: wearesplitting asortedarrayintotwosortedarrays. Thefollowing
pseudocode givestheidea.
1 letY Œ1::Y:lengthandY Œ1::Y:lengthbenewarrays
L R
2 Y :length Y :length 0
L R
D D
3 fori 1toY:length
D
4 ifYŒi P
L
2
5 Y :length Y :length 1
L L
D C
6 Y ŒY :length YŒi
L L
D
7 elseY :length Y :length 1
R R
D C
8 Y ŒY :length YŒi
R R
D
We simply examine the points in array Y in order. If a point YŒi is in P , we
L
append it to the end of array Y ; otherwise, we append it to the end of array Y .
L R
Similarpseudocode worksforformingarraysX ,X ,andY .
L R 0
Theonlyremainingquestionishowtogetthepointssortedinthefirstplace. We
presort them; that is, we sort them once and for all before the first recursive call.
Wepass these sorted arrays into the first recursive call, and from there wewhittle
them down through the recursive calls asnecessary. Presorting adds an additional
O.nlgn/termtotherunning time,butnoweachstepoftherecursion takes linear
time exclusive of the recursive calls. Thus, if we let T.n/ be the running time of
each recursive step and T .n/ be the running time of the entire algorithm, we get
0
T .n/ T.n/ O.nlgn/and
0
D C
2T.n=2/ O.n/ ifn > 3;
T.n/ C
D
(
O.1/ ifn 3:

Thus,T.n/ O.nlgn/andT .n/ O.nlgn/.
0
D D
Exercises
33.4-1
Professor Williams comesupwithascheme thatallowstheclosest-pair algorithm
tocheckonly5pointsfollowingeachpointinarrayY . Theideaisalwaystoplace
0
points on line l into set P . Then, there cannot be pairs of coincident points on
L
line l with one point in P and one in P . Thus, at most 6 points can reside in
L R
theı 2ı rectangle. Whatistheflawintheprofessor’s scheme?

33.4-2
Show that it actually suffices to check only the points in the 5 array positions fol-
lowingeachpointinthearrayY .
0
1044 Chapter33 ComputationalGeometry
33.4-3
We can define the distance between two points in ways other than euclidean. In
the plane, the L m-distance between points p
1
and p
2
is given by the expres-
sion . x x m y y m/1=m . Euclidean distance, therefore, is L -distance.
1 2 1 2 2
j (cid:0) j Cj (cid:0) j
Modify the closest-pair algorithm to use the L -distance, which is also known as
1
theManhattandistance.
33.4-4
Given two points p and p in the plane, the L -distance between them is
1 2
given by max. x x ; y y /. Modify the clos1 est-pair algorithm to use the
1 2 1 2
j (cid:0) j j (cid:0) j
L -distance.
1
33.4-5
Suppose that .n/ of the points given to the closest-pair algorithm are covertical.
Show how to determine the sets P and P and how to determine whether each
L R
point of Y is in P or P so that the running time for the closest-pair algorithm
L R
remainsO.nlgn/.
33.4-6
Suggest a change to the closest-pair algorithm that avoids presorting the Y array
butleaves therunning timeasO.nlgn/. (Hint:Mergesorted arrays Y andY to
L R
formthesortedarrayY.)
Problems
33-1 Convexlayers
GivenasetQofpointsintheplane,wedefinetheconvexlayersofQinductively.
ThefirstconvexlayerofQconsistsofthosepointsinQthatareverticesofCH.Q/.
Fori > 1, define Q toconsist ofthe points of Q with allpoints inconvex layers
i
1;2;:::;i 1removed. Then,theithconvexlayerofQisCH.Q /ifQ and
i i
(cid:0) ¤;
isundefinedotherwise.
a. GiveanO.n2/-timealgorithm tofindtheconvexlayersofasetofnpoints.
b. Provethat.nlgn/timeisrequiredtocomputetheconvexlayersofasetofn
pointswithanymodelofcomputationthatrequires.nlgn/timetosortnreal
numbers.
ProblemsforChapter33 1045
33-2 Maximallayers
Let Q be a set of n points in the plane. We say that point .x;y/ dominates
point .x ;y / if x x and y y . A point in Q that is dominated by no other
0 0 0 0
 
pointsinQissaidtobemaximal. NotethatQmaycontainmanymaximalpoints,
whichcanbeorganizedintomaximallayersasfollows. ThefirstmaximallayerL
1
isthesetofmaximalpointsofQ. Fori > 1,theithmaximallayerL isthesetof
i
maximalpointsinQ
(cid:0)
ji (cid:0)1 1L j.
SupposethatQ hask noDnempty maximallayers, andlety bethey-coordinate
i
S
oftheleftmostpointinL fori 1;2;:::;k. Fornow,assumethatnotwopoints
i
D
inQhavethesamex-ory-coordinate.
a. Showthaty > y > > y .
1 2 k

Consider a point .x;y/ that is to the left of any point in Q and for which y is
distinctfromthey-coordinate ofanypointinQ. LetQ Q .x;y/ .
0
D [f g
b. Let j be the minimum index such that y < y, unless y < y , in which case
j k
weletj k 1. ShowthatthemaximallayersofQ areasfollows:
0
D C
 Ifj k,thenthemaximallayers ofQ arethesameasthemaximallayers
0

ofQ,exceptthatL alsoincludes .x;y/asitsnewleftmostpoint.
j
 Ifj k 1,thenthefirstkmaximallayersofQ arethesameasforQ,but
0
D C
inaddition, Q hasanonempty .k 1/stmaximallayer: L .x;y/ .
0 k 1
C C D f g
c. DescribeanO.nlgn/-timealgorithmtocomputethemaximallayersofasetQ
ofnpoints. (Hint:Moveasweeplinefromrighttoleft.)
d. Do any difficulties arise if we now allow input points to have the same x- or
y-coordinate? Suggestawaytoresolvesuchproblems.
33-3 Ghostbustersandghosts
Agroup of nGhostbusters isbattling n ghosts. EachGhostbuster carries aproton
pack, which shoots a stream at a ghost, eradicating it. A stream goes in a straight
line and terminates when it hits the ghost. The Ghostbusters decide upon the fol-
lowing strategy. They will pair off with the ghosts, forming n Ghostbuster-ghost
pairs, and then simultaneously each Ghostbuster will shoot a stream at his cho-
sen ghost. As we all know, it is very dangerous to let streams cross, and so the
Ghostbusters mustchoosepairings forwhichnostreamswillcross.
Assume that the position of each Ghostbuster and each ghost is afixed point in
theplaneandthatnothreepositions arecolinear.
a. Argue that there exists a line passing through one Ghostbuster and one ghost
suchthatthenumberofGhostbusters ononesideofthelineequalsthenumber
ofghostsonthesameside. DescribehowtofindsuchalineinO.nlgn/time.
1046 Chapter33 ComputationalGeometry
b. Give an O.n2lgn/-time algorithm to pair Ghostbusters with ghosts in such a
waythatnostreamscross.
33-4 Pickingupsticks
Professor Charon has a set of n sticks, which are piled up in some configuration.
Each stick is specified by its endpoints, and each endpoint is an ordered triple
giving its .x;y;´/ coordinates. No stick is vertical. He wishes to pick up all the
sticks, one at a time, subject to the condition that he may pick up a stick only if
thereisnootherstickontopofit.
a. Giveaprocedure that takes twosticks a and b and reports whether a isabove,
below,orunrelated tob.
b. Describeanefficientalgorithmthatdetermineswhetheritispossibletopickup
allthesticks, andifso,provides alegalorderinwhichtopickthemup.
33-5 Sparse-hulleddistributions
Consider the problem of computing the convex hull of a set of points in the plane
that have been drawn according to some known random distribution. Sometimes,
the number of points, or size, of the convex hull of n points drawn from such a
distribution has expectation O.n1 / for some constant  > 0. We call such a
(cid:0)
distribution sparse-hulled. Sparse-hulled distributions includethefollowing:
 Points drawnuniformly from aunit-radius disk. Theconvex hull hasexpected
size‚.n1=3/.
 Pointsdrawnuniformlyfromtheinteriorofaconvexpolygon withk sides,for
anyconstant k. Theconvexhullhasexpected size‚.lgn/.
 Points drawn according to atwo-dimensional normal distribution. The convex
hullhasexpectedsize‚. lgn/.
a. Giventwo convex polygop ns withn and n vertices respectively, show how to
1 2
computetheconvexhullofalln n pointsinO.n n /time. (Thepolygons
1 2 1 2
C C
mayoverlap.)
b. Showhowtocomputetheconvexhullofasetofnpointsdrawnindependently
according to a sparse-hulled distribution in O.n/ average-case time. (Hint:
Recursively find the convex hulls of the first n=2 points and the second n=2
points, andthencombinetheresults.)
NotesforChapter33 1047
Chapter notes
This chapter barely scratches the surface of computational-geometry algorithms
andtechniques. Booksoncomputational geometryinclude thosebyPreparataand
Shamos[282],Edelsbrunner [99],andO’Rourke[269].
Although geometry has been studied since antiquity, the development of algo-
rithms for geometric problems is relatively new. Preparata and Shamos note that
theearliestnotionofthecomplexityofaproblemwasgivenbyE.Lemoinein1902.
Hewasstudyingeuclideanconstructions—those usingacompassandaruler—and
devised a set of five primitives: placing one leg of the compass on a given point,
placingonelegofthecompassonagivenline,drawingacircle,passingtheruler’s
edge through a given point, and drawing a line. Lemoine was interested in the
number of primitives needed to effect a given construction; he called this amount
the“simplicity” oftheconstruction.
The algorithm of Section 33.2, which determines whether any segments inter-
sect,isduetoShamosandHoey[313].
Theoriginal version ofGraham’sscanisgivenbyGraham[150]. Thepackage-
wrapping algorithm is due to Jarvis [189]. Using a decision-tree model of com-
putation, Yao[359]proved aworst-case lower bound of .nlgn/ for the running
time of any convex-hull algorithm. When the number of vertices h of the con-
vex hull is taken into account, the prune-and-search algorithm of Kirkpatrick and
Seidel[206],whichtakesO.nlgh/time,isasymptotically optimal.
TheO.nlgn/-timedivide-and-conquer algorithm forfindingtheclosest pairof
points is by Shamos and appears in Preparata and Shamos [282]. Preparata and
Shamos also show that the algorithm is asymptotically optimal in a decision-tree
model.
34 NP-Completeness
Almostallthealgorithms wehave studied thusfar havebeen polynomial-time al-
gorithms: oninputsofsizen,theirworst-caserunningtimeisO.nk/forsomecon-
stantk. Youmightwonderwhetherallproblemscanbesolvedinpolynomialtime.
Theanswerisno. Forexample,thereareproblems,suchasTuring’sfamous“Halt-
ingProblem,”thatcannotbesolvedbyanycomputer,nomatterhowmuchtimewe
allow. There are also problems that can be solved, but not in time O.nk/ for any
constant k. Generally, wethink of problems that are solvable by polynomial-time
algorithms as being tractable, or easy, and problems that require superpolynomial
timeasbeingintractable, orhard.
The subject of this chapter, however, is an interesting class of problems, called
the “NP-complete” problems, whose status is unknown. No polynomial-time al-
gorithm hasyet been discovered foranNP-complete problem, nor hasanyone yet
beenabletoprovethatnopolynomial-timealgorithmcanexistforanyoneofthem.
Thisso-calledP NPquestionhasbeenoneofthedeepest,mostperplexingopen
¤
research problemsintheoretical computersciencesinceitwasfirstposedin1971.
Several NP-complete problems are particularly tantalizing because they seem
on thesurface tobesimilar toproblems that weknow how tosolve inpolynomial
time. In each of the following pairs of problems, one is solvable in polynomial
timeandtheotherisNP-complete,butthedifferencebetweenproblemsappearsto
beslight:
Shortestvs.longestsimplepaths: InChapter24,wesawthatevenwithnegative
edge weights, we can find shortest paths from a single source in a directed
graphG .V;E/inO.VE/time. Findingalongest simplepathbetweentwo
D
vertices is difficult, however. Merely determining whether a graph contains a
simplepathwithatleastagivennumberofedgesisNP-complete.
Eulertourvs.hamiltoniancycle: An Euler tour of a connected, directed graph
G .V;E/ is a cycle that traverses each edge of G exactly once, although
D
it is allowed to visit each vertex more than once. By Problem 22-3, we can
determine whether a graph has an Euler tour in only O.E/ time and, in fact,
Chapter34 NP-Completeness 1049
wecan findthe edges ofthe Euler tour inO.E/ time. A hamiltonian cycle of
a directed graph G .V;E/ is a simple cycle that contains each vertex in V.
D
Determiningwhetheradirectedgraphhasahamiltonian cycleisNP-complete.
(Later in this chapter, we shall prove that determining whether an undirected
graphhasahamiltonian cycleisNP-complete.)
2-CNFsatisfiability vs.3-CNFsatisfiability: A boolean formula contains vari-
ables whose values are0or1;boolean connectives such as (AND), (OR),
^ _
and (NOT);and parentheses. Aboolean formula is satisfiable ifthere exists
:
someassignment ofthevalues0and1toitsvariables thatcausesittoevaluate
to1. Weshalldefinetermsmoreformallylaterinthischapter,butinformally,a
boolean formula is in k-conjunctive normal form, or k-CNF,if it is the AND
of clauses of ORs of exactly k variables or their negations. For example, the
boolean formula .x x / . x x / . x x /isin2-CNF.(Ithas
1 2 1 3 2 3
_: ^ : _ ^ : _:
the satisfying assignment x 1;x 0;x 1.) Although we can deter-
1 2 3
D D D
mine in polynomial time whether a 2-CNF formula is satisfiable, we shall see
later inthis chapter that determining whether a 3-CNFformula issatisfiable is
NP-complete.
NP-completenessandtheclassesPandNP
Throughout this chapter, we shall refer to three classes of problems: P, NP, and
NPC, the latter class being the NP-complete problems. We describe them infor-
mallyhere,andweshalldefinethemmoreformallylateron.
The class P consists of those problems that are solvable in polynomial time.
More specifically, they are problems that can be solved in time O.nk/ for some
constant k, where n is the size of the input to the problem. Most of the problems
examinedinprevious chaptersareinP.
TheclassNPconsistsofthoseproblemsthatare“verifiable”inpolynomialtime.
What do we mean by a problem being verifiable? If we were somehow given a
“certificate”ofasolution,thenwecouldverifythatthecertificateiscorrectintime
polynomialinthesizeoftheinputtotheproblem. Forexample,inthehamiltonian-
cycle problem, given a directed graph G .V;E/, a certificate would be a se-
D
quence  ; ; ;:::; of V vertices. Wecould easily check inpolynomial
1 2 3 V
timethah t. ; / E fj orji i j 1;j 2;3;:::; V 1andthat. ; / E aswell.
i i 1 V 1
C 2 D j j(cid:0) j j 2
Asanother example, for3-CNFsatisfiability, acertificate wouldbeanassignment
of values to variables. We could check in polynomial time that this assignment
satisfiestheboolean formula.
Any problem in P is also in NP, since if a problem is in P then we can solve it
in polynomial time without even being supplied a certificate. We shall formalize
thisnotionlaterinthischapter, butfornowwecanbelievethatP NP. Theopen

question iswhetherornotPisapropersubsetofNP.
1050 Chapter34 NP-Completeness
Informally, a problem is in the class NPC—and we refer to it as being NP-
complete—if itisinNPandisas“hard” asanyproblem inNP. Weshallformally
define what it means to be as hard as any problem in NP later in this chapter.
In the meantime, we will state without proof that if any NP-complete problem
can be solved in polynomial time, then every problem in NP has a polynomial-
timealgorithm. Mosttheoretical computer scientists believethattheNP-complete
problems are intractable, since given the wide range of NP-complete problems
that have been studied to date—without anyone having discovered a polynomial-
time solution to any of them—it would be truly astounding if all of them could
be solved in polynomial time. Yet, given the effort devoted thus far to proving
that NP-complete problems are intractable—without a conclusive outcome—we
cannot rule out the possibility that the NP-complete problems are in fact solvable
inpolynomial time.
Tobecomeagoodalgorithmdesigner,youmustunderstandtherudimentsofthe
theory of NP-completeness. If you can establish a problem as NP-complete, you
provide good evidence for its intractability. As an engineer, you would then do
bettertospendyourtimedeveloping anapproximation algorithm (seeChapter35)
or solving a tractable special case, rather than searching for a fast algorithm that
solves theproblem exactly. Moreover, manynatural and interesting problems that
on the surface seem no harder than sorting, graph searching, or network flow are
infactNP-complete. Therefore, you should become familiar withthisremarkable
classofproblems.
Overview ofshowingproblemstobeNP-complete
The techniques we use to show that a particular problem is NP-complete differ
fundamentally from the techniques used throughout most of this book to design
and analyze algorithms. When we demonstrate that a problem is NP-complete,
we are making a statement about how hard it is (or at least how hard we think it
is), rather than about how easy it is. We are not trying to prove the existence of
an efficient algorithm, but instead that no efficient algorithm is likely to exist. In
this way,NP-completeness proofs bear somesimilarity totheproof inSection8.1
of an .nlgn/-time lower bound for any comparison sort algorithm; the specific
techniquesusedforshowingNP-completenessdifferfromthedecision-treemethod
usedinSection8.1,however.
Werelyonthreekeyconcepts inshowingaproblemtobeNP-complete:
Decisionproblemsvs.optimization problems
Manyproblemsofinterestareoptimization problems, inwhicheachfeasible(i.e.,
“legal”) solution has an associated value, and we wish to find a feasible solution
with the best value. For example, in a problem that we call SHORTEST-PATH,
Chapter34 NP-Completeness 1051
we are given an undirected graph G and vertices u and , and we wish to find a
path from u to  that uses the fewest edges. In other words, SHORTEST-PATH
is the single-pair shortest-path problem in an unweighted, undirected graph. NP-
completeness applies directly not to optimization problems, however, but to deci-
sionproblems,inwhichtheanswerissimply“yes”or“no”(or,moreformally,“1”
or“0”).
AlthoughNP-completeproblemsareconfinedtotherealmofdecisionproblems,
wecantakeadvantageofaconvenientrelationshipbetweenoptimizationproblems
anddecision problems. Weusually can cast agivenoptimization problem asare-
lated decision problem by imposing a bound on the value to be optimized. For
example, a decision problem related to SHORTEST-PATH is PATH: given a di-
rected graph G, vertices u and , and an integer k, does a path exist from u to 
consisting ofatmostk edges?
Therelationship betweenanoptimizationproblemanditsrelateddecisionprob-
lem works in our favor when we try to show that the optimization problem is
“hard.” That isbecause the decision problem is ina sense “easier,” or at least “no
harder.” Asaspecificexample,wecansolvePATHbysolving SHORTEST-PATH
and then comparing the number of edges in the shortest path found to the value
of the decision-problem parameter k. In other words, if an optimization prob-
lem is easy, its related decision problem is easy as well. Stated in a way that has
more relevance to NP-completeness, if we can provide evidence that a decision
problem ishard, wealso provide evidence that itsrelated optimization problem is
hard. Thus, even though it restricts attention to decision problems, the theory of
NP-completeness oftenhasimplications foroptimization problems aswell.
Reductions
The above notion of showing that one problem is no harder or no easier than an-
otherapplies evenwhenbothproblems aredecision problems. Wetakeadvantage
of this idea in almost every NP-completeness proof, as follows. Letus consider a
decisionproblemA,whichwewouldliketosolveinpolynomialtime. Wecallthe
input to a particular problem an instance of that problem; for example, in PATH,
aninstance would beaparticular graph G, particular vertices uand  ofG, and a
particular integer k. Now suppose that we already know how to solve a different
decisionproblemB inpolynomialtime. Finally,supposethatwehaveaprocedure
that transforms any instance ˛ of A into some instance ˇ of B with the following
characteristics:
 Thetransformation takespolynomial time.
 The answers are the same. That is, the answer for ˛ is “yes” if and only ifthe
answerforˇ isalso“yes.”
1052 Chapter34 NP-Completeness
instance a polynomial-time instance b polynomial-time yes yes
of A reduction algorithm of B algorithm to decide B no no
polynomial-time algorithm to decide A
Figure34.1 Howtouseapolynomial-timereductionalgorithmtosolveadecisionproblemAin
polynomialtime,givenapolynomial-timedecisionalgorithmforanotherproblemB.Inpolynomial
time,wetransformaninstance˛ofAintoaninstanceˇofB,wesolveB inpolynomialtime,and
weusetheanswerforˇastheanswerfor˛.
We call such a procedure a polynomial-time reduction algorithm and, as Fig-
ure34.1shows,itprovidesusawaytosolveproblemAinpolynomial time:
1. Givenan instance ˛ of problem A, use apolynomial-time reduction algorithm
totransform ittoaninstanceˇ ofproblem B.
2. Runthepolynomial-time decisionalgorithm forB ontheinstanceˇ.
3. Usetheanswerforˇ astheanswerfor˛.
Aslongaseachofthesestepstakespolynomialtime,allthreetogetherdoalso,and
sowehaveawaytodecideon˛inpolynomialtime. Inotherwords,by“reducing”
solving problem Atosolving problem B, weusethe “easiness” ofB toprove the
“easiness” ofA.
Recalling that NP-completeness isabout showing how hard aproblem is rather
thanhoweasyitis,weusepolynomial-timereductionsintheoppositewaytoshow
thataproblemisNP-complete. Letustaketheideaastepfurther,andshowhowwe
could use polynomial-time reductions to show that no polynomial-time algorithm
can exist for a particular problem B. Suppose we have a decision problem A for
which we already know that no polynomial-time algorithm can exist. (Let us not
concern ourselves for now with how to find such a problem A.) Suppose further
thatwehaveapolynomial-time reductiontransforminginstancesofAtoinstances
ofB. Nowwecanuseasimpleproofbycontradictiontoshowthatnopolynomial-
time algorithm can exist for B. Suppose otherwise; i.e., suppose that B has a
polynomial-time algorithm. Then, using the method shown in Figure 34.1, we
would have a way to solve problem A in polynomial time, which contradicts our
assumption thatthereisnopolynomial-time algorithm forA.
ForNP-completeness,wecannotassumethatthereisabsolutelynopolynomial-
timealgorithm forproblem A. Theproof methodology issimilar, however, inthat
weprovethatproblemB isNP-completeontheassumptionthatproblemAisalso
NP-complete.
34.1 Polynomialtime 1053
AfirstNP-completeproblem
Because the technique of reduction relies on having a problem already known to
be NP-complete in order to prove a different problem NP-complete, we need a
“first”NP-completeproblem. Theproblem weshalluseisthecircuit-satisfiability
problem,inwhichwearegivenabooleancombinationalcircuitcomposedofAND,
OR,andNOTgates,andwewishtoknowwhetherthereexistssomesetofboolean
inputs to this circuit that causes its output to be 1. We shall prove that this first
problemisNP-completeinSection34.3.
Chapteroutline
ThischapterstudiestheaspectsofNP-completeness thatbearmostdirectly onthe
analysisofalgorithms. InSection34.1,weformalizeournotionof“problem”and
definethe complexity class Pofpolynomial-time solvable decision problems. We
also see how these notions fitinto the framework offormal-language theory. Sec-
tion 34.2 defines the class NPofdecision problems whose solutions are verifiable
inpolynomial time. ItalsoformallyposestheP NPquestion.
¤
Section 34.3 shows we can relate problems via polynomial-time “reductions.”
It defines NP-completeness and sketches a proof that one problem, called “circuit
satisfiability,” isNP-complete. Having found oneNP-complete problem, weshow
inSection34.4howtoproveotherproblemstobeNP-completemuchmoresimply
bythemethodology ofreductions. Weillustratethismethodology byshowingthat
two formula-satisfiability problems are NP-complete. With additional reductions,
weshowinSection34.5avarietyofotherproblemstobeNP-complete.
34.1 Polynomialtime
WebeginourstudyofNP-completeness byformalizing ournotion ofpolynomial-
time solvable problems. We generally regard these problems as tractable, but for
philosophical, not mathematical, reasons. We can offer three supporting argu-
ments.
First, although wemayreasonably regard aproblem that requires time ‚.n100/
to be intractable, very few practical problems require time on the order of such a
high-degree polynomial. The polynomial-time computable problems encountered
in practice typically require much less time. Experience has shown that once the
firstpolynomial-time algorithm for a problem has been discovered, more efficient
algorithms often follow. Even if the current best algorithm for a problem has a
running timeof‚.n100/,analgorithm withamuchbetter running timewilllikely
soonbediscovered.
1054 Chapter34 NP-Completeness
Second, for many reasonable models of computation, a problem that can be
solved in polynomial time in one model can be solved in polynomial time in an-
other. Forexample,theclassofproblemssolvableinpolynomialtimebytheserial
random-access machineusedthroughout mostofthisbookisthesameastheclass
of problems solvable in polynomial time on abstract Turing machines.1 It is also
the same as the class of problems solvable in polynomial time on a parallel com-
puterwhenthenumberofprocessors growspolynomially withtheinputsize.
Third, the class of polynomial-time solvable problems has nice closure proper-
ties,sincepolynomialsareclosedunderaddition, multiplication, andcomposition.
Forexample,iftheoutputofonepolynomial-timealgorithmisfedintotheinputof
another, thecompositealgorithm ispolynomial. Exercise34.1-5asksyoutoshow
that if an algorithm makes a constant number of calls to polynomial-time subrou-
tines and performs an additional amount of work that also takes polynomial time,
thentherunning timeofthecompositealgorithm ispolynomial.
Abstractproblems
Tounderstand the class of polynomial-time solvable problems, wemustfirst have
aformal notion ofwhat a“problem” is. Wedefinean abstract problem Q tobe a
binary relation on a set I of problem instances and a set S of problem solutions.
For example, an instance for SHORTEST-PATHis a triple consisting of a graph
and two vertices. A solution is a sequence of vertices in the graph, with perhaps
theempty sequence denoting thatnopath exists. Theproblem SHORTEST-PATH
itself is the relation that associates each instance of a graph and two vertices with
ashortestpathinthegraphthatconnects thetwovertices. Sinceshortest pathsare
notnecessarilyunique,agivenprobleminstancemayhavemorethanonesolution.
This formulation of an abstract problem is more general than we need for our
purposes. As we saw above, the theory of NP-completeness restricts attention to
decision problems: those having a yes/no solution. In this case, we can view an
abstractdecision problemasafunctionthatmapstheinstancesetI tothesolution
set 0;1 . For example, a decision problem related to SHORTEST-PATH is the
f g
problemPATHthatwesawearlier. Ifi G;u;;k isaninstanceofthedecision
D h i
problem PATH, then PATH.i/ 1 (yes) if a shortest path from u to  has at
D
most k edges, and PATH.i/ 0 (no) otherwise. Many abstract problems are not
D
decision problems, butratheroptimization problems, whichrequire somevalueto
beminimizedormaximized. Aswesawabove,however, wecanusually recastan
optimization problemasadecision problemthatisnoharder.
1SeeHopcroftandUllman[180]orLewisandPapadimitriou[236]forathoroughtreatmentofthe
Turing-machinemodel.
34.1 Polynomialtime 1055
Encodings
In order for a computer program to solve an abstract problem, we must represent
problem instances inawaythat theprogram understands. AnencodingofasetS
ofabstractobjectsisamappingefromS tothesetofbinarystrings.2 Forexample,
we are all familiar with encoding the natural numbers N 0;1;2;3;4;::: as
D f g
the strings 0;1;10;11;100;::: . Using this encoding, e.17/ 10001. If you
f g D
havelookedatcomputerrepresentationsofkeyboardcharacters,youprobablyhave
seen the ASCII code, where, for example, the encoding of A is 1000001. We can
encode a compound object as a binary string by combining the representations of
itsconstituentparts. Polygons,graphs,functions,orderedpairs,programs—allcan
beencoded asbinarystrings.
Thus, acomputer algorithm that “solves” some abstract decision problem actu-
ally takes an encoding of a problem instance as input. We call a problem whose
instance set is the set of binary strings a concrete problem. We say that an algo-
rithmsolvesaconcreteproblemintimeO.T.n//if,whenitisprovidedaproblem
instance i of length n i , the algorithm can produce the solution in O.T.n//
D j j
time.3 A concrete problem is polynomial-time solvable, therefore, if there exists
analgorithm tosolveitintimeO.nk/forsomeconstant k.
Wecan now formally define the complexity class P as the set of concrete deci-
sionproblemsthatarepolynomial-time solvable.
We can use encodings to map abstract problems to concrete problems. Given
an abstract decision problem Q mapping an instance set I to 0;1 , an encoding
f g
e I 0;1  can induce arelated concrete decision problem, which wedenote
W ! f g
by e.Q/.4 If the solution to an abstract-problem instance i I is Q.i/ 0;1 ,
2 2 f g
thenthesolutiontotheconcrete-problem instancee.i/ 0;1 isalsoQ.i/. Asa
2f g
technicality, some binary strings might represent no meaningful abstract-problem
instance. For convenience, we shall assume that any such string maps arbitrarily
to0. Thus,theconcrete problem produces thesamesolutions astheabstract prob-
lem on binary-string instances that represent the encodings of abstract-problem
instances.
Wewouldliketoextendthedefinitionofpolynomial-time solvability fromcon-
creteproblemstoabstractproblemsbyusingencodingsasthebridge,butwewould
2Thecodomain of e need notbebinary strings; anysetof stringsoverafinitealphabet having at
least2symbolswilldo.
3Weassumethatthealgorithm’soutputisseparatefromitsinput. Becauseittakesatleastonetime
steptoproduce eachbitoftheoutput andthealgorithmtakesO.T.n//timesteps, thesizeofthe
outputisO.T.n//.
4Wedenoteby 0;1 thesetofallstringscomposedofsymbolsfromtheset 0;1 .
f g f g
1056 Chapter34 NP-Completeness
like the definition to be independent of any particular encoding. That is, the ef-
ficiency of solving a problem should not depend on how the problem is encoded.
Unfortunately,itdependsquiteheavilyontheencoding. Forexample,supposethat
an integer k is to be provided as the sole input to an algorithm, and suppose that
therunningtimeofthealgorithmis‚.k/. Iftheintegerk isprovidedinunary—a
string ofk 1s—then therunning timeofthealgorithm isO.n/onlength-n inputs,
which is polynomial time. Ifweuse the morenatural binary representation of the
integer k, however, then the input length is n lgk 1. In this case, the run-
D b cC
ningtimeofthealgorithmis‚.k/ ‚.2n/,whichisexponentialinthesizeofthe
D
input. Thus, depending on the encoding, the algorithm runs in either polynomial
orsuperpolynomial time.
How we encode an abstract problem matters quite a bit to how we understand
polynomial time. Wecannot reallytalkabout solving anabstract problem without
first specifying an encoding. Nevertheless, in practice, if we rule out “expensive”
encodings such as unary ones, the actual encoding of a problem makes little dif-
ference to whether the problem can be solved in polynomial time. For example,
representing integers in base 3 instead of binary has no effect on whether a prob-
lemissolvable inpolynomial time, sincewecanconvert aninteger represented in
base3toanintegerrepresented inbase2inpolynomial time.
We say that a function f 0;1  0;1  is polynomial-time computable
W f g ! f g
if there exists a polynomial-time algorithm A that, given any input x 0;1 ,
2 f g
producesasoutputf.x/. ForsomesetI ofprobleminstances, wesaythattwoen-
codingse ande arepolynomiallyrelatedifthereexisttwopolynomial-timecom-
1 2
putable functions f andf such thatforanyi I,wehavef .e .i// e .i/
12 21 12 1 2
2 D
andf .e .i// e .i/.5 Thatis,apolynomial-timealgorithmcancomputetheen-
21 2 1
D
codinge .i/fromtheencodinge .i/,andviceversa. Iftwoencodingse ande of
2 1 1 2
anabstract problem arepolynomially related, whethertheproblem ispolynomial-
time solvable or not is independent of which encoding we use, as the following
lemmashows.
Lemma34.1
Let Q be an abstract decision problem on an instance set I, and let e and e be
1 2
polynomially relatedencodings onI. Then,e .Q/ Pifandonlyife .Q/ P.
1 2
2 2
5Technically, we also require the functions f12 and f21 to “map noninstances to noninstances.”
Anoninstanceofanencoding e isastringx 0;1  suchthatthereisnoinstancei forwhich
2 f g
e.i/ x.Werequirethatf12.x/ yforeverynoninstancexofencodinge1,whereyissomenon-
D D
instanceofe2,andthatf21.x 0/ Dy 0foreverynoninstancex 0ofe2,wherey 0issomenoninstance
ofe1.
34.1 Polynomialtime 1057
Proof Weneedonlyprovetheforwarddirection, sincethebackwarddirection is
symmetric. Suppose, therefore, that e .Q/ can be solved in time O.nk/ for some
1
constant k. Further, suppose that for any problem instance i, the encoding e .i/
1
canbecomputedfromtheencodinge .i/intimeO.nc/forsomeconstantc,where
2
n e .i/ . To solve problem e .Q/, on input e .i/, we first compute e .i/ and
2 2 2 1
D j j
then run the algorithm for e .Q/ on e .i/. How long does this take? Converting
1 1
encodings takes time O.nc/, and therefore e .i/ O.nc/, since the output of
1
j j D
a serial computer cannot be longer than its running time. Solving the problem
one .i/takestimeO. e .i/ k/ O.nck/,whichispolynomialsincebothc andk
1 1
j j D
areconstants.
Thus,whetheranabstract problem hasitsinstances encodedinbinary orbase3
does not affect its “complexity,” that is, whether it is polynomial-time solvable or
not; but if instances are encoded in unary, its complexity maychange. In order to
beabletoconverseinanencoding-independent fashion,weshallgenerallyassume
that problem instances are encoded in any reasonable, concise fashion, unless we
specifically say otherwise. Tobeprecise, weshall assume that theencoding of an
integerispolynomiallyrelatedtoitsbinaryrepresentation,andthattheencodingof
afinitesetispolynomially relatedtoitsencodingasalistofitselements,enclosed
in braces and separated by commas. (ASCIIis one such encoding scheme.) With
such a “standard” encoding in hand, we can derive reasonable encodings of other
mathematicalobjects,suchastuples,graphs,andformulas. Todenotethestandard
encoding of an object, we shall enclose the object in angle braces. Thus, G
h i
denotesthestandard encoding ofagraphG.
As long as we implicitly use an encoding that is polynomially related to this
standard encoding, wecantalkdirectly about abstract problems withoutreference
to any particular encoding, knowing that the choice of encoding has no effect on
whether the abstract problem is polynomial-time solvable. Henceforth, we shall
generally assume that all problem instances are binary strings encoded using the
standardencoding,unlessweexplicitlyspecifythecontrary. Weshallalsotypically
neglectthedistinction between abstract andconcrete problems. Youshould watch
out for problems that arise in practice, however, in which a standard encoding is
notobviousandtheencoding doesmakeadifference.
Aformal-language framework
By focusing on decision problems, we can take advantage of the machinery of
formal-language theory. Let’s review some definitions from that theory. An
alphabet † is a finite set of symbols. A language L over † is any set of
strings made up of symbols from †. For example, if † 0;1 , the set
D f g
L 10;11;101;111;1011;1101;10001;::: isthelanguage ofbinaryrepresen-
D f g
1058 Chapter34 NP-Completeness
tations of prime numbers. We denote the empty string by ", the empty language
by , and the language of all strings over † by † . For example, if † 0;1 ,

; D f g
then † ";0;1;00;01;10;11;000;::: is the set of all binary strings. Every

D f g
language Lover†isasubsetof† .

We can perform a variety of operations on languages. Set-theoretic operations,
such as union and intersection, follow directly from the set-theoretic definitions.
WedefinethecomplementofLbyL † L. TheconcatenationL L oftwo
 1 2
D (cid:0)
languages L andL isthelanguage
1 2
L x x x L andx L :
1 2 1 1 2 2
D f W 2 2 g
TheclosureorKleenestarofalanguage Listhelanguage
L " L L2 L3 ;

D f g[ [ [ [
whereLk isthelanguage obtained byconcatenating Ltoitselfk times.
Fromthe point ofview oflanguage theory, thesetofinstances foranydecision
problem Q issimplytheset† ,where† 0;1 . SinceQ isentirely character-

D f g
ized by those problem instances that produce a1 (yes) answer, wecan view Q as
alanguage Lover† 0;1 ,where
D f g
L x † Q.x/ 1 :

D f 2 W D g
Forexample,thedecision problemPATHhasthecorresponding language
PATH G;u;;k G .V;E/isanundirected graph,
D fh i W D
u; V;
2
k 0isaninteger, and

thereexistsapathfromuto inG
consisting ofatmostk edges :
g
(Whereconvenient, weshallsometimesusethesamename—PATHinthiscase—
torefertobothadecision problemanditscorresponding language.)
The formal-language framework allows us to express concisely the relation be-
tween decision problems and algorithms that solve them. We say that an al-
gorithm A accepts a string x 0;1  if, given input x, the algorithm’s out-
2 f g
put A.x/ is 1. The language accepted by an algorithm A is the set of strings
L x 0;1  A.x/ 1 ,thatis,thesetofstringsthatthealgorithmaccepts.
D f 2 f g W D g
Analgorithm Arejectsastringx ifA.x/ 0.
D
EveniflanguageLisacceptedbyanalgorithm A,thealgorithm willnotneces-
sarilyrejectastringx Lprovidedasinputtoit. Forexample,thealgorithmmay
62
loop forever. A language L is decided by an algorithm A if every binary string
in L is accepted by A and every binary string not in L is rejected by A. A lan-
guage L is accepted in polynomial time by an algorithm A if it is accepted by A
andifinadditionthereexistsaconstant k suchthatforanylength-n stringx L,
2
34.1 Polynomialtime 1059
algorithm A accepts x in time O.nk/. A language L is decided in polynomial
timebyanalgorithmAifthereexistsaconstantk suchthatforanylength-nstring
x 0;1 , the algorithm correctly decides whether x L intime O.nk/. Thus,
2 f g 2
to accept alanguage, an algorithm need only produce an answer when provided a
stringinL,buttodecidealanguage, itmustcorrectly acceptorrejecteverystring
in 0;1 .
f g
As an example, the language PATH can be accepted in polynomial time. One
polynomial-time accepting algorithm verifiesthatG encodes anundirected graph,
verifiesthatuand areverticesinG,usesbreadth-firstsearchtocomputeashort-
estpath from uto inG, and then compares the number of edges on the shortest
pathobtainedwithk. IfG encodesanundirectedgraphandthepathfoundfromu
to  has at most k edges, the algorithm outputs 1 and halts. Otherwise, the algo-
rithm runs forever. This algorithm does not decide PATH, however, since it does
notexplicitlyoutput0forinstancesinwhichashortestpathhasmorethankedges.
Adecisionalgorithm forPATHmustexplicitlyrejectbinarystringsthatdonotbe-
longtoPATH. Foradecision problem suchasPATH,suchadecision algorithm is
easytodesign: insteadofrunningforeverwhenthereisnotapathfromuto with
at most k edges, itoutputs 0 and halts. (It must also output 0 and halt if the input
encoding is faulty.) For other problems, such as Turing’s Halting Problem, there
existsanaccepting algorithm, butnodecisionalgorithm exists.
Wecaninformally define acomplexity class asasetoflanguages, membership
in which is determined by a complexity measure, such as running time, of an
algorithm that determines whether a given string x belongs to language L. The
actualdefinitionofacomplexity classissomewhatmoretechnical.6
Using this language-theoretic framework, we can provide an alternative defini-
tionofthecomplexity classP:
P L 0;1  thereexistsanalgorithm AthatdecidesL
Df  f g W
inpolynomial time :
g
Infact,Pisalsotheclassoflanguages thatcanbeaccepted inpolynomial time.
Theorem34.2
P L Lisaccepted byapolynomial-time algorithm :
D f W g
Proof Because the class of languages decided by polynomial-time algorithms is
a subset of the class of languages accepted by polynomial-time algorithms, we
need only show that if L is accepted by a polynomial-time algorithm, it is de-
cided by a polynomial-time algorithm. Let L be the language accepted by some
6Formoreoncomplexityclasses,seetheseminalpaperbyHartmanisandStearns[162].
1060 Chapter34 NP-Completeness
polynomial-time algorithm A. We shall use a classic “simulation” argument to
construct another polynomial-time algorithm A that decides L. Because A ac-
0
cepts L in time O.nk/ for some constant k, there also exists a constant c such
that A accepts L in at most cnk steps. For any input string x, the algorithm A
0
simulatescnk stepsofA. Aftersimulatingcnk steps,algorithmA inspectsthebe-
0
havior ofA. IfAhasaccepted x,thenA accepts x byoutputting a1. IfAhasnot
0
accepted x, then A rejects x by outputting a 0. The overhead of A simulating A
0 0
does not increase the running time by more than a polynomial factor, and thus A
0
isapolynomial-time algorithm thatdecidesL.
Note that the proof of Theorem 34.2 is nonconstructive. For a given language
L P,wemaynotactuallyknowaboundontherunningtimeforthealgorithmA
2
thatacceptsL. Nevertheless, weknowthatsuchaboundexists,andtherefore, that
an algorithm A exists that can check the bound, even though wemay not be able
0
tofindthealgorithm A easily.
0
Exercises
34.1-1
Define the optimization problem LONGEST-PATH-LENGTHas the relation that
associates each instance of an undirected graph and two vertices with the num-
ber of edges in a longest simple path between the two vertices. Define the de-
cision problem LONGEST-PATH G; u; ; k G .V;E/ is an undi-
D fh i W D
rected graph, u; V, k 0 is an integer, and there exists a simple path
2 
from uto inG consisting ofatleast k edges . Showthattheoptimization prob-
g
lem LONGEST-PATH-LENGTHcan be solved in polynomial time if and only if
LONGEST-PATH P.
2
34.1-2
Give a formal definition for the problem of finding the longest simple cycle in an
undirected graph. Givearelateddecision problem. Givethelanguage correspond-
ingtothedecision problem.
34.1-3
Give a formal encoding of directed graphs as binary strings using an adjacency-
matrix representation. Do the same using an adjacency-list representation. Argue
thatthetworepresentations arepolynomially related.
34.1-4
Isthedynamic-programming algorithmforthe0-1knapsackproblemthatisasked
forinExercise16.2-2apolynomial-time algorithm? Explainyouranswer.
34.2 Polynomial-timeverification 1061
34.1-5
Showthatifanalgorithmmakesatmostaconstantnumberofcallstopolynomial-
timesubroutinesandperformsanadditionalamountofworkthatalsotakespolyno-
mialtime,thenitrunsinpolynomialtime. Alsoshowthatapolynomialnumberof
callstopolynomial-timesubroutines mayresultinanexponential-time algorithm.
34.1-6
Show that the class P, viewed as a set of languages, is closed under union, inter-
section, concatenation, complement, and Kleene star. That is, if L ;L P, then
1 2
2
L L P, L L P, L L P, L P,andL P.
1
[
2
2
1
\
2
2
1 2
2
1
2
1
2
34.2 Polynomial-timeverification
We now look at algorithms that verify membership in languages. For example,
suppose that for a given instance G;u;;k of the decision problem PATH, we
h i
arealsogivenapathp fromuto. Wecaneasily checkwhether p isapathinG
andwhetherthelengthofpisatmostk,andifso,wecanviewpasa“certificate”
that the instance indeed belongs to PATH. For the decision problem PATH, this
certificate doesn’t seem to buy us much. After all, PATH belongs to P—in fact,
we can solve PATH in linear time—and so verifying membership from a given
certificatetakesaslongassolvingtheproblemfromscratch. Weshallnowexamine
a problem for which we know of no polynomial-time decision algorithm and yet,
givenacertificate, verificationiseasy.
Hamiltoniancycles
Theproblem of finding ahamiltonian cycle in anundirected graph has been stud-
iedforoverahundredyears. Formally,ahamiltoniancycleofanundirectedgraph
G .V;E/ is a simple cycle that contains each vertex in V. A graph that con-
D
tains a hamiltonian cycle is said to be hamiltonian; otherwise, it is nonhamilto-
nian. The name honors W. R. Hamilton, who described a mathematical game on
the dodecahedron (Figure 34.2(a)) in which one player sticks five pins in any five
consecutive vertices and the other player must complete the path to form a cycle
1062 Chapter34 NP-Completeness
(a) (b)
Figure 34.2 (a) A graph representing the vertices, edges, and faces of a dodecahedron, with a
hamiltonian cycle shown byshaded edges. (b)A bipartitegraph withan odd number of vertices.
Anysuchgraphisnonhamiltonian.
containing all the vertices.7 The dodecahedron is hamiltonian, and Figure 34.2(a)
shows one hamiltonian cycle. Not all graphs are hamiltonian, however. For ex-
ample, Figure 34.2(b) shows a bipartite graph with an odd number of vertices.
Exercise34.2-2asksyoutoshowthatallsuchgraphs arenonhamiltonian.
We can define the hamiltonian-cycle problem, “Does a graph G have a hamil-
toniancycle?”asaformallanguage:
HAM-CYCLE G G isahamiltonian graph :
D fh i W g
How might an algorithm decide the language HAM-CYCLE? Given a problem
instance G ,onepossible decision algorithm listsallpermutations ofthevertices
h i
of G and then checks each permutation to see if it is a hamiltonian path. What is
therunningtimeofthisalgorithm? Ifweusethe“reasonable” encodingofagraph
as its adjacency matrix, the number m of vertices in the graph is .pn/, where
n G isthelength oftheencoding ofG. TherearemŠpossible permutations
D jh ij
7Inaletterdated17 October 1856 tohisfriendJohn T.Graves, Hamilton[157, p. 624] wrote, “I
havefoundthatsomeyoungpersonshavebeenmuchamusedbytryinganewmathematicalgame
whichtheIcosionfurnishes,onepersonstickingfivepinsinanyfiveconsecutivepoints...andthe
otherplayerthenaimingtoinsert,whichbythetheoryinthislettercanalwaysbedone,fifteenother
pins,incyclicalsuccession,soastocoveralltheotherpoints,andtoendinimmediateproximityto
thepinwherewithhisantagonisthadbegun.”
34.2 Polynomial-timeverification 1063
of the vertices, and therefore the running time is .mŠ/ .pnŠ/ .2pn/,
D D
which is not O.nk/ for any constant k. Thus, this naive algorithm does not run
inpolynomial time. Infact, thehamiltonian-cycle problem isNP-complete, aswe
shallproveinSection34.5.
Verificationalgorithms
Consider a slightly easier problem. Suppose that a friend tells you that a given
graph G is hamiltonian, and then offers to prove it by giving you the vertices in
order along thehamiltonian cycle. Itwouldcertainly beeasy enough toverify the
proof: simply verify that the provided cycle is hamiltonian by checking whether
itisapermutation ofthe vertices ofV and whether each ofthe consecutive edges
along the cycle actually exists in the graph. You could certainly implement this
verification algorithm to run inO.n2/ time, where n isthe length ofthe encoding
of G. Thus, a proof that a hamiltonian cycle exists in a graph can be verified in
polynomial time.
Wedefineaverificationalgorithmasbeingatwo-argumentalgorithmA,where
oneargument isanordinary inputstringx andtheotherisabinarystringy called
acertificate. Atwo-argument algorithm Averifiesaninput string x ifthere exists
a certificate y such that A.x;y/ 1. The language verified by a verification
D
algorithm Ais
L x 0;1

thereexistsy 0;1

suchthatA.x;y/ 1 :
D f 2f g W 2 f g D g
Intuitively, an algorithm A verifies a language L if for any string x L, there
2
exists acertificate y that A can use to prove that x L. Moreover, for any string
2
x L, there must be no certificate proving that x L. For example, in the
62 2
hamiltonian-cycle problem, the certificate is the list of vertices in some hamilto-
nian cycle. If a graph is hamiltonian, the hamiltonian cycle itself offers enough
information to verify this fact. Conversely, if a graph is not hamiltonian, there
canbenolistofverticesthatfoolstheverificationalgorithmintobelievingthatthe
graphishamiltonian,sincetheverificationalgorithmcarefullycheckstheproposed
“cycle”tobesure.
1064 Chapter34 NP-Completeness
ThecomplexityclassNP
The complexity class NP is the class of languages that can be verified by a poly-
nomial-timealgorithm.8 Moreprecisely,alanguageLbelongstoNPifandonlyif
thereexistatwo-inputpolynomial-time algorithm Aandaconstant c suchthat
L x 0;1  thereexistsacertificatey with y O. x c/
Df 2 f g W j j D j j
suchthatA.x;y/ 1 :
D g
Wesaythatalgorithm Averifieslanguage Linpolynomialtime.
From our earlier discussion onthe hamiltonian-cycle problem, wenow see that
HAM-CYCLE NP. (Itisalwaysnicetoknowthatanimportantsetisnonempty.)
2
Moreover, if L P, then L NP, since if there is a polynomial-time algorithm
2 2
to decide L, the algorithm can be easily converted to a two-argument verification
algorithmthatsimplyignoresanycertificateandacceptsexactlythoseinputstrings
itdeterminestobeinL. Thus,P NP.

It isunknown whether P NP, but most researchers believe that P and NPare
D
not the same class. Intuitively, the class Pconsists of problems that can be solved
quickly. The class NP consists of problems for which a solution can be verified
quickly. You may have learned from experience that it is often more difficult to
solveaproblemfromscratchthantoverifyaclearlypresentedsolution, especially
when working under time constraints. Theoretical computer scientists generally
believethatthisanalogyextendstotheclassesPandNP,andthusthatNPincludes
languages thatarenotinP.
There is more compelling, though not conclusive, evidence that P NP—the
¤
existence of languages that are “NP-complete.” We shall study this class in Sec-
tion34.3.
Many other fundamental questions beyond the P NP question remain unre-
¤
solved. Figure 34.3 shows some possible scenarios. Despite much work by many
researchers, no one even knows whether the class NP is closed under comple-
ment. That is, does L NP imply L NP? We can define the complexity class
2 2
co-NP as the set of languages L such that L NP. We can restate the question
2
of whether NP is closed under complement as whether NP co-NP. Since P is
D
closed under complement (Exercise 34.1-6), it follows from Exercise 34.2-9 that
P NP co-NP. Onceagain, however, noone knowswhether P NP co-NP
 \ D \
orwhetherthereissomelanguage inNP co-NP P.
\ (cid:0)
8Thename“NP”standsfor“nondeterministicpolynomialtime.”TheclassNPwasoriginallystudied
inthecontextofnondeterminism,butthisbookusesthesomewhatsimpleryetequivalentnotionof
verification. Hopcroft andUllman[180]giveagoodpresentationofNP-completenessintermsof
nondeterministicmodelsofcomputation.
34.2 Polynomial-timeverification 1065
NP = co-NP
P = NP = co-NP
P
(a) (b)
co-NP NP co-NP NP ˙ co-NP NP
P = NP ˙ co-NP
P
(c) (d)
Figure34.3 Fourpossibilitiesforrelationshipsamongcomplexityclasses. Ineachdiagram, one
regionenclosinganotherindicatesaproper-subsetrelation. (a)P NP co-NP. Mostresearchers
D D
regardthispossibilityasthemostunlikely.(b)IfNPisclosedundercomplement,thenNP co-NP,
D
butitneednotbethecasethatP NP.(c)P NP co-NP,butNPisnotclosedundercomplement.
D D \
(d)NP co-NPandP NP co-NP.Mostresearchersregardthispossibilityasthemostlikely.
¤ ¤ \
Thus, our understanding of the precise relationship between P and NP is woe-
fully incomplete. Nevertheless, even though we might not be able to prove that a
particular problem is intractable, if we can prove that it is NP-complete, then we
havegainedvaluableinformation aboutit.
Exercises
34.2-1
Consider the language GRAPH-ISOMORPHISM G ;G G andG are
1 2 1 2
D fh i W
isomorphic graphs . Prove that GRAPH-ISOMORPHISM NP by describing a
g 2
polynomial-time algorithm toverifythelanguage.
34.2-2
Prove that if G is an undirected bipartite graph with an odd number of vertices,
thenG isnonhamiltonian.
34.2-3
Show that if HAM-CYCLE P, then the problem of listing the vertices of a
2
hamiltonian cycle,inorder, ispolynomial-time solvable.
1066 Chapter34 NP-Completeness
34.2-4
Prove that the class NP of languages is closed under union, intersection, concate-
nation, andKleenestar. DiscusstheclosureofNPundercomplement.
34.2-5
Show that any language in NP can be decided by an algorithm running in
time2O.nk/ forsomeconstant k.
34.2-6
A hamiltonian path in a graph is a simple path that visits every vertex exactly
once. Show that the language HAM-PATH G;u; there is a hamiltonian
D fh i W
pathfromuto ingraphG belongs toNP.
g
34.2-7
Show that the hamiltonian-path problem from Exercise 34.2-6 can be solved in
polynomial time on directed acyclic graphs. Give an efficient algorithm for the
problem.
34.2-8
Let be a boolean formula constructed from the boolean input variables x ;x ;
1 2
:::;x , negations ( ), ANDs ( ), ORs ( ), and parentheses. The formula is a
k
: ^ _
tautologyifitevaluatesto1foreveryassignmentof1and0totheinputvariables.
Define TAUTOLOGY as the language of boolean formulas that are tautologies.
ShowthatTAUTOLOGY co-NP.
2
34.2-9
ProvethatP co-NP.

34.2-10
ProvethatifNP co-NP,thenP NP.
¤ ¤
34.2-11
Let G be a connected, undirected graph with at least 3 vertices, and let G3 be the
graphobtainedbyconnectingallpairsofverticesthatareconnectedbyapathinG
oflengthatmost3. ProvethatG3 ishamiltonian. (Hint:Constructaspanningtree
forG,anduseaninductiveargument.)
34.3 NP-completenessandreducibility 1067
34.3 NP-completeness andreducibility
Perhaps the most compelling reason why theoretical computer scientists believe
that P NP comes from the existence of the class of “NP-complete” problems.
¤
This class has the intriguing property that if any NP-complete problem can be
solved inpolynomial time, thenevery problem inNPhas apolynomial-time solu-
tion,thatis,P NP. Despiteyearsofstudy,though,nopolynomial-timealgorithm
D
haseverbeendiscoveredforanyNP-completeproblem.
The language HAM-CYCLE is one NP-complete problem. If we could decide
HAM-CYCLE in polynomial time, then we could solve every problem in NP in
polynomial time. Infact, ifNP Pshould turn out tobe nonempty, wecould say
(cid:0)
withcertainty thatHAM-CYCLE NP P.
2 (cid:0)
The NP-complete languages are, in a sense, the “hardest” languages in NP. In
this section, we shall show how to compare the relative “hardness” of languages
using a precise notion called “polynomial-time reducibility.” Then we formally
define the NP-complete languages, and we finish by sketching a proof that one
such language, called CIRCUIT-SAT, is NP-complete. In Sections 34.4 and 34.5,
we shall use the notion of reducibility to show that many other problems are NP-
complete.
Reducibility
Intuitively,aproblemQcanbereducedtoanotherproblemQ ifanyinstanceofQ
0
can be “easily rephrased” as an instance of Q, the solution to which provides a
0
solutiontotheinstanceofQ. Forexample,theproblemofsolvinglinearequations
inanindeterminatexreducestotheproblemofsolvingquadraticequations. Given
an instance ax b 0, we transform it to 0x2 ax b 0, whose solution
C D C C D
provides a solution to ax b 0. Thus, if a problem Q reduces to another
C D
problemQ,thenQis,inasense, “nohardertosolve”thanQ.
0 0
Returningtoourformal-language frameworkfordecisionproblems,wesaythat
alanguage L ispolynomial-time reducible to alanguage L , written L L ,
1 2 1 P 2
ifthere exists a polynomial-time computable function f 0;1  0;1  such
W f g ! f g
thatforallx 0;1 ,
2f g
x L ifandonlyiff.x/ L : (34.1)
1 2
2 2
Wecallthefunctionf thereductionfunction,andapolynomial-timealgorithmF
thatcomputesf isareductionalgorithm.
Figure 34.4 illustrates the idea of a polynomial-time reduction from a lan-
guage L
1
to another language L 2. Each language is a subset of 0;1 . The
f g
reduction function f provides a polynomial-time mapping such that if x L ,
1
2
1068 Chapter34 NP-Completeness
{0,1}* f {0,1}*
L
L 2
1
Figure34.4 Anillustrationofapolynomial-timereductionfromalanguageL1toalanguageL2
viaareductionfunctionf.Foranyinputx 0;1 ,thequestionofwhetherx L1hasthesame
2f g 2
answerasthequestionofwhetherf.x/ L2.
2
thenf.x/ L . Moreover, ifx L ,thenf.x/ L . Thus, thereduction func-
2 1 2
2 62 62
tion maps any instance x ofthe decision problem represented by the language L
1
to an instance f.x/ of the problem represented by L . Providing an answer to
2
whetherf.x/ L directlyprovides theanswertowhetherx L .
2 1
2 2
Polynomial-timereductionsgiveusapowerfultoolforprovingthatvariouslan-
guagesbelong toP.
Lemma34.3
If L 1;L 2  f0;1 g are languages such that L 1 P L 2, then L 2 2 P implies
L P.
1
2
Proof Let A be a polynomial-time algorithm that decides L , and let F be a
2 2
polynomial-time reduction algorithm that computes thereduction function f. We
shallconstruct apolynomial-time algorithm A thatdecidesL .
1 1
Figure 34.5 illustrates how we construct A 1. For a given input x 0;1 ,
2 f g
algorithm A uses F totransform x into f.x/,andthenituses A totestwhether
1 2
f.x/ L . Algorithm A takes the output from algorithm A and produces that
2 1 2
2
answerasitsownoutput.
ThecorrectnessofA followsfromcondition(34.1). Thealgorithmrunsinpoly-
1
nomialtime,sincebothF andA runinpolynomial time(seeExercise34.1-5).
2
NP-completeness
Polynomial-time reductions provide a formal means for showing that one prob-
lem is at least as hard as another, to within a polynomial-time factor. That is, if
L L , then L is not more than apolynomial factor harder than L , which is
1 P 2 1 2
34.3 NP-completenessandreducibility 1069
x f.x/
yes,f.x/ 2L
2
yes,x 2L
1
F A
2
A
no,f.x/ 62L
2
no,x 62L
1
1
Figure34.5 TheproofofLemma34.3.ThealgorithmF isareductionalgorithmthatcomputesthe
reductionfunctionf fromL1toL2inpolynomialtime,andA2isapolynomial-timealgorithmthat
decidesL2. AlgorithmA1decideswhetherx L1byusingF totransformanyinputxintof.x/
2
andthenusingA2todecidewhetherf.x/ L2.
2
why the “less than or equal to” notation for reduction is mnemonic. We can now
definethesetofNP-completelanguages, whicharethehardestproblemsinNP.
Alanguage L 0;1  isNP-completeif
 f g
1. L NP,and
2
2. L LforeveryL NP.
0 P 0 2
If a language L satisfies property 2, but not necessarily property 1, we say that L
isNP-hard. WealsodefineNPCtobetheclassofNP-completelanguages.
As the following theorem shows, NP-completeness is at the crux of deciding
whetherPisinfactequaltoNP.
Theorem34.4
If any NP-complete problem is polynomial-time solvable, then P NP. Equiva-
D
lently,ifanyprobleminNPisnotpolynomial-time solvable,thennoNP-complete
problemispolynomial-time solvable.
Proof Suppose that L P and also that L NPC. For any L NP, we
0
2 2 2
have L L by property 2 of the definition of NP-completeness. Thus, by
0 P
Lemma 34.3, we also have that L P, which proves the first statement of the
0
2
theorem.
Toprovethesecondstatement,notethatitisthecontrapositive ofthefirststate-
ment.
It is for this reason that research into the P NP question centers around the
¤
NP-completeproblems. Mosttheoretical computer scientists believethatP NP,
¤
which leads to the relationships among P, NP, and NPC shown in Figure 34.6.
But, for all we know, someone may yet come up with a polynomial-time algo-
rithmforanNP-completeproblem, thusproving thatP NP. Nevertheless, since
D
no polynomial-time algorithm for any NP-complete problem has yet been discov-
1070 Chapter34 NP-Completeness
NP
NPC
P
Figure 34.6 How most theoretical computer scientists view the relationships among P, NP,
andNPC.BothPandNPCarewhollycontainedwithinNP,andP NPC .
\ D;
ered, a proof that a problem is NP-complete provides excellent evidence that it is
intractable.
Circuitsatisfiability
We have defined the notion of an NP-complete problem, but up to this point, we
have not actually proved that any problem is NP-complete. Once weprove that at
least one problem is NP-complete, we can use polynomial-time reducibility as a
tool to prove other problems to be NP-complete. Thus, we now focus on demon-
strating the existence of an NP-complete problem: the circuit-satisfiability prob-
lem.
Unfortunately, the formal proof that the circuit-satisfiability problem is NP-
complete requires technical detail beyond the scope of this text. Instead, weshall
informally describe aproofthatreliesonabasicunderstanding ofboolean combi-
national circuits.
Boolean combinational circuits are built from boolean combinational elements
that areinterconnected bywires. Aboolean combinational element isanycircuit
elementthathasaconstantnumberofbooleaninputsandoutputsandthatperforms
a well-defined function. Boolean values are drawn from the set 0;1 , where 0
f g
represents FALSE and1represents TRUE.
Thebooleancombinationalelementsthatweuseinthecircuit-satisfiabilityprob-
lem compute simple boolean functions, and they are known as logic gates. Fig-
ure 34.7 shows the three basic logic gates that we use in the circuit-satisfiability
problem: the NOT gate (or inverter), the AND gate, and the OR gate. The NOT
gate takes a single binary input x, whose value is either 0 or 1, and produces a
binary output ´whose value isopposite that of the input value. Eachof the other
twogatestakestwobinaryinputsx andy andproduces asinglebinaryoutput´.
We can describe the operation of each gate, and of any boolean combinational
element,byatruthtable,shownundereachgateinFigure34.7. Atruthtablegives
theoutputsofthecombinationalelementforeachpossiblesettingoftheinputs. For
34.3 NP-completenessandreducibility 1071
x x
x z z z
y y
x x x y x y x y x y
: ^ _
0 1 0 0 0 0 0 0
1 0 0 1 0 0 1 1
1 0 0 1 0 1
1 1 1 1 1 1
(a) (b) (c)
Figure34.7 Threebasiclogicgates,withbinaryinputsandoutputs. Undereachgateisthetruth
tablethatdescribesthegate’soperation.(a)TheNOTgate.(b)TheANDgate.(c)TheORgate.
example, the truth table for the OR gate tells us that when the inputs are x 0
D
and y 1, the output value is ´ 1. We use the symbols to denote the NOT
D D :
function, to denote the AND function, and to denote the OR function. Thus,
^ _
forexample, 0 1 1.
_ D
We can generalize AND and OR gates to take more than two inputs. An AND
gate’soutputis1ifallofitsinputsare1,anditsoutputis0otherwise. AnORgate’s
outputis1ifanyofitsinputsare1,anditsoutputis0otherwise.
Abooleancombinationalcircuitconsistsofoneormorebooleancombinational
elements interconnected by wires. A wire can connect the output of one element
totheinputofanother,therebyprovidingtheoutputvalueofthefirstelementasan
input value of the second. Figure 34.8 shows two similar boolean combinational
circuits, differing in only one gate. Part (a) of the figure also shows the values on
the individual wires, given the input x 1;x 1;x 0 . Although asingle
1 2 3
h D D D i
wiremay have no more than one combinational-element output connected to it, it
can feed several element inputs. The number of element inputs fed by a wire is
calledthefan-outofthewire. Ifnoelementoutputisconnectedtoawire,thewire
is a circuit input, accepting input values from an external source. If no element
input is connected to a wire, the wire is a circuit output, providing the results of
the circuit’s computation to the outside world. (An internal wire can also fan out
to a circuit output.) For the purpose of defining the circuit-satisfiability problem,
we limit the number of circuit outputs to 1, though in actual hardware design, a
booleancombinational circuitmayhavemultipleoutputs.
Boolean combinational circuits contain no cycles. In other words, suppose we
createadirectedgraphG .V;E/withonevertexforeachcombinationalelement
D
and with k directed edges for each wire whose fan-out is k; the graph contains
a directed edge .u;/ if a wire connects the output of element u to an input of
element. ThenG mustbeacyclic.
1072 Chapter34 NP-Completeness
x 1 1 x
1 1
x 1 1 x
2 1 2
0
1
1
0
1 1
1
1 1
x 0 1 x
3 3
1
(a) (b)
Figure 34.8 Two instances of the circuit-satisfiability problem. (a) The assignment x1 1;
h D
x2 1;x3 0 to the inputs of this circuit causes the output of the circuit to be 1. The circuit
D D i
isthereforesatisfiable. (b)Noassignment totheinputsof thiscircuit cancause theoutput of the
circuittobe1.Thecircuitisthereforeunsatisfiable.
Atruthassignmentforabooleancombinationalcircuitisasetofbooleaninput
values. We say that a one-output boolean combinational circuit is satisfiable if it
hasasatisfyingassignment: atruthassignmentthatcausestheoutputofthecircuit
to be 1. For example, the circuit in Figure 34.8(a) has the satisfying assignment
x 1;x 1;x 0 , and so it is satisfiable. As Exercise 34.3-1 asks you to
1 2 3
h D D D i
show,noassignmentofvaluestox ,x ,andx causesthecircuitinFigure34.8(b)
1 2 3
toproducea1output;italwaysproduces0,andsoitisunsatisfiable.
The circuit-satisfiability problem is, “Given a boolean combinational circuit
composed of AND, OR, and NOT gates, is it satisfiable?” In order to pose this
question formally, however, we must agree on a standard encoding for circuits.
The size of a boolean combinational circuit is the number of boolean combina-
tionalelementsplusthenumberofwiresinthecircuit. Wecoulddeviseagraphlike
encoding that maps any given circuit C into a binary string C whose length is
h i
polynomial inthesizeofthecircuit itself. Asaformallanguage, wecantherefore
define
CIRCUIT-SAT C C isasatisfiable booleancombinational circuit :
D fh iW g
The circuit-satisfiability problem arises in the area of computer-aided hardware
optimization. If a subcircuit always produces 0, that subcircuit is unnecessary;
the designer can replace it by a simpler subcircuit that omits all logic gates and
providestheconstant0valueasitsoutput. Youcanseewhywewouldliketohave
apolynomial-time algorithm forthisproblem.
Given a circuit C, we might attempt to determine whether it is satisfiable by
simplycheckingallpossibleassignmentstotheinputs. Unfortunately,ifthecircuit
has k inputs, then we would have to check up to 2k possible assignments. When
34.3 NP-completenessandreducibility 1073
the size of C is polynomial in k, checking each one takes .2k/ time, which is
superpolynomial in the size of the circuit.9 In fact, as we have claimed, there is
strong evidence that no polynomial-time algorithm exists that solves the circuit-
satisfiability problem because circuit satisfiability is NP-complete. We break the
proof of this fact into two parts, based on the two parts of the definition of NP-
completeness.
Lemma34.5
Thecircuit-satisfiability problembelongs totheclassNP.
Proof Weshallprovideatwo-input,polynomial-timealgorithmAthatcanverify
CIRCUIT-SAT. OneoftheinputstoAis(astandard encoding of)aboolean com-
binationalcircuitC. Theotherinputisacertificatecorrespondingtoanassignment
ofbooleanvaluestothewiresinC. (SeeExercise34.3-4forasmallercertificate.)
We construct the algorithm A as follows. For each logic gate in the circuit, it
checks that the value provided by the certificate on the output wire is correctly
computed asafunction ofthevaluesontheinput wires. Then, iftheoutput ofthe
entire circuit is 1, the algorithm outputs 1, since the values assigned to the inputs
ofC provideasatisfying assignment. Otherwise,Aoutputs 0.
WheneverasatisfiablecircuitC isinputtoalgorithmA,thereexistsacertificate
whoselengthispolynomialinthesizeofC andthatcausesAtooutputa1. When-
ever an unsatisfiable circuit is input, no certificate can fool A into believing that
thecircuitissatisfiable. AlgorithmArunsinpolynomial time: withagoodimple-
mentation, linear timesuffices. Thus, wecanverify CIRCUIT-SATinpolynomial
time,andCIRCUIT-SAT NP.
2
The second part of proving that CIRCUIT-SAT is NP-complete is to show that
the language is NP-hard. That is, we must show that every language in NP is
polynomial-time reducible to CIRCUIT-SAT. The actual proof of this fact is full
of technical intricacies, and so we shall settle for a sketch of the proof based on
someunderstanding oftheworkings ofcomputerhardware.
A computer program is stored in the computer memory as a sequence of in-
structions. A typical instruction encodes an operation to be performed, addresses
of operands in memory, and an address where the result is to be stored. A spe-
cial memory location, called the program counter, keeps track of which instruc-
9On the other hand, if the size of the circuit C is ‚.2k/, then an algorithm whose running time
is O.2k/ has a running time that is polynomial in the circuit size. Even if P NP, this situa-
¤
tionwouldnotcontradicttheNP-completenessoftheproblem; theexistenceofapolynomial-time
algorithmforaspecialcasedoesnotimplythatthereisapolynomial-timealgorithmforallcases.
1074 Chapter34 NP-Completeness
tion is to be executed next. The program counter automatically increments upon
fetching eachinstruction, thereby causing thecomputer toexecuteinstructions se-
quentially. The execution of an instruction can cause a value to be written to the
programcounter,however,whichaltersthenormalsequentialexecutionandallows
thecomputertoloopandperform conditional branches.
At any point during the execution of a program, the computer’s memory holds
the entire state of the computation. (We take the memory to include the program
itself, the program counter, working storage, and any of the various bits of state
that a computer maintains for bookkeeping.) We call any particular state of com-
puter memory a configuration. We can view the execution of an instruction as
mapping one configuration to another. The computer hardware that accomplishes
this mapping can be implemented as a boolean combinational circuit, which we
denotebyM intheproofofthefollowinglemma.
Lemma34.6
Thecircuit-satisfiability problem isNP-hard.
Proof Let L be any language in NP. We shall describe a polynomial-time algo-
rithm F computing a reduction function f that maps every binary string x to a
circuitC f.x/suchthatx LifandonlyifC CIRCUIT-SAT.
D 2 2
Since L NP, there must exist an algorithm A that verifies L in polynomial
2
time. The algorithm F that we shall construct uses the two-input algorithm A to
computethereduction function f.
Let T.n/ denote the worst-case running time of algorithm A on length-n input
strings, andletk 1beaconstant such thatT.n/ O.nk/andthelength ofthe
 D
certificate is O.nk/. (The running time of A is actually a polynomial in the total
inputsize,whichincludesbothaninputstringandacertificate,butsincethelength
ofthecertificateispolynomial inthelength noftheinput string, therunning time
ispolynomial inn.)
The basic idea of the proof is to represent the computation of A as a sequence
ofconfigurations. AsFigure34.9illustrates, wecanbreak eachconfiguration into
parts consisting ofthe program for A, the program counter and auxiliary machine
state, the input x, the certificate y, and working storage. The combinational cir-
cuit M, which implements the computer hardware, maps each configuration c to
i
thenextconfiguration c ,startingfromtheinitialconfiguration c . AlgorithmA
i 1 0
writes its output—0 or 1C —to some designated location by the time it finishes ex-
ecuting, and if we assume that thereafter A halts, the value never changes. Thus,
if the algorithm runs for at most T.n/ steps, the output appears as one of the bits
inc .
T.n/
The reduction algorithm F constructs a single combinational circuit that com-
putes all configurations produced by a given initial configuration. The idea is to
34.3 NP-completenessandreducibility 1075
A PC aux machine state x y working storage
M
A PC aux machine state x y working storage
M
A PC aux machine state x y working storage
M
A PC aux machine state x y
…
c
0
c
1
c
2
M
c working storage
T(n)
0/1 output
Figure34.9 ThesequenceofconfigurationsproducedbyanalgorithmArunningonaninputxand
certificatey.Eachconfigurationrepresentsthestateofthecomputerforonestepofthecomputation
and,besidesA,x,andy,includestheprogramcounter(PC),auxiliarymachinestate,andworking
storage.Exceptforthecertificatey,theinitialconfigurationc0isconstant.Abooleancombinational
circuitM mapseachconfigurationtothenextconfiguration. Theoutputisadistinguishedbitinthe
workingstorage.
1076 Chapter34 NP-Completeness
paste together T.n/ copies of the circuit M. The output of the ith circuit, which
producesconfigurationc ,feedsdirectlyintotheinputofthe.i 1/stcircuit. Thus,
i
C
the configurations, rather than being stored in the computer’s memory, simply re-
sideasvaluesonthewiresconnecting copiesofM.
Recall what the polynomial-time reduction algorithm F must do. Given an in-
put x, it must compute a circuit C f.x/ that is satisfiable if and only if there
D
exists a certificate y such that A.x;y/ 1. When F obtains an input x, it first
D
computes n x and constructs a combinational circuit C consisting of T.n/
0
D j j
copiesofM. TheinputtoC isaninitialconfiguration corresponding toacompu-
0
tationonA.x;y/,andtheoutputistheconfiguration c .
T.n/
Algorithm F modifies circuit C slightly to construct the circuit C f.x/.
0
D
First, itwires the inputs toC corresponding to the program for A, the initial pro-
0
gram counter, the input x,and the initial state of memory directly to these known
values. Thus, the only remaining inputs to the circuit correspond to the certifi-
cate y. Second, it ignores all outputs from C , except for the one bit of c
0 T.n/
corresponding to the output of A. This circuit C, so constructed, computes
C.y/ A.x;y/ for any input y of length O.nk/. The reduction algorithm F,
D
whenprovidedaninputstringx,computessuchacircuitC andoutputsit.
Weneedtoprovetwoproperties. First,wemustshowthatF correctlycomputes
a reduction function f. That is, we must show that C is satisfiable if and only if
there exists a certificate y such that A.x;y/ 1. Second, we must show that F
D
runsinpolynomial time.
ToshowthatF correctlycomputesareductionfunction,letussupposethatthere
existsacertificatey oflengthO.nk/suchthatA.x;y/ 1. Then,ifweapplythe
D
bits of y to the inputs of C, the output of C is C.y/ A.x;y/ 1. Thus, if a
D D
certificate exists, then C is satisfiable. For the other direction, suppose that C is
satisfiable. Hence, there exists an input y to C such that C.y/ 1, from which
D
weconclude thatA.x;y/ 1. Thus,F correctly computesareduction function.
D
Tocompletetheproofsketch,weneedonlyshowthatF runsintimepolynomial
in n x . The first observation we make is that the number of bits required to
D j j
representaconfigurationispolynomialinn. TheprogramforAitselfhasconstant
size, independent of the length of its input x. The length of the input x is n, and
thelengthofthecertificatey isO.nk/. SincethealgorithmrunsforatmostO.nk/
steps, the amount of working storage required by A is polynomial in n as well.
(We assume that this memory is contiguous; Exercise 34.3-5 asks you to extend
the argument to the situation in which the locations accessed by A are scattered
across amuchlarger regionofmemoryandtheparticular patternofscattering can
differforeachinputx.)
The combinational circuit M implementing the computer hardware has size
polynomial inthelength ofaconfiguration, whichisO.nk/;hence, thesizeofM
is polynomial in n. (Most of this circuitry implements the logic of the memory
34.3 NP-completenessandreducibility 1077
system.) The circuit C consists of at most t O.nk/ copies of M, and hence it
D
has size polynomial in n. The reduction algorithm F can construct C from x in
polynomial time,sinceeachstepoftheconstruction takespolynomial time.
ThelanguageCIRCUIT-SATisthereforeatleastashardasanylanguageinNP,
andsinceitbelongstoNP,itisNP-complete.
Theorem34.7
Thecircuit-satisfiability problemisNP-complete.
Proof Immediate from Lemmas 34.5 and 34.6 and from the definition of NP-
completeness.
Exercises
34.3-1
VerifythatthecircuitinFigure34.8(b)isunsatisfiable.
34.3-2
Showthatthe relationisatransitiverelationonlanguages. Thatis,showthatif
P
L L andL L ,thenL L .
1 P 2 2 P 3 1 P 3
34.3-3
ProvethatL LifandonlyifL L.
P P
34.3-4
Show that we could have used a satisfying assignment as a certificate in an alter-
nativeproofofLemma34.5. Whichcertificatemakesforaneasierproof?
34.3-5
TheproofofLemma34.6assumesthattheworkingstorageforalgorithm Aoccu-
piesacontiguous regionofpolynomialsize. Whereintheproofdoweexploitthis
assumption? Arguethatthisassumption doesnotinvolve anylossofgenerality.
34.3-6
Alanguage Liscomplete foralanguage class C withrespect topolynomial-time
reductions if L 2 C and L 0 P Lfor all L 0 2 C. Show that ;and f0;1 g arethe
only languages in P that are not complete for P with respect to polynomial-time
reductions.
1078 Chapter34 NP-Completeness
34.3-7
Show that, with respect to polynomial-time reductions (see Exercise 34.3-6), Lis
completeforNPifandonlyifLiscompleteforco-NP.
34.3-8
The reduction algorithm F in the proof of Lemma 34.6 constructs the circuit
C f.x/ based on knowledge of x, A, and k. Professor Sartre observes that
D
the string x is input to F, but only the existence of A, k, and the constant factor
implicit in the O.nk/ running time is known to F (since the language L belongs
to NP), not their actual values. Thus, the professor concludes that F can’t possi-
blyconstruct thecircuit C and thatthelanguage CIRCUIT-SATisnotnecessarily
NP-hard. Explaintheflawintheprofessor’s reasoning.
34.4 NP-completeness proofs
We proved that the circuit-satisfiability problem is NP-complete by a direct proof
that L CIRCUIT-SAT for every language L NP. In this section, we shall
P 2
showhowtoprovethatlanguagesareNP-completewithoutdirectlyreducingevery
language in NP to the given language. We shall illustrate this methodology by
provingthatvariousformula-satisfiabilityproblemsareNP-complete. Section34.5
provides manymoreexamplesofthemethodology.
The following lemma is the basis of our method for showing that a language is
NP-complete.
Lemma34.8
IfLisalanguage suchthatL LforsomeL NPC,thenLisNP-hard. If,in
0 P 0 2
addition, L NP,thenL NPC.
2 2
Proof Since L is NP-complete, for all L NP, we have L L. By sup-
0 00 2 00 P 0
position, L L, and thus by transitivity (Exercise 34.3-2), we have L L,
0 P 00 P
whichshowsthatLisNP-hard. IfL NP,wealsohaveL NPC.
2 2
Inotherwords,byreducingaknownNP-completelanguageL toL,weimplic-
0
itly reduce every language in NP to L. Thus, Lemma 34.8 gives us a method for
provingthatalanguage LisNP-complete:
1. ProveL NP.
2
2. SelectaknownNP-completelanguage L.
0
34.4 NP-completenessproofs 1079
3. Describe an algorithm that computes a function f mapping every instance
x 0;1

ofL
0
toaninstancef.x/ofL.
2f g
4. Prove that the function f satisfies x L if and only if f.x/ L for all
0
2 2
x 0;1 .
2f g
5. Provethatthealgorithm computingf runsinpolynomial time.
(Steps 2–5 show that L is NP-hard.) This methodology of reducing from a sin-
gle known NP-complete language is far simpler than the more complicated pro-
cess of showing directly how to reduce from every language in NP. Proving
CIRCUIT-SAT NPChasgivenusa“footinthedoor.” Becauseweknowthatthe
2
circuit-satisfiability problem isNP-complete,wenowcanprovemuchmoreeasily
thatotherproblemsareNP-complete. Moreover,aswedevelopacatalogofknown
NP-complete problems, we will have more and more choices for languages from
whichtoreduce.
Formulasatisfiability
We illustrate the reduction methodology by giving an NP-completeness proof for
theproblem ofdetermining whetherabooleanformula,notacircuit,issatisfiable.
This problem has the historical honor of being the first problem ever shown to be
NP-complete.
Weformulatethe(formula)satisfiability problem intermsofthelanguage SAT
asfollows. Aninstance ofSATisaboolean formula composedof
1. nbooleanvariables: x ;x ;:::;x ;
1 2 n
2. m boolean connectives: any boolean function with one or two inputs and one
output, such as (AND), (OR), (NOT), (implication), (if and only
^ _ : ! $
if);and
3. parentheses. (Withoutlossofgenerality,weassumethattherearenoredundant
parentheses,i.e.,aformulacontainsatmostonepairofparenthesesperboolean
connective.)
Wecaneasilyencodeabooleanformula inalengththatispolynomialinn m.
C
Asinboolean combinational circuits, atruthassignmentforaboolean formula
is a set of values for the variables of , and a satisfying assignment is a truth
assignment that causes itto evaluate to1. A formula withasatisfying assignment
is a satisfiable formula. The satisfiability problem asks whether a given boolean
formulaissatisfiable;informal-language terms,
SAT isasatisfiableboolean formula :
D fh i W g
Asanexample,theformula
1080 Chapter34 NP-Completeness
..x x / .. x x / x // x
1 2 1 3 4 2
D ! _: : $ _ ^:
hasthesatisfying assignment x 0;x 0;x 1;x 1 ,since
1 2 3 4
h D D D D i
..0 0/ .. 0 1/ 1// 0 (34.2)
D ! _: : $ _ ^:
.1 .1 1// 1
D _: _ ^
.1 0/ 1
D _ ^
1;
D
andthusthisformula belongstoSAT.
Thenaivealgorithm todetermine whetheranarbitrary boolean formulaissatis-
fiabledoesnotruninpolynomialtime. Aformulawithnvariableshas2n possible
assignments. If the length of is polynomial in n, then checking every assign-
h i
ment requires .2n/ time, which is superpolynomial in the length of . As the
h i
followingtheorem shows,apolynomial-time algorithm isunlikely toexist.
Theorem34.9
Satisfiability ofboolean formulasisNP-complete.
Proof WestartbyarguingthatSAT NP. ThenweprovethatSATisNP-hardby
2
showingthatCIRCUIT-SAT SAT;byLemma34.8,thiswillprovethetheorem.
P
To show that SAT belongs to NP, we show that a certificate consisting of a
satisfying assignment for an input formula can be verified in polynomial time.
Theverifying algorithm simply replaces each variable in the formula with its cor-
responding value and then evaluates the expression, much as we did in equa-
tion (34.2) above. This task is easy to do in polynomial time. If the expression
evaluates to1,thenthealgorithm hasverifiedthattheformulaissatisfiable. Thus,
thefirstcondition ofLemma34.8forNP-completeness holds.
To prove that SAT is NP-hard, we show that CIRCUIT-SAT SAT. In other
P
words, we need to show how to reduce any instance of circuit satisfiability to an
instance of formula satisfiability in polynomial time. We can use induction to
express any boolean combinational circuit as a boolean formula. We simply look
at the gate that produces the circuit output and inductively express each of the
gate’s inputs asformulas. Wethen obtain theformula forthecircuit bywritingan
expression thatappliesthegate’sfunction toitsinputs’formulas.
Unfortunately, this straightforward method does not amount to a polynomial-
timereduction. AsExercise34.4-1 asksyoutoshow, shared subformulas—which
arise from gates whose output wires have fan-out of 2 or more—can cause the
sizeofthegeneratedformulatogrowexponentially. Thus,thereductionalgorithm
mustbesomewhatmoreclever.
Figure 34.10 illustrates how we overcome this problem, using as an example
the circuit from Figure 34.8(a). For each wire x in the circuit C, the formula
i
34.4 NP-completenessproofs 1081
x 1 x 5
x
2
x
8
x
6
x x
9 10
x x
x 4 7
3
Figure34.10 Reducingcircuitsatisfiabilitytoformulasatisfiability. Theformulaproducedbythe
reductionalgorithmhasavariableforeachwireinthecircuit.
has a variable x . Wecan now express how each gate operates as asmall formula
i
involving the variables of its incident wires. For example, the operation of the
outputANDgateisx .x x x /. Wecalleachofthesesmallformulasa
10 7 8 9
$ ^ ^
clause.
The formula produced by the reduction algorithm is the AND of the circuit-
output variable with the conjunction of clauses describing the operation of each
gate. Forthecircuitinthefigure,theformulais
x .x x /
10 4 3
D ^ $ :
.x .x x //
5 1 2
^ $ _
.x x /
6 4
^ $ :
.x .x x x //
7 1 2 4
^ $ ^ ^
.x .x x //
8 5 6
^ $ _
.x .x x //
9 6 7
^ $ _
.x .x x x //:
10 7 8 9
^ $ ^ ^
Givenacircuit C,itisstraightforward toproduce suchaformula inpolynomial
time.
Whyis the circuit C satisfiable exactly when the formula is satisfiable? IfC
hasasatisfying assignment, theneachwireofthecircuit hasawell-defined value,
and the output of the circuit is 1. Therefore, when we assign wire values to
variables in , each clause of evaluates to 1, and thus the conjunction of all
evaluates to 1. Conversely, if some assignment causes to evaluate to 1, the
circuit C is satisfiable by an analogous argument. Thus, we have shown that
CIRCUIT-SAT SAT,whichcompletes theproof.
P
1082 Chapter34 NP-Completeness
3-CNFsatisfiability
WecanprovemanyproblemsNP-completebyreducingfromformulasatisfiability.
The reduction algorithm must handle any input formula, though, and this require-
ment can lead to a huge number of cases that we must consider. We often prefer
to reduce from a restricted language of boolean formulas, so that weneed to con-
sider fewer cases. Of course, we must not restrict the language so much that it
becomespolynomial-timesolvable. Oneconvenientlanguageis3-CNFsatisfiabil-
ity,or3-CNF-SAT.
Wedefine3-CNFsatisfiability usingthefollowing terms. Aliteral inaboolean
formula is an occurrence of a variable or its negation. A boolean formula is in
conjunctive normal form, or CNF, if it is expressed as an AND of clauses, each
ofwhich istheORofone ormoreliterals. Aboolean formula isin3-conjunctive
normalform,or3-CNF,ifeachclausehasexactlythreedistinct literals.
Forexample,thebooleanformula
.x x x / .x x x / . x x x /
1 1 2 3 2 4 1 3 4
_: _: ^ _ _ ^ : _: _:
isin3-CNF.Thefirstofitsthreeclauses is.x x x /,whichcontains the
1 1 2
_: _:
threeliteralsx , x ,and x .
1 1 2
: :
In 3-CNF-SAT, we are asked whether a given boolean formula in 3-CNF is
satisfiable. The following theorem shows that a polynomial-time algorithm that
candeterminethesatisfiability ofbooleanformulasisunlikely toexist,evenwhen
theyareexpressed inthissimplenormalform.
Theorem34.10
Satisfiability ofboolean formulasin3-conjunctive normalformisNP-complete.
Proof The argument we used in the proof of Theorem 34.9 to show that SAT
2
NP applies equally well here to show that 3-CNF-SAT NP. By Lemma 34.8,
2
therefore, weneedonlyshowthatSAT 3-CNF-SAT.
P
Webreakthereductionalgorithmintothreebasicsteps. Eachstepprogressively
transforms theinputformula closertothedesired3-conjunctive normalform.
The first step is similar to the one used to prove CIRCUIT-SAT SAT in
P
Theorem 34.9. First, we construct a binary “parse” tree for the input formula ,
withliterals asleaves andconnectives asinternal nodes. Figure 34.11 showssuch
aparsetreefortheformula
..x x / .. x x / x // x : (34.3)
1 2 1 3 4 2
D ! _: : $ _ ^:
ShouldtheinputformulacontainaclausesuchastheORofseveralliterals,weuse
associativity toparenthesize theexpression fully sothateveryinternal node inthe
resulting tree has 1 or 2 children. We can now think of the binary parse tree as a
circuitforcomputing thefunction.
34.4 NP-completenessproofs 1083
y
1
^
y
2
x
y _ y : 2
3 4
! :
y
5
x x
1 2 _
y
6
x
$ 4
x x
: 1 3
Figure34.11 Thetreecorrespondingtotheformula ..x1 x2/ .. x1 x3/ x4// x2:
D ! _: : $ _ ^:
Mimicking the reduction in the proof of Theorem 34.9, we introduce a vari-
able y for the output of each internal node. Then, we rewrite the original for-
i
mula astheANDoftherootvariableandaconjunctionofclausesdescribingthe
operation ofeachnode. Fortheformula(34.3),theresulting expression is
y .y .y x //
0 1 1 2 2
D ^ $ ^:
.y .y y //
2 3 4
^ $ _
.y .x x //
3 1 2
^ $ !
.y y /
4 5
^ $ :
.y .y x //
5 6 4
^ $ _
.y . x x //:
6 1 3
^ $ : $
Observe that the formula thus obtained is a conjunction of clauses , each of
0 i0
which has at most 3 literals. The only requirement that we might fail to meet is
thateachclausehastobeanORof3literals.
Thesecondstepofthereductionconvertseachclause intoconjunctivenormal
i0
form. We construct a truth table for by evaluating all possible assignments to
i0
its variables. Each row of the truth table consists of a possible assignment of the
variablesoftheclause,togetherwiththevalueoftheclauseunderthatassignment.
Using the truth-table entries that evaluate to 0, we build a formula in disjunctive
normal form (or DNF)—an OR of ANDs—that is equivalent to . We then
:
i0
negate this formula and convert it into a CNF formula by using DeMorgan’s
i00
1084 Chapter34 NP-Completeness
y1 y2 x2 .y1 .y2 x2//
$ ^:
1 1 1 0
1 1 0 1
1 0 1 0
1 0 0 0
0 1 1 1
0 1 0 0
0 0 1 1
0 0 0 1
Figure34.12 Thetruthtablefortheclause.y1 .y2 x2//.
$ ^:
lawsforpropositional logic,
.a b/ a b ;
: ^ D : _:
.a b/ a b ;
: _ D : ^:
tocomplementallliterals,change ORsintoANDs,andchangeANDsintoORs.
In our example, we convert the clause .y .y x // into CNF
10
D
1
$
2
^ :
2
as follows. The truth table for appears in Figure 34.12. The DNF formula
10
equivalent to is
:
10
.y y x / .y y x / .y y x / . y y x /:
1 2 2 1 2 2 1 2 2 1 2 2
^ ^ _ ^: ^ _ ^: ^: _ : ^ ^:
Negatingandapplying DeMorgan’slaws,wegettheCNFformula
. y y x / . y y x /
100
D :
1
_:
2
_:
2
^ :
1
_
2
_:
2
. y y x / .y y x /;
1 2 2 1 2 2
^ : _ _ ^ _: _
whichisequivalenttotheoriginalclause .
10
At this point, we have converted each clause of the formula into a CNF
i0 0
formula , and thus is equivalent to the CNF formula consisting of the
i00 0 00
conjunction ofthe . Moreover, eachclauseof hasatmost3literals.
i00 00
The third and final step of the reduction further transforms the formula so that
eachclausehasexactly3distinctliterals. Weconstructthefinal3-CNFformula
000
from the clauses of the CNFformula . The formula also uses two auxiliary
00 000
variables that we shall call p and q. For each clause C of , we include the
i 00
followingclauses in :
000
 IfC has3distinct literals, thensimplyinclude C asaclauseof .
i i 000
 IfC has2distinctliterals,thatis,ifC .l l /,wherel andl areliterals,
i i 1 2 1 2
D _
then include .l l p/ .l l p/ as clauses of . The literals
1 2 1 2 000
_ _ ^ _ _ :
p and p merely fulfill the syntactic requirement that each clause of has
000
:
34.4 NP-completenessproofs 1085
exactly 3 distinct literals. Whether p 0 or p 1, one of the clauses is
D D
equivalenttol l ,andtheotherevaluatesto1,whichistheidentityforAND.
1 2
_
 If C has just 1 distinct literal l, then include .l p q/ .l p q/
i
_ _ ^ _ _ : ^
.l p q/ .l p q/asclausesof . Regardlessofthevaluesofp
000
_: _ ^ _: _:
andq,oneofthefourclausesisequivalent tol,andtheother3evaluateto1.
Wecanseethatthe3-CNFformula issatisfiableifandonlyif issatisfiable
000
by inspecting each of the three steps. Like the reduction from CIRCUIT-SAT to
SAT, the construction of from in the first step preserves satisfiability. The
0
secondstepproduces aCNFformula thatisalgebraically equivalent to . The
00 0
third step produces a3-CNFformula that iseffectively equivalent to , since
000 00
any assignment to the variables p and q produces a formula that is algebraically
equivalent to .
00
Wemustalsoshowthatthereductioncanbecomputedinpolynomialtime. Con-
structing from introduces atmost1variableand1clauseperconnective in .
0
Constructing from can introduce at most 8 clauses into for each clause
00 0 00
from , since each clause of has at most 3 variables, and the truth table for
0 0
each clause has at most 23 8 rows. The construction of from introduces
000 00
D
at most 4 clauses into for each clause of . Thus, the size of the resulting
000 00
formula is polynomial in the length of the original formula. Each of the con-
000
structions caneasilybeaccomplished inpolynomial time.
Exercises
34.4-1
Consider thestraightforward (nonpolynomial-time) reduction intheproof ofThe-
orem 34.9. Describe a circuit of size n that, when converted to a formula by this
method,yieldsaformulawhosesizeisexponential inn.
34.4-2
Show the 3-CNF formula that results when we use the method of Theorem 34.10
ontheformula(34.3).
34.4-3
Professor Jagger proposes to show that SAT 3-CNF-SAT by using only the
P
truth-table technique in theproof ofTheorem 34.10, and not the other steps. That
is, the professor proposes to take the boolean formula , form a truth table for
its variables, derive from the truth table a formula in 3-DNF that is equivalent
to , and then negate and apply DeMorgan’s laws to produce a 3-CNF formula
:
equivalentto . Showthatthisstrategydoesnotyieldapolynomial-timereduction.
1086 Chapter34 NP-Completeness
34.4-4
Showthattheproblem ofdetermining whetheraboolean formula isatautology is
completeforco-NP. (Hint:SeeExercise34.3-7.)
34.4-5
Showthattheproblemofdetermining thesatisfiability ofbooleanformulasindis-
junctivenormalformispolynomial-time solvable.
34.4-6
Suppose that someone gives you a polynomial-time algorithm to decide formula
satisfiability. Describe how to use this algorithm to find satisfying assignments in
polynomial time.
34.4-7
Let 2-CNF-SAT be the set of satisfiable boolean formulas in CNF with exactly 2
literalsperclause. Showthat2-CNF-SAT P. Makeyouralgorithmasefficientas
2
possible. (Hint:Observethatx y isequivalentto x y. Reduce2-CNF-SAT
_ : !
toanefficientlysolvable problem onadirected graph.)
34.5 NP-complete problems
NP-completeproblemsariseindiversedomains: booleanlogic,graphs,arithmetic,
network design, setsandpartitions, storage andretrieval, sequencing andschedul-
ing, mathematical programming, algebra and number theory, games and puzzles,
automata andlanguage theory, program optimization, biology, chemistry, physics,
and more. Inthis section, weshall use the reduction methodology toprovide NP-
completeness proofs for a variety of problems drawn from graph theory and set
partitioning.
Figure34.13outlinesthestructureoftheNP-completenessproofsinthissection
and Section 34.4. We prove each language in the figure to be NP-complete by
reduction from the language that points toit. Attheroot isCIRCUIT-SAT, which
weprovedNP-completeinTheorem34.7.
34.5.1 Thecliqueproblem
A clique in an undirected graph G .V;E/ is asubset V V of vertices, each
0
D 
pairofwhichisconnected byanedgeinE. Inotherwords,acliqueisacomplete
subgraph of G. The size of a clique is the number of vertices it contains. The
cliqueproblemistheoptimizationproblemoffindingacliqueofmaximumsizein
34.5 NP-completeproblems 1087
CIRCUIT-SAT
SAT
3-CNF-SAT
CLIQUE SUBSET-SUM
VERTEX-COVER
HAM-CYCLE
TSP
Figure34.13 ThestructureofNP-completenessproofsinSections34.4and34.5. Allproofsulti-
matelyfollowbyreductionfromtheNP-completenessofCIRCUIT-SAT.
agraph. Asadecision problem, weask simply whether a clique of agiven size k
existsinthegraph. Theformaldefinitionis
CLIQUE G;k G isagraphcontaining acliqueofsizek :
D fh i W g
A naive algorithm for determining whether a graph G .V;E/ with V ver-
D j j
tices has a clique of size k is to list all k-subsets of V, and check each one to
see whether it forms a clique. The running time of this algorithm is .k2 jV j /,
k
whichispolynomialifkisaconstant. Ingeneral,however,kcouldbenear V =2,
j(cid:0)j 
in which case the algorithm runs in superpolynomial time. Indeed, an efficient
algorithm forthecliqueproblem isunlikelytoexist.
Theorem34.11
ThecliqueproblemisNP-complete.
Proof To show that CLIQUE NP, for a given graph G .V;E/, we use the
2 D
setV V ofverticesinthecliqueasacertificateforG. WecancheckwhetherV
0 0

is a clique in polynomial time by checking whether, for each pair u; V , the
0
2
edge.u;/belongs toE.
Wenextprovethat3-CNF-SAT CLIQUE,whichshowsthatthecliqueprob-
P
lem is NP-hard. You might be surprised that we should be able to prove such a
result,sinceonthesurfacelogical formulasseemtohavelittletodowithgraphs.
The reduction algorithm begins with an instance of 3-CNF-SAT. Let
D
C C C be a boolean formula in 3-CNF with k clauses. For r
1 2 k
^ ^  ^ D
1088 Chapter34 NP-Completeness
C x x x
1D 1_: 2_: 3
x x x
1 : 2 : 3
x x
: 1 1
C x x x x x C x x x
2D: 1_ 2_ 3 2 2 3D 1_ 2_ 3
x x
3 3
Figure34.14 Thegraph G derivedfromthe3-CNFformula C1 C2 C3, whereC1
D ^ ^ D
.x1 x2 x3/,C2 . x1 x2 x3/,andC3 .x1 x2 x3/,inreducing3-CNF-SATto
_: _: D : _ _ D _ _
CLIQUE. Asatisfyingassignmentoftheformulahasx2 0,x3 1,andx1 either0or1. This
D D
assignment satisfiesC1 with x2,anditsatisfiesC2 andC3 withx3, corresponding totheclique
:
withlightlyshadedvertices.
1;2;:::;k,eachclauseC hasexactlythreedistinctliteralslr,lr,andlr. Weshall
r 1 2 3
constructagraphG suchthat issatisfiableifandonlyifG hasacliqueofsizek.
We construct the graph G .V;E/ as follows. For each clause C
r
D D
.lr lr lr/ in , we place a triple of vertices r, r, and r into V. We put
1 _ 2 _ 3 1 2 3
anedgebetweentwoverticesr ands ifbothofthefollowinghold:
i j
 r ands areindifferent triples,thatis,r s,and
i j ¤
 theircorresponding literalsareconsistent,thatis,lr isnotthenegationofls.
i j
We can easily build this graph from in polynomial time. As an example of this
construction, ifwehave
.x x x / . x x x / .x x x /;
1 2 3 1 2 3 1 2 3
D _: _: ^ : _ _ ^ _ _
thenG isthegraphshowninFigure34.14.
Wemustshowthatthistransformation of intoG isareduction. First,suppose
that has a satisfying assignment. Then each clause C contains at least one
r
literallr thatisassigned1,andeachsuchliteralcorrespondstoavertexr. Picking
i i
onesuch“true”literalfromeachclauseyieldsasetV ofk vertices. Weclaimthat
0
V isaclique. Foranytwovertices r;s V ,wherer s, bothcorresponding
0 i j 2 0 ¤
literals lr andls mapto1bythe given satisfying assignment, and thus the literals
i j
34.5 NP-completeproblems 1089
cannotbecomplements. Thus,bytheconstruction ofG,theedge.r;s/belongs
i j
toE.
Conversely, suppose that G has a clique V of size k. No edges in G connect
0
verticesinthesametriple,andsoV containsexactlyonevertexpertriple. Wecan
0
assign 1 to each literal lr such that r V without fear of assigning 1 to both a
i i 2 0
literalanditscomplement,sinceG containsnoedgesbetweeninconsistentliterals.
Eachclauseissatisfied,andso issatisfied. (Anyvariablesthatdonotcorrespond
toavertexinthecliquemaybesetarbitrarily.)
In the example of Figure 34.14, a satisfying assignment of has x 0 and
2
D
x 1. Acorresponding clique ofsizek 3consists oftheverticescorrespond-
3
D D
ing to x from the first clause, x from the second clause, and x from the third
2 3 3
:
clause. Because theclique contains novertices corresponding toeitherx or x ,
1 1
:
wecansetx toeither0or1inthissatisfying assignment.
1
Observe that in the proof of Theorem 34.11, we reduced an arbitrary instance
of 3-CNF-SAT to an instance of CLIQUE with a particular structure. You might
think that we have shown only that CLIQUE is NP-hard in graphs in which the
vertices are restricted to occur in triples and in which there are no edges between
vertices in the same triple. Indeed, we have shown that CLIQUEis NP-hard only
in this restricted case, but this proof suffices to show that CLIQUE is NP-hard in
generalgraphs. Why? Ifwehadapolynomial-timealgorithmthatsolvedCLIQUE
ongeneral graphs,itwouldalsosolveCLIQUEonrestricted graphs.
Theoppositeapproach—reducing instancesof3-CNF-SATwithaspecialstruc-
ture to general instances of CLIQUE—would not have sufficed, however. Why
not? Perhaps the instances of 3-CNF-SAT that we chose to reduce from were
“easy,”andsowewouldnothavereducedanNP-hardproblem toCLIQUE.
Observe also that the reduction used the instance of 3-CNF-SAT, but not the
solution. We would have erred if the polynomial-time reduction had relied on
knowingwhethertheformula issatisfiable,sincewedonotknowhowtodecide
whether issatisfiable inpolynomial time.
34.5.2 Thevertex-cover problem
A vertex cover of an undirected graph G .V;E/ is a subset V V such that
0
D 
if .u;/ E, then u V or  V (or both). That is, each vertex “covers” its
0 0
2 2 2
incidentedges,andavertexcoverforG isasetofverticesthatcoversalltheedges
in E. The size of a vertex cover is the number of vertices in it. For example, the
graphinFigure34.15(b)hasavertexcover w;´ ofsize2.
f g
The vertex-cover problem is to find a vertex cover of minimum size in a given
graph. Restating this optimization problem as a decision problem, we wish to
1090 Chapter34 NP-Completeness
u v u v
z w z w
y x y x
(a) (b)
Figure34.15 ReducingCLIQUEtoVERTEX-COVER.(a)AnundirectedgraphG .V;E/with
D
cliqueV u;;x;y .(b)ThegraphGproducedbythereductionalgorithmthathasvertexcover
0Df g
V V w;´ .
(cid:0) 0Df g
determine whetheragraphhasavertexcoverofagivensizek. Asalanguage, we
define
VERTEX-COVER G;k graphG hasavertexcoverofsizek :
D fh i W g
Thefollowingtheorem showsthatthisproblem isNP-complete.
Theorem34.12
Thevertex-cover problem isNP-complete.
Proof WefirstshowthatVERTEX-COVER NP. Supposewearegivenagraph
2
G .V;E/andanintegerk. ThecertificatewechooseisthevertexcoverV V
0
D 
itself. Theverificationalgorithmaffirmsthat V k,andthenitchecks,foreach
0
j j D
edge .u;/ E, that u V or  V . We can easily verify the certificate in
0 0
2 2 2
polynomial time.
Weprovethatthevertex-coverproblemisNP-hardbyshowingthatCLIQUE
P
VERTEX-COVER. This reduction relies on the notion of the “complement” of a
graph. Given an undirected graph G .V;E/, we define the complement of G
D
as G .V;E/, where E .u;/ u; V;u ; and.u;/ E . In other
D D f W 2 ¤ 62 g
words,GisthegraphcontainingexactlythoseedgesthatarenotinG. Figure34.15
shows a graph and its complement and illustrates the reduction from CLIQUE to
VERTEX-COVER.
Thereductionalgorithm takesasinputaninstance G;k ofthecliqueproblem.
h i
It computes the complement G, which we can easily do in polynomial time. The
output of the reduction algorithm is the instance G; V k of the vertex-cover
h j j(cid:0) i
problem. To complete the proof, we show that this transformation is indeed a
34.5 NP-completeproblems 1091
reduction: thegraphG hasacliqueofsizekifandonlyifthegraphG hasavertex
coverofsize V k.
j j(cid:0)
Suppose that G hasaclique V V with V k. Weclaim that V V isa
0 0 0
 j j D (cid:0)
vertex cover in G. Let.u;/ be any edge in E. Then, .u;/ E, which implies
62
thatatleastoneofuor doesnotbelongtoV ,sinceeverypairofverticesinV is
0 0
connectedbyanedgeofE. Equivalently, atleastoneofuor isinV V ,which
0
(cid:0)
means that edge .u;/ is covered by V V . Since .u;/ was chosen arbitrarily
0
(cid:0)
fromE,everyedgeofE iscoveredbyavertexinV V . Hence,thesetV V ,
0 0
(cid:0) (cid:0)
whichhassize V k,formsavertexcoverforG.
j j(cid:0)
Conversely, suppose that G has avertex cover V V, where V V k.
0 0
 j j D j j(cid:0)
Then, for all u; V, if .u;/ E, then u V or  V or both. The
0 0
2 2 2 2
contrapositive of this implication is that for all u; V, if u V and  V ,
0 0
2 62 62
then.u;/ E. Inotherwords,V V isaclique,andithassize V V k.
0 0
2 (cid:0) j j(cid:0)j j D
SinceVERTEX-COVERisNP-complete,wedon’texpecttofindapolynomial-
time algorithm for finding a minimum-size vertex cover. Section 35.1 presents a
polynomial-time “approximation algorithm,” however, which produces “approxi-
mate”solutions forthevertex-cover problem. Thesizeofavertexcoverproduced
bythealgorithm isatmosttwicetheminimumsizeofavertexcover.
Thus, we shouldn’t give up hope just because a problem is NP-complete. We
may be able to design a polynomial-time approximation algorithm that obtains
near-optimal solutions, even though finding an optimal solution is NP-complete.
Chapter35givesseveralapproximation algorithms forNP-completeproblems.
34.5.3 Thehamiltonian-cycle problem
Wenowreturntothehamiltonian-cycle problem definedinSection34.2.
Theorem34.13
Thehamiltonian cycleproblem isNP-complete.
Proof We first show that HAM-CYCLE belongs to NP. Given a graph G
D
.V;E/,ourcertificateisthesequenceof V verticesthatmakesupthehamiltonian
j j
cycle. The verification algorithm checks that this sequence contains each vertex
inV exactlyonceandthatwiththefirstvertexrepeatedattheend,itformsacycle
in G. That is, it checks that there is an edge between each pair of consecutive
vertices and between the first and last vertices. We can verify the certificate in
polynomial time.
We now prove that VERTEX-COVER HAM-CYCLE, which shows that
P
HAM-CYCLE is NP-complete. Given an undirected graph G .V;E/ and an
D
1092 Chapter34 NP-Completeness
[u,v,1] [v,u,1] [u,v,1] [v,u,1] [u,v,1] [v,u,1] [u,v,1] [v,u,1]
[u,v,2] [v,u,2]
[u,v,3] [v,u,3]
W W W W
uv uv uv uv
[u,v,4] [v,u,4]
[u,v,5] [v,u,5]
[u,v,6] [v,u,6] [u,v,6] [v,u,6] [u,v,6] [v,u,6] [u,v,6] [v,u,6]
(a) (b) (c) (d)
Figure34.16 Thewidgetusedinreducingthevertex-coverproblemtothehamiltonian-cycleprob-
lem.Anedge.u;/ofgraphGcorrespondstowidgetWu inthegraphG 0createdinthereduction.
(a)Thewidget,withindividualverticeslabeled.(b)–(d)Theshadedpathsaretheonlypossibleones
throughthewidgetthatincludeallvertices,assumingthattheonlyconnectionsfromthewidgetto
theremainderofG arethroughverticesŒu;;1,Œu;;6,Œ;u;1,andŒ;u;6.
0
integer k, we construct an undirected graph G .V ;E / that has a hamiltonian
0 0 0
D
cycleifandonlyifG hasavertexcoverofsizek.
Ourconstruction usesawidget,whichisapieceofagraphthatenforces certain
properties. Figure34.16(a)showsthewidgetweuse. Foreachedge.u;/ E,the
2
graph G that weconstruct will contain one copy of this widget, which wedenote
0
byW . Wedenote eachvertexinW byŒu;;iorŒ;u;i,where1 i 6,so
u u
 
thateachwidgetW contains 12vertices. Widget W alsocontains the14edges
u u
showninFigure34.16(a).
Along with the internal structure of the widget, we enforce the properties we
want by limiting the connections between the widget and the remainder of the
graph G that we construct. In particular, only vertices Œu;;1, Œu;;6, Œ;u;1,
0
and Œ;u;6 will have edges incident from outside W . Any hamiltonian cycle
u
of G must traverse the edges of W in one of the three ways shown in Fig-
0 u
ures 34.16(b)–(d). If the cycle enters through vertex Œu;;1, it must exit through
vertex Œu;;6, anditeither visits all12ofthewidget’s vertices (Figure 34.16(b))
or the six vertices Œu;;1 through Œu;;6 (Figure 34.16(c)). In the latter case,
the cycle willhave toreenter the widget to visit vertices Œ;u;1 through Œ;u;6.
Similarly, if the cycle enters through vertex Œ;u;1, it must exit through ver-
tex Œ;u;6, and it either visits all 12 of the widget’s vertices (Figure 34.16(d)) or
thesixverticesŒ;u;1throughŒ;u;6(Figure34.16(c)). Nootherpathsthrough
the widget that visit all 12 vertices are possible. In particular, it is impossible to
construct twovertex-disjoint paths, one of whichconnects Œu;;1 to Œ;u;6 and
theotherofwhichconnectsŒ;u;1toŒu;;6,suchthattheunionofthetwopaths
contains allofthewidget’svertices.
34.5 NP-completeproblems 1093
w x
(a)
z y
s
1
s
2
(b)
[w,x,1] [x,w,1] [x,y,1] [y,x,1] [w,y,1] [y,w,1] [w,z,1] [z,w,1]
W W W W
wx xy wy wz
[w,x,6] [x,w,6] [x,y,6] [y,x,6] [w,y,6] [y,w,6] [w,z,6] [z,w,6]
Figure34.17 Reducinganinstanceofthevertex-coverproblemtoaninstanceofthehamiltonian-
cycleproblem. (a) Anundirected graph G withavertexcover of size2, consisting of thelightly
shadedverticeswandy. (b)TheundirectedgraphG 0producedbythereduction,withthehamilto-
nianpathcorrespondingtothevertexcovershaded. Thevertexcover w;y correspondstoedges
f g
.s1;Œw;x;1/and.s2;Œy;x;1/appearinginthehamiltoniancycle.
The only other vertices in V other than those of widgets are selector vertices
0
s ;s ;:::;s . We use edges incident on selector vertices in G to select the k
1 2 k 0
verticesofthecoverinG.
Inaddition totheedgesinwidgets,E contains twoothertypesofedges,which
0
Figure 34.17 shows. First, for each vertex u V, we add edges to join pairs
2
of widgets in order to form a path containing all widgets corresponding to edges
incident on u in G. We arbitrarily order the vertices adjacent to each vertex
u V as u.1/;u.2/;:::;u.degree.u//, where degree.u/ is the number of vertices
2
adjacent to u. We create a path in G through all the widgets corresponding
0
to edges incident on u by adding to E the edges .Œu;u.i/;6;Œu;u.i 1/;1/
0 C
f W
1 i degree.u/ 1 . In Figure 34.17, for example, we order the vertices ad-
  (cid:0) g
jacent to w as x;y;´, and so graph G in part (b) of the figure includes the edges
0
1094 Chapter34 NP-Completeness
.Œw;x;6;Œw;y;1/ and .Œw;y;6;Œw;´;1/. For each vertex u V, these edges
2
in G fill in a path containing all widgets corresponding to edges incident on u
0
inG.
Theintuitionbehindtheseedgesisthatifwechooseavertexu V inthevertex
2
cover of G, wecan construct apath from Œu;u.1/;1 to Œu;u.degree.u//;6 in G that
0
“covers”allwidgetscorrespondingtoedgesincidentonu. Thatis,foreachofthese
widgets, say W , the path either includes all 12 vertices (if u is in the vertex
u;u.i/
coverbutu.i/isnot)orjustthesixverticesŒu;u.i/;1;Œu;u.i/;2;:::;Œu;u.i/;6(if
bothuandu.i/ areinthevertexcover).
The final type of edge in E joins the first vertex Œu;u.1/;1 and the last vertex
0
Œu;u.degree.u//;6 ofeachofthese paths toeach oftheselector vertices. Thatis, we
include theedges
.s ;Œu;u.1/;1/ u V and1 j k
j
f W 2   g
.s ;Œu;u.degree.u//;6/ u V and1 j k :
j
[f W 2   g
Next, weshow that the size of G ispolynomial in the size ofG, and hence we
0
can construct G intimepolynomial inthe sizeofG. Thevertices ofG arethose
0 0
inthewidgets,plustheselectorvertices. With12verticesperwidget,plusk V
 j j
selector vertices, wehaveatotalof
V 12 E k
0
j j D j jC
12 E V
 j jCj j
vertices. Theedges ofG arethose inthewidgets, those thatgobetween widgets,
0
and those connecting selector vertices towidgets. Eachwidgetcontains 14edges,
totaling 14 E inallwidgets. Foreachvertexu V,graphG hasdegree.u/ 1
0
j j 2 (cid:0)
edgesgoingbetweenwidgets, sothatsummedoverallverticesinV,
.degree.u/ 1/ 2 E V
(cid:0) D j j(cid:0)j j
u V
X2
edges gobetween widgets. Finally, G has twoedges foreach pair consisting ofa
0
selector vertex andavertex ofV,totaling 2k V such edges. Thetotal numberof
j j
edgesofG istherefore
0
E .14 E / .2 E V / .2k V /
0
j j D j j C j j(cid:0)j j C j j
16 E .2k 1/ V
D j jC (cid:0) j j
16 E .2 V 1/ V :
 j jC j j(cid:0) j j
NowweshowthatthetransformationfromgraphG toG isareduction. Thatis,
0
wemustshowthatGhasavertexcoverofsizekifandonlyifG hasahamiltonian
0
cycle.
34.5 NP-completeproblems 1095
Suppose that G .V;E/ has a vertex cover V V of size k. Let

D 
V u ;u ;:::;u . As Figure 34.17 shows, we form a hamiltonian cy-
 1 2 k
D f g
cle in G by including the following edges10 for each vertex u V . Include
0 j 
2
edges .Œu j;u j.i/;6;Œu j;u j.i C1/;1/
W
1

i

degree.u j/ (cid:0)1 , which connect all
widgets corresponding to edges incident on u . We also include the edges within
j
˚ (cid:9)
thesewidgetsasFigures34.16(b)–(d)show,dependingonwhethertheedgeiscov-
eredbyoneortwoverticesinV . Thehamiltonian cyclealsoincludes theedges

.s ;Œu ;u.1/;1/ 1 j k
f j j j W   g
.s ;Œu
;u.degree.uj//
;6/ 1 j k 1
[f j C1 j j W   (cid:0) g
.s ;Œu ;u.degree.uk//;6/ :
[f 1 k k g
ByinspectingFigure34.17,youcanverifythattheseedgesformacycle. Thecycle
startsats ,visitsallwidgetscorresponding toedgesincident onu ,thenvisitss ,
1 1 2
visitsallwidgetscorresponding toedgesincident onu ,andsoon,until itreturns
2
tos . Thecyclevisitseachwidgeteitheronceortwice,dependingonwhetherone
1
or two vertices of V cover its corresponding edge. Because V is a vertex cover
 
forG,eachedgeinE isincidentonsomevertexinV ,andsothecyclevisitseach

vertex in each widget of G . Because the cycle also visits every selector vertex, it
0
ishamiltonian.
Conversely, suppose that G .V ;E / has a hamiltonian cycle C E . We
0 0 0 0
D 
claimthattheset
V u V .s ;Œu;u.1/;1/ C forsome1 j k (34.4)
 j
Df 2 W 2   g
is a vertex cover for G. To see why, partition C into maximal paths that start at
someselector vertex s ,traverse anedge .s ;Œu;u.1/;1/ forsomeu V, andend
i i
2
ataselectorvertexs withoutpassingthroughanyotherselectorvertex. Letuscall
j
each such path a“cover path.” From how G isconstructed, each cover path must
0
startatsomes ,taketheedge.s ;Œu;u.1/;1/forsomevertexu V,passthrough
i i
2
all the widgets corresponding to edges in E incident on u, and then end at some
selector vertex s . We refer to this cover path as p , and by equation (34.4), we
j u
put u into V . Each widget visited by p must be W or W for some  V.
 u u u
2
For each widget visited by p , its vertices are visited by either one or two cover
u
paths. If they are visited by one cover path, then edge .u;/ E iscovered in G
2
by vertex u. If two cover paths visit the widget, then the other cover path must
bep ,whichimpliesthat V ,andedge.u;/ E iscoveredbybothuand.
 
2 2
10Technically, we define a cycle in terms of vertices rather than edges (see Section B.4). In the
interestofclarity,weabusenotationhereanddefinethehamiltoniancycleintermsofedges.
1096 Chapter34 NP-Completeness
4
u v
1
3 2
1
x w
5
Figure34.18 Aninstanceofthetraveling-salesmanproblem.Shadededgesrepresentaminimum-
costtour,withcost7.
Becauseeachvertexineachwidgetisvisitedbysomecoverpath,weseethateach
edgeinE iscoveredbysomevertexinV .

34.5.4 Thetraveling-salesman problem
In the traveling-salesman problem, which is closely related to the hamiltonian-
cycleproblem,asalesmanmustvisitncities. Modelingtheproblemasacomplete
graph with n vertices, we can say that the salesman wishes to make a tour, or
hamiltonian cycle,visitingeachcityexactlyonceandfinishingatthecityhestarts
from. The salesman incurs a nonnegative integer cost c.i;j/ to travel from city i
to city j, and the salesman wishes to make the tour whose total cost is minimum,
where the total cost is the sum of the individual costs along the edges of the tour.
Forexample, in Figure 34.18, a minimum-cost tour is u;w;;x;u , withcost 7.
h i
Theformallanguage forthecorresponding decisionproblem is
TSP G;c;k G .V;E/isacompletegraph;
Dfh i W c isD afunctionfromV V N;
k N ,and  !
2
G hasatraveling-salesman tourwithcostatmostk :
g
The following theorem shows that a fast algorithm for the traveling-salesman
problem isunlikely toexist.
Theorem34.14
Thetraveling-salesman problemisNP-complete.
Proof Wefirst show that TSPbelongs to NP. Given an instance of the problem,
we use as a certificate the sequence of n vertices in the tour. The verification
algorithmchecksthatthissequencecontainseachvertexexactlyonce,sumsupthe
edgecosts,andcheckswhetherthesumisatmostk. Thisprocesscancertainlybe
doneinpolynomial time.
34.5 NP-completeproblems 1097
To prove that TSP is NP-hard, we show that HAM-CYCLE TSP. Let
P
G .V;E/beaninstance ofHAM-CYCLE. Weconstruct aninstance ofTSPas
D
follows. WeformthecompletegraphG .V;E /,whereE .i;j/ i;j V
0 0 0
D Df W 2
andi j ,andwedefinethecostfunction c by
¤ g
0 if.i;j/ E ;
c.i;j/ 2
D
(
1 if.i;j/ E :
62
(Notethatbecause G isundirected, ithasnoself-loops, and soc.;/ 1forall
D
vertices V.) TheinstanceofTSPisthen G ;c;0 ,whichwecaneasilycreate
0
2 h i
inpolynomial time.
WenowshowthatgraphG hasahamiltoniancycleifandonlyifgraphG hasa
0
tourofcostatmost0. SupposethatgraphG hasahamiltonian cycleh. Eachedge
in h belongs to E and thus has cost 0 in G . Thus, h is a tour in G with cost 0.
0 0
Conversely, suppose that graph G has a tour h of cost at most 0. Since the costs
0 0
oftheedgesinE are0and1,thecostoftourh isexactly0andeachedgeonthe
0 0
tourmusthavecost0. Therefore,h containsonlyedgesinE. Weconcludethath
0 0
isahamiltonian cycleingraphG.
34.5.5 Thesubset-sumproblem
WenextconsideranarithmeticNP-completeproblem. Inthesubset-sumproblem,
wearegivenafinitesetS ofpositive integers andanintegertarget t > 0. Weask
whether there exists a subset S S whose elements sum to t. For example,
0

if S 1;2;7;14;49;98;343;686;2409;2793;16808;17206;117705;117993
D f g
andt 138457, thenthesubsetS 1;2;7;98;343;686;2409;17206;117705
0
D D f g
isasolution.
Asusual, wedefinetheproblem asalanguage:
SUBSET-SUM S;t thereexistsasubsetS S suchthatt s :
D fh i W 0  D s S0 g
2
Aswithanyarithmeticproblem,itisimportanttorecallthatourstandaPrdencoding
assumesthattheinputintegers arecodedinbinary. Withthisassumption inmind,
wecanshowthatthesubset-sum problem isunlikelytohaveafastalgorithm.
Theorem34.15
Thesubset-sum problemisNP-complete.
Proof ToshowthatSUBSET-SUMisinNP,foraninstance S;t oftheproblem,
h i
welet thesubset S be thecertificate. Averification algorithm can check whether
0
t s inpolynomial time.
D s S0
We no2w show that 3-CNF-SAT SUBSET-SUM. Given a 3-CNF formula
P
P
overvariables x ;x ;:::;x withclauses C ;C ;:::;C ,each containing exactly
1 2 n 1 2 k
1098 Chapter34 NP-Completeness
three distinct literals, the reduction algorithm constructs an instance S;t of the
h i
subset-sum problem such that is satisfiable if and only if there exists a subset
ofS whosesumisexactlyt. Without lossofgenerality, wemaketwosimplifying
assumptions about the formula . First, no clause contains both avariable and its
negation, for such a clause is automatically satisfied by any assignment of values
to the variables. Second, each variable appears in at least one clause, because it
doesnotmatterwhatvalueisassigned toavariablethatappears innoclauses.
ThereductioncreatestwonumbersinsetS foreachvariablex andtwonumbers
i
in S for each clause C . We shall create numbers in base 10, where each number
j
containsn kdigitsandeachdigitcorrespondstoeitheronevariableoroneclause.
C
Base 10 (and other bases, as weshall see) has the property weneed of preventing
carriesfromlowerdigitstohigherdigits.
As Figure 34.19 shows, we construct set S and target t as follows. We label
eachdigitpositionbyeitheravariableoraclause. Theleastsignificantkdigitsare
labeled bytheclauses, andthemostsignificant ndigitsarelabeledbyvariables.
 The target t has a 1 in each digit labeled by a variable and a 4 in each digit
labeledbyaclause.
 For each variable x , set S contains two integers  and  . Each of  and 
i i i0 i i0
hasa1inthedigitlabeled byx and0sintheothervariable digits. Ifliteral x
i i
appears in clause C , then the digit labeled by C in  contains a 1. If lit-
j j i
eral x appears in clause C , then the digit labeled by C in  contains a 1.
:
i j j i0
Allotherdigitslabeledbyclausesin and are0.
i i0
All and valuesinsetS areunique. Why? Forl i,no or valuescan
i i0
¤
l l0
equal and inthemostsignificantndigits. Furthermore,byoursimplifying
i i0
assumptions above, no  and  can be equal in all k least significant digits.
i i0
If  and  were equal, then x and x would have to appear in exactly the
i i0 i
:
i
same set of clauses. But we assume that no clause contains both x and x
i i
:
and that either x or x appears in some clause, and so there must be some
i i
:
clauseC forwhich and differ.
j i i0
 ForeachclauseC ,setS containstwointegerss ands . Eachofs ands has
j j j0 j j0
0s in all digits other than the one labeled by C . For s , there is a 1 in the C
j j j
digit,ands hasa2inthisdigit. Theseintegersare“slackvariables,”whichwe
j0
usetogeteachclause-labeled digitposition toaddtothetargetvalueof4.
Simple inspection of Figure 34.19 demonstrates that all s and s values in S
j j0
areuniqueinsetS.
Notethatthegreatestsumofdigitsinanyonedigitpositionis6,whichoccursin
thedigitslabeled byclauses (three1sfromthe and values, plus1and2from
i i0
34.5 NP-completeproblems 1099
x x x C C C C
1 2 3 1 2 3 4
 1 = 1 0 0 1 0 0 1
 = 1 0 0 0 1 1 0
10
 2 = 0 1 0 0 0 0 1
 = 0 1 0 1 1 1 0
20
 3 = 0 0 1 0 0 1 1
 = 0 0 1 1 1 0 0
30
s 1 = 0 0 0 1 0 0 0
s = 0 0 0 2 0 0 0
10
s 2 = 0 0 0 0 1 0 0
s = 0 0 0 0 2 0 0
20
s 3 = 0 0 0 0 0 1 0
s = 0 0 0 0 0 2 0
30
s 4 = 0 0 0 0 0 0 1
s = 0 0 0 0 0 0 2
40
t = 1 1 1 4 4 4 4
Figure 34.19 The reduction of 3-CNF-SAT to SUBSET-SUM. The formula in 3-CNF is
D
C1 C2 C3 C4,whereC1 .x1 x2 x3/,C2 . x1 x2 x3/,C3 . x1 x2 x3/,
^ ^ ^ D _: _: D : _: _: D : _: _
andC4 .x1 x2 x3/. Asatisfyingassignmentof is x1 0;x2 0;x3 1 . ThesetS
D _ _ h D D D i
producedbythereductionconsistsofthebase-10numbersshown;readingfromtoptobottom,S
D
1001001;1000110;100001;101110;10011;11100;1000;2000;100;200;10;20;1;2 . Thetargett
f g
is1114444. ThesubsetS 0Sislightlyshaded,anditcontains 10, 20,and3,correspondingtothe
satisfyingassignment. Italsocontainsslackvariabless1,s 10,s 20,s3,s4,ands
40
toachievethetarget
valueof4inthedigitslabeledbyC1throughC4.
the s and s values). Interpreting these numbers in base 10, therefore, no carries
j j0
canoccurfromlowerdigitstohigherdigits.11
Wecan perform the reduction in polynomial time. The set S contains 2n 2k
C
values, eachofwhichhasn k digits, andthetimetoproduce eachdigit ispoly-
C
nomialinn k. Thetargett hasn k digits, andthereduction produces eachin
C C
constanttime.
Wenow show that the 3-CNF formula is satisfiable ifand only ifthere exists
asubsetS S whosesumist. First,supposethat hasasatisfying assignment.
0

Fori 1;2;:::;n,ifx 1inthisassignment, theninclude inS . Otherwise,
i i 0
D D
include  . In other words, we include in S exactly the  and  values that cor-
i0 0 i i0
11Infact,anybaseb,whereb 7,wouldwork. Theinstanceatthebeginningofthissubsectionis

thesetS andtargett inFigure34.19interpretedinbase7,withS listedinsortedorder.
1100 Chapter34 NP-Completeness
respond to literals with the value 1 in the satisfying assignment. Having included
either  or  , but not both, for all i, and having put 0 in the digits labeled by
i i0
variablesinalls ands ,weseethatforeachvariable-labeled digit,thesumofthe
j j0
values of S must be 1, which matches those digits of the target t. Because each
0
clause is satisfied, the clause contains some literal with the value 1. Therefore,
eachdigitlabeledbyaclausehasatleastone1contributed toitssumbya or
i i0
value inS . Infact, 1,2, or3literals maybe1ineach clause, andsoeach clause-
0
labeleddigithasasumof1,2,or3fromthe and valuesinS . InFigure34.19
i i0 0
forexample,literals x , x ,andx havethevalue1inasatisfying assignment.
1 2 3
: :
EachofclausesC andC containsexactlyoneoftheseliterals,andsotogether ,
1 4 10
 , and  contribute 1tothe sum in thedigits for C and C . Clause C contains
20 3 1 4 2
twoofthese literals, and ,  ,and  contribute 2tothe sum inthe digit forC .
10 20 3 2
Clause C contains allthree ofthese literals, and , ,and contribute 3tothe
3 10 20 3
suminthedigitforC . Weachievethetargetof4ineachdigitlabeledbyclauseC
3 j
by including in S the appropriate nonempty subset of slack variables s ;s . In
0
f
j j0
g
Figure34.19,S includess ,s ,s ,s ,s ,ands . Sincewehavematchedthetarget
0 1 10 20 3 4 40
inalldigitsofthesum,andnocarriescanoccur, thevaluesofS sumtot.
0
Now, suppose that there is a subset S S that sums to t. The subset S must
0 0

include exactly one of  and  for each i 1;2;:::;n, for otherwise the digits
i i0
D
labeled by variables would not sum to 1. If  S , we set x 1. Otherwise,
i 0 i
2 D
 S ,andwesetx 0. Weclaim thateveryclauseC ,forj 1;2;:::;k,is
i0
2
0 i
D
j
D
satisfiedbythisassignment. Toprovethisclaim,notethattoachieveasumof4in
the digit labeled by C , the subset S must include at least one  or  value that
j 0 i i0
has a 1 in the digit labeled by C , since the contributions of the slack variables s
j j
and s together sum to at most 3. If S includes a  that has a 1 in C ’s position,
j0 0 i j
then the literal x appears in clause C . Since we have set x 1 when  S ,
i j i i 0
D 2
clause C is satisfied. If S includes a  that has a 1 in that position, then the
j 0 i0
literal x appears in C . Since we have set x 0 when  S , clause C is
:
i j i
D
i0
2
0 j
againsatisfied. Thus,allclauses of aresatisfied, whichcompletestheproof.
Exercises
34.5-1
Thesubgraph-isomorphism problemtakestwoundirected graphsG andG ,and
1 2
it asks whether G is isomorphic to a subgraph of G . Show that the subgraph-
1 2
isomorphism problem isNP-complete.
34.5-2
Given an integer m n matrix A and an integer m-vector b, the 0-1 integer-

programming problem asks whether there exists an integer n-vector x with ele-
ProblemsforChapter34 1101
ments in the set 0;1 such that Ax b. Prove that 0-1 integer programming is
f g 
NP-complete. (Hint:Reducefrom3-CNF-SAT.)
34.5-3
The integer linear-programming problem is like the 0-1 integer-programming
problem given in Exercise 34.5-2, except that the values of the vector x may be
any integers rather than just 0 or 1. Assuming that the 0-1 integer-programming
problem is NP-hard, show that the integer linear-programming problem is NP-
complete.
34.5-4
Showhowtosolvethesubset-sumprobleminpolynomialtimeifthetargetvaluet
isexpressed inunary.
34.5-5
The set-partition problem takes as input a set S of numbers. The question is
whether the numbers can be partitioned into two sets A and A S A such
D (cid:0)
that x x. Showthattheset-partition problemisNP-complete.
x A D x A
2 2
34.5-P6 P
Showthatthehamiltonian-path problem isNP-complete.
34.5-7
The longest-simple-cycle problem is the problem of determining a simple cycle
(norepeatedvertices)ofmaximumlengthinagraph. Formulatearelateddecision
problem,andshowthatthedecision problem isNP-complete.
34.5-8
Inthe half3-CNFsatisfiability problem, weare given a3-CNFformula with n
variables and m clauses, where m is even. We wish to determine whether there
exists a truth assignment to the variables of such that exactly half the clauses
evaluate to0and exactly half the clauses evaluate to1. Provethat the half 3-CNF
satisfiability problem isNP-complete.
Problems
34-1 Independentset
An independent set of a graph G .V;E/ is a subset V V of vertices such
0
D 
that each edge in E is incident on at most one vertex in V . The independent-set
0
problem istofindamaximum-sizeindependent setinG.
1102 Chapter34 NP-Completeness
a. Formulate a related decision problem for the independent-set problem, and
provethatitisNP-complete. (Hint:Reducefromthecliqueproblem.)
b. Supposethatyouaregivena“black-box”subroutinetosolvethedecisionprob-
lemyoudefinedinpart(a). Giveanalgorithmtofindanindependentsetofmax-
imum size. The running time of your algorithm should be polynomial in V
j j
and E ,counting queriestotheblackboxasasinglestep.
j j
Although the independent-set decision problem is NP-complete, certain special
casesarepolynomial-time solvable.
c. Giveanefficientalgorithmtosolvetheindependent-setproblemwheneachver-
texinG hasdegree2. Analyzetherunningtime,andprovethatyouralgorithm
workscorrectly.
d. Give an efficient algorithm to solve the independent-set problem when G is
bipartite. Analyze the running time, and prove that your algorithm works cor-
rectly. (Hint:UsetheresultsofSection26.3.)
34-2 BonnieandClyde
Bonnie and Clyde have just robbed a bank. They have a bag of money and want
to divide itup. Foreach ofthe following scenarios, either give apolynomial-time
algorithm, or prove that the problem is NP-complete. The input in each case is a
listofthenitemsinthebag,alongwiththevalueofeach.
a. The bag contains n coins, but only 2 different denominations: some coins are
worthxdollars,andsomeareworthydollars. BonnieandClydewishtodivide
themoneyexactlyevenly.
b. Thebagcontainsncoins,withanarbitrarynumberofdifferentdenominations,
but each denomination is a nonnegative integer power of 2, i.e., the possible
denominations are1dollar, 2dollars, 4dollars, etc. Bonnie and Clydewishto
dividethemoneyexactlyevenly.
c. Thebagcontains nchecks, whichare,inanamazing coincidence, madeoutto
“Bonnie or Clyde.” They wish to divide the checks so that they each get the
exactsameamountofmoney.
d. The bag contains n checks as in part (c), but this time Bonnie and Clyde are
willingtoacceptasplitinwhichthedifference isnolargerthan100dollars.
ProblemsforChapter34 1103
34-3 Graphcoloring
Mapmakerstrytouseasfewcolorsaspossible whencoloringcountries onamap,
aslongasnotwocountries thatshareaborderhavethesamecolor. Wecanmodel
this problem with an undirected graph G .V;E/ in which each vertex repre-
D
sentsacountryandverticeswhoserespectivecountriesshareaborderareadjacent.
Then, ak-coloring is afunction c V 1;2;:::;k such that c.u/ c./for
W ! f g ¤
everyedge.u;/ E. Inotherwords,thenumbers1;2;:::;k representthekcol-
2
ors,andadjacentverticesmusthavedifferentcolors. Thegraph-coloringproblem
istodeterminetheminimumnumberofcolorsneededtocoloragivengraph.
a. Giveanefficientalgorithm todeterminea2-coloring ofagraph,ifoneexists.
b. Cast the graph-coloring problem as a decision problem. Show that your deci-
sion problem is solvable in polynomial time if and only if the graph-coloring
problem issolvable inpolynomial time.
c. Let the language 3-COLOR be the set of graphs that can be 3-colored. Show
that if 3-COLOR is NP-complete, then your decision problem from part (b) is
NP-complete.
To prove that 3-COLOR is NP-complete, we use a reduction from 3-CNF-SAT.
Givenaformula ofmclausesonnvariablesx ,x ,...,x ,weconstructagraph
1 2 n
G .V;E/ as follows. The set V consists of a vertex for each variable, a vertex
D
forthenegation ofeachvariable, 5verticesforeachclause, and3specialvertices:
TRUE, FALSE, and RED. The edges of the graph are of two types: “literal” edges
thatare independent ofthe clauses and “clause” edges that depend ontheclauses.
Theliteraledgesformatriangleonthespecialverticesandalsoformatriangleon
x i, x i,and RED fori 1;2;:::;n.
: D
d. Argue that in any 3-coloring c of a graph containing the literal edges, exactly
one of a variable and its negation is colored c.TRUE/ and the other is colored
c.FALSE/. Argue that for any truth assignment for , there exists a 3-coloring
ofthegraphcontaining justtheliteraledges.
ThewidgetshowninFigure34.20helpstoenforcethecondition corresponding to
aclause .x y ´/. Eachclauserequires aunique copyofthe5vertices thatare
_ _
heavilyshadedinthefigure;theyconnectasshowntotheliteralsoftheclauseand
thespecialvertex TRUE.
e. Argue that if each of x, y, and ´ is colored c.TRUE/ or c.FALSE/, then the
widgetis3-colorableifandonlyifatleastoneofx,y,or´iscoloredc.TRUE/.
f. Completetheproofthat3-COLORisNP-complete.
1104 Chapter34 NP-Completeness
x
y
TRUE
z
Figure34.20 Thewidgetcorrespondingtoaclause.x y ´/,usedinProblem34-3.
_ _
34-4 Schedulingwithprofitsanddeadlines
Suppose that we have one machine and a set of n tasks a ;a ;:::;a , each of
1 2 n
which requires time on the machine. Each task a requires t time units on the
j j
machine (its processing time), yields a profit of p , and has a deadline d . The
j j
machine can process only one task at a time, and task a must run without inter-
j
ruptionfort consecutive timeunits. Ifwecompletetaska byitsdeadlined ,we
j j j
receiveaprofitp ,butifwecompleteitafteritsdeadline, wereceivenoprofit. As
j
anoptimization problem,wearegiventheprocessing times,profits,anddeadlines
forasetofntasks, andwewishtofindaschedule thatcompletes allthetasksand
returns the greatest amount of profit. The processing times, profits, and deadlines
areallnonnegativenumbers.
a. Statethisproblem asadecisionproblem.
b. Showthatthedecision problem isNP-complete.
c. Give a polynomial-time algorithm for the decision problem, assuming that all
processing timesareintegersfrom1ton. (Hint:Usedynamicprogramming.)
d. Giveapolynomial-time algorithm fortheoptimization problem, assuming that
allprocessing timesareintegers from1ton.
Chapter notes
ThebookbyGareyandJohnson[129]providesawonderfulguidetoNP-complete-
ness, discussing the theory at length and providing a catalogue of many problems
that were known to be NP-complete in 1979. The proof of Theorem 34.13 is
adaptedfromtheirbook,andthelistofNP-completeproblemdomainsatthebegin-
ning of Section 34.5 is drawn from their table ofcontents. Johnson wrote aseries
NotesforChapter34 1105
of23columnsintheJournal ofAlgorithmsbetween1981and1992reporting new
developments in NP-completeness. Hopcroft, Motwani, and Ullman [177], Lewis
and Papadimitriou [236], Papadimitriou [270], and Sipser [317] have good treat-
ments of NP-completeness in the context of complexity theory. NP-completeness
and several reductions also appear in books by Aho, Hopcroft, and Ullman [5];
Dasgupta, Papadimitriou, and Vazirani [82]; Johnsonbaugh and Schaefer [193];
andKleinbergandTardos[208].
TheclassPwasintroducedin1964byCobham[72]and,independently, in1965
byEdmonds[100],whoalsointroducedtheclassNPandconjecturedthatP NP.
¤
The notion of NP-completeness was proposed in 1971 by Cook [75], who gave
the first NP-completeness proofs for formula satisfiability and 3-CNF satisfiabil-
ity. Levin [234] independently discovered the notion, giving an NP-completeness
proof for a tiling problem. Karp [199] introduced the methodology of reductions
in 1972 and demonstrated the rich variety of NP-complete problems. Karp’s pa-
per included the original NP-completeness proofs of the clique, vertex-cover, and
hamiltonian-cycle problems. Sincethen, thousands ofproblems havebeen proven
tobe NP-complete by manyresearchers. In atalk at ameeting celebrating Karp’s
60thbirthdayin1995,Papadimitriouremarked,“about6000paperseachyearhave
the term ‘NP-complete’ on their title, abstract, or list of keywords. This is more
than each of the terms ‘compiler,’ ‘database,’ ‘expert,’ ‘neural network,’ or ‘oper-
atingsystem.’ ”
Recentworkincomplexitytheoryhasshedlightonthecomplexityofcomputing
approximate solutions. Thiswork gives anew definition of NPusing “probabilis-
tically checkable proofs.” This new definition implies that for problems such as
clique, vertex cover, the traveling-salesman problem with the triangle inequality,
andmanyothers, computing good approximate solutions isNP-hardandhence no
easierthancomputingoptimalsolutions. Anintroduction tothisareacanbefound
in Arora’s thesis [20]; a chapter by Arora and Lund in Hochbaum [172]; a survey
article by Arora [21]; a book edited by Mayr, Pro¨mel, and Steger [246]; and a
surveyarticlebyJohnson[191].
35 Approximation Algorithms
Manyproblems ofpractical significance areNP-complete, yettheyaretooimpor-
tant toabandon merely because wedon’t know how tofindanoptimal solution in
polynomialtime. EvenifaproblemisNP-complete,theremaybehope. Wehaveat
leastthreewaystogetaroundNP-completeness. First,iftheactualinputsaresmall,
analgorithmwithexponential runningtimemaybeperfectly satisfactory. Second,
wemaybeable toisolate important special cases that wecansolve inpolynomial
time. Third, we might come up with approaches to find near-optimal solutions in
polynomial time (either in the worst case or the expected case). In practice, near-
optimality is often good enough. We call an algorithm that returns near-optimal
solutions anapproximation algorithm. Thischapterpresents polynomial-time ap-
proximation algorithms forseveralNP-completeproblems.
Performanceratiosforapproximation algorithms
Suppose that we are working on an optimization problem in which each potential
solutionhasapositivecost,andwewishtofindanear-optimalsolution. Depending
on the problem, we may define an optimal solution as one with maximum possi-
ble cost or one with minimum possible cost; that is, the problem may be either a
maximization oraminimizationproblem.
We say that an algorithm for a problem has an approximation ratio of .n/ if,
for any input of size n, the cost C of the solution produced by the algorithm is
withinafactorof.n/ofthecostC ofanoptimalsolution:

C C
max ;  .n/: (35.1)
C C 
  
Ifanalgorithm achieves anapproximation ratio of.n/,wecallita.n/-approx-
imation algorithm. The definitions of the approximation ratio and of a .n/-
approximation algorithm apply tobothminimization and maximization problems.
For a maximization problem, 0 < C C , and the ratio C =C gives the factor
 

bywhichthecost ofanoptimal solution islarger thanthecost oftheapproximate
Chapter35 ApproximationAlgorithms 1107
solution. Similarly,foraminimizationproblem,0< C C,andtheratioC=C
 

gives the factor by which the cost of the approximate solution is larger than the
cost of an optimal solution. Because we assume that all solutions have positive
cost, these ratios are always well defined. The approximation ratio of an approx-
imation algorithm is never less than 1, since C=C 1 implies C =C 1.
 
 
Therefore, a1-approximation algorithm1 produces anoptimalsolution, andanap-
proximation algorithm withalarge approximation ratio mayreturn asolution that
ismuchworsethanoptimal.
For many problems, we have polynomial-time approximation algorithms with
small constant approximation ratios, although for other problems, the best known
polynomial-time approximation algorithms have approximation ratios that grow
as functions of the input size n. An example of such a problem is the set-cover
problempresented inSection35.3.
SomeNP-complete problems allow polynomial-time approximation algorithms
that can achieve increasingly better approximation ratios by using more and more
computation time. That is, we can trade computation time for the quality of the
approximation. An example is the subset-sum problem studied in Section 35.5.
Thissituation isimportant enoughtodeserveanameofitsown.
Anapproximation schemeforanoptimization problem isanapproximation al-
gorithm that takes as input not only an instance of the problem, but also a value
 > 0such that foranyfixed,thescheme isa.1 /-approximation algorithm.
C
Wesaythatanapproximationschemeisapolynomial-timeapproximationscheme
ifforanyfixed > 0,theschemerunsintimepolynomialinthesizenofitsinput
instance.
Therunningtimeofapolynomial-timeapproximationschemecanincreasevery
rapidly as  decreases. For example, the running time of a polynomial-time ap-
proximation schememightbeO.n2=/. Ideally, if decreases byaconstantfactor,
therunningtimetoachievethedesiredapproximationshouldnotincreasebymore
thanaconstant factor (though notnecessarily thesameconstant factor bywhich
decreased).
Wesaythatanapproximationschemeisafullypolynomial-timeapproximation
scheme if it is an approximation scheme and its running time is polynomial in
both1= andthesizenoftheinputinstance. Forexample,theschememighthave
arunning timeofO..1=/2n3/. Withsuchascheme, anyconstant-factor decrease
in comeswithacorresponding constant-factor increaseintherunning time.
1Whentheapproximationratioisindependentofn,weusetheterms“approximationratioof”and
“-approximationalgorithm,”indicatingnodependenceonn.
1108 Chapter35 ApproximationAlgorithms
Chapteroutline
The first four sections of this chapter present some examples of polynomial-time
approximationalgorithmsforNP-completeproblems,andthefifthsectionpresents
a fully polynomial-time approximation scheme. Section 35.1 begins with a study
of the vertex-cover problem, an NP-complete minimization problem that has an
approximation algorithm with an approximation ratio of 2. Section 35.2 presents
an approximation algorithm with an approximation ratio of 2 for the case of the
traveling-salesman problem in which the cost function satisfies the triangle in-
equality. Italsoshowsthatwithout thetriangle inequality, foranyconstant  1,

a -approximation algorithm cannot exist unless P NP. In Section 35.3, we
D
show how touse agreedy method asaneffective approximation algorithm forthe
set-covering problem, obtaining a covering whose cost is at worst a logarithmic
factor larger thantheoptimal cost. Section 35.4presents twomoreapproximation
algorithms. First we study the optimization version of 3-CNF satisfiability and
giveasimple randomized algorithm that produces asolution withanexpected ap-
proximation ratioof8=7. Thenweexamineaweightedvariantofthevertex-cover
problem and show how to use linear programming to develop a 2-approximation
algorithm. Finally, Section 35.5 presents a fully polynomial-time approximation
schemeforthesubset-sum problem.
35.1 The vertex-cover problem
Section34.5.2definedthevertex-coverproblemandproveditNP-complete. Recall
that a vertex cover of an undirected graph G .V;E/ is a subset V V such
0
D 
thatif.u;/isanedgeofG,theneither u V or V (orboth). Thesizeofa
0 0
2 2
vertexcoveristhenumberofvertices init.
The vertex-cover problem is to find a vertex cover of minimum size in a given
undirected graph. Wecallsuchavertexcoveranoptimalvertex cover. Thisprob-
lemistheoptimization versionofanNP-completedecision problem.
Even though we don’t know how to find an optimal vertex cover in a graph G
in polynomial time, we can efficiently find a vertex cover that is near-optimal.
The following approximation algorithm takes as input an undirected graph G and
returns a vertex cover whose size is guaranteed to be no more than twice the size
ofanoptimalvertexcover.
35.1 Thevertex-coverproblem 1109
b c d b c d
a e f g a e f g
(a) (b)
b c d b c d
a e f g a e f g
(c) (d)
b c d b c d
a e f g a e f g
(e) (f)
Figure 35.1 The operation of APPROX-VERTEX-COVER. (a) The input graph G, which has 7
verticesand8edges.(b)Theedge.b;c/,shownheavy,isthefirstedgechosenbyAPPROX-VERTEX-
COVER. Verticesbandc,shownlightlyshaded,areaddedtothesetC containingthevertexcover
beingcreated.Edges.a;b/,.c;e/,and.c;d/,showndashed,areremovedsincetheyarenowcovered
bysomevertexinC. (c)Edge.e;f/ischosen; verticese andf areaddedtoC. (d)Edge.d;g/
ischosen; verticesd andg areaddedtoC. (e)ThesetC,whichisthevertexcoverproducedby
APPROX-VERTEX-COVER,containsthesixverticesb;c;d;e;f;g.(f)Theoptimalvertexcoverfor
thisproblemcontainsonlythreevertices:b,d,ande.
APPROX-VERTEX-COVER.G/
1 C
D ;
2 E G:E
0
D
3 whileE
0
¤;
4 let.u;/beanarbitrary edgeofE
0
5 C C u;
D [f g
6 removefromE everyedgeincidentoneitheruor
0
7 returnC
Figure 35.1 illustrates how APPROX-VERTEX-COVER operates on an example
graph. The variable C contains the vertex cover being constructed. Line 1 ini-
tializes C to the empty set. Line 2 sets E to be a copy of the edge set G:E of
0
thegraph. Theloop oflines 3–6 repeatedly picks anedge .u;/ from E , adds its
0
1110 Chapter35 ApproximationAlgorithms
endpoints u and  to C, and deletes all edges in E that are covered by either u
0
or. Finally,line7returnsthevertexcoverC. Therunningtimeofthisalgorithm
isO.V E/,usingadjacency liststorepresent E .
0
C
Theorem35.1
APPROX-VERTEX-COVER isapolynomial-time 2-approximation algorithm.
Proof We have already shown that APPROX-VERTEX-COVER runs in polyno-
mialtime.
The set C of vertices that is returned by APPROX-VERTEX-COVER is a vertex
cover,sincethealgorithmloopsuntileveryedgeinG:Ehasbeencoveredbysome
vertexinC.
ToseethatAPPROX-VERTEX-COVER returnsavertexcoverthatisatmosttwice
the size of an optimal cover, let A denote the set of edges that line 4 of APPROX-
VERTEX-COVER picked. In order to cover the edges in A, any vertex cover—in
particular, an optimal cover C —must include at least one endpoint of each edge

inA. Notwoedges inAshareanendpoint, sinceonceanedgeispicked inline4,
allotheredgesthatareincidentonitsendpointsaredeletedfromE inline6. Thus,
0
notwoedgesinAarecoveredbythesamevertexfromC ,andwehavethelower

bound
C A (35.2)

j j  j j
on the size of an optimal vertex cover. Each execution of line 4 picks an edge for
which neither of its endpoints is already in C, yielding an upper bound (an exact
upperbound, infact)onthesizeofthevertexcoverreturned:
C 2 A : (35.3)
j j D j j
Combiningequations (35.2)and(35.3),weobtain
C 2 A
j j D j j
2 C ;

 j j
thereby provingthetheorem.
Let us reflect on this proof. At first, you might wonder how we can possibly
provethatthesizeofthevertexcoverreturned byAPPROX-VERTEX-COVER isat
mosttwicethesizeofanoptimalvertexcover,whenwedonotevenknowthesize
of an optimal vertex cover. Instead of requiring that weknow the exact size of an
optimalvertexcover,werelyonalowerboundonthesize. AsExercise35.1-2asks
youtoshow,thesetAofedgesthatline4of APPROX-VERTEX-COVER selectsis
actually amaximalmatchinginthegraphG. (Amaximalmatchingisamatching
thatisnotapropersubsetofanyothermatching.) Thesizeofamaximalmatching
35.2 Thetraveling-salesmanproblem 1111
is, as we argued in the proof of Theorem 35.1, a lower bound on the size of an
optimal vertex cover. The algorithm returns a vertex cover whose size is at most
twice the size of the maximal matching A. By relating the size of the solution
returned to the lower bound, we obtain our approximation ratio. We will use this
methodology inlatersections aswell.
Exercises
35.1-1
Giveanexample ofagraph for which APPROX-VERTEX-COVER always yields a
suboptimal solution.
35.1-2
Prove that the set of edges picked in line 4 of APPROX-VERTEX-COVER forms a
maximalmatchinginthegraphG.
35.1-3 ?
Professor Bu¨ndchen proposes the following heuristic to solve the vertex-cover
problem. Repeatedly select a vertex of highest degree, and remove all of its in-
cidentedges. Giveanexampletoshowthattheprofessor’s heuristic doesnothave
an approximation ratio of 2. (Hint: Try a bipartite graph with vertices of uniform
degreeontheleftandverticesofvaryingdegreeontheright.)
35.1-4
Give an efficient greedy algorithm that finds an optimal vertex cover for a tree in
lineartime.
35.1-5
FromtheproofofTheorem34.12,weknowthatthevertex-cover problem andthe
NP-completecliqueproblemarecomplementaryinthesensethatanoptimalvertex
coveristhecomplementofamaximum-sizecliqueinthecomplementgraph. Does
this relationship imply that there is a polynomial-time approximation algorithm
withaconstant approximation ratioforthecliqueproblem? Justifyyouranswer.
35.2 The traveling-salesmanproblem
In the traveling-salesman problem introduced in Section 34.5.4, we are given a
completeundirected graphG .V;E/thathasanonnegative integercostc.u;/
D
associated with each edge .u;/ E, and we must find a hamiltonian cycle (a
2
tour) of G with minimum cost. As an extension of our notation, let c.A/ denote
thetotalcostoftheedgesinthesubsetA E:

1112 Chapter35 ApproximationAlgorithms
c.A/ c.u;/:
D
.u;/ A
X2
Inmanypracticalsituations,theleastcostlywaytogofromaplaceutoaplacew
istogodirectly, withnointermediate steps. Putanother way, cutting out aninter-
mediate stop never increases the cost. Weformalize this notion by saying that the
costfunction c satisfiesthetriangleinequalityif,forallverticesu;;w V,
2
c.u;w/ c.u;/ c.;w/:
 C
The triangle inequality seems as though it should naturally hold, and it is au-
tomatically satisfied in several applications. For example, if the vertices of the
graph are points in the plane and the cost of traveling between two vertices is the
ordinary euclidean distance betweenthem,then thetriangle inequality issatisfied.
Furthermore,manycostfunctionsotherthaneuclideandistancesatisfythetriangle
inequality.
AsExercise35.2-2shows,thetraveling-salesman problemisNP-completeeven
ifwerequire thatthecostfunction satisfy thetriangle inequality. Thus,weshould
not expect to find a polynomial-time algorithm for solving this problem exactly.
Instead, welookforgoodapproximation algorithms.
In Section 35.2.1, we examine a 2-approximation algorithm for the traveling-
salesman problem with the triangle inequality. In Section 35.2.2, we show that
withoutthetriangle inequality, apolynomial-time approximation algorithm witha
constant approximation ratiodoesnotexistunlessP NP.
D
35.2.1 Thetraveling-salesman problemwiththetriangleinequality
Applying the methodology ofthe previous section, weshall firstcompute astruc-
ture—aminimumspanningtree—whoseweightgivesalowerboundonthelength
of an optimal traveling-salesman tour. We shall then use the minimum spanning
treetocreateatourwhosecostisnomorethantwicethatoftheminimumspanning
tree’s weight, aslongasthecostfunction satisfies thetriangle inequality. Thefol-
lowing algorithm implements this approach, calling the minimum-spanning-tree
algorithm MST-PRIM from Section 23.2 as a subroutine. The parameter G is a
completeundirected graph, andthecostfunctionc satisfiesthetriangle inequality.
APPROX-TSP-TOUR.G;c/
1 selectavertexr G:V tobea“root”vertex
2
2 computeaminimumspanning treeT forG fromrootr
using MST-PRIM.G;c;r/
3 letH bealistofvertices, orderedaccording towhentheyarefirstvisited
inapreorder treewalkofT
4 returnthehamiltonian cycleH
35.2 Thetraveling-salesmanproblem 1113
a d a d a d
e e e
b f g b f g b f g
c c c
h h h
(a) (b) (c)
a d a d
e e
b f g b f g
c c
h h
(d) (e)
Figure35.2 TheoperationofAPPROX-TSP-TOUR.(a)Acompleteundirectedgraph.Verticeslie
onintersectionsofintegergridlines.Forexample,f isoneunittotherightandtwounitsupfromh.
Thecostfunctionbetweentwopointsistheordinaryeuclideandistance. (b)Aminimumspanning
treeT ofthecompletegraph,ascomputedbyMST-PRIM. Vertexaistherootvertex. Onlyedges
intheminimumspanningtreeareshown. Theverticeshappentobelabeledinsuchawaythatthey
areaddedtothemaintreeby MST-PRIMinalphabetical order. (c)AwalkofT,startingata. A
fullwalkofthetreevisitstheverticesintheordera;b;c;b;h;b;a;d;e;f;e;g;e;d;a. Apreorder
walkofT listsavertexjustwhenitisfirstencountered,asindicatedbythedotnexttoeachvertex,
yielding theordering a;b;c;h;d;e;f;g. (d) A tour obtained by visitingtheverticesin theorder
given by the preorder walk, which is the tour H returned by APPROX-TSP-TOUR. Itstotal cost
isapproximately19:074. (e)AnoptimaltourH fortheoriginalcompletegraph. Itstotalcostis

approximately14:715.
RecallfromSection12.1thatapreordertreewalkrecursivelyvisitseveryvertex
in the tree, listing a vertex when it is first encountered, before visiting any of its
children.
Figure35.2illustrates theoperation ofAPPROX-TSP-TOUR. Part(a)ofthefig-
ureshowsacompleteundirected graph,andpart(b)showstheminimumspanning
tree T grown from root vertex a by MST-PRIM. Part (c) shows how a preorder
walkofT visitsthevertices,andpart(d)displaysthecorresponding tour,whichis
thetourreturnedbyAPPROX-TSP-TOUR. Part(e)displaysanoptimaltour,which
isabout23%shorter.
1114 Chapter35 ApproximationAlgorithms
ByExercise23.2-2,evenwithasimpleimplementation ofMST-PRIM,therun-
ningtimeofAPPROX-TSP-TOURis‚.V2/. Wenowshowthatifthecostfunction
for an instance of the traveling-salesman problem satisfies the triangle inequality,
thenAPPROX-TSP-TOUR returnsatourwhosecostisnotmorethantwicethecost
ofanoptimaltour.
Theorem35.2
APPROX-TSP-TOUR is a polynomial-time 2-approximation algorithm for the
traveling-salesman problemwiththetriangle inequality.
Proof Wehavealreadyseenthat APPROX-TSP-TOUR runsinpolynomial time.
LetH denoteanoptimaltourforthegivensetofvertices. Weobtainaspanning

treebydeletinganyedgefromatour,andeachedgecostisnonnegative. Therefore,
theweightoftheminimumspanning treeT computed inline2of APPROX-TSP-
TOUR providesalowerboundonthecostofanoptimaltour:
c.T/ c.H /: (35.4)


A full walk of T lists the vertices when they are first visited and also whenever
theyarereturned toafteravisittoasubtree. LetuscallthisfullwalkW. Thefull
walkofourexamplegivestheorder
a;b;c;b;h;b;a;d;e;f;e;g;e;d;a :
Since the full walk traverses every edge of T exactly twice, we have (extending
ourdefinitionofthecostc inthenatural mannertohandle multisetsofedges)
c.W/ 2c.T/: (35.5)
D
Inequality (35.4)andequation (35.5)implythat
c.W/ 2c.H /; (35.6)


andsothecostofW iswithinafactorof2ofthecostofanoptimaltour.
Unfortunately, the full walk W is generally not a tour, since it visits some ver-
tices more than once. By the triangle inequality, however, wecan delete avisit to
anyvertexfromW andthecostdoesnotincrease. (Ifwedeleteavertex fromW
between visits to u and w, the resulting ordering specifies going directly from u
to w.) By repeatedly applying this operation, we can remove from W all but the
firstvisittoeachvertex. Inourexample,thisleavestheordering
a;b;c;h;d;e;f;g :
Thisordering isthesameasthatobtained byapreorder walkofthetreeT. LetH
bethecyclecorrespondingtothispreorderwalk. Itisahamiltoniancycle,sinceev-
35.2 Thetraveling-salesmanproblem 1115
eryvertexisvisitedexactlyonce,andinfactitisthecyclecomputedby APPROX-
TSP-TOUR. Since H is obtained by deleting vertices from the full walk W, we
have
c.H/ c.W/: (35.7)

Combininginequalities (35.6)and(35.7)givesc.H/ 2c.H /,whichcompletes


theproof.
In spite of the nice approximation ratio provided by Theorem 35.2, APPROX-
TSP-TOURisusuallynotthebestpracticalchoiceforthisproblem. Thereareother
approximation algorithms that typically perform much better in practice. (Seethe
references attheendofthischapter.)
35.2.2 Thegeneraltraveling-salesman problem
If wedrop the assumption that the cost function c satisfies the triangle inequality,
thenwecannotfindgoodapproximate toursinpolynomial timeunlessP NP.
D
Theorem35.3
IfP NP,thenforanyconstant 1,thereisnopolynomial-timeapproximation
¤ 
algorithm withapproximation ratioforthegeneraltraveling-salesman problem.
Proof Theproofisbycontradiction. Supposetothecontrarythatforsomenum-
ber  1, there is a polynomial-time approximation algorithm A with approx-

imation ratio . Without loss of generality, we assume that  is an integer, by
rounding it up if necessary. We shall then show how to use A to solve instances
of the hamiltonian-cycle problem (defined in Section 34.2) in polynomial time.
Since Theorem 34.13 tells us that the hamiltonian-cycle problem is NP-complete,
Theorem34.4impliesthatifwecansolveitinpolynomial time,thenP NP.
D
Let G .V;E/ be an instance of the hamiltonian-cycle problem. We wish to
D
determine efficiently whether G contains a hamiltonian cycle by making use of
the hypothesized approximation algorithm A. We turn G into an instance of the
traveling-salesman problem as follows. Let G .V;E / be the complete graph
0 0
D
onV;thatis,
E .u;/ u; V andu  :
0
D f W 2 ¤ g
AssignanintegercosttoeachedgeinE asfollows:
0
1 if.u;/ E ;
c.u;/ 2
D (  V 1 otherwise :
j jC
Wecancreaterepresentations ofG andc fromarepresentation ofG intimepoly-
0
nomialin V and E .
j j j j
1116 Chapter35 ApproximationAlgorithms
Now, consider the traveling-salesman problem .G ;c/. If the original graph G
0
has a hamiltonian cycle H, then the cost function c assigns to each edge of H a
cost of 1, and so .G ;c/ contains a tour of cost V . On the other hand, if G does
0
j j
notcontain ahamiltonian cycle,thenanytourofG mustusesomeedgenotinE.
0
ButanytourthatusesanedgenotinE hasacostofatleast
. V 1/ . V 1/  V V
j jC C j j(cid:0) D j jCj j
>  V :
j j
BecauseedgesnotinGaresocostly,thereisagapofatleast V betweenthecost
j j
of atour that is a hamiltonian cycle in G (cost V )and the cost of any other tour
j j
(cost at least  V V ). Therefore, the cost of a tour that is not a hamiltonian
j jCj j
cycle in G is at least a factor of  1 greater than the cost of a tour that is a
C
hamiltonian cycleinG.
Now, suppose that we apply the approximation algorithm A to the traveling-
salesman problem .G ;c/. Because A is guaranteed to return a tour of cost no
0
more than  times the cost of an optimal tour, if G contains a hamiltonian cycle,
then Amustreturn it. IfG hasno hamiltonian cycle, then Areturns atour ofcost
more than  V . Therefore, wecan use Atosolve the hamiltonian-cycle problem
j j
inpolynomial time.
The proof of Theorem 35.3 serves as an example of a general technique for
proving that we cannot approximate a problem very well. Suppose that given an
NP-hard problem X, we can produce in polynomial time a minimization prob-
lem Y such that “yes” instances of X correspond to instances of Y with value at
most k (for some k), but that “no” instances of X correspond to instances of Y
withvaluegreater thank. Then,wehaveshownthat, unless P NP,thereisno
D
polynomial-time -approximation algorithm forproblem Y.
Exercises
35.2-1
Suppose thatacomplete undirected graph G .V;E/withatleast 3vertices has
D
acostfunctionc thatsatisfiesthetriangleinequality. Provethatc.u;/ 0forall

u; V.
2
35.2-2
Show how in polynomial time we can transform one instance of the traveling-
salesman problem into another instance whose cost function satisfies the triangle
inequality. The two instances must have the same set of optimal tours. Explain
whysuchapolynomial-time transformation doesnotcontradict Theorem35.3,as-
sumingthatP NP.
¤
35.3 Theset-coveringproblem 1117
35.2-3
Consider the following closest-point heuristic for building an approximate trav-
eling-salesman tour whose cost function satisfies the triangle inequality. Begin
with a trivial cycle consisting of a single arbitrarily chosen vertex. At each step,
identifythevertexuthatisnotonthecyclebutwhosedistancetoanyvertexonthe
cycleisminimum. Supposethatthevertexonthecyclethatisnearestuisvertex.
Extend the cycle to include u by inserting u just after . Repeat until all vertices
areonthecycle. Provethatthisheuristicreturnsatourwhosetotalcostisnotmore
thantwicethecostofanoptimaltour.
35.2-4
Inthebottlenecktraveling-salesman problem,wewishtofindthehamiltoniancy-
clethatminimizesthecostofthemostcostlyedgeinthecycle. Assumingthatthe
cost function satisfies the triangle inequality, show that there exists a polynomial-
timeapproximation algorithm withapproximation ratio 3forthis problem. (Hint:
Show recursively that we can visit all the nodes in a bottleneck spanning tree, as
discussed inProblem23-3, exactlyoncebytakingafullwalkofthetreeandskip-
ping nodes, but without skipping more than two consecutive intermediate nodes.
Showthatthecostliest edgeinabottleneck spanning treehasacostthatisatmost
thecostofthecostliest edgeinabottleneck hamiltonian cycle.)
35.2-5
Suppose that the vertices for an instance of the traveling-salesman problem are
points in the plane and that the cost c.u;/ is the euclidean distance between
pointsuand. Showthatanoptimaltournevercrossesitself.
35.3 The set-covering problem
Theset-covering problem is an optimization problem that models many problems
thatrequireresources tobeallocated. Itscorresponding decisionproblem general-
izestheNP-completevertex-coverproblemandisthereforealsoNP-hard. Theap-
proximationalgorithmdevelopedtohandlethevertex-coverproblemdoesn’tapply
here,however,andsoweneedtotryotherapproaches. Weshallexamineasimple
greedy heuristic withalogarithmic approximation ratio. Thatis, asthesize ofthe
instance gets larger, thesize ofthe approximate solution maygrow, relative tothe
size of an optimal solution. Because the logarithm function grows rather slowly,
however,thisapproximation algorithm maynonetheless giveusefulresults.
1118 Chapter35 ApproximationAlgorithms
S
1
S
2
S
6
S S S
3 4 5
Figure35.3 Aninstance .X;F/ of the set-covering problem, where X consists of the 12 black
points and F S1;S2;S3;S4;S5;S6 . A minimum-size set cover is C S3;S4;S5 , with
D f g D f g
size 3. The greedy algorithm produces a cover of size 4 by selecting either the sets S1, S4, S5,
andS3orthesetsS1,S4,S5,andS6,inorder.
An instance .X;F/ of the set-covering problem consists of a finite set X and
a family F of subsets of X, such that every element of X belongs to at least one
F
subsetin :
X S :
D
S F
[2
WesaythatasubsetS F coversitselements. Theproblemistofindaminimum-
sizesubsetC F who2 sememberscoverallofX:

X S : (35.8)
D
S C
[2
Wesay that any C satisfying equation (35.8) covers X. Figure 35.3 illustrates the
C
set-covering problem. The size of is the number of sets it contains, rather than
thenumberofindividualelementsinthesesets,sinceeverysubsetC thatcoversX
must contain all X individual elements. In Figure 35.3, the minimum set cover
j j
hassize3.
Theset-coveringproblemabstractsmanycommonlyarisingcombinatorialprob-
lems. Asasimpleexample,supposethatX representsasetofskillsthatareneeded
tosolveaproblem andthatwehaveagivensetofpeople available toworkonthe
problem. We wish to form a committee, containing as few people as possible,
such that for every requisite skill in X, at least one member of the committee has
that skill. In the decision version of the set-covering problem, we ask whether a
covering exists with size at most k, where k is an additional parameter specified
in the problem instance. The decision version of the problem is NP-complete, as
Exercise35.3-2asksyoutoshow.
35.3 Theset-coveringproblem 1119
Agreedyapproximation algorithm
Thegreedymethodworksbypicking,ateachstage,thesetS thatcoversthegreat-
estnumberofremaining elementsthatareuncovered.
GREEDY-SET-COVER.X;F/
1 U X
C D
2
D ;
3 whileU
4 selec¤ tan; S F thatmaximizes S U
2 j \ j
5 U U S
6 C D C (cid:0) S
DC [f g
7 return
C
In the example of Figure 35.3, GREEDY-SET-COVER adds to , inorder, the sets
S ,S ,andS ,followedbyeitherS orS .
1 4 5 3 6
The algorithm works as follows. The set U contains, at each stage, the set of
C
remaining uncovered elements. The set contains the cover being constructed.
Line4isthegreedydecision-makingstep,choosingasubsetS thatcoversasmany
uncovered elements as possible (breaking ties arbitrarily). After S is selected,
line5removesitselementsfromU,andline6placesS intoC . Whenthealgorithm
terminates, thesetC contains asubfamilyofF thatcoversX.
WecaneasilyimplementGREEDY-SET-COVER torunintimepolynomialin X
F j j
and . Since the number of iterations of the loop on lines 3–6 is bounded from
abovj e bj y min. X ; F /, and we can implement the loop body to run in time
O. X F /,asj imj plej imj plementationrunsintimeO. X F min. X ; F //. Ex-
j jj j j jj j j j j j
ercise35.3-3asksforalinear-time algorithm.
Analysis
We now show that the greedy algorithm returns a set cover that is not too much
largerthananoptimalsetcover. Forconvenience,inthischapterwedenotethedth
harmonic number H d 1=i (see Section A.1) by H.d/. As a boundary
d D i 1
condition, wedefineH.0/ D0.
PD
Theorem35.4
GREEDY-SET-COVER isapolynomial-time .n/-approximation algorithm, where
.n/ H.max S S F /:
D fj j W 2 g
Proof We have already shown that GREEDY-SET-COVER runs in polynomial
time.
1120 Chapter35 ApproximationAlgorithms
To show that GREEDY-SET-COVER is a .n/-approximation algorithm, we as-
sign a cost of 1 to each set selected by the algorithm, distribute this cost over
the elements covered for the first time, and then use these costs to derive the de-
C
sired relationship between the size of an optimal set cover and the size of the

set cover C returned by the algorithm. Let S denote the ith subset selected by
i
GREEDY-SET-COVER; the algorithm incurs a cost of 1 when it adds S
i
to C . We
spreadthiscostofselectingS evenlyamongtheelementscoveredforthefirsttime
i
byS . Letc denotethecostallocatedtoelementx,foreachx X. Eachelement
i x
2
is assigned a cost only once, when it is covered for the first time. If x is covered
forthefirsttimebyS ,then
i
1
c :
x
D S .S S S /
i 1 2 i 1
j (cid:0) [ [[ (cid:0) j
Eachstepofthealgorithm assigns 1unitofcost,andso
C c : (35.9)
x
j j D
x X
EachelX e2 mentx X isinatleastonesetintheoptimalcoverC ,andsowehave

2
c c : (35.10)
x x

S Cx S x X
X2 X2 X2
Combiningequation (35.9)andinequality (35.10),wehavethat
C c : (35.11)
x
j j 
S Cx S
X2 X2
Theremainderoftheproofrestsonthefollowingkeyinequality, whichweshall
proveshortly. ForanysetS belonging tothefamilyF ,
c H. S /: (35.12)
x
 j j
x S
X2
Frominequalities (35.11)and(35.12),itfollowsthat
C H. S /
j j  j j
S C
X C2 H.max S S F /;

 j j fj j W 2 g
thusprovingthetheorem.
Allthatremainsistoproveinequality (35.12). ConsideranysetS F andany
i 1;2;:::; C ,andlet 2
D j j
u S .S S S /
i 1 2 i
D j (cid:0) [ [[ j
be the number of elements in S that remain uncovered after the algorithm has
selected sets S ;S ;:::;S . We define u S to be the number of elements
1 2 i 0
D j j
35.3 Theset-coveringproblem 1121
ofS, which areall initially uncovered. Letk bethe least index such that u 0,
k
D
sothateveryelement inS iscovered byatleastoneofthesetsS ;S ;:::;S and
1 2 k
some element in S is uncovered by S S S . Then, u u , and
1 2 k 1 i 1 i
u u elements of S are covered fo[ r the [ firs t ti[ me b(cid:0) y S , for i (cid:0) 1; 2;:::;k.
i 1 i i
(cid:0) (cid:0) D
Thus,
k
1
c .u u / :
x i 1 i
D (cid:0) (cid:0)  S i .S 1 S 2 S i 1/
Xx 2S Xi D1 j (cid:0) [ [[ (cid:0) j
Observethat
S .S S S / S .S S S /
i 1 2 i 1 1 2 i 1
j (cid:0) [ [[ (cid:0) j  j (cid:0) [ [[ (cid:0) j
u ;
i 1
D (cid:0)
because the greedy choice of S guarantees that S cannot cover more new ele-
i
mentsthan S does (otherwise, thealgorithm wouldhavechosen S instead ofS ).
i i
Consequently, weobtain
k
1
c .u u / :
x i 1 i
 (cid:0) (cid:0)  u i 1
Xx 2S Xi D1 (cid:0)
Wenowboundthisquantity asfollows:
k
1
c .u u /
x i 1 i
 (cid:0) (cid:0)  u i 1
Xx 2S Xi D1 (cid:0)
k ui(cid:0)1
1
D u
i 1
Xi D1j DXui C1 (cid:0)
k ui(cid:0)1
1
(because j u )
i 1
 j  (cid:0)
Xi D1j DXui C1
k ui(cid:0)1
1
ui
1
D j (cid:0) j
!
i 1 j 1 j 1
XD XD XD
k
.H.u / H.u //
i 1 i
D (cid:0) (cid:0)
i 1
XD
H.u / H.u / (because thesumtelescopes)
0 k
D (cid:0)
H.u / H.0/
0
D (cid:0)
H.u / (because H.0/ 0)
0
D D
H. S /;
D j j
whichcompletestheproofofinequality (35.12).
1122 Chapter35 ApproximationAlgorithms
Corollary 35.5
GREEDY-SET-COVER isapolynomial-time.ln X 1/-approximationalgorithm.
j jC
Proof Useinequality (A.14)andTheorem35.4.
Insomeapplications, max S S F isasmallconstant,andsothesolution
fj j W 2 g
returned by GREEDY-SET-COVER is at most a small constant times larger than
optimal. One such application occurs when this heuristic finds an approximate
vertex cover for a graph whose vertices have degree at most 3. In this case, the
solution found by GREEDY-SET-COVER is not more than H.3/ 11=6 times as
D
large as an optimal solution, a performance guarantee that is slightly better than
thatof APPROX-VERTEX-COVER.
Exercises
35.3-1
Consider each of the following words as a set of letters: arid;dash;drain;
f
heard;lost;nose;shun;slate;snare;thread . Show which set cover
g
GREEDY-SET-COVER produces when we break ties in favor of the word that ap-
pearsfirstinthedictionary.
35.3-2
Show that the decision version of the set-covering problem is NP-complete by
reducing itfromthevertex-coverproblem.
35.3-3
Show how to implement GREEDY-SET-COVER in such a way that it runs in time
O S .
S F j j
2
 
P
35.3-4
ShowthatthefollowingweakerformofTheorem35.4istriviallytrue:
C C max S S F :

j j  j j fj j W 2 g
35.3-5
GREEDY-SET-COVER can return a number of different solutions, depending on
how we break ties in line 4. Give a procedure BAD-SET-COVER-INSTANCE.n/
that returns ann-element instance oftheset-covering problem for which, depend-
ing on how we break ties in line 4, GREEDY-SET-COVER can return a number of
different solutions thatisexponential inn.
35.4 Randomizationandlinearprogramming 1123
35.4 Randomizationandlinearprogramming
Inthis section, westudy twouseful techniques for designing approximation algo-
rithms: randomizationandlinearprogramming. Weshallgiveasimplerandomized
algorithmforanoptimizationversionof3-CNFsatisfiability,andthenweshalluse
linearprogrammingtohelpdesignanapproximation algorithmforaweightedver-
sion of the vertex-cover problem. This section only scratches the surface of these
two powerful techniques. The chapter notes give references for further study of
theseareas.
Arandomizedapproximation algorithm forMAX-3-CNFsatisfiability
Just as some randomized algorithms compute exact solutions, some randomized
algorithms compute approximate solutions. We say that a randomized algorithm
for a problem has an approximation ratio of .n/ if, for any input of size n, the
expected cost C ofthesolution produced bytherandomized algorithm iswithina
factorof.n/ofthecostC ofanoptimalsolution:

C C
max ;  .n/: (35.13)
C C 
  
We call a randomized algorithm that achieves an approximation ratio of .n/ a
randomized .n/-approximation algorithm. In other words, a randomized ap-
proximationalgorithm islikeadeterministic approximation algorithm, exceptthat
theapproximation ratioisforanexpectedcost.
Aparticular instance of3-CNF satisfiability, asdefined in Section 34.4, mayor
maynotbesatisfiable. Inordertobesatisfiable, theremustexistanassignment of
thevariablessothateveryclauseevaluatesto1. Ifaninstanceisnotsatisfiable,we
maywanttocomputehow“close”tosatisfiableitis,thatis,wemaywishtofindan
assignment of the variables that satisfies as many clauses as possible. Wecall the
resultingmaximizationproblemMAX-3-CNFsatisfiability. TheinputtoMAX-3-
CNF satisfiability is the same as for 3-CNF satisfiability, and the goal is to return
an assignment of the variables that maximizes the number of clauses evaluating
to 1. We now show that randomly setting each variable to 1 with probability 1=2
and to 0 with probability 1=2 yields a randomized 8=7-approximation algorithm.
According to the definition of 3-CNF satisfiability from Section 34.4, we require
each clause to consist of exactly three distinct literals. We further assume that
no clause contains both a variable and its negation. (Exercise 35.4-1 asks you to
removethislastassumption.)
1124 Chapter35 ApproximationAlgorithms
Theorem35.6
Given an instance of MAX-3-CNF satisfiability with n variables x ;x ;:::;x
1 2 n
and m clauses, the randomized algorithm that independently sets each vari-
able to 1 with probability 1=2 and to 0 with probability 1=2 is a randomized
8=7-approximation algorithm.
Proof Suppose that we have independently set each variable to 1 with probabil-
ity1=2andto0withprobability 1=2. Fori 1;2;:::;m,wedefinetheindicator
D
random variable
Y I clausei issatisfied ;
i
D f g
so that Y 1 as long as we have set at least one of the literals in the ith clause
i
D
to1. Sincenoliteralappearsmorethanonceinthesameclause,andsincewehave
assumedthatnovariableanditsnegationappearinthesameclause,thesettingsof
thethreeliteralsineachclauseareindependent. Aclauseisnotsatisfiedonlyifall
threeofitsliteralsaresetto0,andsoPr clausei isnotsatisfied .1=2/3 1=8.
f g D D
Thus, we have Pr clausei issatisfied 1 1=8 7=8, and by Lemma 5.1,
f g D (cid:0) D
we have EŒY  7=8. Let Y be the number of satisfied clauses overall, so that
i
D
Y Y Y Y . Then,wehave
1 2 m
D C CC
m
EŒY E Y
i
D
" #
i 1
XD
m
EŒY  (bylinearity ofexpectation)
i
D
i 1
XD
m
7=8
D
i 1
XD
7m=8:
D
Clearly, m is an upper bound on the number of satisfied clauses, and hence the
approximation ratioisatmostm=.7m=8/ 8=7.
D
Approximatingweightedvertexcoverusinglinearprogramming
In the minimum-weight vertex-cover problem, we are given an undirected graph
G .V;E/ in which each vertex  V has an associated positive weight w./.
D 2
For any vertex cover V V, we define the weight of the vertex cover w.V /
0 0
 D
w./. Thegoalistofindavertexcoverofminimumweight.
 V0
W2ecannotapplythealgorithmusedforunweightedvertexcover,norcanweuse
P
a random solution; both methods may return solutions that are far from optimal.
Weshall, however, compute alower bound on the weight ofthe minimum-weight
35.4 Randomizationandlinearprogramming 1125
vertex cover, by using a linear program. We shall then “round” this solution and
useittoobtainavertexcover.
Suppose that we associate a variable x./ with each vertex  V, and let us
2
requirethatx./equalseither0or1foreach V. Weputintothevertexcover
2
ifandonlyifx./ 1. Then,wecanwritetheconstraintthatforanyedge.u;/,
D
atleastoneofuand mustbeinthevertexcoverasx.u/ x./ 1. Thisview
C 
gives rise to the following 0-1 integer program for finding a minimum-weight
vertexcover:
minimize w./x./ (35.14)
 V
X2
subjectto
x.u/ x./ 1 foreach.u;/ E (35.15)
C  2
x./ 0;1 foreach V : (35.16)
2 f g 2
In the special case in which all the weights w./ are equal to 1, this formu-
lation is the optimization version of the NP-hard vertex-cover problem. Sup-
pose, however, that we remove the constraint that x./ 0;1 and replace it
2 f g
by0 x./ 1. Wethenobtainthefollowinglinearprogram,whichisknownas
 
thelinear-programming relaxation:
minimize w./x./ (35.17)
 V
X2
subjectto
x.u/ x./ 1 foreach.u;/ E (35.18)
C  2
x./ 1 foreach V (35.19)
 2
x./ 0 foreach V : (35.20)
 2
Any feasible solution to the 0-1 integer program in lines (35.14)–(35.16) is also
a feasible solution to the linear program in lines (35.17)–(35.20). Therefore, the
valueofanoptimalsolutiontothelinearprogramgivesalowerboundonthevalue
ofanoptimal solution tothe0-1integer program, andhence alowerbound onthe
optimalweightintheminimum-weightvertex-coverproblem.
Thefollowingprocedure usesthesolution tothelinear-programming relaxation
toconstructanapproximatesolutiontotheminimum-weightvertex-coverproblem:
1126 Chapter35 ApproximationAlgorithms
APPROX-MIN-WEIGHT-VC.G;w/
1 C
D ;
2 computex,anoptimalsolution tothelinearprogram inlines(35.17)–(35.20)
N
3 foreach V
2
4 ifx./ 1=2
N 
5 C C 
D [f g
6 returnC
The APPROX-MIN-WEIGHT-VC procedure works as follows. Line 1 initial-
izes the vertex cover to be empty. Line 2 formulates the linear program in
lines (35.17)–(35.20) and then solves this linear program. An optimal solution
gives each vertex  an associated value x./, where 0 x./ 1. We use this
N  N 
valuetoguidethechoiceofwhichverticestoaddtothevertexcoverC inlines3–5.
If x./ 1=2, weadd  to C; otherwise wedo not. In effect, weare “rounding”
N 
each fractional variable in the solution to the linear program to 0 or 1 in order to
obtainasolutiontothe0-1integerprograminlines(35.14)–(35.16). Finally,line6
returnsthevertexcoverC.
Theorem35.7
Algorithm APPROX-MIN-WEIGHT-VC isapolynomial-time 2-approximation al-
gorithm fortheminimum-weight vertex-coverproblem.
Proof Because there is apolynomial-time algorithm to solve the linear program
inline2,andbecause theforloopoflines3–5runsinpolynomial time, APPROX-
MIN-WEIGHT-VC isapolynomial-time algorithm.
Now we show that APPROX-MIN-WEIGHT-VC is a 2-approximation algo-
rithm. Let C be an optimal solution to the minimum-weight vertex-cover prob-

lem, and let ´ be the value of an optimal solution to the linear program in

lines (35.17)–(35.20). Since an optimal vertex cover is a feasible solution to the
linearprogram,´ mustbealowerboundonw.C /,thatis,
 
´ w.C /: (35.21)
 

Next, we claim that by rounding the fractional values of the variables x./, we
N
produce a set C that is a vertex cover and satisfies w.C/ 2´ . Tosee that C is


avertexcover, consider anyedge.u;/ E. Byconstraint (35.18), weknowthat
2
x.u/ x./ 1,whichimplies thatatleastoneofx.u/andx./isatleast1=2.
C  N N
Therefore,atleastoneofuand isincludedinthevertexcover,andsoeveryedge
iscovered.
Now,weconsider theweightofthecover. Wehave
35.4 Randomizationandlinearprogramming 1127
´ w./x./

D N
 V
X2
w./x./
 N
 V x./ 1=2
2 WXN 
1
w./
  2
 V x./ 1=2
2 WXN 
1
w./
D  2
 C
X2
1
w./
D 2
 C
X2
1
w.C/: (35.22)
D 2
Combininginequalities (35.21)and(35.22)gives
w.C/ 2´ 2w.C /;
 
 
andhence APPROX-MIN-WEIGHT-VC isa2-approximation algorithm.
Exercises
35.4-1
Showthatevenifweallowaclausetocontainbothavariableanditsnegation,ran-
domlysettingeachvariableto1withprobability1=2andto0withprobability1=2
stillyieldsarandomized 8=7-approximation algorithm.
35.4-2
TheMAX-CNFsatisfiability problem is like the MAX-3-CNFsatisfiability prob-
lem, except that it does not restrict each clause to have exactly 3 literals. Give a
randomized 2-approximation algorithm fortheMAX-CNFsatisfiability problem.
35.4-3
In the MAX-CUT problem, we are given an unweighted undirected graph G
D
.V;E/. Wedefineacut.S;V S/asinChapter23andtheweightofacutasthe
(cid:0)
number of edges crossing the cut. The goal is to find a cut of maximum weight.
Suppose thatforeach vertex,werandomly andindependently place  inS with
probability 1=2 and in V S with probability 1=2. Show that this algorithm is a
(cid:0)
randomized 2-approximation algorithm.
1128 Chapter35 ApproximationAlgorithms
35.4-4
Show that the constraints in line (35.19) are redundant in the sense that if we re-
movethem from thelinear program inlines (35.17)–(35.20), anyoptimal solution
totheresultinglinearprogram mustsatisfy x./ 1foreach V.
 2
35.5 The subset-sum problem
Recall from Section 34.5.5 that an instance of the subset-sum problem is a
pair .S;t/, where S is a set x ;x ;:::;x of positive integers and t is a posi-
1 2 n
f g
tive integer. This decision problem asks whether there exists a subset of S that
adds upexactly tothetarget value t. AswesawinSection34.5.5, thisproblem is
NP-complete.
The optimization problem associated with this decision problem arises in prac-
tical applications. In the optimization problem, we wish to find a subset of
x ;x ;:::;x whose sum is as large as possible but not larger than t. For ex-
1 2 n
f g
ample, wemayhaveatruck that cancarry nomorethan t pounds, andndifferent
boxestoship,theithofwhichweighsx pounds. Wewishtofillthetruckwithas
i
heavyaloadaspossible withoutexceeding thegivenweightlimit.
In thissection, wepresent an exponential-time algorithm that computes the op-
timal value for this optimization problem, and then we show how to modify the
algorithm sothatitbecomes afullypolynomial-time approximation scheme. (Re-
call that a fully polynomial-time approximation scheme has arunning timethat is
polynomial in1= aswellasinthesizeoftheinput.)
Anexponential-timeexactalgorithm
Suppose that we computed, for each subset S of S, the sum of the elements
0
in S , and then we selected, among the subsets whose sum does not exceed t,
0
the one whose sum was closest to t. Clearly this algorithm would return the op-
timal solution, but it could take exponential time. To implement this algorithm,
we could use an iterative procedure that, in iteration i, computes the sums of
all subsets of x ;x ;:::;x , using as a starting point the sums of all subsets
1 2 i
f g
of x ;x ;:::;x . Indoingso,wewouldrealizethatonceaparticularsubsetS
1 2 i 1 0
hadf a sum exceed(cid:0) ing g t, there would be no reason to maintain it, since no super-
set of S could be the optimal solution. We now give an implementation of this
0
strategy.
The procedure EXACT-SUBSET-SUM takes an input set S x 1;x 2;:::;x
n
D f g
and a target value t; we’ll see its pseudocode in a moment. This procedure it-
35.5 Thesubset-sumproblem 1129
eratively computes L , the list of sums of all subsets of x ;:::;x that do not
i 1 i
f g
exceedt,andthenitreturnsthemaximumvalueinL .
n
If L is a list of positive integers and x is another positive integer, then we let
L x denote the list of integers derived from Lby increasing each element ofL
C
byx. Forexample,ifL 1;2;3;5;9 ,thenL 2 3;4;5;7;11 . Wealsouse
Dh i C Dh i
thisnotation forsets,sothat
S x s x s S :
C D f C W 2 g
We also use an auxiliary procedure MERGE-LISTS.L;L 0/, which returns the
sorted list that is the merge of its two sorted input lists L and L with duplicate
0
valuesremoved. LiketheMERGEprocedureweusedinmergesort(Section2.3.1),
MERGE-LISTS runsintimeO. L L
0
/. Weomitthepseudocode for MERGE-
j jCj j
LISTS.
EXACT-SUBSET-SUM.S;t/
1 n S
D j j
2 L 0
0
D h i
3 fori 1ton
D
4 L
i
MERGE-LISTS.L
i
1;L
i 1
x i/
5 remD ovefromL everyele(cid:0) ment(cid:0) thaC tisgreaterthant
i
6 returnthelargest elementinL
n
To see how EXACT-SUBSET-SUM works, let P
i
denote the set of all values
obtained by selecting a (possibly empty) subset of x ;x ;:::;x and summing
1 2 i
f g
itsmembers. Forexample,ifS 1;4;5 ,then
D f g
P 0;1 ;
1
D f g
P 0;1;4;5 ;
2
D f g
P 0;1;4;5;6;9;10 :
3
D f g
Giventheidentity
P P .P x /; (35.23)
i i 1 i 1 i
D (cid:0) [ (cid:0) C
wecanprovebyinduction oni (seeExercise35.5-1)thatthelistL isasortedlist
i
containing every element of P whose value is not more than t. Since the length
i
ofL
i
canbeasmuchas2i,EXACT-SUBSET-SUM isanexponential-timealgorithm
ingeneral,althoughitisapolynomial-timealgorithminthespecialcasesinwhicht
ispolynomial in S orallthenumbersinS areboundedbyapolynomial in S .
j j j j
Afullypolynomial-timeapproximation scheme
We can derive a fully polynomial-time approximation scheme for the subset-sum
problemby“trimming”eachlistL afteritiscreated. Theideabehindtrimmingis
i
1130 Chapter35 ApproximationAlgorithms
thatiftwovaluesinLareclose toeach other, then sincewewantjust anapproxi-
matesolution, wedonotneedtomaintainbothofthemexplicitly. Moreprecisely,
weuse a trimming parameter ı such that 0 < ı < 1. When wetrim a list Lby ı,
we remove as many elements from L as possible, in such a way that if L is the
0
result of trimming L, then for every element y that wasremoved from L, there is
anelement´stillinL thatapproximates y,thatis,
0
y
´ y : (35.24)
1 ı  
C
We can think of such a ´ as “representing” y in the new list L. Each removed
0
element y is represented by a remaining element ´ satisfying inequality (35.24).
Forexample,ifı 0:1and
D
L 10;11;12;15;20;21;22;23;24;29 ;
Dh i
thenwecantrimLtoobtain
L 10;12;15;20;23;29 ;
0
Dh i
where the deleted value 11 is represented by 10, the deleted values 21 and 22
are represented by 20, and the deleted value 24 is represented by 23. Because
every element of the trimmed version of the list is also an element of the original
versionofthelist,trimmingcandramaticallydecreasethenumberofelementskept
whilekeepingaclose(andslightlysmaller)representative valueinthelistforeach
deleted element.
ThefollowingproceduretrimslistL y ;y ;:::;y intime‚.m/,givenL
1 2 m
Dh i
and ı, and assuming that L is sorted into monotonically increasing order. The
outputoftheprocedure isatrimmed,sortedlist.
TRIM.L;ı/
1 letmbethelengthofL
2 L y
0 1
D h i
3 last y
1
D
4 fori 2tom
D
5 ify > last .1 ı/ //y last becauseLissorted
i i
 C 
6 appendy ontotheendofL
i 0
7 last y
i
D
8 returnL
0
Theprocedure scanstheelementsofLinmonotonically increasing order. Anum-
ber is appended onto the returned list L only if it is the first element of L or if it
0
cannotberepresented bythemostrecentnumberplacedintoL.
0
Given the procedure TRIM, wecan construct our approximation scheme as fol-
lows. This procedure takes as input a set S x ;x ;:::;x of n integers (in
1 2 n
D f g
arbitrary order), atargetintegert,andan“approximation parameter” ,where
35.5 Thesubset-sumproblem 1131
0<  < 1: (35.25)
Itreturnsavalue´whosevalueiswithina1  factoroftheoptimalsolution.
C
APPROX-SUBSET-SUM.S;t;/
1 n S
D j j
2 L 0
0
D h i
3 fori 1ton
D
4 L
i
MERGE-LISTS.L
i
1;L
i 1
x i/
5 L
i
D TRIM.L i;=2n/ (cid:0) (cid:0) C
D
6 removefromL everyelementthatisgreaterthant
i
7 let´ bethelargest valueinL
 n
8 return´

Line 2 initializes the list L to be the list containing just the element 0. The for
0
loop in lines 3–6 computes L as a sorted list containing a suitably trimmed ver-
i
sion of the set P , with all elements larger than t removed. Since we create L
i i
fromL ,wemustensurethattherepeated trimmingdoesn’t introduce toomuch
i 1
(cid:0)
compounded inaccuracy. In a moment, we shall see that APPROX-SUBSET-SUM
returnsacorrectapproximation ifoneexists.
Asanexample,suppose wehavetheinstance
S 104;102;201;101
D h i
witht 308 and  0:40. Thetrimming parameter ı is =8 0:05. APPROX-
D D D
SUBSET-SUM computesthefollowingvaluesontheindicated lines:
line2: L 0 ;
0
D h i
line4: L 0;104 ;
1
D h i
line5: L 0;104 ;
1
D h i
line6: L 0;104 ;
1
D h i
line4: L 0;102;104;206 ;
2
D h i
line5: L 0;102;206 ;
2
D h i
line6: L 0;102;206 ;
2
D h i
line4: L 0;102;201;206;303;407 ;
3
D h i
line5: L 0;102;201;303;407 ;
3
D h i
line6: L 0;102;201;303 ;
3
D h i
line4: L 0;101;102;201;203;302;303;404 ;
4
D h i
line5: L 0;101;201;302;404 ;
4
D h i
line6: L 0;101;201;302 :
4
D h i
1132 Chapter35 ApproximationAlgorithms
The algorithm returns ´ 302 as its answer, which is well within  40% of

D D
theoptimalanswer307 104 102 101;infact,itiswithin2%.
D C C
Theorem35.8
APPROX-SUBSET-SUM is a fully polynomial-time approximation scheme for the
subset-sum problem.
Proof Theoperations of trimmingL inline 5and removing from L every ele-
i i
mentthat isgreater than t maintain theproperty thatevery element ofL isalso a
i
memberofP . Therefore,thevalue´ returnedinline8isindeedthesumofsome
i 
subset of S. Let y P denote an optimal solution to the subset-sum problem.
 n
2
Then, from line 6, weknow that ´ y . By inequality (35.1), weneed to show
 

that y =´ 1 . Wemustalso show thattherunning timeofthis algorithm is
 
 C
polynomial inboth1= andthesizeoftheinput.
AsExercise35.5-2asksyoutoshow,foreveryelementy inP thatisatmostt,
i
thereexistsanelement´ L suchthat
i
2
y
´ y : (35.26)
.1 =2n/i  
C
Inequality (35.26) must hold for y P , and therefore there exists an element
 n
2
´ L suchthat
n
2
y
 ´ y ;

.1 =2n/n  
C
andthus
y  n
 1 : (35.27)
´  C 2n
 
Since there exists an element ´ L fulfilling inequality (35.27), the inequality
n
2
mustholdfor´ ,whichisthelargestvalueinL ;thatis,
 n
y  n
 1 : (35.28)
´  C 2n

 
Now, we show that y =´ 1 . We do so by showing that .1 =2n/n
 
 C C 
1 . Byequation (3.14), wehave lim .1 =2n/n e=2. Exercise 35.5-3
n
C !1 C D
asksyoutoshowthat
d  n
1 > 0: (35.29)
dn C 2n
 
Therefore, the function .1 =2n/n increases with n as it approaches its limit
C
ofe=2,andwehave
35.5 Thesubset-sumproblem 1133
 n
1 e=2
C 2n 
  1 =2 .=2/2 (byinequality (3.13))
 C C
1  (byinequality (35.25)) . (35.30)
 C
Combininginequalities (35.28) and(35.30)completes theanalysis oftheapproxi-
mationratio.
Toshow that APPROX-SUBSET-SUM isafully polynomial-time approximation
scheme, we derive a bound on the length of L . After trimming, successive ele-
i
ments´and´ ofL musthavetherelationship´=´ >1 =2n. Thatis,theymust
0 i 0
C
differ by a factor of at least 1 =2n. Each list, therefore, contains the value 0,
C
possibly the value 1, and up to log t additional values. The number of
1 =2n
elementsineachlistL isatmost C
i
 ˘
lnt
log t 2 2
1 C=2n C D ln.1 =2n/ C
C
2n.1 =2n/lnt
C 2 (byinequality (3.17))
  C
3nlnt
< 2 (byinequality (35.25)) .
 C
Thisboundispolynomialinthesizeoftheinput—whichisthenumberofbitslgt
neededtorepresentt plusthenumberofbitsneededtorepresentthesetS,whichis
inturnpolynomialinn—andin1=. SincetherunningtimeofAPPROX-SUBSET-
SUM is polynomial in the lengths of the L i, we conclude that APPROX-SUBSET-
SUM isafullypolynomial-time approximation scheme.
Exercises
35.5-1
Proveequation (35.23). Thenshowthatafterexecutingline5of EXACT-SUBSET-
SUM, L
i
is a sorted list containing every element of P
i
whose value is not more
thant.
35.5-2
Usinginduction oni,proveinequality (35.26).
35.5-3
Proveinequality (35.29).
1134 Chapter35 ApproximationAlgorithms
35.5-4
Howwouldyoumodifytheapproximation schemepresentedinthissectiontofind
a good approximation to the smallest value not less than t that is a sum of some
subsetofthegiveninputlist?
35.5-5
Modify the APPROX-SUBSET-SUM procedure to also return the subset of S that
sumstothevalue´ .

Problems
35-1 Binpacking
Suppose that we are given a set of n objects, where the size s of the ith object
i
satisfies 0 < s < 1. Wewishtopackallthe objects into theminimum numberof
i
unit-size bins. Each bin can hold any subset of the objects whose total size does
notexceed1.
a. Provethattheproblemofdeterminingtheminimumnumberofbinsrequiredis
NP-hard. (Hint:Reducefromthesubset-sum problem.)
The first-fit heuristic takes each object in turn and places it into the first bin that
canaccommodate it. LetS n s .
D i 1 i
D
b. ArguethattheoptimalnumPberofbinsrequiredisatleast S .
d e
c. Arguethatthefirst-fitheuristic leavesatmostonebinlessthanhalffull.
d. Prove that the number of bins used by the first-fit heuristic is never more
than 2S .
d e
e. Proveanapproximation ratioof2forthefirst-fitheuristic.
f. Giveanefficientimplementationofthefirst-fitheuristic,andanalyzeitsrunning
time.
35-2 Approximatingthesizeofamaximumclique
LetG .V;E/beanundirectedgraph. Foranyk 1,defineG.k/ tobetheundi-
D 
rected graph .V.k/;E.k//, where V.k/ is the set of all ordered k-tuples of vertices
fromV andE.k/ isdefinedsothat. ; ;:::; /isadjacentto.w ;w ;:::;w /
1 2 k 1 2 k
if and only if for i 1;2;:::;k, either vertex  is adjacent to w in G, or else
i i
D
 w .
i i
D
ProblemsforChapter35 1135
a. Provethat thesize ofthemaximum clique inG.k/ isequal tothe kthpowerof
thesizeofthemaximumcliqueinG.
b. Argue that if there is an approximation algorithm that has a constant approxi-
mationratioforfindingamaximum-sizeclique,thenthereisapolynomial-time
approximation schemefortheproblem.
35-3 Weightedset-covering problem
Suppose that we generalize the set-covering problem so that each set S in the
i
family F has an associated weight w and the weight of a cover C is w .
i Si C i
Wewishtodetermine aminimum-weight cover. (Section 35.3handles thec2ase in
P
whichw 1foralli.)
i
D
Show how to generalize the greedy set-covering heuristic in a natural manner
to provide an approximate solution for any instance of the weighted set-covering
problem. Showthatyourheuristichasanapproximation ratioofH.d/,whered is
themaximumsizeofanysetS .
i
35-4 Maximummatching
Recall that for an undirected graph G, a matching is a set of edges such that no
twoedges inthesetareincident onthesamevertex. InSection 26.3, wesawhow
tofindamaximum matching inabipartite graph. Inthis problem, wewilllook at
matchings in undirected graphs in general (i.e., the graphs are not required to be
bipartite).
a. A maximal matching is a matching that is not a proper subset of any other
matching. Showthatamaximalmatchingneednotbeamaximummatchingby
exhibitinganundirectedgraphG andamaximalmatchingM inG thatisnota
maximummatching. (Hint:Youcanfindsuchagraphwithonlyfourvertices.)
b. Consider an undirected graph G .V;E/. Give an O.E/-time greedy algo-
D
rithmtofindamaximalmatchinginG.
In this problem, we shall concentrate on a polynomial-time approximation algo-
rithmformaximummatching. Whereasthefastestknownalgorithmformaximum
matching takes superlinear (but polynomial) time, the approximation algorithm
here will run in linear time. You will show that the linear-time greedy algorithm
for maximal matching in part (b) is a 2-approximation algorithm for maximum
matching.
c. Show that the size of a maximum matching in G is a lower bound on the size
ofanyvertexcoverforG.
1136 Chapter35 ApproximationAlgorithms
d. ConsideramaximalmatchingM inG .V;E/. Let
D
T  V someedgeinM isincident on :
D f 2 W g
What can you say about the subgraph of G induced by the vertices of G that
arenotinT?
e. Concludefrompart(d)that2 M isthesizeofavertexcoverforG.
j j
f. Usingparts(c)and(e),provethatthegreedyalgorithminpart(b)isa2-approx-
imationalgorithm formaximummatching.
35-5 Parallelmachinescheduling
Intheparallel-machine-scheduling problem, wearegiven njobs, J ;J ;:::;J ,
1 2 n
where each job J has an associated nonnegative processing time of p . We are
k k
also given m identical machines, M ;M ;:::;M . Any job can run on any ma-
1 2 m
chine. A schedule specifies, for each job J , the machine on which it runs and
k
the time period during which it runs. Each job J must run on some machine M
k i
for p consecutive time units, and during that time period no other job may run
k
on M . Let C denote the completion time of job J , that is, the time at which
i k k
jobJ completesprocessing. Givenaschedule, wedefineC max C to
k max D 1 j n j
be the makespan of the schedule. The goal is to find a schedule whose makespan
isminimum.
Forexample, suppose that wehave twomachines M and M and thatwehave
1 2
four jobs J ;J ;J ;J , with p 2, p 12, p 4, and p 5. Then one
1 2 3 4 1 2 3 4
D D D D
possible schedule runs, on machine M , job J followed by job J , and on ma-
1 1 2
chine M , itruns job J followed by job J . Forthis schedule, C 2, C 14,
2 4 3 1 2
D D
C 9,C 5,andC 14. AnoptimalschedulerunsJ onmachineM ,and
3 D 4 D max D 2 1
it runs jobs J , J , and J on machine M . For this schedule, C 2, C 12,
1 3 4 2 1 2
D D
C 6,C 11,andC 12.
3 D 4 D max D
Givenaparallel-machine-scheduling problem,weletC denotethemakespan
max
ofanoptimalschedule.
a. Show that the optimal makespan is at least as large as the greatest processing
time,thatis,
C max p :
max
 1 k n
k
 
b. Showthattheoptimalmakespanisatleastaslargeastheaveragemachineload,
thatis,
1
C p :
max
 m
k
1 k n
X
ProblemsforChapter35 1137
Supposethatweusethefollowing greedy algorithm forparallel machine schedul-
ing: wheneveramachineisidle,schedule anyjobthathasnotyetbeenscheduled.
c. Write pseudocode to implement this greedy algorithm. What is the running
timeofyouralgorithm?
d. Fortheschedule returnedbythegreedyalgorithm, showthat
1
C p max p :
max  m k C1 k n k
1 k n  
X
Concludethatthisalgorithm isapolynomial-time 2-approximation algorithm.
35-6 Approximating amaximumspanningtree
LetG .V;E/beanundirectedgraphwithdistinctedgeweightsw.u;/oneach
D
edge .u;/ E. For each vertex  V, let max./ argmax w.u;/
2 2 D .u;/ Ef g
bethemaximum-weightedgeincidentonthatvertex. LetS max2./  V
G
D f W 2 g
be the set of maximum-weight edges incident on each vertex, and let T be the
G
maximum-weight spanning tree of G, that is, the spanning tree of maximum total
weight. ForanysubsetofedgesE E,definew.E / w.u;/.
0  0 D .u;/ E0
2
a. Giveanexampleofagraphwithatleast4vertices forwPhichS T .
G G
D
b. Giveanexampleofagraphwithatleast4vertices forwhichS T .
G G
¤
c. ProvethatS T foranygraphG.
G G

d. Provethatw.S / w.T /=2foranygraphG.
G G

e. GiveanO.V E/-timealgorithm tocomputea2-approximation tothemaxi-
C
mumspanning tree.
35-7 Anapproximation algorithmforthe0-1knapsackproblem
Recall the knapsack problem from Section 16.2. There are n items, where the ith
item is worth  dollars and weighs w pounds. We are also given a knapsack
i i
that can hold at most W pounds. Here, we add the further assumptions that each
weightw isatmostW andthattheitemsareindexedinmonotonicallydecreasing
i
orderoftheirvalues:    .
1 2 n
  
In the 0-1 knapsack problem, we wish to find a subset of the items whose total
weight is at most W and whose total value is maximum. The fractional knapsack
problem is like the 0-1 knapsack problem, except that we are allowed to take a
fraction of each item, rather than being restricted to taking either all or none of
1138 Chapter35 ApproximationAlgorithms
each item. If we take a fraction x of item i, where 0 x 1, we contribute
i i
 
x w to the weight of the knapsack and receive value x  . Ourgoal isto develop
i i i i
apolynomial-time 2-approximation algorithm forthe0-1knapsack problem.
Inordertodesignapolynomial-time algorithm,weconsiderrestrictedinstances
of the 0-1 knapsack problem. Given an instance I of the knapsack problem, we
formrestrictedinstancesI ,forj 1;2;:::;n,byremovingitems1;2;:::;j 1
j
D (cid:0)
and requiring the solution to include item j (all of item j in both the fractional
and0-1knapsackproblems). NoitemsareremovedininstanceI . ForinstanceI ,
1 j
let P denote an optimal solution to the 0-1 problem and Q denote an optimal
j j
solution tothefractional problem.
a. ArguethatanoptimalsolutiontoinstanceI ofthe0-1knapsackproblemisone
of P ;P ;:::;P .
1 2 n
f g
b. Provethatwecanfindanoptimalsolution Q tothefractional problem forin-
j
stance I by including item j and then using the greedy algorithm in which
j
at each step, we take as much as possible of the unchosen item in the set
j 1;j 2;:::;n withmaximumvalueperpound =w .
i i
f C C g
c. Prove that we can always construct an optimal solution Q to the fractional
j
problem forinstance I thatincludes atmostoneitem fractionally. Thatis,for
j
all items except possibly one, we either include all of the item or none of the
itemintheknapsack.
d. Given an optimal solution Q to the fractional problem for instance I , form
j j
solutionR fromQ bydeletinganyfractionalitemsfromQ . Let.S/denote
j j j
the total value of items taken in a solution S. Prove that .R / .Q /=2
j j
 
.P /=2.
j
e. Giveapolynomial-time algorithmthatreturnsamaximum-valuesolutionfrom
the set R ;R ;:::;R , and prove that your algorithm is a polynomial-time
1 2 n
f g
2-approximation algorithm forthe0-1knapsack problem.
Chapter notes
Although methods that do not necessarily compute exact solutions have been
known for thousands of years (for example, methods to approximate the value
of ), the notion of an approximation algorithm is much more recent. Hochbaum
[172] credits Garey, Graham, and Ullman [128] and Johnson [190] with formal-
izing the concept of a polynomial-time approximation algorithm. The first such
algorithm isoftencredited toGraham[149].
NotesforChapter35 1139
Since this early work, thousands of approximation algorithms have been de-
signed for a wide range of problems, and there is a wealth of literature on this
field. Recent texts by Ausiello et al. [26], Hochbaum [172], and Vazirani [345]
deal exclusively with approximation algorithms, as do surveys by Shmoys [315]
and Kleinand Young [207]. Several other texts, such asGareyand Johnson [129]
andPapadimitriou andSteiglitz [271],havesignificant coverage ofapproximation
algorithms as well. Lawler, Lenstra, Rinnooy Kan, and Shmoys [225] provide an
extensive treatment of approximation algorithms for the traveling-salesman prob-
lem.
Papadimitriou and Steiglitz attribute the algorithm APPROX-VERTEX-COVER
toF.GavrilandM.Yannakakis. Thevertex-coverproblemhasbeenstudiedexten-
sively (Hochbaum [172]lists 16 different approximation algorithms forthis prob-
lem),butalltheapproximation ratiosareatleast2 o.1/.
(cid:0)
ThealgorithmAPPROX-TSP-TOUR appearsinapaperbyRosenkrantz,Stearns,
andLewis[298]. Christofides improved onthisalgorithm andgavea3=2-approx-
imation algorithm forthe traveling-salesman problem with the triangle inequality.
Arora [22] and Mitchell [257] have shown that if the points are in the euclidean
plane, there is a polynomial-time approximation scheme. Theorem 35.3 is due to
SahniandGonzalez[301].
The analysis of the greedy heuristic for the set-covering problem is modeled
afterthe proof published byChva´tal [68]ofamoregeneral result; the basic result
aspresented hereisduetoJohnson [190]andLova´sz[238].
ThealgorithmAPPROX-SUBSET-SUM anditsanalysisarelooselymodeledafter
related approximation algorithms for the knapsack and subset-sum problems by
IbarraandKim[187].
Problem35-7isacombinatorialversionofamoregeneralresultonapproximat-
ingknapsack-type integerprogramsbyBienstockandMcClosky[45].
TherandomizedalgorithmforMAX-3-CNFsatisfiabilityisimplicitinthework
of Johnson [190]. The weighted vertex-cover algorithm is by Hochbaum [171].
Section 35.4 only touches on the power of randomization and linear program-
minginthedesignofapproximation algorithms. Acombinationofthesetwoideas
yields a technique called “randomized rounding,” which formulates a problem as
anintegerlinearprogram,solvesthelinear-programming relaxation,andinterprets
the variables in the solution as probabilities. These probabilities then help guide
the solution of the original problem. This technique was first used by Raghavan
and Thompson [290], and it has had many subsequent uses. (See Motwani, Naor,
and Raghavan [261] for a survey.) Several other notable recent ideas in the field
of approximation algorithms include the primal-dual method (see Goemans and
Williamson [135] for a survey), finding sparse cuts for use in divide-and-conquer
algorithms[229],andtheuseofsemidefiniteprogramming [134].
1140 Chapter35 ApproximationAlgorithms
Asmentioned inthechapter notes forChapter 34, recent results inprobabilisti-
cally checkable proofs have led to lower bounds on the approximability of many
problems, including several in this chapter. In addition to the references there,
the chapter by Arora and Lund [23] contains a good description of the relation-
shipbetweenprobabilistically checkableproofsandthehardnessofapproximating
variousproblems.
VIII Appendix: Mathematical Background
Introduction
Whenweanalyzealgorithms, weoftenneedtodrawuponabodyofmathematical
tools. Someofthesetools areassimpleashigh-school algebra, but othersmaybe
new to you. In Part I, we saw how to manipulate asymptotic notations and solve
recurrences. Thisappendixcomprisesacompendiumofseveralotherconceptsand
methods weuse toanalyze algorithms. Asnoted in the introduction to PartI, you
mayhaveseen muchofthematerialinthisappendix before having read thisbook
(althoughthespecificnotationalconventionsweusemightoccasionallydifferfrom
thoseyouhaveseenelsewhere). Hence,youshouldtreatthisappendixasreference
material. As in the rest of this book, however, we have included exercises and
problems, inorderforyoutoimproveyourskillsintheseareas.
Appendix A offers methods for evaluating and bounding summations, which
occur frequently in the analysis of algorithms. Many of the formulas here appear
inanycalculustext,butyouwillfinditconvenienttohavethesemethodscompiled
inoneplace.
AppendixBcontainsbasicdefinitionsandnotationsforsets,relations,functions,
graphs,andtrees. Italsogivessomebasicpropertiesofthesemathematicalobjects.
AppendixCbeginswithelementary principles ofcounting: permutations, com-
binations, andthe like. Theremainder contains definitions and properties ofbasic
probability. Most of the algorithms in this book require no probability for their
analysis, and thus you can easily omit the latter sections of the chapter on a first
reading, even without skimming them. Later, when you encounter a probabilistic
analysis that you want to understand better, you will find Appendix C well orga-
nizedforreference purposes.
1144 PartVIII Appendix:MathematicalBackground
Appendix D defines matrices, their operations, and some of their basic prop-
erties. You have probably seen most of this material already if you have taken a
course inlinearalgebra, butyoumightfindithelpful tohaveoneplacetolookfor
ournotation anddefinitions.
A Summations
When an algorithm contains an iterative control construct such as a while or for
loop, we can express its running time as the sum of the times spent on each exe-
cution of the body of the loop. Forexample, wefound in Section 2.2 that the jth
iteration ofinsertion sorttooktimeproportional toj intheworstcase. Byadding
upthetimespentoneachiteration, weobtained thesummation(orseries)
n
j :
j 2
XD
When we evaluated this summation, we attained a bound of ‚.n2/ on the worst-
caserunningtimeofthealgorithm. Thisexampleillustrateswhyyoushouldknow
howtomanipulate andboundsummations.
Section A.1lists several basic formulas involving summations. Section A.2 of-
fers useful techniques for bounding summations. Wepresent the formulas in Sec-
tion A.1 without proof, though proofs for some of them appear in Section A.2 to
illustrate themethods ofthatsection. Youcanfindmostoftheotherproofs inany
calculustext.
A.1 Summation formulasandproperties
Givenasequence a ;a ;:::;a ofnumbers, wherenisanonnegative integer, we
1 2 n
canwritethefinitesuma a a as
1 2 n
C CC
n
a :
k
k 1
XD
Ifn 0,thevalueofthesummationisdefinedtobe0. Thevalueofafiniteseries
D
isalwayswelldefined,andwecanadditstermsinanyorder.
Givenaninfinite sequence a ;a ;:::ofnumbers, wecanwrite theinfinite sum
1 2
a a as
1 2
C C
1146 AppendixA Summations
1
a ;
k
k 1
XD
whichweinterpret tomean
n
lim a :
k
n
!1k 1
XD
If the limit does not exist, the series diverges; otherwise, it converges. The terms
of a convergent series cannot always be added in any order. We can, however,
rearrange the terms of an absolutely convergent series, that is, a series 1k 1a k
forwhichtheseries 1k D1ja k jalsoconverges.
P
D
P
Linearity
Foranyrealnumberc andanyfinitesequences a ;a ;:::;a andb ;b ;:::;b ,
1 2 n 1 2 n
n n n
.ca b / c a b :
k k k k
C D C
k 1 k 1 k 1
XD XD XD
Thelinearity property alsoappliestoinfiniteconvergent series.
We can exploit the linearity property to manipulate summations incorporating
asymptotic notation. Forexample,
n n
‚.f.k// ‚ f.k/ :
D
!
k 1 k 1
XD XD
Inthisequation, the‚-notation ontheleft-hand side applies tothevariable k,but
on the right-hand side, it applies to n. We can also apply such manipulations to
infiniteconvergent series.
Arithmeticseries
Thesummation
n
k 1 2 n;
D C CC
k 1
XD
isanarithmeticseriesandhasthevalue
n
1
k n.n 1/ (A.1)
D 2 C
k 1
XD
‚.n2/: (A.2)
D
A.1 Summationformulasandproperties 1147
Sumsofsquaresandcubes
Wehavethefollowingsummations ofsquaresandcubes:
n
n.n 1/.2n 1/
k2 C C ; (A.3)
D 6
k 0
XD
n n2.n 1/2
k3 C : (A.4)
D 4
k 0
XD
Geometricseries
Forrealx 1,thesummation
¤
n
xk 1 x x2 xn
D C C CC
k 0
XD
isageometric orexponentialseriesandhasthevalue
n xn 1 1
xk C (cid:0) : (A.5)
D x 1
k 0 (cid:0)
XD
When the summation is infinite and x < 1, wehave the infinite decreasing geo-
j j
metricseries
1 1
xk : (A.6)
D 1 x
k 0 (cid:0)
XD
Becauseweassumethat00 1,theseformulasapplyevenwhenx 0.
D D
Harmonicseries
Forpositiveintegers n,thenthharmonicnumberis
1 1 1 1
H 1
n
D C 2 C 3 C 4 CC n
n
1
D k
k 1
XD
lnn O.1/: (A.7)
D C
(WeshallprovearelatedboundinSectionA.2.)
Integratinganddifferentiating series
Byintegratingordifferentiating theformulasabove,additionalformulasarise. For
example, by differentiating both sides of the infinite geometric series (A.6) and
multiplying byx,weget
1148 AppendixA Summations
1 x
kxk (A.8)
D .1 x/2
k 0 (cid:0)
XD
for x <1.
j j
Telescopingseries
Foranysequencea ;a ;:::;a ,
0 1 n
n
.a a / a a ; (A.9)
k k 1 n 0
(cid:0) (cid:0) D (cid:0)
k 1
XD
sinceeachofthetermsa ;a ;:::;a isaddedinexactlyonceandsubtractedout
1 2 n 1
(cid:0)
exactlyonce. Wesaythatthesumtelescopes. Similarly,
n 1
(cid:0)
.a a / a a :
k k 1 0 n
(cid:0) C D (cid:0)
k 0
XD
Asanexampleofatelescoping sum,consider theseries
n 1
(cid:0) 1
:
k.k 1/
k 1 C
XD
Sincewecanrewriteeachtermas
1 1 1
;
k.k 1/ D k (cid:0) k 1
C C
weget
n 1 n 1
(cid:0) 1 (cid:0) 1 1
k.k 1/ D k (cid:0) k 1
k 1 C k 1 C 
XD XD
1
1 :
D (cid:0) n
Products
Wecanwritethefiniteproducta a a as
1 2 n

n
a :
k
k 1
YD
Ifn 0,thevalueoftheproductisdefinedtobe1. Wecanconvertaformulawith
D
aproduct toaformulawithasummationbyusingtheidentity
n n
lg a lga :
k k
D
!
k 1 k 1
YD XD
A.2 Boundingsummations 1149
Exercises
A.1-1
Findasimpleformulafor n .2k 1/.
k 1 (cid:0)
D
A.1-2 ? P
Show that n 1=.2k 1/ ln.pn/ O.1/ by manipulating the harmonic
k 1 (cid:0) D C
series. D
P
A.1-3
Showthat 1k 0k2xk
D
x.1 Cx/=.1 (cid:0)x/3 for jx j< 1.
D
A.1-4 ? P
Showthat 1k 0.k (cid:0)1/=2k
D
0.
D
A.1-5 ? P
Evaluatethesum 1k 1.2k C1/x2k for jx j< 1.
D
A.1-6 P
Prove that n O.f .i// O n f .i/ by using the linearity property of
k 1 k D k 1 k
summations. D D
P (cid:0)P 
A.1-7
Evaluatetheproduct n 2 4k.
k 1 
D
A.1-8 ? Q
Evaluatetheproduct n .1 1=k2/.
k 2 (cid:0)
D
Q
A.2 Bounding summations
We have many techniques at our disposal for bounding the summations that de-
scribetherunning timesofalgorithms. Herearesomeofthemostfrequently used
methods.
Mathematicalinduction
The most basic way to evaluate a series is to use mathematical induction. As an
example,letusprovethatthearithmeticseries n kevaluatesto 1n.n 1/. We
k 1 2 C
can easily verify this assertion for n 1. WemakDe the inductive assumption that
D P
1150 AppendixA Summations
itholdsforn,andweprovethatitholdsforn 1. Wehave
C
n 1 n
C
k k .n 1/
D C C
k 1 k 1
XD XD
1
n.n 1/ .n 1/
D 2 C C C
1
.n 1/.n 2/:
D 2 C C
You don’t always need to guess the exact value of a summation in order to use
mathematicalinduction. Instead,youcanuseinductiontoproveaboundonasum-
mation. As an example, let us prove that the geometric series n 3k is O.3n/.
k 0
More specifically, let us prove that n 3k c3n for some consDtant c. For the
initialcondition n 0,wehave 0 k 3Dk0 1 c 1aslongaP sc 1. Assuming
D kP0 D   
thattheboundholdsforn,letusproDvethatitholdsforn 1. Wehave
P C
n 1 n
C
3k 3k 3n 1
C
D C
k 0 k 0
XD XD
c3n 3n 1 (bytheinductivehypothesis)
C
 C
1 1
c3n 1
C
D 3 C c
 
c3n 1
C

as long as .1=3 1=c/ 1 or, equivalently, c 3=2. Thus, n 3k O.3n/,
C   k 0 D
aswewishedtoshow. D
P
Wehave to becareful when weuse asymptotic notation to prove bounds by in-
duction. Consider the following fallacious proof that n k O.n/. Certainly,
k 1 D
1 k O.1/. Assumingthattheboundholdsforn,wD enowproveitforn 1:
k 1 D P C
D
Pn 1 n
C
k k .n 1/
D C C
k 1 k 1
XD XD
O.n/ .n 1/ wrong!!
D C C 
O.n 1/:
D C
Thebugintheargumentisthatthe“constant”hiddenbythe“big-oh”growswithn
andthusisnotconstant. Wehavenotshownthatthesameconstantworksforalln.
Boundingtheterms
We can sometimes obtain a good upper bound on a series by bounding each term
of the series, and it often suffices to use the largest term to bound the others. For
A.2 Boundingsummations 1151
example,aquickupperboundonthearithmeticseries(A.1)is
n n
k n

k 1 k 1
XD XD
n2 :
D
Ingeneral, foraseries n a ,ifweleta max a ,then
k D1 k max D 1 k n k
n P
a n a :
k   max
k 1
XD
The technique of bounding each term in a series by the largest term is a weak
method when the series can in fact be bounded by a geometric series. Given the
series n a , suppose that a =a r for all k 0, where 0 < r < 1 is a
constant.k WD0
e
ck
an bound the
sumk C b1
y
ak n
infinite
decrea
sing geometric series, since
P
a a rk,andthus
k 0

n
1
a a rk
k 0

k 0 k 0
XD XD
1
a rk
0
D
k 0
XD
1
a :
0
D 1 r
(cid:0)
We can apply this method to bound the summation 1k 1.k=3k/. In order to
start the summation at k
D
0, we rewrite it as 1k 0. P.k CD 1/=3k C1/. The first
term(a )is1=3,andtheratio(r)ofconsecutive termDsis
0
P
.k 2/=3k 2 1 k 2
C
C C
.k 1/=3k 1 D 3  k 1
C C C
2
 3
forallk 0. Thus,wehave

1 k 1 k 1
C
3k D 3k 1
k 1 k 0 C
XD XD
1 1
 3  1 2=3
(cid:0)
1:
D
1152 AppendixA Summations
A common bug in applying this method is to show that the ratio of consecu-
tive terms is less than 1 and then to assume that the summation is bounded by a
geometric series. Anexampleistheinfiniteharmonicseries, whichdivergessince
n
1 1 1
lim
k D n k
k 1 !1k 1
XD XD
lim ‚.lgn/
D n
!1:
D 1
Theratioofthe.k 1/standkthtermsinthisseriesisk=.k 1/ <1,buttheseries
C C
isnotbounded byadecreasing geometricseries. Toboundaseriesbyageometric
series,wemustshowthatthereisanr < 1,whichisaconstant, suchthattheratio
ofallpairsofconsecutivetermsneverexceedsr. Intheharmonicseries,nosuchr
existsbecausetheratiobecomesarbitrarily closeto1.
Splittingsummations
One way to obtain bounds on a difficult summation is to express the series as the
sumoftwoormoreseriesbypartitioning therangeoftheindexandthentobound
each of the resulting series. For example, suppose we try to find a lower bound
on the arithmetic series n k, which we have already seen has an upper bound
k 1
ofn2. WemightattempttobDoundeachterminthesummationbythesmallestterm,
P
butsincethattermis1,wegetalowerboundofnforthesummation—farofffrom
ourupperboundofn2.
Wecanobtainabetterlowerboundbyfirstsplittingthesummation. Assumefor
convenience thatniseven. Wehave
n n=2 n
k k k
D C
k 1 k 1 k n=2 1
XD XD DXC
n=2 n
0 .n=2/
 C
k 1 k n=2 1
XD DXC
.n=2/2
D
.n2/;
D
whichisanasymptotically tightbound,since n k O.n2/.
k 1 D
For a summation arising from the analysis of Dan algorithm, we can often split
P
the summation and ignore a constant number of the initial terms. Generally, this
techniqueapplieswheneachterma inasummation n a isindependentofn.
k k 0 k
D
P
A.2 Boundingsummations 1153
Thenforanyconstantk > 0,wecanwrite
0
n k0 1 n
(cid:0)
a a a
k k k
D C
Xk D0 Xk D0 k XDk0
n
‚.1/ a ;
k
D C
k XDk0
since the initial terms of the summation are all constant and there are a constant
number of them. We can then use other methods to bound n a . This tech-
k k0 k
nique applies to infinite summations as well. For example, to fiDnd an asymptotic
P
upperboundon
1
k2
;
2k
k 0
XD
weobserve thattheratioofconsecutive termsis
.k 1/2=2k 1 .k 1/2
C
C C
k2=2k D 2k2
8
 9
ifk 3. Thus,thesummationcanbesplitinto

1
k2 2 k2
1
k2
2k D 2k C 2k
k 0 k 0 k 3
XD XD XD
2 k2 9 1 8 k
 2k C 8 9
k 0 k 0 
XD XD
O.1/;
D
sincethefirstsummationhasaconstantnumberoftermsandthesecondsummation
isadecreasing geometricseries.
Thetechniqueofsplittingsummationscanhelpusdetermineasymptoticbounds
inmuch more difficult situations. Forexample, wecan obtain abound ofO.lgn/
ontheharmonicseries(A.7):
n
1
H :
n
D k
k 1
XD
We do so by splitting the range 1 to n into lgn 1 pieces and upper-bounding
b cC
thecontribution ofeachpieceby1. Fori 0;1;:::; lgn ,theithpiececonsists
D b c
1154 AppendixA Summations
of the terms starting at 1=2i and going up to but not including 1=2i 1. The last
C
piecemightcontaintermsnotintheoriginal harmonicseries,andthuswehave
n
1
blgn c2i (cid:0)1
1
k  2i j
k 1 i 0 j 0 C
XD XD XD
blgn c2i (cid:0)1
1
 2i
i 0 j 0
XD XD
lgn
b c
1
D
i 0
XD
lgn 1: (A.10)
 C
Approximationbyintegrals
When a summation has the form n f.k/, where f.k/ is a monotonically in-
k m
creasing function, wecanapproximatDe itbyintegrals:
P
n n n 1
C
f.x/dx f.k/ f.x/dx : (A.11)
 
Zm (cid:0)1
k XDm
Zm
Figure A.1justifies this approximation. Thesummation isrepresented asthe area
oftherectanglesinthefigure,andtheintegralistheshadedregionunderthecurve.
When f.k/ is a monotonically decreasing function, we can use a similar method
toprovidethebounds
n 1 n n
C
f.x/dx f.k/ f.x/dx : (A.12)
 
Zm
k XDm
Zm (cid:0)1
The integral approximation (A.12) gives a tight estimate for the nth harmonic
number. Foralowerbound, weobtain
n 1 n 1 dx
C
k  x
k 1
Z1
XD
ln.n 1/: (A.13)
D C
Fortheupperbound, wederivetheinequality
n 1 n dx
k  x
k 2
Z1
XD
lnn;
D
A.2 Boundingsummations 1155
m –1 m m+2 n–2 n–1 n+1
f (m) m(
f
+
1
)
f
(m+2)
f
(n–2)
f (x)
… …
f
(n – f (n
1 )
)
x
… …
m+1 n
(a)
m –1 m m+2 n–2 n–1 n+1
f (m) m(
f
+
1
)
f
(m+2)
f
(n–2)
f (x)
… …
f
(n – f (n
1 )
)
x
… …
m+1 n
(b)
Figure A.1 Approximation of n f.k/ by integrals. The area of each rectangle is shown
k m
within the rectangle, and the total reDctangle area represents the value of the summation. The in-
P
tegral is represented by the shaded area under the curve. By comparing areas in (a), we get
n f.x/dx n f.k/, and then by shifting the rectangles one unit to the right, we get
m 1  k m
R
n k(cid:0) mf.k/

mn PC1 fD .x/dxin(b).
D
P R
1156 AppendixA Summations
whichyieldsthebound
n
1
lnn 1: (A.14)
k  C
k 1
XD
Exercises
A.2-1
Showthat n 1=k2 isboundedabovebyaconstant.
k 1
D
A.2-2 P
Findanasymptotic upperboundonthesummation
lgn
b c
n=2k :
k 0
XD ˙ 
A.2-3
Showthatthenthharmonicnumberis.lgn/bysplittingthesummation.
A.2-4
Approximate n k3 withanintegral.
k 1
D
A.2-5 P
Why didn’t we use the integral approximation (A.12) directly on n 1=k to
k 1
obtainanupperboundonthenthharmonicnumber? D
P
Problems
A-1 Boundingsummations
Giveasymptoticallytightboundsonthefollowingsummations. Assumethatr 0

ands 0areconstants.

n
a. kr.
k 1
XD
n
b. lgsk.
k 1
XD
NotesforAppendixA 1157
n
c. krlgsk.
k 1
XD
Appendix notes
Knuth [209] provides an excellent reference for the material presented here. You
canfindbasicproperties ofseriesinanygoodcalculus book, suchasApostol[18]
orThomasetal.[334].
B Sets, Etc.
Many chapters of this book touch on the elements of discrete mathematics. This
appendixreviewsmorecompletelythenotations,definitions,andelementaryprop-
ertiesofsets,relations, functions, graphs, andtrees. Ifyouarealreadywellversed
inthismaterial,youcanprobably justskimthischapter.
B.1 Sets
Aset isacollection ofdistinguishable objects, called itsmembers orelements. If
an object x is a member of a set S, we write x S (read “x is a member of S”
2
or, more briefly, “x is in S”). If x is not a member of S, we write x S. We
62
can describe a set by explicitly listing its members as a list inside braces. For
example, we can define a set S to contain precisely the numbers 1, 2, and 3 by
writing S 1;2;3 . Since 2 is a member of the set S, we can write 2 S, and
D f g 2
since4isnotamember,wehave4 S. Asetcannotcontainthesameobjectmore
…
than once,1 and itselements arenot ordered. TwosetsA andB areequal, written
A B,iftheycontain thesameelements. Forexample, 1;2;3;1 1;2;3
D f g D f g D
3;2;1 .
f g
Weadoptspecialnotations forfrequently encountered sets:
 denotestheemptyset,thatis,thesetcontaining nomembers.
;
 Z denotesthesetofintegers, thatis,theset :::; 2; 1;0;1;2;::: .
f (cid:0) (cid:0) g
R
 denotesthesetofrealnumbers.
 N denotesthesetofnaturalnumbers,thatis,theset 0;1;2;::: .2
f g
1Avariationofaset,whichcancontainthesameobjectmorethanonce,iscalledamultiset.
2Someauthorsstartthenaturalnumberswith1insteadof0. Themoderntrendseemstobetostart
with0.
B.1 Sets 1159
If all the elements of a set A are contained in a set B, that is, if x A implies
2
x B, then we write A B and say that A is a subset of B. A set A is a
2 
proper subset ofB,written A B,ifA B but A B. (Someauthors usethe
  ¤
symbol “ ” to denote the ordinary subset relation, rather than the proper-subset

relation.) Forany set A, wehave A A. Fortwosets A and B, wehave A B
 D
if and only if A B and B A. For any three sets A, B, and C, if A B
  
andB C,thenA C. ForanysetA,wehave A.
  ;
Wesometimes definesetsintermsofother sets. Givenaset A,wecandefinea
setB Abystatingapropertythatdistinguishes theelementsofB. Forexample,
we can define the set of even integers by x x Z andx=2isaninteger . The
f W 2 g
coloninthisnotation isread“suchthat.” (Someauthorsuseavertical barinplace
ofthecolon.)
GiventwosetsAandB,wecanalsodefinenewsetsbyapplyingsetoperations:
 Theintersection ofsetsAandB istheset
A B x x Aandx B :
\ D f W 2 2 g
 TheunionofsetsAandB istheset
A B x x Aorx B :
[ D f W 2 2 g
 Thedifference betweentwosetsAandB istheset
A B x x Aandx B :
(cid:0) D f W 2 … g
Setoperations obeythefollowinglaws:
Emptysetlaws:
A ;
\; D ;
A A:
[; D
Idempotencylaws:
A A A;
\ D
A A A:
[ D
Commutativelaws:
A B B A;
\ D \
A B B A:
[ D [
1160 AppendixB Sets,Etc.
A B A B A B A B A B
(cid:0) D D [
C C C C C
A .B C/ A .B C/ .A B/ .A C/
(cid:0) \ D (cid:0) \ D (cid:0) [ (cid:0)
FigureB.1 AVenndiagramillustratingthefirstofDeMorgan’slaws(B.2).EachofthesetsA,B,
andC isrepresentedasacircle.
Associative laws:
A .B C/ .A B/ C ;
\ \ D \ \
A .B C/ .A B/ C :
[ [ D [ [
Distributivelaws:
A .B C/ .A B/ .A C/;
\ [ D \ [ \ (B.1)
A .B C/ .A B/ .A C/:
[ \ D [ \ [
Absorptionlaws:
A .A B/ A;
\ [ D
A .A B/ A:
[ \ D
DeMorgan’slaws:
A .B C/ .A B/ .A C/;
(cid:0) \ D (cid:0) [ (cid:0) (B.2)
A .B C/ .A B/ .A C/:
(cid:0) [ D (cid:0) \ (cid:0)
FigureB.1illustratesthefirstofDeMorgan’slaws,usingaVenndiagram: agraph-
icalpictureinwhichsetsarerepresented asregionsoftheplane.
Often,allthesetsunderconsiderationaresubsetsofsomelargersetU calledthe
universe. Forexample,ifweareconsideringvarioussetsmadeuponlyofintegers,
thesetZ ofintegers isanappropriate universe. Givenauniverse U,wedefinethe
complement of a set A as A U A x x U andx A . For any set
D (cid:0) D f W 2 62 g
A U,wehavethefollowinglaws:

A A;
D
A A ;
\ D ;
A A U :
[ D
B.1 Sets 1161
We can rewrite DeMorgan’s laws (B.2) with set complements. For any two sets
B;C U,wehave

B C B C ;
\ D [
B C B C :
[ D \
Two sets A and B are disjoint if they have no elements in common, that is, if
A B . AcollectionS S ofnonemptysetsformsapartitionofasetS if
i
\ D ; D f g
 thesetsarepairwisedisjoint,thatis,S ;S S andi j implyS S ,
i j i j
2 ¤ \ D ;
and
 theirunionisS,thatis,
S S :
i
D
S [i 2S
In other words, S forms a partition of S if each element of S appears in exactly
oneS S .
i
2
Thenumberofelementsinasetisthecardinality(orsize)oftheset,denoted S .
j j
Two sets have the same cardinality if their elements can be put into a one-to-one
correspondence. Thecardinality oftheemptysetis 0. Ifthecardinality ofa
j;j D
setisanaturalnumber, wesaythesetisfinite;otherwise, itisinfinite. Aninfinite
N
setthatcanbeputintoaone-to-onecorrespondence withthenaturalnumbers is
Z
countably infinite; otherwise, it is uncountable. For example, the integers are
R
countable, butthereals areuncountable.
ForanytwofinitesetsAandB,wehavetheidentity
A B A B A B ; (B.3)
j [ j D j jCj j(cid:0)j \ j
fromwhichwecanconclude that
A B A B :
j [ j  j jCj j
If A and B are disjoint, then A B 0 and thus A B A B . If
j \ j D j [ j D j j C j j
A B,then A B .
 j j  j j
A finite set of n elements is sometimes called an n-set. A 1-set is called a
singleton. Asubsetofk elementsofasetissometimescalledak-subset.
Wedenote thesetofallsubsets ofasetS, including the emptysetandS itself,
by2S; wecall 2S thepower set ofS. Forexample, 2 a;b ; a ; b ; a;b .
f g
D f; f g f g f gg
ThepowersetofafinitesetS hascardinality 2S (seeExerciseB.1-5).
j j
We sometimes care about setlike structures in which the elements are ordered.
Anorderedpairoftwoelementsaandb isdenoted .a;b/andisdefinedformally
astheset .a;b/ a; a;b . Thus, theordered pair .a;b/isnot thesameasthe
D f f gg
orderedpair.b;a/.
1162 AppendixB Sets,Etc.
The Cartesian product of two sets A and B, denoted A B, is the set of all

ordered pairs such that the first element of the pair is an element of A and the
secondisanelementofB. Moreformally,
A B .a;b/ a Aandb B :
 D f W 2 2 g
Forexample, a;b a;b;c .a;a/;.a;b/;.a;c/;.b;a/;.b;b/;.b;c/ . When
f gf g D f g
AandB arefinitesets,thecardinality oftheirCartesianproduct is
A B A B : (B.4)
j  j D j jj j
TheCartesianproduct ofnsetsA ;A ;:::;A isthesetofn-tuples
1 2 n
A A A .a ;a ;:::;a / a A fori 1;2;:::;n ;
1 2 n 1 2 n i i
  D f W 2 D g
whosecardinality is
A A A A A A
1 2 n 1 2 n
j   j D j jj jj j
if all sets are finite. Wedenote an n-fold Cartesian product over asingle set A by
theset
An A A A;
D  
whose cardinality is An A n if A is finite. We can also view an n-tuple as a
j j D j j
finitesequence oflengthn(seepage1166).
Exercises
B.1-1
DrawVenndiagramsthatillustrate thefirstofthedistributive laws(B.1).
B.1-2
Provethegeneralization ofDeMorgan’slawstoanyfinitecollection ofsets:
A A A A A A ;
1 2 n 1 2 n
\ \\ D [ [[
A A A A A A :
1 2 n 1 2 n
[ [[ D \ \\
B.2 Relations 1163
B.1-3 ?
Prove the generalization of equation (B.3), which is called the principle of inclu-
sionandexclusion:
A A A
1 2 n
j [ [[ j D
A A A
1 2 n
j jCj jCCj j
A A A A (allpairs)
1 2 1 3
(cid:0)j \ j(cid:0)j \ j(cid:0)
A A A (alltriples)
1 2 3
Cj \ \ jC
:
:
:
. 1/n 1 A A A :
(cid:0) 1 2 n
C (cid:0) j \ \\ j
B.1-4
Showthatthesetofoddnatural numbersiscountable.
B.1-5
Show that for any finite set S, the power set 2S has 2S elements (that is, there
j j
are2S distinctsubsets ofS).
j j
B.1-6
Giveaninductivedefinitionforann-tuplebyextendingtheset-theoretic definition
foranorderedpair.
B.2 Relations
AbinaryrelationRontwosetsAandB isasubsetoftheCartesianproductA B.

If.a;b/ R,wesometimeswriteaR b. WhenwesaythatR isabinary relation
2
on a set A, we mean that R is a subset of A A. For example, the “less than”
relation on the natural numbers is the set .a;b / a;b N anda <b . Ann-ary
f W 2 g
relationonsetsA ;A ;:::;A isasubsetofA A A .
1 2 n 1 2 n
 
AbinaryrelationR A Aisreflexiveif
 
aR a
for all a A. Forexample, “ ” and “ ” are reflexive relations on N , but “<” is
2 D 
not. TherelationR issymmetricif
aR b impliesbR a
for all a;b A. For example, “ ” is symmetric, but “<” and “ ” are not. The
2 D 
relationR istransitiveif
aR b andbR c implyaR c
1164 AppendixB Sets,Etc.
foralla;b;c A. Forexample,therelations“<,”“ ,”and“ ”aretransitive, but
the relation R2 .a;b/ a;b N anda b 1  is not, siD nce 3R 4 and 4 R 5
D f W 2 D (cid:0) g
donotimply3R 5.
Arelationthatisreflexive,symmetric,andtransitiveisanequivalencerelation.
Forexample,“ ”isanequivalencerelationonthenaturalnumbers,but“<”isnot.
D
If R is an equivalence relation on a set A, then for a A, the equivalence class
2
ofaisthesetŒa b A aR b ,thatis,thesetofallelementsequivalenttoa.
For example, if wD e f defi2 ne RW .ag ;b/ a;b N anda b isanevennumber ,
D f W 2 C g
then R is an equivalence relation, since a a is even (reflexive), a b is even
C C
implies b a is even (symmetric), and a b is even and b c is even imply
C C C
a c iseven (transitive). Theequivalence class of 4is Œ4 0;2;4;6;::: , and
C D f g
theequivalence classof3isŒ3 1;3;5;7;::: . Abasic theorem ofequivalence
D f g
classesisthefollowing.
TheoremB.1(Anequivalencerelation isthesameasapartition)
The equivalence classes of any equivalence relation R on a set A form a partition
ofA,andanypartitionofAdeterminesanequivalencerelationonAforwhichthe
setsinthepartitionaretheequivalence classes.
Proof For the first part of the proof, we must show that the equivalence classes
of R are nonempty, pairwise-disjoint sets whose union is A. Because R is reflex-
ive, a Œa, and so the equivalence classes are nonempty; moreover, since every
2
element a A belongs to the equivalence class Œa, the union of the equivalence
2
classes is A. It remains to show that the equivalence classes are pairwise disjoint,
that is, if two equivalence classes Œa and Œb have an element c in common, then
they areinfactthe sameset. Suppose that aR c andbR c. Bysymmetry, c R b,
andbytransitivity, aR b. Thus, foranyarbitrary elementx Œa,wehavex R a
2
and, by transitivity, x R b, and thus Œa Œb. Similarly, Œb Œa, and thus
 
Œa Œb.
FD or the second part of the proof, let A A be a partition of A, and define
i
D f g
R .a;b/ thereexistsi suchthata A andb A . We claim that R is an
i i
D f W 2 2 g
equivalencerelationonA. Reflexivityholds,sincea A impliesaR a. Symme-
i
2
try holds, because ifaR b, then a and b are inthe sameset A , and hence bR a.
i
If aR b and bR c, then all three elements are in the same set A , and thus aR c
i
and transitivity holds. To see that the sets in the partition are the equivalence
classes of R, observe that if a A , then x Œa implies x A , and x A
i i i
2 2 2 2
impliesx Œa.
2
AbinaryrelationRonasetAisantisymmetricif
aR b andbR aimplya b :
D
B.2 Relations 1165
Forexample,the“ ”relationonthenaturalnumbersisantisymmetric,sincea b
 
andb a implya b. A relation that isreflexive, antisymmetric, and transitive
 D
is a partial order, and we call a set on which a partial order is defined a partially
orderedset. Forexample,therelation“isadescendant of”isapartialorderonthe
setofallpeople(ifweviewindividuals asbeingtheirowndescendants).
Inapartially ordered setA,theremaybenosingle“maximum”elementa such
thatbR a forallb A. Instead, thesetmaycontain several maximal elements a
2
such that for no b A, where b a, is it the case that aR b. For example, a
2 ¤
collection of different-sized boxes may contain several maximal boxes that don’t
fitinside anyother box, yetithasnosingle “maximum” box intowhich anyother
boxwillfit.3
A relation R on a set A is a total relation if for all a;b A, we have aR b
2
or bR a (or both), that is, if every pairing of elements of A is related by R. A
partialorderthatisalsoatotalrelationisatotalorderorlinearorder. Forexample,
the relation “ ” is a total order on the natural numbers, but the “is a descendant

of” relation is not a total order on the set of all people, since there are individuals
neitherofwhomisdescended fromtheother. Atotalrelation thatistransitive, but
notnecessarily reflexiveandantisymmetric, isatotalpreorder.
Exercises
B.2-1
Z
Prove that the subset relation “ ” on all subsets of is a partial order but not a

totalorder.
B.2-2
Showthatforanypositiveintegern,therelation“equivalentmodulon”isanequiv-
alence relation on the integers. (We say that a b .mod n/ if there exists an

integer q such that a b qn.) Into what equivalence classes does this relation
(cid:0) D
partition theintegers?
B.2-3
Giveexamplesofrelations thatare
a. reflexiveandsymmetricbutnottransitive,
b. reflexiveandtransitive butnotsymmetric,
c. symmetricandtransitivebutnotreflexive.
3Tobeprecise,inorderforthe“fitinside”relationtobeapartialorder,weneedtoviewaboxas
fittinginsideitself.
1166 AppendixB Sets,Etc.
B.2-4
Let S be a finite set, and let R be an equivalence relation on S S. Show that if

inadditionRisantisymmetric,thentheequivalenceclassesofS withrespecttoR
aresingletons.
B.2-5
ProfessorNarcissusclaimsthatifarelationRissymmetricandtransitive,thenitis
also reflexive. Heoffers the following proof. Bysymmetry, aR b implies bR a.
Transitivity, therefore, impliesaR a. Istheprofessor correct?
B.3 Functions
Given two sets A and B, a function f is a binary relation on A and B such that
for all a A, there exists precisely one b B such that .a;b/ f. The set Ais
2 2 2
calledthedomainoff,andthesetB iscalledthecodomainoff. Wesometimes
write f A B; and if .a;b/ f, we write b f.a/, since b is uniquely
W ! 2 D
determined bythechoiceofa.
Intuitively, the function f assigns an element of B to each element of A. No
element of A is assigned two different elements of B, but the same element of B
canbeassigned totwodifferent elementsofA. Forexample,thebinaryrelation
f .a;b/ a;b N andb a mod2
D f W 2 D g
isafunction f N 0;1 ,sinceforeachnaturalnumbera,thereisexactlyone
W ! f g
value b in 0;1 such that b a mod 2. Forthis example, 0 f.0/, 1 f.1/,
f g D D D
0 f.2/,etc. Incontrast, thebinaryrelation
D
g .a;b/ a;b N anda b iseven
D f W 2 C g
isnotafunction,since.1;3/and.1;5/arebothing,andthusforthechoicea 1,
D
thereisnotprecisely oneb suchthat.a;b/ g.
2
Givenafunction f A B,ifb f.a/,wesaythat a istheargumentoff
W ! D
and that b isthe value of f ata. Wecan define afunction by stating its value for
everyelementofitsdomain. Forexample,wemightdefinef.n/ 2nforn N ,
which means f .n;2n/ n N . Two functions f and g aD re equal if2 they
D f W 2 g
havethesamedomainandcodomain andif,forallainthedomain, f.a/ g.a/.
D
A finite sequence of length n is a function f whose domain is the set of n
integers 0;1;:::;n 1 . We often denote a finite sequence by listing its values:
f (cid:0) g
f.0/;f.1/;:::;f.n 1/ . An infinite sequence is a function whose domain is
h N (cid:0) i
the set of natural numbers. For example, the Fibonacci sequence, defined by
recurrence (3.22),istheinfinitesequence 0;1;1;2;3;5;8;13;21;::: .
h i
B.3 Functions 1167
Whenthedomainofafunctionf isaCartesianproduct,weoftenomittheextra
parentheses surrounding the argument of f. For example, if we had a function
f A A A B, we would write b f.a ;a ;:::;a / instead
1 2 n 1 2 n
W     ! D
of b f..a ;a ;:::;a //. We also call each a an argument to the function f,
1 2 n i
D
thoughtechnically the(single) argumenttof isthen-tuple.a ;a ;:::;a /.
1 2 n
Iff A B isafunction and b f.a/,then wesometimes say thatb isthe
W ! D
imageofaunderf. TheimageofasetA Aunderf isdefinedby
0

f.A/ b B b f.a/forsomea A :
0 0
D f 2 W D 2 g
The range of f is the image of its domain, that is, f.A/. Forexample, the range
of the function f N N defined by f.n/ 2n is f.N/ m m 2n for
some n N ,inoW ther! words,thesetofnonnegD ative evenintegD erf s. W D
2 g
Afunctionisasurjectionifitsrangeisitscodomain. Forexample,thefunction
f.n/ n=2 is a surjective function from N to N , since every element in N
D b c
appearsasthevalueoff forsomeargument. Incontrast, thefunction f.n/ 2n
isnotasurjectivefunctionfromN toN ,sincenoargumenttof canproduceD 3asa
value. Thefunction f.n/ 2nis, however, asurjective function fromthenatural
D
numberstotheevennumbers. Asurjection f A B issometimesdescribed as
W !
mappingAontoB. Whenwesaythatf isonto,wemeanthatitissurjective.
A function f A B is an injection if distinct arguments to f produce
W !
distinct values, thatis,ifa a impliesf.a/ f.a/. Forexample, thefunction
0 0
f.n/ 2nisaninjective f¤ unction from N to¤N ,since each evennumber b isthe
D
image under f of at most one element of the domain, namely b=2. The function
f.n/ n=2 isnot injective, since the value 1isproduced by twoarguments: 2
D b c
and3. Aninjection issometimescalledaone-to-onefunction.
Afunctionf A B isabijectionifitisinjectiveandsurjective. Forexample,
thefunctionf.nW / ! . 1/n n=2 isabijection fromN toZ :
D (cid:0) d e
0 0;
!
1 1;
! (cid:0)
2 1;
!
3 2;
! (cid:0)
4 2;
!:
:
:
Z
The function is injective, since no element of is the image of more than one
N Z
element of . It is surjective, since every element of appears as the image of
N
some element of . Hence, the function is bijective. A bijection is sometimes
called a one-to-one correspondence, since it pairs elements in the domain and
codomain. AbijectionfromasetAtoitselfissometimescalledapermutation.
Whenafunctionf isbijective, wedefineitsinversef 1 as
(cid:0)
f 1.b/ aifandonlyiff.a/ b :
(cid:0)
D D
1168 AppendixB Sets,Etc.
Forexample,theinverseofthefunction f.n/ . 1/n n=2 is
D (cid:0) d e
2m ifm 0;
f 1.m/ 
(cid:0)
D
(
2m 1 ifm < 0:
(cid:0) (cid:0)
Exercises
B.3-1
LetAandB befinitesets,andletf A B beafunction. Showthat
W !
a. iff isinjective,then A B ;
j j j j
b. iff issurjective, then A B .
j j  j j
B.3-2
Isthefunctionf.x/ x 1bijectivewhenthedomainandthecodomainareN ?
D C Z
Isitbijectivewhenthedomainandthecodomainare ?
B.3-3
Give a natural definition for the inverse of a binary relation such that if a relation
isinfactabijectivefunction, itsrelational inverseisitsfunctional inverse.
B.3-4 ?
Z Z Z
Giveabijection from to .

B.4 Graphs
This section presents two kinds of graphs: directed and undirected. Certain def-
initions in the literature differ from those given here, but for the most part, the
differences are slight. Section 22.1 shows how we can represent graphs in com-
putermemory.
Adirected graph (ordigraph) G isapair .V;E/,whereV isafiniteset andE
isabinary relation on V. ThesetV iscalled thevertex setofG, and itselements
arecalled vertices(singular: vertex). ThesetE iscalledtheedgesetofG,andits
elements are called edges. Figure B.2(a) is apictorial representation of adirected
graph on the vertex set 1;2;3;4;5;6 . Vertices are represented by circles in the
f g
figure, and edges are represented by arrows. Note that self-loops—edges from a
vertextoitself—arepossible.
In an undirected graph G .V;E/, the edge set E consists of unordered
D
pairs of vertices, rather than ordered pairs. That is, an edge is a set u; , where
f g
B.4 Graphs 1169
1 2 3 1 2 3 1 2 3
4 5 6 4 5 6 6
(a) (b) (c)
Figure B.2 Directed and undirected graphs. (a) A directed graph G .V;E/, where V
D D
1;2;3;4;5;6 and E .1;2/;.2;2/;.2;4/;.2;5/;.4;1/;.4;5/;.5;4/;.6;3/ . The edge .2;2/
f g D f g
is a self-loop. (b) An undirected graph G .V;E/, where V 1;2;3;4;5;6 and E
D D f g D
.1;2/;.1;5/;.2;5/;.3;6/ . The vertex 4 is isolated. (c) The subgraph of the graph in part (a)
f g
inducedbythevertexset 1;2;3;6 .
f g
u; V andu . Byconvention, weusethenotation .u;/foranedge, rather
2 ¤
thanthesetnotation u; ,andweconsider.u;/and.;u/tobethesameedge.
f g
Inanundirected graph,self-loops areforbidden, andsoeveryedgeconsistsoftwo
distinct vertices. FigureB.2(b)isapictorial representation ofanundirected graph
onthevertexset 1;2;3;4;5;6 .
f g
Manydefinitions fordirected andundirected graphsarethesame,although cer-
taintermshaveslightlydifferentmeaningsinthetwocontexts. If.u;/isanedge
in a directed graph G .V;E/, we say that .u;/ is incident from or leaves
D
vertexuandisincidenttoorentersvertex. Forexample, theedges leaving ver-
tex2inFigureB.2(a)are.2;2/,.2;4/,and.2;5/. Theedgesentering vertex2are
.1;2/ and .2;2/. If .u;/ is an edge in an undirected graph G .V;E/, we say
D
that.u;/ isincident onvertices uand . InFigureB.2(b), theedges incident on
vertex2are.1;2/and.2;5/.
If .u;/ is an edge in a graph G .V;E/, we say that vertex  is adjacent to
D
vertexu. Whenthegraphisundirected,theadjacencyrelationissymmetric. When
the graph is directed, the adjacency relation is not necessarily symmetric. If  is
adjacent to uinadirected graph, wesometimes writeu . Inparts (a) and (b)
!
ofFigureB.2,vertex2isadjacenttovertex1,sincetheedge.1;2/belongstoboth
graphs. Vertex 1 isnot adjacent to vertex 2in Figure B.2(a), since the edge .2;1/
doesnotbelongtothegraph.
Thedegreeofavertexinanundirectedgraphisthenumberofedgesincidenton
it. Forexample,vertex2inFigureB.2(b)hasdegree2. Avertexwhosedegreeis0,
such as vertex 4 in Figure B.2(b), is isolated. In a directed graph, the out-degree
of a vertex is the number of edges leaving it, and the in-degree of a vertex is the
number of edges entering it. The degree of a vertex in a directed graph is its in-
1170 AppendixB Sets,Etc.
degreeplusitsout-degree. Vertex2inFigureB.2(a)hasin-degree2,out-degree 3,
anddegree5.
A path of length k from a vertex u to a vertex u in a graph G .V;E/
0
D
is a sequence  ;  ;  ; :::;  of vertices such that u  , u  , and
0 1 2 k 0 0 k
h i D D
. ; / E for i 1;2;:::;k. The length of the path is the number of
i 1 i
edg(cid:0) es in th2 e path. ThD e path contains the vertices  ; ;:::; and the edges
0 1 k
. ; /;. ; /;:::;. ; /. (There is always a 0-length path from uto u.) If
0 1 1 2 k 1 k
there isapathp fromut(cid:0) ou,wesaythatu isreachable fromuviap,whichwe
0 0
sometimes writeasu
;p
u ifG isdirected. Apathissimple 4ifallvertices inthe
0
patharedistinct. InFigureB.2(a),thepath 1;2;5;4 isasimplepathoflength3.
h i
Thepath 2;5;4;5 isnotsimple.
h i
A subpath ofpath p  ; ;:::; isa contiguous subsequence ofits ver-
0 1 k
D h i
tices. Thatis,forany0 i j k,thesubsequenceofvertices  ; ;:::;
i i 1 j
isasubpathofp.    h C i
In a directed graph, a path  ; ;:::; forms a cycle if   and the
0 1 k 0 k
h i D
path contains at least one edge. The cycle is simple if, in addition,  ; ;:::;
1 2 k
aredistinct. Aself-loop isacycleoflength 1. Twopaths  ; ; ;:::; ;
0 1 2 k 1 0
and  ; ; ;:::; ; form thesamecycle ifthereh exists aninteger j(cid:0) suchi
h 00 10 20 k0 1 00 i
that  fo(cid:0)ri 0;1;:::;k 1. InFigureB.2(a),thepath 1;2;4;1
formsi0 tD hesa.i mCj e/ cm yod clk eastheD
paths
2;4;1(cid:0)
;2 and 4;1;2;4 .
Thiscycleh issimplei
,
h i h i
but the cycle 1;2;4;5;4;1 is not. The cycle 2;2 formed by the edge .2;2/ is
h i h i
aself-loop. Adirected graph withnoself-loops issimple. Inanundirected graph,
a path  ;  ; :::;  forms a cycle if k > 0,   , and all edges on the
0 1 k 0 k
h i D
path are distinct; the cycle is simple if  ; ;:::; are distinct. For example, in
1 2 k
FigureB.2(b),thepath 1;2;5;1 isasimplecycle. Agraphwithnosimplecycles
h i
isacyclic.
An undirected graph is connected if every vertex is reachable from all other
vertices. The connected components of an undirected graph are the equivalence
classes of vertices under the “is reachable from” relation. The graph in Fig-
ureB.2(b)hasthreeconnectedcomponents: 1;2;5 , 3;6 ,and 4 . Everyvertex
f g f g f g
in 1;2;5 is reachable from every other vertex in 1;2;5 . An undirected graph
f g f g
isconnected ifithasexactly oneconnected component. Theedges ofaconnected
component are those that are incident on only the vertices of the component; in
otherwords,edge.u;/isanedgeofaconnected componentonlyifbothuand
areverticesofthecomponent.
4Someauthorsrefertowhatwecallapathasa“walk”andtowhatwecallasimplepathasjusta
“path.”Weusetheterms“path”and“simplepath”throughoutthisbookinamannerconsistentwith
theirdefinitions.
B.4 Graphs 1171
1 2 1 2
G 6 3
5 3
5 4 4
G¢ u v w x y z u v w x y
(a) (b)
Figure B.3 (a) A pair of isomorphic graphs. The vertices of the top graph are mapped to the
verticesofthebottomgraphbyf.1/ u;f.2/ ;f.3/ w;f.4/ x;f.5/ y;f.6/ ´.
D D D D D D
(b)Twographsthatarenotisomorphic,sincethetopgraphhasavertexofdegree4andthebottom
graphdoesnot.
Adirected graph isstronglyconnected ifeverytwovertices arereachable from
eachother. Thestronglyconnectedcomponentsofadirected grapharetheequiv-
alence classes of vertices under the “are mutually reachable” relation. A directed
graph isstrongly connected ifithasonly one strongly connected component. The
graph in Figure B.2(a) has three strongly connected components: 1;2;4;5 , 3 ,
f g f g
and 6 . All pairs of vertices in 1;2;4;5 are mutually reachable. The ver-
f g f g
tices 3;6 do not form a strongly connected component, since vertex 6 cannot
f g
bereached fromvertex3.
Two graphs G .V;E/ and G .V ;E / are isomorphic if there exists a
0 0 0
D D
bijection f V V such that .u;/ E if and only if .f.u/;f.// E .
0 0
W ! 2 2
In other words, we can relabel the vertices of G to be vertices of G , maintain-
0
ing the corresponding edges in G and G . Figure B.3(a) shows a pair of iso-
0
morphic graphs G and G with respective vertex sets V 1;2;3;4;5;6 and
0
D f g
V u;;w;x;y;´ . ThemappingfromV toV givenbyf.1/ u;f.2/ ;
0 0
D f g D D
f.3/ w;f.4/ x;f.5/ y;f.6/ ´ provides the required bijective func-
D D D D
tion. The graphs inFigure B.3(b) are not isomorphic. Although both graphs have
5verticesand7edges,thetopgraphhasavertexofdegree4andthebottomgraph
doesnot.
We say that a graph G .V ;E / is a subgraph of G .V;E/ if V V
0 0 0 0
D D 
and E E. Given aset V V, the subgraph of G induced by V is the graph
0 0 0
 
G .V ;E /,where
0 0 0
D
E .u;/ E u; V :
0 0
D f 2 W 2 g
1172 AppendixB Sets,Etc.
The subgraph induced by the vertex set 1;2;3;6 in Figure B.2(a) appears in
f g
FigureB.2(c)andhastheedgeset .1;2/;.2;2/;.6;3/ .
f g
GivenanundirectedgraphG .V;E/,thedirectedversionofG isthedirected
D
graph G .V;E /, where .u;/ E if and only if .u;/ E. That is, we
0 0 0
D 2 2
replaceeachundirectededge.u;/inGbythetwodirectededges.u;/and.;u/
inthedirectedversion. GivenadirectedgraphG .V;E/,theundirectedversion
D
ofG istheundirected graphG .V;E /,where.u;/ E ifandonlyifu 
0 0 0
D 2 ¤
and E contains at least one of the edges .u;/ and .;u/. That is, the undirected
versioncontainstheedgesofG“withtheirdirectionsremoved”andwithself-loops
eliminated. (Since .u;/ and .;u/ are the same edge in an undirected graph, the
undirected version of a directed graph contains it only once, even if the directed
graph contains both edges .u;/ and .;u/.) In a directed graph G .V;E/, a
D
neighbor ofavertex uisanyvertexthat isadjacent touintheundirected version
ofG. Thatis, isaneighborofuifu  andeither.u;/ E or.;u/ E. In
¤ 2 2
anundirected graph, uand areneighbors iftheyareadjacent.
Severalkindsofgraphs havespecial names. Acomplete graphisanundirected
graphinwhicheverypairofverticesisadjacent. Abipartitegraphisanundirected
graphG .V;E/inwhichV canbepartitioned intotwosetsV andV suchthat
1 2
D
.u;/ E implies either u V and  V or u V and  V . That is, all
1 2 2 1
2 2 2 2 2
edgesgobetweenthetwosetsV andV . Anacyclic, undirected graphisaforest,
1 2
and a connected, acyclic, undirected graph is a (free) tree (see Section B.5). We
oftentakethefirstlettersof“directed acyclicgraph”andcallsuchagraphadag.
Therearetwovariantsofgraphs thatyoumayoccasionally encounter. Amulti-
graphislikeanundirectedgraph,butitcanhavebothmultipleedgesbetweenver-
ticesandself-loops. Ahypergraphislikeanundirectedgraph,buteachhyperedge,
ratherthanconnectingtwovertices,connectsanarbitrarysubsetofvertices. Many
algorithms written for ordinary directed and undirected graphs can be adapted to
runonthesegraphlike structures.
Thecontraction ofanundirected graph G .V;E/byanedge e .u;/ isa
D D
graphG .V ;E /,whereV V u; x andx isanewvertex. Theset
0 0 0 0
D D (cid:0)f g[f g
of edges E is formed from E by deleting the edge .u;/ and, for each vertex w
0
adjacent touor,deleting whichever of.u;w/and.;w/isinE andadding the
newedge.x;w/. Ineffect,uand are“contracted” intoasinglevertex.
Exercises
B.4-1
Attendees of a faculty party shake hands to greet each other, and each professor
remembers how many times he or she shook hands. At the end of the party, the
department head adds up the number of times that each professor shook hands.
B.5 Trees 1173
Showthattheresultisevenbyprovingthehandshakinglemma: ifG .V;E/is
D
anundirected graph,then
degree./ 2 E :
D j j
 V
X2
B.4-2
Showthatifadirected orundirected graphcontains apathbetweentwoverticesu
and,thenitcontainsasimplepathbetweenuand. Showthatifadirectedgraph
containsacycle,thenitcontains asimplecycle.
B.4-3
Showthatanyconnected, undirected graphG .V;E/satisfies E V 1.
D j j  j j(cid:0)
B.4-4
Verify that in an undirected graph, the “is reachable from” relation is an equiv-
alence relation on the vertices of the graph. Which of the three properties of an
equivalence relation hold in general for the “is reachable from” relation on the
verticesofadirected graph?
B.4-5
Whatistheundirected version ofthedirected graph inFigureB.2(a)? Whatisthe
directedversionoftheundirected graphinFigureB.2(b)?
B.4-6 ?
Showthatwecanrepresentahypergraph byabipartitegraphifweletincidencein
the hypergraph correspond to adjacency in the bipartite graph. (Hint: Let one set
of vertices in the bipartite graph correspond to vertices of the hypergraph, and let
theothersetofverticesofthebipartitegraphcorrespond tohyperedges.)
B.5 Trees
Aswithgraphs,therearemanyrelated,butslightlydifferent,notionsoftrees. This
section presents definitions and mathematical properties of several kinds of trees.
Sections10.4and22.1describe howwecanrepresent treesincomputermemory.
B.5.1 Freetrees
AsdefinedinSectionB.4,afreetreeisaconnected,acyclic,undirectedgraph. We
oftenomittheadjective “free”whenwesaythatagraphisatree. Ifanundirected
graphisacyclicbutpossiblydisconnected,itisaforest. Manyalgorithmsthatwork
1174 AppendixB Sets,Etc.
(a) (b) (c)
FigureB.4 (a)Afreetree. (b)Aforest. (c)Agraphthatcontainsacycleandisthereforeneither
atreenoraforest.
for trees also work for forests. Figure B.4(a) shows a free tree, and Figure B.4(b)
showsaforest. TheforestinFigureB.4(b)isnotatreebecauseitisnotconnected.
The graph in Figure B.4(c) is connected but neither a tree nor a forest, because it
contains acycle.
Thefollowingtheorem captures manyimportantfactsaboutfreetrees.
TheoremB.2(Properties offreetrees)
LetG .V;E/beanundirected graph. Thefollowingstatementsareequivalent.
D
1. G isafreetree.
2. AnytwoverticesinG areconnected byauniquesimplepath.
3. G is connected, but if any edge is removed from E, the resulting graph is dis-
connected.
4. G isconnected, and E V 1.
j j D j j(cid:0)
5. G isacyclic,and E V 1.
j j D j j(cid:0)
6. G isacyclic,butifanyedgeisaddedtoE,theresultinggraphcontainsacycle.
Proof (1) (2): Sinceatreeisconnected, anytwovertices inG are connected
)
by atleast one simple path. Suppose, for the sake ofcontradiction, that vertices u
andareconnectedbytwodistinctsimplepathsp andp ,asshowninFigureB.5.
1 2
Let w be the vertex at which the paths first diverge; that is, w is the first vertex
on both p and p whose successor on p is x and whose successor on p is y,
1 2 1 2
where x y. Let ´ be the first vertex at which the paths reconverge; that is, ´is
¤
the first vertex following w on p that is also on p . Let p be the subpath of p
1 2 0 1
from w through x to ´, and let p be the subpath of p from w through y to ´.
00 2
Pathsp andp sharenoverticesexcepttheirendpoints. Thus,thepathobtainedby
0 00
concatenatingp andthereverseofp isacycle,whichcontradictsourassumption
0 00
B.5 Trees 1175
p¢
x
w
v
z
y
u
p¢¢
FigureB.5 AstepintheproofofTheoremB.2: if(1)G isafreetree,then(2)anytwovertices
inG areconnected byauniquesimplepath. Assumeforthesakeofcontradiction thatverticesu
and areconnectedbytwodistinctsimplepathsp1 andp2. Thesepathsfirstdivergeatvertexw,
andtheyfirstreconvergeatvertex´.Thepathp concatenatedwiththereverseofthepathp forms
0 00
acycle,whichyieldsthecontradiction.
thatG isatree. Thus,ifG isatree,therecanbeatmostonesimplepathbetween
twovertices.
(2) (3): If any two vertices in G are connected by a unique simple path,
)
thenG isconnected. Let.u;/beanyedgeinE. Thisedgeisapathfromuto,
and so it must be the unique path from uto . If we remove .u;/ from G, there
isnopathfromuto,andhenceitsremovaldisconnects G.
(3) (4): Byassumption, thegraphG isconnected, andbyExerciseB.4-3,we
)
have E V 1. We shall prove E V 1 by induction. A connected
j j  j j (cid:0) j j  j j (cid:0)
graph with n 1 or n 2 vertices has n 1 edges. Suppose that G has n 3
D D (cid:0) 
vertices and that all graphs satisfying (3) with fewer than n vertices also satisfy
E V 1. RemovinganarbitraryedgefromG separatesthegraphintok 2
j j  j j(cid:0) 
connected components (actually k 2). Each component satisfies (3), or else G
D
wouldnotsatisfy (3). Ifwevieweachconnected component V ,withedge setE ,
i i
as its ownfree tree, then because each component has fewer than V vertices, by
j j
theinductivehypothesiswehave E V 1. Thus,thenumberofedgesinall
i i
j j  j j(cid:0)
components combined isatmost V k V 2. Addingintheremovededge
j j(cid:0)  j j(cid:0)
yields E V 1.
j j  j j(cid:0)
(4) (5): SupposethatG isconnected andthat E V 1. Wemustshow
) j j D j j(cid:0)
that G is acyclic. Suppose that G has acycle containing k vertices  ; ;:::; ,
1 2 k
andwithoutlossofgenerality assumethatthiscycleissimple. LetG .V ;E /
k k k
D
be the subgraph of G consisting of the cycle. Note that V E k.
k k
j j D j j D
Ifk < V , there must be a vertex  V V that is adjacent to some ver-
k 1 k
tex  j Vj , since G is connected. DC efin2 e G (cid:0) .V ;E / to be the sub-
i k k 1 k 1 k 1
graph o2 f G with V V  and E C D E C . ;C / . Note that
k 1 k k 1 k 1 k i k 1
V E CkD 1. [ Iff k C1g < V ,C weD can co[ ntif nue, deC finig ng G in
k 1 k 1 k 2
tj heC saj mD e mj annC erj ,D and sC o forth, unC til we oj btaj in G .V ;E /, where n CV ,
n n n
D D j j
1176 AppendixB Sets,Etc.
V V,and E V V . SinceG isasubgraph ofG, wehaveE E,
n n n n n
D j j D j j D j j 
andhence E V ,whichcontradicts theassumption that E V 1. Thus,
j j  j j j j D j j(cid:0)
G isacyclic.
(5) (6): Suppose that G is acyclic and that E V 1. Let k be the
) j j D j j (cid:0)
number of connected components of G. Each connected component is a free tree
bydefinition, andsince (1)implies(5), thesumofalledges inallconnected com-
ponents of G is V k. Consequently, we must have k 1, and G is in fact a
j j(cid:0) D
tree. Since(1)implies(2),anytwoverticesinG areconnected byauniquesimple
path. Thus,addinganyedgetoG createsacycle.
(6) (1): Suppose that G is acyclic but that adding any edge to E creates a
)
cycle. Wemust show that G is connected. Letu and  be arbitrary vertices in G.
Ifuand arenotalreadyadjacent, addingtheedge.u;/createsacycleinwhich
alledges but.u;/belong toG. Thus,thecycleminusedge.u;/mustcontain a
pathfromuto,andsinceuand werechosenarbitrarily, G isconnected.
B.5.2 Rootedandorderedtrees
A rooted tree is a free tree in which one of the vertices is distinguished from the
others. We call the distinguished vertex the root of the tree. We often refer to a
vertexofarooted treeasanode5 ofthetree. FigureB.6(a)showsarooted treeon
asetof12nodeswithroot7.
Consider a node x in a rooted tree T with root r. We call any node y on the
uniquesimplepathfromr tox anancestorofx. Ify isanancestorofx,thenx is
adescendantofy. (Everynodeisbothanancestorandadescendantofitself.) Ify
is anancestor ofx and x y, then y isaproper ancestor of x andx is aproper
¤
descendant ofy. Thesubtreerootedatx isthetreeinduced bydescendants ofx,
rooted at x. For example, the subtree rooted at node 8 in Figure B.6(a) contains
nodes8,6,5,and9.
Ifthelastedgeonthesimplepathfromtherootr ofatreeT toanodexis.y;x/,
theny istheparentofx,andx isachildofy. TherootistheonlynodeinT with
no parent. If two nodes have the same parent, they are siblings. A node with no
children isaleaf orexternalnode. Anonleafnodeisaninternalnode.
5Theterm“node”isoftenusedinthegraphtheoryliteratureasasynonymfor“vertex.”Wereserve
theterm“node”tomeanavertexofarootedtree.
B.5 Trees 1177
7 depth 0 7
3 10 4 depth 1 3 10 4
height = 4 8 12 11 2 depth 2 12 8 11 2
6 5 1 depth 3 1 6 5
9 depth 4 9
(a) (b)
Figure B.6 Rooted and ordered trees. (a) A rooted tree with height 4. The tree is drawn in a
standardway: theroot(node7)isatthetop,itschildren(nodeswithdepth1)arebeneathit,their
children(nodeswithdepth2)arebeneaththem,andsoforth. Ifthetreeisordered,therelativeleft-
to-rightorderofthechildrenofanodematters;otherwiseitdoesn’t. (b)Anotherrootedtree. Asa
rootedtree,itisidenticaltothetreein(a),butasanorderedtreeitisdifferent,sincethechildrenof
node3appearinadifferentorder.
Thenumber of children of a node x in a rooted tree T equals the degree of x.6
The length of the simple path from the root r to a node x is the depth of x in T.
Alevel ofatreeconsists ofallnodes atthesamedepth. Theheightofanode ina
treeisthenumberofedgesonthelongestsimpledownwardpathfromthenodeto
aleaf, and the height of atree is the height of its root. Theheight of atree is also
equaltothelargestdepthofanynodeinthetree.
Anordered treeisarooted treeinwhichthechildren ofeachnodeareordered.
That is, if a node has k children, then there is a first child, a second child, ...,
and a kth child. The two trees in Figure B.6 are different when considered to be
orderedtrees,butthesamewhenconsidered tobejustrootedtrees.
B.5.3 Binaryandpositionaltrees
Wedefinebinarytreesrecursively. AbinarytreeT isastructuredefinedonafinite
setofnodesthateither
 contains nonodes,or
6NoticethatthedegreeofanodedependsonwhetherweconsiderT tobearootedtreeorafreetree.
Thedegreeofavertexinafreetreeis,asinanyundirectedgraph,thenumberofadjacentvertices.
Inarootedtree,however,thedegreeisthenumberofchildren—theparentofanodedoesnotcount
towarditsdegree.
1178 AppendixB Sets,Etc.
3 3 3
2 7 2 7 2 7
4 1 5 4 1 5 4 1 5
6 6 6
(a) (b) (c)
FigureB.7 Binarytrees. (a)Abinarytreedrawninastandard way. Theleftchildofanodeis
drawnbeneaththenodeandtotheleft.Therightchildisdrawnbeneathandtotheright.(b)Abinary
treedifferentfromtheonein(a). In(a),theleftchildofnode7is5andtherightchildisabsent.
In(b), theleftchild of node 7isabsent andthe right childis5. Asordered trees, these treesare
thesame,butasbinarytrees,theyaredistinct. (c)Thebinarytreein(a)representedbytheinternal
nodesofafullbinarytree: anorderedtreeinwhicheachinternalnodehasdegree2. Theleavesin
thetreeareshownassquares.
 iscomposedofthreedisjoint setsofnodes: arootnode, abinary treecalledits
leftsubtree,andabinarytreecalleditsrightsubtree.
Thebinary tree that contains nonodes is called the empty tree ornulltree, some-
timesdenoted NIL. Iftheleftsubtreeisnonempty,itsrootiscalledtheleftchildof
therootoftheentire tree. Likewise, theroot ofanonnull rightsubtree istheright
child oftheroot ofthe entire tree. Ifasubtree isthenull tree NIL, wesaythat the
childisabsentormissing. FigureB.7(a)showsabinarytree.
A binary tree is not simply an ordered tree in which each node has degree at
most 2. For example, in a binary tree, if a node has just one child, the position
of the child—whether it is the left child or the right child—matters. In an or-
dered tree, there is nodistinguishing a sole child asbeing either left or right. Fig-
ureB.7(b)showsabinarytreethatdiffersfromthetreeinFigureB.7(a)becauseof
the position of one node. Considered as ordered trees, however, the two trees are
identical.
We can represent the positioning information in a binary tree by the internal
nodes of an ordered tree, as shown in Figure B.7(c). The idea is to replace each
missing child in the binary tree with a node having no children. These leaf nodes
are drawn as squares in the figure. The tree that results is a full binary tree: each
node iseither aleaf orhasdegree exactly 2. Therearenodegree-1 nodes. Conse-
quently, theorderofthechildren ofanodepreservestheposition information.
We can extend the positioning information that distinguishes binary trees from
ordered trees totrees withmorethan 2children pernode. Inapositional tree, the
B.5 Trees 1179
depth 0
depth 1
height = 3
depth 2
depth 3
FigureB.8 Acompletebinarytreeofheight3with8leavesand7internalnodes.
children of a node are labeled with distinct positive integers. The ith child of a
nodeisabsentifnochildislabeledwithintegeri. Ak-arytreeisapositionaltree
inwhichforeverynode, allchildren withlabels greater than k aremissing. Thus,
abinarytreeisak-arytreewithk 2.
D
A complete k-ary tree is a k-ary tree in which all leaves have the same depth
and all internal nodes have degree k. Figure B.8 shows a complete binary tree of
height 3. Howmanyleavesdoes acompletek-arytreeofheighthhave? Theroot
has k children at depth 1, each of which has k children at depth 2, etc. Thus, the
number of leaves at depth h is kh. Consequently, the height of a complete k-ary
treewithnleavesislog n. Thenumberofinternalnodesofacompletek-arytree
k
ofheighthis
h 1
(cid:0)
1 k k2 kh 1 ki
(cid:0)
C C CC D
i 0
XD
kh 1
(cid:0)
D k 1
(cid:0)
byequation (A.5). Thus,acompletebinarytreehas2h 1internal nodes.
(cid:0)
Exercises
B.5-1
Draw all the free trees composed of the three vertices x, y, and ´. Draw all the
rooted trees with nodes x, y, and ´ with x as the root. Draw all the ordered trees
withnodes x,y, and ´withx asthe root. Drawall thebinary trees withnodes x,
y,and´withx astheroot.
1180 AppendixB Sets,Etc.
B.5-2
Let G .V;E/ be a directed acyclic graph in which there is a vertex  V
0
D 2
such that there exists aunique path from  toevery vertex  V. Provethat the
0
2
undirected versionofG formsatree.
B.5-3
Showbyinduction thatthenumberofdegree-2nodesinanynonemptybinarytree
is 1 fewer than the number of leaves. Conclude that the number of internal nodes
inafullbinarytreeis1fewerthanthenumberofleaves.
B.5-4
Use induction to show that a nonempty binary tree with n nodes has height at
least lgn .
b c
B.5-5 ?
The internal path length of a full binary tree is the sum, taken over all internal
nodes ofthetree, ofthedepthofeachnode. Likewise, theexternal pathlengthis
thesum,taken overallleavesofthetree,ofthedepthofeachleaf. Considerafull
binarytreewithninternalnodes,internalpathlengthi,andexternalpathlengthe.
Provethate i 2n.
D C
B.5-6 ?
Let us associate a “weight” w.x/ 2 d with each leaf x of depth d in a binary
(cid:0)
D
tree T, and let L be the set of leaves of T. Prove that w.x/ 1. (This is
x L 
knownastheKraftinequality.) 2
P
B.5-7 ?
ShowthatifL 2,theneverybinarytreewithLleavescontainsasubtreehaving

betweenL=3and2L=3leaves,inclusive.
Problems
B-1 Graphcoloring
Given an undirected graph G .V;E/, ak-coloring of G is afunction c V
D W !
0;1;:::;k 1 suchthatc.u/ c./foreveryedge.u;/ E. Inotherwords,
f (cid:0) g ¤ 2
thenumbers0;1;:::;k 1representthek colors,andadjacentverticesmusthave
(cid:0)
different colors.
a. Showthatanytreeis2-colorable.
ProblemsforAppendixB 1181
b. Showthatthefollowingareequivalent:
1. G isbipartite.
2. G is2-colorable.
3. G hasnocyclesofoddlength.
c. Let d be the maximum degree of any vertex in a graph G. Prove that we can
colorG withd 1colors.
C
d. ShowthatifG hasO. V /edges,thenwecancolorG withO. V /colors.
j j j j
p
B-2 Friendlygraphs
Reword each of the following statements as a theorem about undirected graphs,
andthenproveit. Assumethatfriendship issymmetricbutnotreflexive.
a. Any group of at least two people contains at least two people with the same
numberoffriendsinthegroup.
b. Everygroupofsixpeoplecontainseitheratleastthreemutualfriendsoratleast
threemutualstrangers.
c. Any group of people can be partitioned into two subgroups such that at least
half the friends of each person belong to the subgroup of which that person is
not amember.
d. Ifeveryoneinagroupisthefriendofatleasthalfthepeopleinthegroup, then
the group can be seated around a table in such a way that everyone is seated
betweentwofriends.
B-3 Bisectingtrees
Manydivide-and-conquer algorithms thatoperateongraphsrequire thatthegraph
bebisectedintotwonearlyequal-sizedsubgraphs,whichareinducedbyapartition
ofthevertices. Thisprobleminvestigatesbisectionsoftreesformedbyremovinga
smallnumber ofedges. Werequire thatwhenevertwovertices endupinthesame
subtreeafterremovingedges, thentheymustbeinthesamepartition.
a. Show that we can partition the vertices of any n-vertex binary tree into two
sets A and B, such that A 3n=4 and B 3n=4, by removing a single
j j  j j 
edge.
b. Show that the constant 3=4 in part (a) is optimal in the worst case by giving
anexampleofasimple binarytreewhosemostevenlybalanced partition upon
removalofasingleedgehas A 3n=4.
j j D
1182 AppendixB Sets,Etc.
c. Show that by removing at most O.lgn/ edges, we can partition the vertices
of any n-vertex binary tree into two sets A and B such that A n=2
j j D b c
and B n=2 .
j j D d e
Appendix notes
G.Boolepioneeredthedevelopmentofsymboliclogic,andheintroducedmanyof
thebasicsetnotations inabookpublishedin1854. Modernsettheorywascreated
by G. Cantor during the period 1874–1895. Cantor focused primarily on sets of
infinitecardinality. Theterm“function” isattributed toG.W.Leibniz,whousedit
torefertoseveralkindsofmathematical formulas. Hislimiteddefinition hasbeen
generalized many times. Graph theory originated in 1736, when L. Euler proved
that itwasimpossible tocross each ofthe seven bridges in thecity of Ko¨nigsberg
exactlyonceandreturntothestartingpoint.
The book by Harary [160] provides a useful compendium of many definitions
andresultsfromgraphtheory.
C Counting and Probability
This appendix reviews elementary combinatorics and probability theory. If you
haveagoodbackgroundintheseareas,youmaywanttoskimthebeginningofthis
appendixlightlyandconcentrateonthelatersections. Mostofthisbook’schapters
donotrequireprobability, butforsomechapters itisessential.
Section C.1 reviews elementary results in counting theory, including standard
formulas for counting permutations and combinations. The axioms of probability
and basic facts concerning probability distributions form Section C.2. Random
variables are introduced in Section C.3, along with the properties of expectation
and variance. Section C.4 investigates the geometric and binomial distributions
that arise from studying Bernoulli trials. The study of the binomial distribution
continues inSectionC.5,anadvanceddiscussion ofthe“tails”ofthedistribution.
C.1 Counting
Counting theory tries to answer the question “How many?” without actually enu-
merating all the choices. For example, we might ask, “How many different n-bit
numbersarethere?”or “Howmanyorderingsofndistinctelementsarethere?” In
thissection,wereviewtheelementsofcountingtheory. Sincesomeofthematerial
assumes a basic understanding of sets, you might wish to start by reviewing the
materialinSectionB.1.
Rulesofsumandproduct
Wecansometimesexpressasetofitemsthatwewishtocountasaunionofdisjoint
setsorasaCartesianproductofsets.
Therule of sum says that the number of ways to choose one element from one
of two disjoint sets is the sum of the cardinalities of the sets. That is, if A and B
aretwofinitesetswithnomembersincommon,then A B A B ,which
j [ j D j jCj j
1184 AppendixC CountingandProbability
follows from equation (B.3). For example, each position on a car’s license plate
is a letter or a digit. The number of possibilities for each position is therefore
26 10 36, since there are 26 choices if it is a letter and 10 choices if it is a
C D
digit.
Theruleofproductsaysthatthenumberofwaystochooseanorderedpairisthe
numberofwaystochoosethefirstelementtimesthenumberofwaystochoosethe
second element. That is, if A and B are two finite sets, then A B A B ,
j  j D j jj j
which is simply equation (B.4). For example, if an ice-cream parlor offers 28
flavorsoficecreamand4toppings,thenumberofpossiblesundaeswithonescoop
oficecreamandonetopping is28 4 112.
 D
Strings
AstringoverafinitesetS isasequenceofelementsofS. Forexample,thereare8
binarystringsoflength3:
000;001;010;011;100;101;110;111 :
We sometimes call a string of length k a k-string. A substring s of a string s
0
is an ordered sequence of consecutive elements of s. A k-substring of a string
is a substring of length k. For example, 010 is a 3-substring of 01101001 (the
3-substring thatbeginsinposition 4),but111isnotasubstring of01101001.
We can view a k-string over a set S as an element of the Cartesian product Sk
of k-tuples; thus, there are S k strings of length k. For example, the number of
j j
binary k-strings is2k. Intuitively, toconstruct ak-string overan n-set, wehave n
waystopickthefirstelement;foreachofthesechoices,wehavenwaystopickthe
secondelement;andsoforthk times. Thisconstructionleadstothek-foldproduct
n n n nk asthenumberofk-strings.
  D
Permutations
A permutation of a finite set S is an ordered sequence of all the elements of S,
with each element appearing exactly once. For example, if S a;b;c , then S
D f g
has6permutations:
abc;acb;bac;bca;cab;cba:
There are nŠ permutations of a set of n elements, since we can choose the first
element of the sequence in n ways, the second in n 1 ways, the third in n 2
(cid:0) (cid:0)
ways,andsoon.
A k-permutation of S is an ordered sequence of k elements of S, with no ele-
mentappearingmorethanonceinthesequence. (Thus,anordinarypermutationis
ann-permutationofann-set.) Thetwelve2-permutationsoftheset a;b;c;d are
f g
C.1 Counting 1185
ab;ac;ad;ba;bc;bd;ca;cb;cd;da;db;dc :
Thenumberofk-permutations ofann-setis
nŠ
n.n 1/.n 2/ .n k 1/ ; (C.1)
(cid:0) (cid:0)  (cid:0) C D .n k/Š
(cid:0)
sincewehavenwaystochoosethefirstelement,n 1waystochoosethesecond
(cid:0)
element, and so on, until we have selected k elements, the last being a selection
fromtheremainingn k 1elements.
(cid:0) C
Combinations
A k-combination of an n-set S is simply a k-subset of S. Forexample, the 4-set
a;b;c;d hassix2-combinations:
f g
ab;ac;ad;bc;bd;cd :
(Here we use the shorthand of denoting the 2-subset a;b by ab, and so on.)
f g
We can construct a k-combination of an n-set by choosing k distinct (different)
elementsfromthen-set. Theorderinwhichweselecttheelementsdoesnotmatter.
Wecanexpressthenumberofk-combinationsofann-setintermsofthenumber
of k-permutations of an n-set. Every k-combination has exactly kŠ permutations
of its elements, each of which is a distinct k-permutation of the n-set. Thus, the
number of k-combinations of an n-set is the number of k-permutations divided
bykŠ;fromequation (C.1),thisquantity is
nŠ
: (C.2)
kŠ.n k/Š
(cid:0)
Fork 0,thisformulatellsusthatthenumberofwaystochoose0elementsfrom
D
ann-setis1(not0),since0Š 1.
D
Binomialcoefficients
The notation n (read “n choose k”) denotes the number of k-combinations of
k
ann-set. Fromequation (C.2),wehave
(cid:0) 
n nŠ
:
k D kŠ.n k/Š
!
(cid:0)
Thisformulaissymmetricink andn k:
(cid:0)
n n
: (C.3)
k D n k
! !
(cid:0)
1186 AppendixC CountingandProbability
Thesenumbersarealsoknownasbinomialcoefficients,duetotheirappearancein
thebinomialexpansion:
n
n
.x y/n xkyn k : (C.4)
(cid:0)
C D k
!
k 0
XD
Aspecialcaseofthebinomialexpansion occurswhenx y 1:
D D
n
n
2n :
D k
!
k 0
XD
This formula corresponds to counting the2n binary n-strings by thenumber of1s
they contain: n binary n-strings contain exactly k 1s, since wehave n ways to
k k
choosek outofthenpositions inwhichtoplacethe1s.
(cid:0)  (cid:0) 
Many identities involve binomial coefficients. The exercises at the end of this
section giveyoutheopportunity toproveafew.
Binomialbounds
We sometimes need to bound the size of a binomial coefficient. For 1 k n,
 
wehavethelowerbound
n n.n 1/ .n k 1/
(cid:0)  (cid:0) C
k D k.k 1/ 1
!
(cid:0) 
n n 1 n k 1
(cid:0) (cid:0) C
D k k 1  1
  (cid:0)   
n k
:
 k
 
Taking advantage of the inequality kŠ .k=e/k derived from Stirling’s approxi-

mation(3.18),weobtaintheupperbounds
n n.n 1/ .n k 1/
(cid:0)  (cid:0) C
k D k.k 1/ 1
!
(cid:0) 
nk
 kŠ
en k
: (C.5)
 k
 
Forallintegersk suchthat0 k n,wecanuseinduction(seeExerciseC.1-12)
 
toprovethebound
C.1 Counting 1187
n nn
; (C.6)
k  kk.n k/n k
! (cid:0) (cid:0)
whereforconvenience weassumethat00 1. Fork n,where0  1,we
D D  
canrewritethisboundas
n nn
n  .n/n..1 /n/.1 /n
! (cid:0) (cid:0)
n
1  1 1 
(cid:0)
D  1 
!
   (cid:0) 
2nH./ ;
D
where
H./ lg .1 /lg.1 / (C.7)
D(cid:0) (cid:0) (cid:0) (cid:0)
is the (binary) entropy function and where, for convenience, we assume that
0lg0 0,sothatH.0/ H.1/ 0.
D D D
Exercises
C.1-1
Howmanyk-substrings doesann-stringhave? (Consideridenticalk-substrings at
different positions to bedifferent.) Howmanysubstrings does an n-string have in
total?
C.1-2
An n-input, m-output boolean function is a function from TRUE;FALSE n to
TRUE;FALSE m . Howmanyn-input, 1-output boolean functiof ns arethere?g How
f g
manyn-input, m-outputbooleanfunctions arethere?
C.1-3
In how many ways can n professors sit around a circular conference table? Con-
sidertwoseatings tobethesameifonecanberotatedtoformtheother.
C.1-4
Inhowmanywayscanwechoosethreedistinctnumbersfromtheset 1;2;:::;99
f g
sothattheirsumiseven?
1188 AppendixC CountingandProbability
C.1-5
Provetheidentity
n n n 1
(cid:0) (C.8)
k D k k 1
! !
(cid:0)
for0 < k n.

C.1-6
Provetheidentity
n n n 1
(cid:0)
k D n k k
! !
(cid:0)
for0 k <n.

C.1-7
To choose k objects from n, you can make one of the objects distinguished and
consider whether the distinguished object is chosen. Use this approach to prove
that
n n 1 n 1
(cid:0) (cid:0) :
k D k C k 1
! ! !
(cid:0)
C.1-8
UsingtheresultofExerciseC.1-7,makeatableforn 0;1;:::;6and0 k n
n 0 1 D 1  
ofthebinomialcoefficients with atthetop, and onthenextline,and
k 0 0 1
soforth. Suchatableofbinomialcoefficients iscalledPascal’striangle.
(cid:0)  (cid:0)  (cid:0)  (cid:0) 
C.1-9
Provethat
n
n 1
i C :
D 2
!
i 1
XD
C.1-10
Show that for any integers n 0 and 0 k n, the expression n achieves its
   k
maximumvaluewhenk n=2 ork n=2 .
D b c D d e (cid:0) 
C.1-11 ?
Arguethatforanyintegersn 0,j 0,k 0,andj k n,
   C 
n n n j
(cid:0) : (C.9)
j k  j k
! ! !
C
C.2 Probability 1189
Provide both an algebraic proof and an argument based on amethod for choosing
j k itemsoutofn. Giveanexampleinwhichequalitydoesnothold.
C
C.1-12 ?
Use induction on all integers k such that 0 k n=2 to prove inequality (C.6),
 
anduseequation(C.3)toextendittoallintegersk suchthat0 k n.
 
C.1-13 ?
UseStirling’sapproximation toprovethat
2n 22n
.1 O.1=n//: (C.10)
n D pn C
!
C.1-14 ?
By differentiating the entropy function H./, show that it achieves its maximum
valueat 1=2. WhatisH.1=2/?
D
C.1-15 ?
Showthatforanyintegern 0,

n
n
k n2n 1 : (C.11)
(cid:0)
k D
!
k 0
XD
C.2 Probability
Probability isanessential toolforthedesignandanalysis ofprobabilistic andran-
domizedalgorithms. Thissectionreviewsbasicprobability theory.
We define probability in terms of a sample space S, which is a set whose ele-
ments are called elementary events. We can think of each elementary event as a
possibleoutcomeofanexperiment. Fortheexperimentofflippingtwodistinguish-
ablecoins,witheachindividualflipresultinginahead(H)oratail(T),wecanview
thesamplespaceasconsisting ofthesetofallpossible 2-strings over H;T :
f g
S HH;HT;TH;TT :
D f g
1190 AppendixC CountingandProbability
Aneventisasubset1 of thesample space S. Forexample, in theexperiment of
flipping two coins, the event of obtaining one head and one tail is HT;TH . The
f g
eventS iscalledthecertainevent,andtheevent iscalledthenullevent. Wesay
;
thattwoeventsAandB aremutuallyexclusiveifA B . Wesometimestreat
\ D;
an elementary event s S as the event s . By definition, all elementary events
2 f g
aremutuallyexclusive.
Axiomsofprobability
AprobabilitydistributionPr onasamplespaceS isamappingfromeventsofS
fg
torealnumberssatisfying thefollowingprobability axioms:
1. Pr A 0foranyeventA.
f g
2. Pr S 1.
f g D
3. Pr A B Pr A Pr B for any two mutually exclusive events A
f [ g D f g C f g
andB. Moregenerally, forany(finiteorcountably infinite)sequenceofevents
A ;A ;:::thatarepairwisemutuallyexclusive,
1 2
Pr A Pr A :
i i
D f g
( )
i i
[ X
We call Pr A the probability of the event A. We note here that axiom 2 is a
f g
normalization requirement: there is really nothing fundamental about choosing 1
astheprobability ofthecertainevent,exceptthatitisnaturalandconvenient.
Several results follow immediately from these axioms and basic set theory (see
Section B.1). The null event has probability Pr 0. If A B, then
; f;g D 
Pr A Pr B . Using A to denote the event S A (the complement of A),
f g  f g (cid:0)
wehavePr A 1 Pr A . ForanytwoeventsAandB,
D (cid:0) f g
Pr A B ˚ (cid:9) Pr A Pr B Pr A B (C.12)
f [ g D f gC f g(cid:0) f \ g
Pr A Pr B : (C.13)
 f gC f g
1Forageneralprobabilitydistribution,theremaybesomesubsetsofthesamplespaceS thatarenot
consideredtobeevents. Thissituationusuallyariseswhenthesamplespaceisuncountablyinfinite.
Themainrequirementforwhatsubsetsareeventsisthatthesetofeventsofasamplespacebeclosed
undertheoperationsoftakingthecomplementofanevent,formingtheunionofafiniteorcountable
number of events, and taking the intersection of a finite or countable number of events. Most of
the probability distributionswe shall see are over finiteor countable sample spaces, and we shall
generallyconsiderallsubsetsofasamplespacetobeevents. Anotableexceptionisthecontinuous
uniformprobabilitydistribution,whichweshallseeshortly.
C.2 Probability 1191
In our coin-flipping example, suppose that each of the four elementary events
hasprobability 1=4. Thentheprobability ofgettingatleastoneheadis
Pr HH;HT;TH Pr HH Pr HT Pr TH
f g D f gC f gC f g
3=4:
D
Alternatively, since the probability of getting strictly less than one head is
Pr TT 1=4,theprobability ofgetting atleastoneheadis1 1=4 3=4.
f g D (cid:0) D
Discreteprobabilitydistributions
Aprobabilitydistributionisdiscreteifitisdefinedoverafiniteorcountablyinfinite
samplespace. LetS bethesamplespace. ThenforanyeventA,
Pr A Pr s ;
f g D f g
s A
X2
since elementary events, specifically those in A, are mutually exclusive. If S is
finiteandeveryelementary events S hasprobability
2
Pr s 1= S ;
f g D j j
thenwehavetheuniformprobability distribution onS. Insuchacasetheexperi-
mentisoftendescribed as“picking anelementofS atrandom.”
As an example, consider the process of flipping a fair coin, one for which the
probabilityofobtainingaheadisthesameastheprobabilityofobtainingatail,that
is, 1=2. If we flip the coin n times, we have the uniform probability distribution
defined on the sample space S H;T n , aset ofsize 2n. Wecan represent each
D f g
elementaryeventinS asastringoflengthnover H;T ,eachstringoccurringwith
f g
probability 1=2n. Theevent
A exactlyk headsandexactlyn k tailsoccur
D f (cid:0) g
isasubset ofS ofsize A n ,since n strings oflength nover H;T contain
j j D k k f g
exactlyk H’s. Theprobability ofeventAisthusPr A n =2n.
(cid:0)  (cid:0)  f g D k
(cid:0) 
Continuousuniformprobabilitydistribution
The continuous uniform probability distribution is an example of a probability
distribution in which not all subsets of the sample space are considered to be
events. The continuous uniform probability distribution is defined over a closed
interval Œa;bof the reals, where a < b. Ourintuition isthat each point inthe in-
tervalŒa;bshouldbe“equallylikely.” Thereareanuncountablenumberofpoints,
however,soifwegiveallpointsthesamefinite,positiveprobability, wecannotsi-
multaneously satisfyaxioms2and3. Forthisreason, wewouldliketoassociate a
1192 AppendixC CountingandProbability
probability only with some of the subsets of S, in such a way that the axioms are
satisfiedfortheseevents.
For any closed interval Œc;d, where a c d b, the continuous uniform
  
probability distribution definestheprobability oftheeventŒc;dtobe
d c
Pr Œc;d (cid:0) :
f g D b a
(cid:0)
Note that for any point x Œx;x, the probability of x is 0. If we remove
D
the endpoints of an interval Œc;d, we obtain the open interval .c;d/. Since
Œc;d Œc;c .c;d/ Œd;d, axiom 3 gives us Pr Œc;d Pr .c;d/ . Gen-
D [ [ f g D f g
erally,thesetofeventsforthecontinuousuniformprobabilitydistributioncontains
any subset of the sample space Œa;b that can be obtained by a finite or countable
unionofopenandclosedintervals, aswellascertain morecomplicated sets.
Conditionalprobabilityandindependence
Sometimes wehave someprior partial knowledge about theoutcome of anexper-
iment. For example, suppose that a friend has flipped two fair coins and has told
you that atleast oneofthe coins showed ahead. Whatistheprobability thatboth
coinsareheads? Theinformationgiveneliminatesthepossibility oftwotails. The
three remaining elementary events are equally likely, so weinfer that each occurs
withprobability 1=3. Sinceonlyoneoftheseelementary eventsshowstwoheads,
theanswertoourquestionis1=3.
Conditional probability formalizes thenotion ofhaving priorpartial knowledge
oftheoutcomeofanexperiment. Theconditional probability ofaneventAgiven
thatanother eventB occursisdefinedtobe
Pr A B
Pr A B f \ g (C.14)
f j g D Pr B
f g
whenever Pr B 0. (We read “Pr A B ” as “the probability of A given B.”)
f g ¤ f j g
Intuitively, since we are given that event B occurs, the event that A also occurs
is A B. That is, A B is the set of outcomes in which both A and B occur.
\ \
BecausetheoutcomeisoneoftheelementaryeventsinB,wenormalizetheprob-
abilities ofalltheelementary eventsinB bydividing thembyPr B ,sothatthey
f g
sum to 1. The conditional probability of A given B is, therefore, the ratio of the
probability ofeventA B totheprobability ofeventB. Intheexampleabove,A
\
isthe event that both coins areheads, and B istheevent that atleast onecoin isa
head. Thus,Pr A B .1=4/=.3=4/ 1=3.
f j g D D
Twoeventsareindependentif
Pr A B Pr A Pr B ; (C.15)
f \ g D f g f g
whichisequivalent, ifPr B 0,tothecondition
f g ¤
C.2 Probability 1193
Pr A B Pr A :
f j g D f g
For example, suppose that we flip two fair coins and that the outcomes are inde-
pendent. Then the probability of two heads is .1=2/.1=2/ 1=4. Now suppose
D
that one event is that the first coin comes up heads and the other event is that the
coins come up differently. Each of these events occurs with probability 1=2, and
the probability that both events occur is 1=4; thus, according to the definition of
independence, theeventsareindependent—even though youmightthinkthatboth
events depend on the first coin. Finally, suppose that the coins are welded to-
gethersothattheybothfallheadsorbothfalltailsandthatthetwopossibilities are
equally likely. Then the probability that each coin comes up heads is 1=2, but the
probability thattheyboth comeupheads is1=2 .1=2/.1=2/. Consequently, the
¤
eventthatonecomesupheadsandtheeventthattheothercomesupheadsarenot
independent.
Acollection A ;A ;:::;A ofeventsissaidtobepairwiseindependentif
1 2 n
Pr A A Pr A Pr A
i j i j
f \ g D f g f g
for all 1 i < j n. We say that the events of the collection are (mutually)
 
independentifeveryk-subsetA ;A ;:::;A ofthecollection,where2 k n
i1 i2 ik
 
and1 i <i < < i n,satisfies
1 2 k
  
Pr A A A Pr A Pr A Pr A :
f
i1
\
i2
\\
ikg
D f
i1g
f
i2g
f
ikg
Forexample, suppose wefliptwofaircoins. LetA betheeventthatthefirstcoin
1
isheads, letA be the event that the second coin isheads, and let A be the event
2 3
thatthetwocoinsaredifferent. Wehave
Pr A 1=2;
1
f g D
Pr A 1=2;
2
f g D
Pr A 1=2;
3
f g D
Pr A A 1=4;
1 2
f \ g D
Pr A A 1=4;
1 3
f \ g D
Pr A A 1=4;
2 3
f \ g D
Pr A A A 0:
1 2 3
f \ \ g D
Since for 1 i < j 3, we have Pr A A Pr A Pr A 1=4, the
i j i j
  f \ g D f g f g D
eventsA ,A ,andA arepairwiseindependent. Theeventsarenotmutuallyinde-
1 2 3
pendent, however,because Pr A A A 0andPr A Pr A Pr A
1 2 3 1 2 3
f \ \ g D f g f g f g D
1=8 0.
¤
1194 AppendixC CountingandProbability
Bayes’stheorem
From the definition of conditional probability (C.14) and the commutative law
A B B A, it follows that for two events A and B, each with nonzero
\ D \
probability,
Pr A B Pr B Pr A B (C.16)
f \ g D f g f j g
Pr A Pr B A :
D f g f j g
SolvingforPr A B ,weobtain
f j g
Pr A Pr B A
Pr A B f g f j g ; (C.17)
f j g D Pr B
f g
which is known as Bayes’s theorem. The denominator Pr B is a normalizing
f g
constant, which we can reformulate as follows. Since B .B A/ .B A/,
D \ [ \
andsinceB AandB Aaremutuallyexclusiveevents,
\ \
Pr B Pr B A Pr B A
f g D f \ gC \
Pr A Pr B A ˚ Pr A (cid:9) Pr B A :
D f g f j gC j
Substituting into equation (C.17), we obtain an equivalent form of Bayes’s theo-
˚ (cid:9) ˚ (cid:9)
rem:
Pr A Pr B A
Pr A B f g f j g : (C.18)
f j g D Pr A Pr B A Pr A Pr B A
f g f j gC j
Bayes’s theorem can simplify the computing of conditional probabilities. For
˚ (cid:9) ˚ (cid:9)
example, suppose that wehave afair coinand abiased cointhat always comesup
heads. We run an experiment consisting of three independent events: we choose
one of the two coins at random, we flip that coin once, and then we flip it again.
Suppose that the coin we have chosen comes up heads both times. What is the
probability thatitisbiased?
WesolvethisproblemusingBayes’stheorem. LetAbetheeventthatwechoose
the biased coin, and let B be the event that the chosen coin comes up heads both
times. Wewishto determine Pr A B . Wehave Pr A 1=2, Pr B A 1,
f j g f g D f j g D
Pr A 1=2,andPr B A 1=4;hence,
D j D
.1=2/ 1
˚ (cid:9) ˚ (cid:9)
Pr A B 
f j g D .1=2/ 1 .1=2/ .1=4/
 C 
4=5:
D
Exercises
C.2-1
Professor Rosencrantz flips a fair coin once. Professor Guildenstern flips a fair
cointwice. Whatistheprobability thatProfessorRosencrantz obtainsmoreheads
thanProfessorGuildenstern?
C.2 Probability 1195
C.2-2
Prove Boole’s inequality: For any finite or countably infinite sequence of events
A ;A ;:::,
1 2
Pr A A Pr A Pr A : (C.19)
1 2 1 2
f [ [g  f gC f gC
C.2-3
Supposeweshuffleadeckof10cards,eachbearingadistinctnumberfrom1to10,
to mix the cards thoroughly. We then remove three cards, one at a time, from the
deck. What is the probability that we select the three cards in sorted (increasing)
order?
C.2-4
Provethat
Pr A B Pr A B 1:
f j gC j D
˚ (cid:9)
C.2-5
Provethatforanycollection ofeventsA ;A ;:::;A ,
1 2 n
Pr A A A Pr A Pr A A Pr A A A
1 2 n 1 2 1 3 1 2
f \ \\ g D f g f j g f j \ g
Pr A A A A :
n 1 2 n 1
f j \ \\ (cid:0) g
C.2-6 ?
Describe aprocedure thattakes asinput twointegers a and b suchthat 0< a < b
and, using fair coin flips, produces as output heads with probability a=b and tails
with probability .b a/=b. Give a bound on the expected number of coin flips,
(cid:0)
whichshouldbeO.1/. (Hint:Representa=b inbinary.)
C.2-7 ?
Showhowtoconstructasetofneventsthatarepairwiseindependent butsuchthat
nosubsetofk > 2ofthemismutuallyindependent.
C.2-8 ?
TwoeventsAandB areconditionally independent,givenC,if
Pr A B C Pr A C Pr B C :
f \ j g D f j g f j g
Giveasimplebutnontrivialexampleoftwoeventsthatarenotindependentbutare
conditionally independent givenathirdevent.
C.2-9 ?
You are a contestant in a game show in which a prize is hidden behind one of
three curtains. You will win the prize if you select the correct curtain. After you
1196 AppendixC CountingandProbability
have picked one curtain but before the curtain is lifted, the emcee lifts one of the
other curtains, knowing that it will reveal an empty stage, and asks if you would
like to switch from your current selection to the remaining curtain. How would
your chances change if you switch? (This question is the celebrated Monty Hall
problem, namedafteragame-showhostwhooftenpresented contestants withjust
thisdilemma.)
C.2-10 ?
A prison warden has randomly picked one prisoner among three to go free. The
other twowillbeexecuted. Theguardknowswhichonewillgofreebutisforbid-
dentogiveanyprisoner information regarding hisstatus. Letuscalltheprisoners
X, Y, and Z. Prisoner X asks the guard privately which of Y or Z will be exe-
cuted, arguing that since he already knows that at least one of them must die, the
guard won’t berevealing anyinformation about hisownstatus. Theguard tellsX
thatY istobeexecuted. PrisonerX feels happier now,sincehefiguresthateither
he or prisoner Z will go free, which means that his probability of going free is
now1=2. Isheright,orarehischances still1=3? Explain.
C.3 Discreterandom variables
A (discrete) random variable X is a function from a finite or countably infinite
samplespaceS totherealnumbers. Itassociatesarealnumberwitheachpossible
outcome of an experiment, which allows us to work with the probability distribu-
tioninducedontheresultingsetofnumbers. Randomvariablescanalsobedefined
for uncountably infinite sample spaces, but they raise technical issues that are un-
necessary to address for our purposes. Henceforth, we shall assume that random
variables arediscrete.
Forarandom variable X andarealnumberx,wedefinetheeventX x tobe
D
s S X.s/ x ;thus,
f 2 W D g
Pr X x Pr s :
f D gD f g
s SX.s/ x
2 XW D
Thefunction
f.x/ Pr X x
D f D g
istheprobability densityfunctionoftherandomvariableX. Fromtheprobability
axioms,Pr X x 0and Pr X x 1.
f D g x f D gD
As an example, consider the experiment of rolling a pair of ordinary, 6-sided
P
dice. There are 36 possible elementary events in the sample space. We assume
C.3 Discreterandomvariables 1197
thattheprobability distribution isuniform, sothat eachelementary event s S is
2
equallylikely: Pr s 1=36. DefinetherandomvariableX tobethemaximumof
f g D
thetwovaluesshowingonthedice. WehavePr X 3 5=36,sinceX assigns
f D g D
avalueof3to5ofthe36possibleelementaryevents,namely,.1;3/,.2;3/,.3;3/,
.3;2/,and.3;1/.
Weoftendefineseveralrandomvariablesonthesamesamplespace. IfX andY
arerandom variables, thefunction
f.x;y/ Pr X x andY y
D f D D g
isthejointprobability densityfunctionofX andY. Forafixedvaluey,
Pr Y y Pr X x andY y ;
f D g D f D D g
x
X
andsimilarly, forafixedvaluex,
Pr X x Pr X x andY y :
f D gD f D D g
y
X
Usingthedefinition (C.14)ofconditional probability, wehave
Pr X x andY y
Pr X x Y y f D D g :
f D j D g D Pr Y y
f D g
Wedefinetworandom variablesX andY tobeindependentifforallx andy,the
events X x and Y y are independent or, equivalently, if for all x and y, we
D D
havePr X x andY y Pr X x Pr Y y .
f D D g D f D g f D g
Given a set of random variables defined over the same sample space, we can
define new random variables as sums, products, or other functions of the original
variables.
Expectedvalueofarandomvariable
Thesimplest and most useful summary of the distribution of arandom variable is
the “average” of the values it takes on. The expected value (or, synonymously,
expectation ormean)ofadiscreterandom variableX is
EŒX x Pr X x ; (C.20)
D  f D g
x
X
which is well defined if the sum is finite or converges absolutely. Sometimes the
expectation ofX isdenoted by or, whenthe random variable isapparent from
X
context,simplyby.
Consideragameinwhichyoufliptwofaircoins. Youearn$3foreachheadbut
lose $2 for each tail. The expected value of the random variable X representing
1198 AppendixC CountingandProbability
yourearningsis
EŒX 6 Pr 2H’s 1 Pr 1H,1T 4 Pr 2 T’s
D  f gC  f g(cid:0)  f g
6.1=4/ 1.1=2/ 4.1=4/
D C (cid:0)
1:
D
Theexpectation ofthesumoftworandomvariablesisthesumoftheirexpecta-
tions, thatis,
EŒX Y EŒX EŒY ; (C.21)
C D C
whenever EŒX and EŒY are defined. We call this property linearity of expecta-
tion,anditholdsevenifX andY arenotindependent. Italsoextendstofiniteand
absolutely convergent summations ofexpectations. Linearity ofexpectation isthe
key property that enables us to perform probabilistic analyses by using indicator
random variables(seeSection5.2).
If X is any random variable, any function g.x/ defines a new random vari-
ableg.X/. Iftheexpectation ofg.X/isdefined,then
EŒg.X/ g.x/ Pr X x :
D  f D g
x
X
Lettingg.x/ ax,wehaveforanyconstant a,
D
EŒaX aEŒX : (C.22)
D
Consequently, expectations arelinear: foranytworandom variables X andY and
anyconstant a,
EŒaX Y aEŒX EŒY : (C.23)
C D C
When two random variables X and Y are independent and each has a defined
expectation,
EŒXY xy Pr X x andY y
D  f D D g
x y
XX
xy Pr X x Pr Y y
D  f D g f D g
x y
XX
x Pr X x y Pr Y y
D  f D g  f D g
! !
x y
X X
EŒXEŒY :
D
Ingeneral, whennrandomvariables X ;X ;:::;X aremutuallyindependent,
1 2 n
EŒX X X  EŒX EŒX  EŒX  : (C.24)
1 2 n 1 2 n
 D 
C.3 Discreterandomvariables 1199
When a random variable X takes on values from the set of natural numbers
N 0;1;2;::: ,wehaveaniceformulaforitsexpectation:
D f g
1
EŒX i Pr X i
D  f D g
i 0
XD
1
i.Pr X i Pr X i 1 /
D f  g(cid:0) f  C g
i 0
XD
1
Pr X i ; (C.25)
D f  g
i 1
XD
since each term Pr X i is added in i times and subtracted out i 1 times
f  g (cid:0)
(exceptPr X 0 ,whichisaddedin0timesandnotsubtracted outatall).
f  g
When we apply a convex function f.x/ to a random variable X, Jensen’s in-
equalitygivesus
EŒf.X/ f.EŒX/; (C.26)

provided that the expectations exist and are finite. (A function f.x/ is convex
if for all x and y and for all 0  1, we have f.x .1 /y/
  C (cid:0) 
f.x/ .1 /f.y/.)
C (cid:0)
Varianceandstandarddeviation
The expected value of a random variable does not tell us how “spread out” the
variable’svaluesare. Forexample,ifwehaverandomvariablesX andY forwhich
Pr X 1=4 Pr X 3=4 1=2 and Pr Y 0 Pr Y 1 1=2,
f D g D f D g D f D g D f D g D
then both EŒX and EŒY are 1=2, yet the actual values taken on by Y are farther
fromthemeanthantheactualvaluestakenonbyX.
The notion of variance mathematically expresses how far from the mean a ran-
dom variable’s values are likely to be. Thevariance of a random variable X with
meanEŒXis
VarŒX E .X EŒX/2
D (cid:0)
EX2 2XEŒX  E2ŒX
D (cid:0) C
EX2 2EŒXEŒX E2 ŒX
D (cid:0) C
EX2

2E2ŒX E2ŒX
D (cid:0) C
E X2 E2ŒX : (C.27)
 
D (cid:0)
Tojustify the equality EŒE2ŒX E2ŒX, note that because EŒXisareal num-
D
ber and not a random variable, so is E2ŒX. The equality EŒXEŒX E2ŒX
D
1200 AppendixC CountingandProbability
follows from equation (C.22), with a EŒX. Rewriting equation (C.27) yields
D
anexpression fortheexpectation ofthesquareofarandomvariable:
E X2 VarŒX E2ŒX : (C.28)
D C
Thevariance of a random variable X and the variance of aX are related (see
ExerciseC.3-10):
VarŒaX a2VarŒX :
D
WhenX andY areindependent random variables,
VarŒX Y VarŒX VarŒY :
C D C
Ingeneral, ifnrandomvariables X ;X ;:::;X arepairwiseindependent, then
1 2 n
n n
Var X VarŒX  : (C.29)
i i
D
" #
i 1 i 1
XD XD
The standard deviation of a random variable X is the nonnegative square root
ofthevarianceofX. Thestandard deviationofarandomvariable X issometimes
denoted  or simply  when the random variable X is understood from context.
X
Withthisnotation, thevarianceofX isdenoted 2.
Exercises
C.3-1
Suppose we roll two ordinary, 6-sided dice. What is the expectation of the sum
of the two values showing? What is the expectation of the maximum of the two
valuesshowing?
C.3-2
AnarrayAŒ1::ncontainsndistinctnumbersthatarerandomlyordered,witheach
permutation ofthe n numbers being equally likely. Whatis the expectation of the
index of the maximum element in the array? What isthe expectation of the index
oftheminimumelementinthearray?
C.3-3
A carnival gameconsists of three dice ina cage. Aplayer can bet a dollar on any
ofthenumbers1through6. Thecageisshaken,andthepayoffisasfollows. Ifthe
player’s number doesn’t appear on any of the dice, he loses his dollar. Otherwise,
if his number appears on exactly k of the three dice, for k 1;2;3, he keeps his
D
dollarandwinskmoredollars. Whatishisexpectedgainfromplayingthecarnival
gameonce?
C.4 Thegeometricandbinomialdistributions 1201
C.3-4
ArguethatifX andY arenonnegativerandom variables, then
EŒmax.X;Y/ EŒX EŒY :
 C
C.3-5 ?
Let X and Y be independent random variables. Prove that f.X/ and g.Y/ are
independent foranychoiceoffunctions f andg.
C.3-6 ?
Let X be a nonnegative random variable, and suppose that EŒX is well defined.
ProveMarkov’sinequality:
Pr X t EŒX=t (C.30)
f  g 
forallt > 0.
C.3-7 ?
Let S be a sample space, and let X and X be random variables such that
0
X.s/ X .s/foralls S. Provethatforanyrealconstantt,
0
 2
Pr X t Pr X t :
0
f  g  f  g
C.3-8
Which is larger: the expectation of the square of a random variable, or the square
ofitsexpectation?
C.3-9
ShowthatforanyrandomvariableX thattakesononlythevalues0and1,wehave
VarŒX EŒXEŒ1 X.
D (cid:0)
C.3-10
ProvethatVarŒaX a2VarŒXfromthedefinition(C.27)ofvariance.
D
C.4 The geometricand binomialdistributions
Wecan think ofa coin flipas an instance of aBernoulli trial, which is anexperi-
mentwithonly twopossible outcomes: success, whichoccurs withprobability p,
andfailure,whichoccurswithprobabilityq 1 p. WhenwespeakofBernoulli
D (cid:0)
trialscollectively, wemeanthatthetrialsaremutuallyindependent and,unlesswe
specifically say otherwise, that each has the same probability p for success. Two
1202 AppendixC CountingandProbability
2 k (cid:0)1 1
3 3
   
0.35
0.30
0.25
0.20
0.15
0.10
0.05
k
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Figure C.1 A geometric distribution with probability p 1=3 of success and a probability
D
q 1 poffailure.Theexpectationofthedistributionis1=p 3.
D (cid:0) D
important distributions arise from Bernoulli trials: the geometric distribution and
thebinomialdistribution.
Thegeometricdistribution
Suppose wehave asequence ofBernoulli trials, each with a probability p of suc-
cessandaprobabilityq 1 poffailure. Howmanytrialsoccurbeforeweobtain
D (cid:0)
a success? Letus define the random variable X be the number of trials needed to
obtainasuccess. ThenX hasvaluesintherange 1;2;::: ,andfork 1,
f g 
Pr X k qk 1p ; (C.31)
(cid:0)
f D g D
sincewehavek 1failuresbeforetheonesuccess. Aprobability distribution sat-
(cid:0)
isfyingequation(C.31)issaidtobeageometricdistribution. FigureC.1illustrates
suchadistribution.
C.4 Thegeometricandbinomialdistributions 1203
Assuming that q < 1, wecan calculate the expectation of a geometric distribu-
tionusingidentity(A.8):
1
EŒX kqk 1p
(cid:0)
D
k 1
XD
p 1
kqk
D q
k 0
p
XD
q
D q  .1 q/2
(cid:0)
p q
D q  p2
1=p : (C.32)
D
Thus,onaverage,ittakes1=p trialsbeforeweobtainasuccess,anintuitiveresult.
Thevariance, whichcanbecalculated similarly,butusingExerciseA.1-3,is
VarŒX q=p2 : (C.33)
D
As an example, suppose we repeatedly roll two dice until we obtain either a
seven or an eleven. Of the 36 possible outcomes, 6 yield a seven and 2 yield an
eleven. Thus, the probability of success is p 8=36 2=9, and we must roll
D D
1=p 9=2 4:5timesonaveragetoobtainasevenoreleven.
D D
Thebinomialdistribution
Howmanysuccesses occur during n Bernoulli trials, whereasuccess occurs with
probability p and a failure with probability q 1 p? Define the random vari-
D (cid:0)
able X to be the number of successes in n trials. Then X has values in the range
0;1;:::;n ,andfork 0;1;:::;n,
f g D
n
Pr X k pkqn k ; (C.34)
(cid:0)
f D gD k
!
since there are n ways to pick which k of the n trials are successes, and the
k
probability thateach occurs ispkqn k. Aprobability distribution satisfying equa-
(cid:0)  (cid:0)
tion (C.34) is said to be a binomial distribution. For convenience, we define the
familyofbinomialdistributions usingthenotation
n
b.k n;p/ pk.1 p/n k : (C.35)
(cid:0)
I D k (cid:0)
!
FigureC.2illustratesabinomialdistribution. Thename“binomial”comesfromthe
right-handsideofequation(C.34)beingthekthtermoftheexpansionof.p q/n.
C
Consequently, sincep q 1,
C D
1204 AppendixC CountingandProbability
b (k; 15, 1/3)
0.25
0.20
0.15
0.10
0.05
k
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
FigureC.2 Thebinomial distributionb.k 15;1=3/ resultingfromn 15Bernoullitrials,each
I D
withprobabilityp 1=3ofsuccess.Theexpectationofthedistributionisnp 5.
D D
n
b.k n;p/ 1; (C.36)
I D
k 0
XD
asaxiom2oftheprobability axiomsrequires.
Wecan compute the expectation of arandom variable having a binomial distri-
bution from equations (C.8) and (C.36). Let X be a random variable that follows
thebinomialdistributionb.k n;p/,andletq 1 p. Bythedefinitionofexpec-
I D (cid:0)
tation, wehave
n
EŒX k Pr X k
D  f D g
k 0
XD
n
k b.k n;p/
D  I
k 0
XD
n
n
k pkqn k
(cid:0)
D k
!
k 1
XD
n
n 1
np (cid:0) pk 1qn k (byequation (C.8))
(cid:0) (cid:0)
D k 1
!
k 1 (cid:0)
XD
n 1
(cid:0) n 1
np (cid:0) pkq.n 1/ k
(cid:0) (cid:0)
D k
!
k 0
XD
C.4 Thegeometricandbinomialdistributions 1205
n 1
(cid:0)
np b.k n 1;p/
D I (cid:0)
k 0
XD
np (byequation (C.36)) . (C.37)
D
By using the linearity of expectation, we can obtain the same result with sub-
stantially less algebra. Let X be the random variable describing the number of
i
successes in the ith trial. Then EŒX  p 1 q 0 p, and by linearity of
i
D  C  D
expectation (equation (C.21)),theexpectednumberofsuccesses forntrialsis
n
EŒX E X
i
D
" #
i 1
XD
n
EŒX 
i
D
i 1
XD
n
p
D
i 1
XD
np : (C.38)
D
Wecanusethesameapproachtocalculatethevarianceofthedistribution. Using
equation (C.27),wehaveVarŒX  EŒX2 E2ŒX . SinceX onlytakes onthe
i D i (cid:0) i i
values0and1,wehaveX2 X ,whichimpliesEŒX2 EŒX  p. Hence,
i D i i D i D
VarŒX  p p2 p.1 p/ pq : (C.39)
i
D (cid:0) D (cid:0) D
Tocompute the variance of X, wetake advantage of the independence ofthe n
trials;thus,byequation (C.29),
n
VarŒX Var X
i
D
" #
i 1
XD
n
VarŒX 
i
D
i 1
XD
n
pq
D
i 1
XD
npq : (C.40)
D
AsFigureC.2shows,thebinomialdistribution b.k n;p/increases withk until
I
it reaches the mean np, and then it decreases. We can prove that the distribution
alwaysbehavesinthismannerbylooking attheratioofsuccessiveterms:
1206 AppendixC CountingandProbability
b.k n;p/ n pkqn k
I k (cid:0)
b.k 1 n;p/ D n pk 1qn k 1
(cid:0) I k 1(cid:0)  (cid:0) (cid:0) C
(cid:0)
nŠ.k 1/Š.n k 1/Šp
(cid:0) (cid:0) (cid:0) C
D kŠ.n k/ŠnŠq
(cid:0)
.n k 1/p
(cid:0) C (C.41)
D kq
.n 1/p k
1 C (cid:0) :
D C kq
This ratio is greater than 1 precisely when .n 1/p k is positive. Conse-
C (cid:0)
quently, b.k n;p/ >b.k 1 n;p/fork < .n 1/p (thedistribution increases),
I (cid:0) I C
and b.k n;p/ < b.k 1 n;p/ for k > .n 1/p (the distribution decreases).
I (cid:0) I C
If k .n 1/p is an integer, then b.k n;p/ b.k 1 n;p/, and so the distri-
D C I D (cid:0) I
butionthenhastwomaxima: atk .n 1/pandatk 1 .n 1/p 1 np q.
D C (cid:0) D C (cid:0) D (cid:0)
Otherwise, it attains a maximum at the unique integer k that lies in the range
np q < k < .n 1/p.
(cid:0) C
Thefollowinglemmaprovides anupperboundonthebinomialdistribution.
LemmaC.1
Letn 0,let0< p < 1,letq 1 p,andlet0 k n. Then
 D (cid:0)  
np k nq n k
b.k n;p/ (cid:0) :
I  k n k
   (cid:0) 
Proof Usingequation (C.6),wehave
n
b.k n;p/ pkqn k
(cid:0)
I D k
!
n k n n k
(cid:0) pkqn k
(cid:0)
 k n k
np k (cid:0) nq n k
(cid:0) :
D k n k
   (cid:0) 
Exercises
C.4-1
Verifyaxiom2oftheprobability axiomsforthegeometric distribution.
C.4-2
How many times on average must we flip 6 fair coins before we obtain 3 heads
and3tails?
C.4 Thegeometricandbinomialdistributions 1207
C.4-3
Showthatb.k n;p/ b.n k n;q/,whereq 1 p.
I D (cid:0) I D (cid:0)
C.4-4
Showthatvalueofthemaximumofthebinomialdistribution b.k n;p/isapprox-
I
imately1=p2npq,whereq 1 p.
D (cid:0)
C.4-5 ?
ShowthattheprobabilityofnosuccessesinnBernoullitrials,eachwithprobability
p 1=n, is approximately 1=e. Show that the probability of exactly one success
D
isalsoapproximately 1=e.
C.4-6 ?
ProfessorRosencrantzflipsafaircoinntimes,andsodoesProfessorGuildenstern.
Showthattheprobabilitythattheygetthesamenumberofheadsis 2n =4n. (Hint:
n
For Professor Rosencrantz, call a head a success; for Professor Guildenstern, call
(cid:0) 
atailasuccess.) Useyourargument toverifytheidentity
n 2
n 2n
:
k D n
! !
k 0
XD
C.4-7 ?
Showthatfor0 k n,
 
b.k n;1=2/ 2nH.k=n/ n ;
(cid:0)
I 
whereH.x/istheentropyfunction (C.7).
C.4-8 ?
Consider n Bernoulli trials, where for i 1;2;:::;n, the ith trial has probabil-
D
ity p of success, and let X be the random variable denoting the total number of
i
successes. Letp p foralli 1;2;:::;n. Provethatfor1 k n,
i
 D  
k 1
(cid:0)
Pr X < k b.i n;p/:
f g  I
i 0
XD
C.4-9 ?
Let X be the random variable for the total number of successes in a set A of n
Bernoulli trials, where the ith trial has a probability p of success, and let X
i 0
be the random variable for the total number of successes in a second set A of n
0
Bernoullitrials,wheretheithtrialhasaprobabilityp p ofsuccess. Provethat
i0

i
for0 k n,
 
1208 AppendixC CountingandProbability
Pr X k Pr X k :
0
f  g  f  g
(Hint: Show how to obtain the Bernoulli trials in A by an experiment involving
0
thetrialsofA,andusetheresultofExerciseC.3-7.)
? C.5 The tailsofthe binomialdistribution
The probability of having at least, or at most, k successes in n Bernoulli trials,
eachwithprobability p ofsuccess, isoftenofmoreinterestthantheprobability of
havingexactlyk successes. Inthissection, weinvestigate thetailsofthebinomial
distribution: the two regions of the distribution b.k n;p/ that are far from the
I
mean np. We shall prove several important bounds on (the sum of all terms in) a
tail.
We firstprovide a bound on the right tail of the distribution b.k n;p/. Wecan
I
determine boundsonthelefttailbyinverting therolesofsuccesses andfailures.
TheoremC.2
ConsiderasequenceofnBernoullitrials,wheresuccessoccurswithprobabilityp.
Let X be the random variable denoting the total number of successes. Then for
0 k n,theprobability ofatleastk successes is
 
n
Pr X k b.i n;p/
f  g D I
i k
XD
n
pk :
 k
!
Proof For S 1;2;:::;n , we let A denote the event that the ith trial is a
S
 f g
success foreveryi S. ClearlyPr A pk if S k. Wehave
S
2 f gD j j D
Pr X k Pr thereexistsS 1;2;:::;n S k andA
S
f  g D f  f g W j j D g
Pr A S
D
S 1;2;:::;n S k
 f [gWj jD 
Pr A (byinequality (C.19))
S
 f g
S 1;2;:::;n S k
f XgWj jD
n
pk :
D k
!
C.5 Thetailsofthebinomialdistribution 1209
The following corollary restates the theorem for the left tail of the binomial
distribution. Ingeneral,weshallleaveittoyoutoadapttheproofsfromonetailto
theother.
CorollaryC.3
Consider a sequence of n Bernoulli trials, where success occurs with probabil-
ityp. IfX istherandom variabledenoting thetotalnumberofsuccesses, thenfor
0 k n,theprobability ofatmostk successes is
 
k
Pr X k b.i n;p/
f  g D I
i 0
XD
n
.1 p/n k
(cid:0)
 n k (cid:0)
!
(cid:0)
n
.1 p/n k :
(cid:0)
D k (cid:0)
!
Our next bound concerns the left tail of the binomial distribution. Its corollary
showsthat,farfromthemean,thelefttaildiminishes exponentially.
TheoremC.4
ConsiderasequenceofnBernoullitrials,wheresuccessoccurswithprobability p
andfailurewithprobabilityq 1 p. LetX betherandomvariabledenotingthe
D (cid:0)
total number of successes. Then for 0 < k < np, the probability of fewer than k
successes is
k 1
(cid:0)
Pr X < k b.i n;p/
f g D I
i 0
XD
kq
< b.k n;p/:
np k I
(cid:0)
Proof Weboundtheseries k i(cid:0)01b.i In;p/byageometricseriesusingthetech-
nique from Section A.2, page 1D151. For i 1;2;:::;k, we have from equa-
P D
tion(C.41),
b.i 1 n;p/ iq
(cid:0) I
b.i n;p/ D .n i 1/p
I (cid:0) C
iq
<
.n i/p
(cid:0)
kq
:
 .n k/p
(cid:0)
1210 AppendixC CountingandProbability
Ifwelet
kq
x
D .n k/p
(cid:0)
kq
<
.n np/p
(cid:0)
kq
D nqp
k
D np
< 1;
itfollowsthat
b.i 1 n;p/ < xb.i n;p/
(cid:0) I I
for0 < i k. Iteratively applying thisinequality k i times,weobtain
 (cid:0)
b.i n;p/ <xk i b.k n;p/
(cid:0)
I I
for0 i < k,andhence

k 1 k 1
(cid:0) (cid:0)
b.i n;p/ < xk ib.k n;p/
(cid:0)
I I
i 0 i 0
XD XD
1
< b.k n;p/ xi
I
i 1
x
XD
b.k n;p/
D 1 x I
(cid:0)
kq
b.k n;p/:
D np k I
(cid:0)
Corollary C.5
ConsiderasequenceofnBernoullitrials,wheresuccessoccurswithprobability p
andfailurewithprobabilityq 1 p. Thenfor0 <k np=2,theprobabilityof
D (cid:0) 
fewerthan k successes islessthan onehalfoftheprobability offewerthank 1
C
successes.
Proof Becausek np=2,wehave

kq .np=2/q
np k  np .np=2/
(cid:0) (cid:0)
C.5 Thetailsofthebinomialdistribution 1211
.np=2/q
D np=2
1; (C.42)

since q 1. Letting X betherandom variable denoting thenumber ofsuccesses,

TheoremC.4andinequality (C.42)implythattheprobability offewerthank suc-
cessesis
k 1
(cid:0)
Pr X < k b.i n;p/ < b.k n;p/:
f g D I I
i 0
XD
Thuswehave
Pr fX < k
g
k i(cid:0)01b.i In;p/
D
Pr X < k 1 D k b.i n;p/
f C g Pi 0 I
D
k 1b.i n;p/
P i(cid:0)0
I
D
D k 1b.i n;p/ b.k n;p/
i(cid:0)0 P
I C I
D
< 1=2;
P
since k i(cid:0)01b.i In;p/ < b.k In;p/.
D
P
Boundsontherighttailfollowsimilarly. ExerciseC.5-2asksyoutoprovethem.
CorollaryC.6
ConsiderasequenceofnBernoullitrials,wheresuccessoccurswithprobabilityp.
Let X be the random variable denoting the total number of successes. Then for
np < k < n,theprobability ofmorethank successes is
n
Pr X > k b.i n;p/
f g D I
i k 1
DXC
.n k/p
< (cid:0) b.k n;p/:
k np I
(cid:0)
CorollaryC.7
ConsiderasequenceofnBernoullitrials,wheresuccessoccurswithprobability p
and failure with probability q 1 p. Then for .np n/=2 < k < n, the
D (cid:0) C
probability of more than k successes is less than one half of the probability of
morethank 1successes.
(cid:0)
The next theorem considers n Bernoulli trials, each with a probability p of
i
success, for i 1;2;:::;n. As the subsequent corollary shows, we can use the
D
1212 AppendixC CountingandProbability
theorem toprovide aboundontherighttailofthebinomial distribution bysetting
p p foreachtrial.
i
D
TheoremC.8
ConsiderasequenceofnBernoullitrials,whereintheithtrial,fori 1;2;:::;n,
D
successoccurswithprobabilityp andfailureoccurswithprobabilityq 1 p .
i i i
D (cid:0)
Let X be the random variable describing the total number of successes, and let
 EŒX. Thenforr > ,
D
e r
Pr X  r :
f (cid:0)  g  r
 
Proof Sinceforany˛ > 0,thefunctione˛x isstrictlyincreasing inx,
Pr X  r Pr e˛.X / e˛r ; (C.43)
(cid:0)
f (cid:0)  g D 
wherewewilldetermi˚ne˛ later. Usin(cid:9)gMarkov’sinequality (C.30),weobtain
Pr e˛.X / e˛r E e˛.X / e ˛r : (C.44)
(cid:0) (cid:0) (cid:0)
 
T˚he bulk of the(cid:9)proofconsistsof bounding E e˛.X (cid:0)/ and substituting a suit-
able value for ˛ in inequality (C.44). First, we evaluate E e˛.X / . Using the
  (cid:0)
technique of indicator random variables (see Section 5.2), let X I theith
i
 D f
Bernoulli trial is a success for i 1;2;:::;n; that is, X is the random vari-
i
g D
ablethatis1iftheithBernoullitrialisasuccessand0ifitisafailure. Thus,
n
X X ;
i
D
i 1
XD
andbylinearity ofexpectation,
n n n
 EŒX E X EŒX  p ;
i i i
D D D D
" #
i 1 i 1 i 1
XD XD XD
whichimplies
n
X  .X p /:
i i
(cid:0) D (cid:0)
i 1
XD
ToevaluateE e˛.X / ,wesubstitute forX ,obtaining
(cid:0)
(cid:0)
E e˛.X (cid:0)/  E e˛ n iD1.Xi (cid:0)pi/
D
P
n
   
E e˛.Xi pi/
(cid:0)
D
" #
i 1
YD
n
E e˛.Xi pi/ ;
(cid:0)
D
i 1
YD  
C.5 Thetailsofthebinomialdistribution 1213
which follows from (C.24), since the mutual independence of the random vari-
ables X implies the mutual independence of the random variables e˛.Xi pi/ (see
i (cid:0)
ExerciseC.3-5). Bythedefinition ofexpectation,
E e˛.Xi pi/ e˛.1 pi/p e˛.0 pi/q
(cid:0) (cid:0) i (cid:0) i
D C
p e˛qi q e ˛pi
  D i C i (cid:0)
p e˛ 1 (C.45)
i
 C
exp.p e˛/;
i

where exp.x/ denotes the exponential function: exp.x/ ex. (Inequality (C.45)
D
followsfromtheinequalities ˛ > 0,q 1,e˛qi e˛,ande ˛pi 1,andthelast
i (cid:0)
  
linefollowsfrominequality (3.12).) Consequently,
n
E e˛.X / E e˛.Xi pi/
(cid:0) (cid:0)
D
i 1
  YD  
n
exp.p e˛/
i

i 1
YD
n
exp p e˛
i
D
!
i 1
exp.X eD˛/;
(C.46)
D
since  n p . Therefore, from equation (C.43) and inequalities (C.44)
D i 1 i
and(C.46),itfolDlowsthat
P
Pr X  r exp.e˛ ˛r/: (C.47)
f (cid:0)  g  (cid:0)
Choosing˛ ln.r=/(seeExerciseC.5-7),weobtain
D
Pr X  r exp.eln.r=/ rln.r=//
f (cid:0)  g  (cid:0)
exp.r rln.r=//
D (cid:0)
er
D .r=/r
e r
:
D r
 
Whenapplied toBernoulli trials in whicheach trial has thesameprobability of
success, Theorem C.8 yields the following corollary bounding the right tail of a
binomialdistribution.
1214 AppendixC CountingandProbability
Corollary C.9
Consider a sequence of n Bernoulli trials, where in each trial success occurs with
probability p andfailureoccurswithprobability q 1 p. Thenforr > np,
D (cid:0)
n
Pr X np r b.k n;p/
f (cid:0)  g D I
k np r
DXd C e
npe r
:
 r
 
Proof Byequation (C.37),wehave EŒX np.
D D
Exercises
C.5-1 ?
Which is less likely: obtaining no heads when you flip a fair coin n times, or
obtaining fewerthannheadswhenyouflipthecoin4ntimes?
C.5-2 ?
ProveCorollaries C.6andC.7.
C.5-3 ?
Showthat
k 1
(cid:0) n k
ai < .a 1/n b.k n;a=.a 1//
i C na k.a 1/ I C
!
i 0 (cid:0) C
XD
foralla > 0andallk suchthat0 < k < na=.a 1/.
C
C.5-4 ?
Provethatif0< k < np,where0< p < 1andq 1 p,then
D (cid:0)
k 1
(cid:0) kq np k nq n k
piqn i < (cid:0) :
(cid:0)
np k k n k
Xi D0 (cid:0)    (cid:0) 
C.5-5 ?
UseTheoremC.8toshowthat
.n /e r
Pr  X r (cid:0)
f (cid:0)  g  r
 
forr > n . Similarly,useCorollaryC.9toshowthat
(cid:0)
nqe r
Pr np X r
f (cid:0)  g  r
 
forr > n np.
(cid:0)
ProblemsforAppendixC 1215
C.5-6 ?
ConsiderasequenceofnBernoullitrials,whereintheithtrial,fori 1;2;:::;n,
D
successoccurswithprobability p andfailureoccurswithprobability q 1 p .
i i i
D (cid:0)
Let X be the random variable describing the total number of successes, and let
 EŒX. Showthatforr 0,
D 
Pr X  r e
r2=2n
:
(cid:0)
f (cid:0)  g 
(Hint: Prove that p e˛qi q e ˛pi e˛2=2. Then follow the outline of the proof
i i (cid:0)
C 
ofTheoremC.8,usingthisinequality inplaceofinequality (C.45).)
C.5-7 ?
Show that choosing ˛ ln.r=/ minimizes the right-hand side of inequal-
D
ity(C.47).
Problems
C-1 Ballsandbins
Inthisproblem, weinvestigate theeffectofvariousassumptions onthenumberof
waysofplacingnballsintob distinctbins.
a. Suppose that the n balls are distinct and that their order within a bin does not
matter. Arguethatthenumberofwaysofplacingtheballsinthebinsisbn.
b. Suppose that the balls are distinct and that the balls in each bin are ordered.
Provethatthereareexactly.b n 1/Š=.b 1/Šwaystoplacetheballsinthe
C (cid:0) (cid:0)
bins. (Hint:Considerthenumberofwaysofarrangingndistinctballsandb 1
(cid:0)
indistinguishable sticksinarow.)
c. Supposethattheballsareidentical, andhencetheirorderwithinabindoesnot
b n 1
matter. Showthatthenumberofwaysofplacingtheballsinthebinsis C (cid:0) .
n
(Hint: Of the arrangements in part (b), how many are repeated if the balls are
(cid:0) 
madeidentical?)
d. Suppose thatthe ballsare identical and thatnobin maycontain morethan one
ball,sothatn b. Showthatthenumberofwaysofplacingtheballsis b .
 n
(cid:0) 
e. Supposethattheballsareidenticalandthatnobinmaybeleftempty. Assuming
thatn b,showthatthenumberofwaysofplacingtheballsis n (cid:0)1 .
 b 1
(cid:0)
(cid:0) 
1216 AppendixC CountingandProbability
Appendix notes
The first general methods for solving probability problems were discussed in a
famouscorrespondence betweenB.PascalandP.deFermat,whichbeganin1654,
andinabookbyC.Huygens in1657. Rigorous probability theory beganwiththe
work of J. Bernoulli in 1713 and A. DeMoivre in 1730. Further developments of
thetheorywereprovided byP.-S.Laplace,S.-D.Poisson,andC.F.Gauss.
SumsofrandomvariableswereoriginallystudiedbyP.L.ChebyshevandA.A.
Markov. A.N.Kolmogorovaxiomatizedprobabilitytheoryin1933. Chernoff[66]
and Hoeffding [173] provided bounds on the tails of distributions. Seminal work
inrandomcombinatorial structures wasdonebyP.Erdo˝s.
Knuth [209] and Liu [237] are good references for elementary combinatorics
andcounting. StandardtextbookssuchasBillingsley[46],Chung[67],Drake[95],
Feller[104],andRozanov[300]offercomprehensive introductions toprobability.
D Matrices
Matrices arise in numerous applications, including, but by no means limited to,
scientificcomputing. Ifyouhaveseenmatricesbefore,muchofthematerialinthis
appendix willbefamiliar toyou, but someofitmight benew. Section D.1covers
basicmatrixdefinitionsandoperations,andSectionD.2presentssomebasicmatrix
properties.
D.1 Matrices and matrixoperations
In this section, we review some basic concepts of matrix theory and some funda-
mentalproperties ofmatrices.
Matricesandvectors
Amatrixisarectangular arrayofnumbers. Forexample,
a a a
A 11 12 13
D a 21 a 22 a 23
 
1 2 3
(D.1)
D 4 5 6
 
is a 2 3 matrix A .a /, where for i 1;2 and j 1;2;3, we denote the
ij
 D D D
element of the matrix in row i and column j by a . We use uppercase letters
ij
todenote matrices andcorresponding subscripted lowercase letters todenote their
elements. Wedenotethesetofallm nmatriceswithreal-valuedentriesbyRm n


and,ingeneral,thesetofm nmatriceswithentriesdrawnfromasetS bySm n.


Thetranspose of a matrix A is the matrix AT obtained by exchanging the rows
andcolumnsofA. ForthematrixAofequation (D.1),
1218 AppendixD Matrices
1 4
AT 2 5 :
D
3 6
Ave(cid:0)ctorisaone-dimensional arrayofnumbers. Forexample,
2
x 3
D
5
is a v(cid:0)ector of size 3. We sometimes call a vector of length n an n-vector. We
use lowercase letters to denote vectors, and wedenote the ith element of a size-n
vector x by x , for i 1;2;:::;n. We take the standard form of a vector to be
i
D
asacolumnvector equivalent toann 1matrix; thecorresponding row vectoris

obtained bytakingthetranspose:
xT . 2 3 5 /:
D
The unit vector e is the vector whose ith element is 1 and all of whose other
i
elementsare0. Usually, thesizeofaunitvectorisclearfromthecontext.
A zero matrix is a matrix all of whose entries are 0. Such a matrix is often
denoted 0,sincetheambiguitybetweenthenumber0andamatrixof0sisusually
easily resolved from context. If a matrix of 0s is intended, then the size of the
matrixalsoneedstobederivedfromthecontext.
Squarematrices
Square n n matrices arise frequently. Several special cases of square matrices

areofparticular interest:
1. Adiagonalmatrixhasa 0wheneveri j. Becausealloftheoff-diagonal
ij
D ¤
elements are zero, we can specify the matrix by listing the elements along the
diagonal:
a 0 ::: 0
11
0 a ::: 0
22
diag.a 11;a 22;:::;a nn/ D : : : : : : ::: : : : :
˙ 0 0 ::: a 
nn
2. Then nidentitymatrixI isadiagonal matrixwith1salongthediagonal:
n

I diag.1;1;:::;1/
n
D
1 0 ::: 0
0 1 ::: 0
D : : : : : : :: : : : : :
˙0 0 ::: 1
D.1 Matricesandmatrixoperations 1219
WhenI appearswithoutasubscript,wederiveitssizefromthecontext. Theith
columnofanidentitymatrixistheunitvectore .
i
3. AtridiagonalmatrixT isoneforwhicht 0if i j >1. Nonzeroentries
ij
D j (cid:0) j
appearonlyonthemaindiagonal, immediately abovethemaindiagonal(t
i;i 1
for i 1;2;:::;n 1), or immediately below the main diagonal (t fC or
i 1;i
i 1;D 2;:::;n 1):(cid:0) C
D (cid:0)
t t 0 0 ::: 0 0 0
11 12
t t t 0 ::: 0 0 0
21 22 23
0 t t t ::: 0 0 0
32 33 34
T : : : : : : : : : : : : ::: : : : : : : : : : :
D
 0 0 0 0 ::: t t 0 ˘
n 2;n 2 n 2;n 1
0 0 0 0 ::: t (cid:0) (cid:0) t (cid:0) (cid:0) t
n 1;n 2 n 1;n 1 n 1;n
0 0 0 0 ::: (cid:0)0 (cid:0) t(cid:0) (cid:0) t(cid:0)
n;n 1 nn
(cid:0)
4. An upper-triangular matrix U is one for which u 0 if i > j. All entries
ij
D
belowthediagonal arezero:
u u ::: u
11 12 1n
0 u ::: u
22 2n
U D : : : : : : :: : : : : :
˙ 0 0 ::: u 
nn
An upper-triangular matrix is unit upper-triangular if it has all 1s along the
diagonal.
5. A lower-triangular matrix L is one for which l 0 if i < j. All entries
ij
D
abovethediagonal arezero:
l 0 ::: 0
11
l l ::: 0
21 22
L D : : : : : : :: : : : : :
˙ l l ::: l 
n1 n2 nn
A lower-triangular matrix is unit lower-triangular if it has all 1s along the
diagonal.
1220 AppendixD Matrices
6. A permutation matrix P has exactly one 1 in each row or column, and 0s
elsewhere. Anexampleofapermutation matrixis
0 1 0 0 0
0 0 0 1 0
P 1 0 0 0 0 :
D
0 0 0 0 1
ˇ
0 0 1 0 0
Such a matrix is called a permutation matrix because multiplying a vector x
byapermutation matrixhastheeffectofpermuting (rearranging) theelements
ofx. ExerciseD.1-4exploresadditional properties ofpermutation matrices.
7. AsymmetricmatrixAsatisfiesthecondition A AT. Forexample,
D
1 2 3
2 6 4
3 4 5
(cid:0) 
isasymmetricmatrix.
Basicmatrixoperations
The elements of a matrix or vector are numbers from a number system, such as
the real numbers, the complex numbers, or integers modulo a prime. Thenumber
system defines howtoaddandmultiplynumbers. Wecanextend thesedefinitions
toencompassaddition andmultiplication ofmatrices.
We define matrix addition as follows. If A .a / and B .b / are m n
ij ij
D D 
matrices,thentheirmatrixsumC .c / A B isthem nmatrixdefinedby
ij
D D C 
c a b
ij ij ij
D C
for i 1;2;:::;m and j 1;2;:::;n. That is, matrix addition is performed
D D
componentwise. Azeromatrixistheidentityformatrixaddition:
A 0 A 0 A:
C D D C
If  is a number and A .a / is a matrix, then A .a / is the scalar
ij ij
D D
multipleofAobtainedbymultiplyingeachofitselementsby. Asaspecialcase,
wedefine the negative of amatrix A .a / tobe 1 A A,so that the ijth
ij
D (cid:0)  D (cid:0)
entryof Ais a . Thus,
ij
(cid:0) (cid:0)
A . A/ 0 . A/ A:
C (cid:0) D D (cid:0) C
D.1 Matricesandmatrixoperations 1221
Weusethenegativeofamatrixtodefinematrixsubtraction: A B A . B/.
(cid:0) D C (cid:0)
Wedefinematrixmultiplicationasfollows. WestartwithtwomatricesAandB
thatarecompatibleinthesensethatthenumberofcolumnsofAequalsthenumber
ofrowsofB. (Ingeneral,anexpressioncontainingamatrixproductAB isalways
assumedtoimplythatmatricesAandB arecompatible.) IfA .a /isanm n
ik
D 
matrixandB .b /isann p matrix,thentheirmatrixproductC AB isthe
kj
D  D
m p matrixC .c /,where
ij
 D
n
c a b (D.2)
ij ik kj
D
k 1
XD
for i 1;2;:::;m and j 1;2;:::;p. The procedure SQUARE-MATRIX-
D D
MULTIPLY in Section 4.2 implements matrix multiplication in the straightfor-
ward manner based on equation (D.2), assuming that the matrices are square:
m n p. To multiply n n matrices, SQUARE-MATRIX-MULTIPLY per-
D D 
formsn3multiplicationsandn2.n 1/additions, andsoitsrunningtimeis‚.n3/.
(cid:0)
Matrices havemany(but notall)ofthealgebraic properties typical ofnumbers.
Identitymatricesareidentities formatrixmultiplication:
I A AI A
m n
D D
foranym nmatrixA. Multiplying byazeromatrixgivesazeromatrix:

A0 0:
D
Matrixmultiplication isassociative:
A.BC/ .AB/C
D
forcompatible matrices A,B, and C. Matrix multiplication distributes over addi-
tion:
A.B C/ AB AC ;
C D C
.B C/D BD CD :
C D C
For n > 1, multiplication of n n matrices is not commutative. For example, if

0 1 0 0
A andB ,then
D 0 0 D 1 0
   
1 0
AB
D 0 0
 
and
0 0
BA :
D 0 1
 
1222 AppendixD Matrices
Wedefinematrix-vectorproductsorvector-vector productsasifthevectorwere
the equivalent n 1matrix (ora1 n matrix, inthe case ofarow vector). Thus,
 
if A is an m n matrix and x is an n-vector, then Ax is an m-vector. If x and y

aren-vectors, then
n
xTy x y
i i
D
i 1
XD
isanumber(actuallya1 1matrix)calledtheinnerproductofx andy. Thema-

trixxyTisann nmatrixZ calledtheouterproductofx andy,with´ x y .
ij i j
 D
The(euclidean)norm x ofann-vectorx isdefinedby
k k
x .x2 x2 x2/1=2
k k D 1 C 2 CC n
.xTx/1=2 :
D
Thus,thenormofx isitslengthinn-dimensional euclidean space.
Exercises
D.1-1
ShowthatifAandB aresymmetricn nmatrices,thensoareA B andA B.
 C (cid:0)
D.1-2
Provethat.AB/T BTAT andthatATAisalwaysasymmetricmatrix.
D
D.1-3
Provethattheproduct oftwolower-triangular matricesislower-triangular.
D.1-4
Prove that if P isan n npermutation matrix and A is an n n matrix, then the
 
matrix product PA is A with its rows permuted, and the matrix product AP is A
with its columns permuted. Prove that the product of two permutation matrices is
apermutation matrix.
D.2 Basicmatrix properties
In this section, we define some basic properties pertaining to matrices: inverses,
linear dependence and independence, rank, and determinants. We also define the
classofpositive-definite matrices.
D.2 Basicmatrixproperties 1223
Matrixinverses, ranks,anddeterminants
Wedefinetheinverseofann nmatrixAtobethen nmatrix,denotedA 1 (if
(cid:0)
 
itexists),suchthatAA 1 I A 1A. Forexample,
(cid:0) n (cid:0)
D D
1
1 1 (cid:0) 0 1
:
1 0 D 1 1
   (cid:0) 
Manynonzeron nmatricesdonothaveinverses. Amatrixwithoutaninverseis

callednoninvertible,orsingular. Anexampleofanonzero singular matrixis
1 0
:
1 0
 
If amatrix has an inverse, it is called invertible, or nonsingular. Matrix inverses,
when they exist, are unique. (See Exercise D.2-1.) If A and B are nonsingular
n nmatrices, then

.BA/ 1 A 1B 1 :
(cid:0) (cid:0) (cid:0)
D
Theinverseoperation commuteswiththetranspose operation:
.A 1/T .AT/ 1 :
(cid:0) (cid:0)
D
The vectors x ;x ;:::;x are linearly dependent if there exist coefficients
1 2 n
c ;c ;:::;c , not all of which are zero, such that c x c x c x 0.
1 2 n 1 1 2 2 n n
C CC D
The row vectors x . 1 2 3 /, x . 2 6 4/, and x .4 11 9/ are
1 2 3
D D D
linearly dependent, for example, since 2x 3x 2x 0. If vectors are not
1 2 3
C (cid:0) D
linearlydependent, theyarelinearlyindependent. Forexample,thecolumnsofan
identitymatrixarelinearly independent.
The column rank of a nonzero m n matrix A is the size of the largest set

of linearly independent columns of A. Similarly, the row rank of A is the size
of the largest set of linearly independent rows of A. A fundamental property of
any matrix A is that its row rank always equals its column rank, so that we can
simplyrefertotherankofA. Therankofanm nmatrixisanintegerbetween0

andmin.m;n/,inclusive. (Therankofazeromatrixis0,andtherankofann n

identitymatrixisn.) Analternate, butequivalentandoftenmoreuseful,definition
is that the rank of a nonzero m n matrix A is the smallest number r such that

thereexistmatricesB andC ofrespective sizesm r andr nsuchthat
 
A BC :
D
A square n n matrix has full rank if its rank is n. An m n matrix has full
 
columnrankifitsrankisn. Thefollowingtheorem givesafundamental property
ofranks.
1224 AppendixD Matrices
TheoremD.1
Asquarematrixhasfullrankifandonlyifitisnonsingular.
A null vector for a matrix A is a nonzero vector x such that Ax 0. The
D
following theorem (whose proof is left as Exercise D.2-7) and its corollary relate
thenotions ofcolumnrankandsingularity tonullvectors.
TheoremD.2
AmatrixAhasfullcolumnrankifandonlyifitdoesnothaveanullvector.
Corollary D.3
AsquarematrixAissingularifandonlyifithasanullvector.
Theijthminorofann nmatrixA,forn > 1,isthe.n 1/ .n 1/matrixA
Œij
 (cid:0)  (cid:0)
obtained bydeleting theithrowandjthcolumnofA. Wedefinethedeterminant
ofann nmatrixArecursively intermsofitsminorsby

a ifn 1;
11
D
det.A/ n
D . 1/1 ja det.A / ifn > 1:
C 1j Œ1j
(cid:0)
‚j 1
XD
Theterm. 1/i j det.A /isknownasthecofactor oftheelementa .
C Œij ij
(cid:0)
The following theorems, whose proofs are omitted here, express fundamental
properties ofthedeterminant.
TheoremD.4(Determinantproperties)
Thedeterminant ofasquarematrixAhasthefollowingproperties:
 IfanyroworanycolumnofAiszero,thendet.A/ 0.
D
 The determinant of A is multiplied by  if the entries of any one row (or any
onecolumn)ofAareallmultipliedby.
 Thedeterminant of Aisunchanged ifthe entries inone row(respectively, col-
umn)areaddedtothoseinanotherrow(respectively, column).
 Thedeterminant ofAequalsthedeterminant ofAT.
 ThedeterminantofAismultipliedby 1ifanytworows(oranytwocolumns)
(cid:0)
areexchanged.
Also,foranysquarematricesAandB,wehavedet.AB/ det.A/det.B/.
D
D.2 Basicmatrixproperties 1225
TheoremD.5
Ann nmatrixAissingular ifandonlyifdet.A/ 0.
 D
Positive-definitematrices
Positive-definite matrices play an important role in many applications. An n n

matrix A is positive-definite if xTAx > 0 for all n-vectors x 0. For
¤
example, the identity matrix is positive-definite, since for any nonzero vector
x . x x x /T,
1 2 n
D 
xTI x xTx
n
D
n
x2
D i
i 1
XD
> 0:
Matricesthatariseinapplicationsareoftenpositive-definiteduetothefollowing
theorem.
TheoremD.6
ForanymatrixAwithfullcolumnrank,thematrixATAispositive-definite.
Proof We must show that xT.ATA/x > 0 for any nonzero vector x. For any
vectorx,
xT.ATA/x .Ax/T.Ax/ (byExerciseD.1-2)
D
Ax 2 :
D k k
Note that Ax 2 is just the sum of the squares of the elements of the vector Ax.
Therefore,k Axk 2 0. If Ax 2 0, every element of Ax is 0, which is to say
k k  k k D
Ax 0. SinceAhas fullcolumn rank, Ax 0implies x 0, by Theorem D.2.
D D D
Hence,ATAispositive-definite.
Section28.3exploresotherproperties ofpositive-definite matrices.
Exercises
D.2-1
Prove that matrix inverses are unique, that is, if B and C are inverses of A, then
B C.
D
D.2-2
Provethatthedeterminantofalower-triangular orupper-triangular matrixisequal
totheproductofitsdiagonalelements. Provethattheinverseofalower-triangular
matrix,ifitexists,islower-triangular.
1226 AppendixD Matrices
D.2-3
Prove that if P is a permutation matrix, then P is invertible, its inverse is PT,
andPT isapermutation matrix.
D.2-4
Let A and B be n n matrices such that AB I. Prove that if A is obtained
0
 D
fromAbyaddingrowj intorowi,thensubtractingcolumni fromcolumnj ofB
yieldstheinverseB ofA.
0 0
D.2-5
Let Abe a nonsingular n n matrix with complex entries. Show that every entry

ofA 1 isrealifandonlyifeveryentryofAisreal.
(cid:0)
D.2-6
Show that ifA is anonsingular, symmetric, n n matrix, then A 1 is symmetric.
(cid:0)

Show that if B is an arbitrary m n matrix, then the m m matrix given by the
 
product BABT issymmetric.
D.2-7
ProveTheoremD.2. Thatis,showthatamatrixAhasfullcolumnrankifandonly
ifAx 0implies x 0. (Hint:Express thelinear dependence ofonecolumnon
D D
theothersasamatrix-vector equation.)
D.2-8
Provethatforanytwocompatible matricesAandB,
rank.AB/ min.rank.A/;rank.B//;

where equality holds if either A or B is a nonsingular square matrix. (Hint: Use
thealternate definitionoftherankofamatrix.)
Problems
D-1 Vandermondematrix
Given numbers x ;x ;:::;x , prove that the determinant of the Vandermonde
0 1 n 1
(cid:0)
matrix
1 x x2 xn 1
0 0  0(cid:0)
1 x x2 xn 1
V.x 0;x 1;:::;x n (cid:0)1/ D : : : : : :1 : : :1  :: : 1 : : :(cid:0)
˙ 1 x x2 xn 1 
n (cid:0)1 n (cid:0)1  n (cid:0)(cid:0)1
ProblemsforAppendixD 1227
is
det.V.x ;x ;:::;x // .x x /:
0 1 n 1 k j
(cid:0) D (cid:0)
0 j<k n 1
 Y (cid:0)
(Hint: Multiply column i by x and add it to column i 1 for i n 1;
0
(cid:0) C D (cid:0)
n 2;:::;1,andthenuseinduction.)
(cid:0)
D-2 Permutationsdefinedbymatrix-vector multiplication overGF.2/
One class of permutations of the integers in the set S 0;1;2;:::;2n 1 is
n
D f (cid:0) g
definedbymatrixmultiplication overGF.2/. Foreachintegerx inS ,weviewits
n
binaryrepresentation asann-bitvector
x
0
x
1
x
2 ;
:
:
:
(cid:0)x 
n 1
(cid:0)
where x
D
n i(cid:0)01x i2i. If A is an n

n matrix in which each entry is either 0
or1, then wecanDdefine apermutation mapping each value x in S tothe number
n
P
whose binary representation is the matrix-vector product Ax. Here, we perform
all arithmetic over GF.2/: all values are either 0 or 1, and withone exception the
usual rules of addition and multiplication apply. Theexception isthat 1 1 0.
C D
YoucanthinkofarithmeticoverGF.2/asbeingjustlikeregularintegerarithmetic,
exceptthatyouuseonlytheleastsignificant bit.
Asanexample,forS 0;1;2;3 ,thematrix
2
D f g
1 0
A
D 1 1
 
defines the following permutation  :  .0/ 0,  .1/ 3,  .2/ 2,
A A A A
D D D
 .3/ 1. Toseewhy .3/ 1,observethat,workinginGF.2/,
A A
D D
1 0 1
 .3/
A D 1 1 1
  
1 1 0 1
 C 
D 1 1 1 1
  C  
1
;
D 0
 
whichisthebinaryrepresentation of1.
1228 AppendixD Matrices
For the remainder of this problem, we work over GF.2/, and all matrix and
vector entries are 0 or 1. We define the rank of a 0-1 matrix (a matrix for which
eachentryiseither0or1)overGF.2/thesameasforaregularmatrix,butwithall
arithmetic that determines linear independence performed over GF.2/. Wedefine
therangeofann n0-1matrixAby

R.A/ y y Ax forsomex S ;
n
D f W D 2 g
so that R.A/is the set ofnumbers in S that wecan produce bymultiplying each
n
valuex inS byA.
n
a. Ifr istherankofmatrixA,provethat R.A/ 2r. Conclude thatAdefinesa
j j D
permutation onS onlyifAhasfullrank.
n
Foragivenn nmatrixAandagivenvaluey R.A/,wedefinethepreimage
 2
ofy by
P.A;y/ x Ax y ;
D f W D g
sothatP.A;y/isthesetofvaluesinS thatmaptoy whenmultiplied byA.
n
b. Ifr istherankofn nmatrixAandy R.A/,provethat P.A;y/ 2n r.
(cid:0)
 2 j j D
Let 0 m n, and suppose we partition the set S into blocks of consec-
n
 
utive numbers, where the ith block consists of the 2m numbers i2m;i2m 1;
C
i2m 2;:::;.i 1/2m 1. For any subset S S , define B.S;m/ to be the
n
C C (cid:0) 
set of size-2m blocks of S containing some element of S. As an example, when
n
n 3, m 1,and S 1;4;5 ,then B.S;m/ consists ofblocks 0(since 1isin
D D D f g
the0thblock)and2(sinceboth4and5areinblock2).
c. Let r be the rank of the lower left .n m/ m submatrix of A, that is, the
(cid:0) 
matrix formed by taking the intersection of the bottom n m rows and the
(cid:0)
leftmost m columns of A. Let S be any size-2m block of S , and let S
n 0
D
y y Ax forsomex S . Prove that B.S ;m/ 2r and that for each
0
f W D 2 g j j D
blockinB.S ;m/,exactly2m r numbersinS maptothatblock.
0 (cid:0)
Because multiplying the zero vector by any matrix yields a zero vector, the set
ofpermutations ofS definedbymultiplying byn n0-1matrices withfullrank
n

over GF.2/ cannot include all permutations of S . Letus extend the class of per-
n
mutations defined by matrix-vector multiplication to include an additive term, so
that x S mapstoAx c,wherec isann-bitvector andaddition isperformed
n
2 C
overGF.2/. Forexample,when
1 0
A
D 1 1
 
NotesforAppendixD 1229
and
0
c ;
D 1
 
we get the following permutation  :  .0/ 2,  .1/ 1,  .2/ 0,
A;c A;c A;c A;c
D D D
 .3/ 3. Wecallanypermutationthatmapsx S toAx c,forsomen n
A;c n
D 2 C 
0-1matrixAwithfullrankandsomen-bitvectorc,alinearpermutation.
d. Useacounting argument toshow thatthenumberoflinearpermutations ofS
n
ismuchlessthanthenumberofpermutations ofS .
n
e. GiveanexampleofavalueofnandapermutationofS thatcannotbeachieved
n
by any linear permutation. (Hint: For a given permutation, think about how
multiplying amatrixbyaunitvectorrelatestothecolumnsofthematrix.)
Appendix notes
Linear-algebra textbooks provide plenty of background information on matrices.
ThebooksbyStrang[323,324]areparticularly good.
Bibliography
[1] MiltonAbramowitzand IreneA.Stegun, editors. Handbook of Mathematical Functions.
Dover,1965.
[2] G.M.Adel’son-Vel’ski˘ıandE.M.Landis. Analgorithmfortheorganizationofinformation.
SovietMathematicsDoklady,3(5):1259–1263,1962.
[3] AlokAggarwalandJeffreyScottVitter. Theinput/outputcomplexityofsortingandrelated
problems. CommunicationsoftheACM,31(9):1116–1127,1988.
[4] ManindraAgrawal,NeerajKayal,andNitinSaxena. PRIMESisinP. AnnalsofMathe-
matics,160(2):781–793,2004.
[5] Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman. The Design and Analysis of
ComputerAlgorithms. Addison-Wesley,1974.
[6] AlfredV.Aho,JohnE.Hopcroft,andJeffreyD.Ullman. DataStructuresandAlgorithms.
Addison-Wesley,1983.
[7] Ravindra K. Ahuja, Thomas L. Magnanti, and James B. Orlin. Network Flows: Theory,
Algorithms,andApplications. PrenticeHall,1993.
[8] RavindraK.Ahuja,KurtMehlhorn,JamesB.Orlin,andRobertE.Tarjan. Fasteralgorithms
fortheshortestpathproblem. JournaloftheACM,37(2):213–223,1990.
[9] RavindraK.AhujaandJamesB.Orlin. Afastandsimplealgorithmforthemaximumflow
problem. OperationsResearch,37(5):748–759,1989.
[10] RavindraK.Ahuja, JamesB.Orlin,andRobertE.Tarjan. Improvedtimeboundsforthe
maximumflowproblem. SIAMJournalonComputing,18(5):939–954,1989.
[11] Miklo´s Ajtai, Nimrod Megiddo, and Orli Waarts. Improved algorithms and analysis for
secretaryproblemsandgeneralizations. InProceedingsofthe36thAnnualSymposiumon
FoundationsofComputerScience,pages473–482,1995.
[12] SelimG.Akl. TheDesignandAnalysisofParallelAlgorithms. PrenticeHall,1989.
[13] MohamadAkraandLouayBazzi. Onthesolutionoflinearrecurrenceequations. Compu-
tationalOptimizationandApplications,10(2):195–210,1998.
[14] NogaAlon. Generatingpseudo-random permutationsandmaximumflowalgorithms. In-
formationProcessingLetters,35:201–204,1990.
1232 Bibliography
[15] ArneAndersson. Balancedsearchtreesmadesimple. InProceedingsoftheThirdWorkshop
on Algorithms and Data Structures, volume 709 of Lecture Notes in Computer Science,
pages60–71.Springer,1993.
[16] ArneAndersson. Fasterdeterministicsortingandsearchinginlinearspace. InProceedings
ofthe37thAnnualSymposiumonFoundationsofComputerScience,pages135–141,1996.
[17] Arne Andersson, Torben Hagerup, Stefan Nilsson, and Rajeev Raman. Sorting in linear
time? JournalofComputerandSystemSciences,57:74–93,1998.
[18] TomM.Apostol. Calculus,volume1. BlaisdellPublishingCompany,secondedition,1967.
[19] NimarS.Arora,RobertD.Blumofe,andC.GregPlaxton. Threadschedulingformultipro-
grammedmultiprocessors. InProceedingsofthe10thAnnualACMSymposiumonParallel
AlgorithmsandArchitectures,pages119–129,1998.
[20] SanjeevArora. Probabilisticcheckingofproofsandthehardness ofapproximationprob-
lems. PhDthesis,UniversityofCalifornia,Berkeley,1994.
[21] Sanjeev Arora. The approximability of NP-hard problems. In Proceedings of the 30th
AnnualACMSymposiumonTheoryofComputing,pages337–348,1998.
[22] SanjeevArora. Polynomialtimeapproximationschemesforeuclideantravelingsalesman
andothergeometricproblems. JournaloftheACM,45(5):753–782,1998.
[23] Sanjeev Arora and Carsten Lund. Hardness of approximations. In Dorit S. Hochbaum,
editor,ApproximationAlgorithmsforNP-HardProblems,pages399–446.PWSPublishing
Company,1997.
[24] JavedA.Aslam. Asimpleboundontheexpectedheightofarandomlybuiltbinarysearch
tree. TechnicalReportTR2001-387,DartmouthCollegeDepartmentofComputerScience,
2001.
[25] MikhailJ.Atallah,editor. AlgorithmsandTheoryofComputationHandbook. CRCPress,
1999.
[26] G.Ausiello,P.Crescenzi,G.Gambosi,V.Kann,A.Marchetti-Spaccamela,andM.Protasi.
ComplexityandApproximation: CombinatorialOptimizationProblemsandTheirApprox-
imabilityProperties. Springer,1999.
[27] ShaiAvidanandArielShamir. Seamcarvingforcontent-awareimageresizing. ACMTrans-
actionsonGraphics,26(3),article10,2007.
[28] SaraBaaseandAlanVanGelder. ComputerAlgorithms: IntroductiontoDesignandAnal-
ysis. Addison-Wesley,thirdedition,2000.
[29] EricBach. Privatecommunication,1989.
[30] EricBach. Number-theoreticalgorithms. InAnnualReviewofComputerScience,volume4,
pages119–172.AnnualReviews,Inc.,1990.
[31] EricBachandJeffreyShallit. AlgorithmicNumberTheory—VolumeI:EfficientAlgorithms.
TheMITPress,1996.
[32] DavidH.Bailey,KingLee,andHorstD.Simon. UsingStrassen’salgorithmtoaccelerate
thesolutionoflinearsystems. TheJournalofSupercomputing,4(4):357–371,1990.
Bibliography 1233
[33] Surender Baswana, Ramesh Hariharan, and Sandeep Sen. Improved decremental algo-
rithmsformaintainingtransitiveclosureandall-pairsshortestpaths. JournalofAlgorithms,
62(2):74–92,2007.
[34] R. Bayer. Symmetric binary B-trees: Data structure and maintenance algorithms. Acta
Informatica,1(4):290–306,1972.
[35] R.Bayer and E. M. McCreight. Organizationand maintenance of largeordered indexes.
ActaInformatica,1(3):173–189,1972.
[36] PierreBeauchemin,GillesBrassard,ClaudeCre´peau,ClaudeGoutier,andCarlPomerance.
Thegenerationofrandomnumbersthatareprobablyprime. JournalofCryptology,1(1):53–
64,1988.
[37] RichardBellman. DynamicProgramming. PrincetonUniversityPress,1957.
[38] RichardBellman. Onaroutingproblem. QuarterlyofAppliedMathematics,16(1):87–90,
1958.
[39] Michael Ben-Or. Lower bounds for algebraic computation trees. In Proceedings of the
FifteenthAnnualACMSymposiumonTheoryofComputing,pages80–86,1983.
[40] MichaelA.Bender,ErikD.Demaine,andMartinFarach-Colton. Cache-obliviousB-trees.
InProceedingsofthe41stAnnualSymposiumonFoundationsofComputerScience,pages
399–409,2000.
[41] SamuelW.BentandJohnW.John. Findingthemedianrequires2ncomparisons. InPro-
ceedingsoftheSeventeenthAnnualACMSymposiumonTheoryofComputing,pages213–
216,1985.
[42] JonL.Bentley. WritingEfficientPrograms. PrenticeHall,1982.
[43] JonL.Bentley. ProgrammingPearls. Addison-Wesley,1986.
[44] JonL.Bentley,DorotheaHaken,andJamesB.Saxe. Ageneralmethodforsolvingdivide-
and-conquerrecurrences. SIGACTNews,12(3):36–44,1980.
[45] Daniel Bienstock and Benjamin McClosky. Tightening simplex mixed-integer sets with
guaranteedbounds. OptimizationOnline,July2008.
[46] PatrickBillingsley. ProbabilityandMeasure. JohnWiley&Sons,secondedition,1986.
[47] GuyE.Blelloch. ScanPrimitivesandParallelVectorModels. PhDthesis,Departmentof
ElectricalEngineeringandComputerScience,MIT,1989. AvailableasMITLaboratoryfor
ComputerScienceTechnicalReportMIT/LCS/TR-463.
[48] Guy E. Blelloch. Programming parallel algorithms. Communications of the ACM,
39(3):85–97,1996.
[49] GuyE.Blelloch, PhillipB.Gibbons, andYossiMatias. Provablyefficientschedulingfor
languageswithfine-grainedparallelism. InProceedingsofthe7thAnnualACMSymposium
onParallelAlgorithmsandArchitectures,pages1–12,1995.
[50] Manuel Blum, Robert W. Floyd, Vaughan Pratt, Ronald L. Rivest, and Robert E. Tarjan.
Timeboundsforselection. JournalofComputerandSystemSciences,7(4):448–461,1973.
[51] Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. Leiserson,
KeithH.Randall,andYuliZhou. Cilk:Anefficientmultithreadedruntimesystem. Journal
ofParallelandDistributedComputing,37(1):55–69,1996.
1234 Bibliography
[52] RobertD.BlumofeandCharlesE.Leiserson. Schedulingmultithreadedcomputationsby
workstealing. JournaloftheACM,46(5):720–748,1999.
[53] Be´laBolloba´s. RandomGraphs. AcademicPress,1985.
[54] GillesBrassardandPaulBratley. FundamentalsofAlgorithmics. PrenticeHall,1996.
[55] RichardP.Brent. Theparallelevaluationofgeneralarithmeticexpressions. Journalofthe
ACM,21(2):201–206,1974.
[56] RichardP.Brent. AnimprovedMonteCarlofactorizationalgorithm. BIT,20(2):176–184,
1980.
[57] J.P.Buhler, H.W.Lenstra, Jr., andCarlPomerance. Factoringintegerswiththenumber
fieldsieve. InA.K.LenstraandH.W.Lenstra,Jr.,editors,TheDevelopmentoftheNumber
FieldSieve,volume1554ofLectureNotesinMathematics,pages50–94.Springer,1993.
[58] J.LawrenceCarterandMarkN.Wegman. Universalclassesofhashfunctions. Journalof
ComputerandSystemSciences,18(2):143–154,1979.
[59] BarbaraChapman,GabrieleJost,andRuudvanderPas. UsingOpenMP:PortableShared
MemoryParallelProgramming. TheMITPress,2007.
[60] BernardChazelle. Aminimumspanningtreealgorithmwithinverse-Ackermanntypecom-
plexity. JournaloftheACM,47(6):1028–1047,2000.
[61] Joseph Cheriyan and Torben Hagerup. A randomized maximum-flow algorithm. SIAM
JournalonComputing,24(2):203–226,1995.
[62] JosephCheriyanandS.N.Maheshwari. Analysisofpreflowpushalgorithmsformaximum
networkflow. SIAMJournalonComputing,18(6):1057–1086,1989.
[63] BorisV.CherkasskyandAndrewV.Goldberg. Onimplementingthepush-relabelmethod
forthemaximumflowproblem. Algorithmica,19(4):390–410,1997.
[64] BorisV.Cherkassky,AndrewV.Goldberg,andTomaszRadzik. Shortestpathsalgorithms:
Theoryandexperimentalevaluation. MathematicalProgramming,73(2):129–174,1996.
[65] BorisV.Cherkassky,AndrewV.Goldberg,andCraigSilverstein. Buckets,heaps,listsand
monotonepriorityqueues. SIAMJournalonComputing,28(4):1326–1346,1999.
[66] H.Chernoff. Ameasureofasymptoticefficiencyfortestsofahypothesisbasedonthesum
ofobservations. AnnalsofMathematicalStatistics,23(4):493–507,1952.
[67] KaiLaiChung. ElementaryProbabilityTheorywithStochasticProcesses. Springer,1974.
[68] V. Chva´tal. Agreedy heuristicfor theset-coveringproblem. MathematicsofOperations
Research,4(3):233–235,1979.
[69] V.Chva´tal. LinearProgramming. W.H.FreemanandCompany,1983.
[70] V. Chva´tal, D. A. Klarner, and D. E. Knuth. Selected combinatorial research problems.
Technical Report STAN-CS-72-292, Computer Science Department, Stanford University,
1972.
[71] CilkArts,Inc.,Burlington,Massachusetts. Cilk++Programmer’sGuide,2008. Available
athttp://www.cilk.com/archive/docs/cilk1guide.
Bibliography 1235
[72] AlanCobham. Theintrinsiccomputational difficultyof functions. InProceedings ofthe
1964CongressforLogic,Methodology,andthePhilosophyofScience,pages24–30.North-
Holland,1964.
[73] H.CohenandH.W.Lenstra,Jr. PrimalitytestingandJacobisums. MathematicsofCom-
putation,42(165):297–330,1984.
[74] DouglasComer. TheubiquitousB-tree. ACMComputingSurveys,11(2):121–137,1979.
[75] StephenCook. Thecomplexityoftheoremprovingprocedures. InProceedingsoftheThird
AnnualACMSymposiumonTheoryofComputing,pages151–158,1971.
[76] JamesW.CooleyandJohnW.Tukey. Analgorithmforthemachinecalculationofcomplex
Fourierseries. MathematicsofComputation,19(90):297–301,1965.
[77] Don Coppersmith. Modifications to the number field sieve. Journal of Cryptology,
6(3):169–180,1993.
[78] DonCoppersmithandShmuelWinograd. Matrixmultiplicationviaarithmeticprogression.
JournalofSymbolicComputation,9(3):251–280,1990.
[79] ThomasH.Cormen,ThomasSundquist,andLeonardF.Wisniewski. Asymptoticallytight
bounds for performing BMMC permutations on parallel disk systems. SIAM Journal on
Computing,28(1):105–136,1998.
[80] DonDaileyandCharlesE.Leiserson. UsingCilktowritemultiprocessorchessprograms.
InH.J.vandenHerikandB.Monien, editors, Advances inComputerGames, volume9,
pages25–52.UniversityofMaastricht,Netherlands,2001.
[81] Paolo D’Alberto and Alexandru Nicolau. Adaptive Strassen’s matrix multiplication. In
Proceedingsofthe21stAnnualInternational ConferenceonSupercomputing, pages284–
292,June2007.
[82] SanjoyDasgupta,ChristosPapadimitriou,andUmeshVazirani. Algorithms. McGraw-Hill,
2008.
[83] RomanDementiev,LutzKettner,JensMehnert,andPeterSanders. Engineeringasortedlist
datastructurefor32bitkeys. InProceedingsoftheSixthWorkshoponAlgorithmEngineer-
ingandExperimentsandtheFirstWorkshoponAnalyticAlgorithmicsandCombinatorics,
pages142–151,January2004.
[84] CamilDemetrescuandGiuseppeF.Italiano. Fullydynamicallpairsshortestpathswithreal
edgeweights. JournalofComputerandSystemSciences,72(5):813–837,2006.
[85] EricV. Denardo andBennett L.Fox. Shortest-routemethods: 1. Reaching, pruning, and
buckets. OperationsResearch,27(1):161–186,1979.
[86] MartinDietzfelbinger,AnnaKarlin,KurtMehlhorn,FriedhelmMeyeraufderHeide,Hans
Rohnert,andRobertE.Tarjan. Dynamicperfecthashing: Upperandlowerbounds. SIAM
JournalonComputing,23(4):738–761,1994.
[87] WhitfieldDiffieandMartinE.Hellman. Newdirectionsincryptography. IEEETransac-
tionsonInformationTheory,IT-22(6):644–654,1976.
[88] E.W.Dijkstra. Anoteontwoproblemsinconnexionwithgraphs. NumerischeMathematik,
1(1):269–271,1959.
1236 Bibliography
[89] E.A.Dinic. Algorithmforsolutionofaproblemofmaximumflowinanetworkwithpower
estimation. SovietMathematicsDoklady,11(5):1277–1280,1970.
[90] BrandonDixon,MonikaRauch,andRobertE.Tarjan. Verificationandsensitivityanalysis
ofminimumspanningtreesinlineartime. SIAMJournalonComputing,21(6):1184–1192,
1992.
[91] John D. Dixon. Factorizationand primalitytests. TheAmerican Mathematical Monthly,
91(6):333–352,1984.
[92] DoritDor,JohanHa°stad, StaffanUlfberg, andUriZwick. Onlowerbounds forselecting
themedian. SIAMJournalonDiscreteMathematics,14(3):299–311,2001.
[93] DoritDorandUriZwick. Selectingthemedian. SIAMJournalonComputing,28(5):1722–
1758,1999.
[94] DoritDorandUriZwick. Medianselectionrequires.2 /ncomparisons. SIAMJournal
C
onDiscreteMathematics,14(3):312–325,2001.
[95] AlvinW.Drake. FundamentalsofAppliedProbabilityTheory. McGraw-Hill,1967.
[96] JamesR.Driscoll,HaroldN.Gabow,RuthShrairman,andRobertE.Tarjan. Relaxedheaps:
AnalternativetoFibonacciheapswithapplicationstoparallelcomputation. Communica-
tionsoftheACM,31(11):1343–1354,1988.
[97] James R. Driscoll, Neil Sarnak, Daniel D. Sleator, and Robert E. Tarjan. Making data
structurespersistent. JournalofComputerandSystemSciences,38(1):86–124,1989.
[98] Derek L. Eager, John Zahorjan, and Edward D. Lazowska. Speedup versus efficiency in
parallelsystems. IEEETransactionsonComputers,38(3):408–423,1989.
[99] HerbertEdelsbrunner. AlgorithmsinCombinatorialGeometry,volume10ofEATCSMono-
graphsonTheoreticalComputerScience. Springer,1987.
[100] JackEdmonds. Paths,trees,andflowers. CanadianJournalofMathematics,17:449–467,
1965.
[101] JackEdmonds. Matroidsandthegreedyalgorithm. MathematicalProgramming,1(1):127–
136,1971.
[102] Jack Edmonds and Richard M. Karp. Theoretical improvements in the algorithmic effi-
ciencyfornetworkflowproblems. JournaloftheACM,19(2):248–264,1972.
[103] ShimonEven. GraphAlgorithms. ComputerSciencePress,1979.
[104] WilliamFeller. AnIntroductiontoProbabilityTheoryandItsApplications. JohnWiley&
Sons,thirdedition,1968.
[105] Robert W. Floyd. Algorithm 97 (SHORTEST PATH). Communications of the ACM,
5(6):345,1962.
[106] RobertW.Floyd. Algorithm245(TREESORT). CommunicationsoftheACM,7(12):701,
1964.
[107] Robert W. Floyd. Permuting information inidealized two-level storage. In Raymond E.
MillerandJamesW.Thatcher,editors,ComplexityofComputerComputations,pages105–
109.PlenumPress,1972.
Bibliography 1237
[108] RobertW.FloydandRonaldL.Rivest. Expectedtimeboundsforselection. Communica-
tionsoftheACM,18(3):165–172,1975.
[109] LestorR.Ford, Jr.andD.R.Fulkerson. FlowsinNetworks. PrincetonUniversityPress,
1962.
[110] LestorR.Ford,Jr.andSelmerM.Johnson. Atournamentproblem. TheAmericanMathe-
maticalMonthly,66(5):387–389,1959.
[111] MichaelL.Fredman. Newboundsonthecomplexityoftheshortestpathproblem. SIAM
JournalonComputing,5(1):83–89,1976.
[112] MichaelL.Fredman,Ja´nosKomlo´s,andEndreSzemere´di. StoringasparsetablewithO.1/
worstcaseaccesstime. JournaloftheACM,31(3):538–544,1984.
[113] MichaelL.FredmanandMichaelE.Saks. Thecellprobecomplexityofdynamicdatastruc-
tures. InProceedingsoftheTwentyFirstAnnualACMSymposiumonTheoryofComputing,
pages345–354,1989.
[114] Michael L. Fredman and Robert E. Tarjan. Fibonacci heaps and their uses in improved
networkoptimizationalgorithms. JournaloftheACM,34(3):596–615,1987.
[115] MichaelL.FredmanandDanE.Willard. Surpassingtheinformationtheoreticboundwith
fusiontrees. JournalofComputerandSystemSciences,47(3):424–436,1993.
[116] MichaelL.FredmanandDanE.Willard. Trans-dichotomousalgorithmsforminimumspan-
ningtreesandshortest paths. Journal ofComputerand System Sciences, 48(3):533–551,
1994.
[117] MatteoFrigoandStevenG.Johnson. ThedesignandimplementationofFFTW3. Proceed-
ingsoftheIEEE,93(2):216–231,2005.
[118] MatteoFrigo,CharlesE.Leiserson,andKeithH.Randall. TheimplementationoftheCilk-5
multithreadedlanguage. InProceedings ofthe1998 ACMSIGPLANConference onPro-
grammingLanguageDesignandImplementation,pages212–223,1998.
[119] HaroldN. Gabow. Path-based depth-first search for strong and biconnected components.
InformationProcessingLetters,74(3–4):107–114,2000.
[120] HaroldN.Gabow,Z.Galil,T.Spencer,andRobertE.Tarjan. Efficientalgorithmsforfind-
ingminimumspanningtreesinundirectedanddirectedgraphs. Combinatorica,6(2):109–
122,1986.
[121] HaroldN.GabowandRobertE.Tarjan. Alinear-timealgorithmforaspecialcaseofdisjoint
setunion. JournalofComputerandSystemSciences,30(2):209–221,1985.
[122] HaroldN.Gabow andRobert E.Tarjan. Fasterscalingalgorithmsfor networkproblems.
SIAMJournalonComputing,18(5):1013–1036,1989.
[123] Zvi Galil and Oded Margalit. All pairs shortest distances for graphs with small integer
lengthedges. InformationandComputation,134(2):103–139,1997.
[124] ZviGalilandOdedMargalit. Allpairsshortestpathsforgraphswithsmallintegerlength
edges. JournalofComputerandSystemSciences,54(2):243–254,1997.
[125] ZviGalilandKunsooPark. Dynamicprogrammingwithconvexity,concavityandsparsity.
TheoreticalComputerScience,92(1):49–76,1992.
1238 Bibliography
[126] ZviGalilandJoelSeiferas. Time-space-optimalstringmatching. JournalofComputerand
SystemSciences,26(3):280–294,1983.
[127] IgalGalperinandRonaldL.Rivest. Scapegoattrees. InProceedingsofthe4thACM-SIAM
SymposiumonDiscreteAlgorithms,pages165–174,1993.
[128] Michael R. Garey, R. L. Graham, and J. D. Ullman. Worst-case analyis of memory al-
locationalgorithms. InProceedingsoftheFourthAnnual ACMSymposium onTheoryof
Computing,pages143–150,1972.
[129] Michael R. Garey and David S. Johnson. Computers and Intractability: A Guide to the
TheoryofNP-Completeness. W.H.Freeman,1979.
[130] SaulGass. LinearProgramming: MethodsandApplications. InternationalThomsonPub-
lishing,fourthedition,1975.
[131] Fa˘nica˘Gavril. Algorithmsforminimumcoloring,maximumclique,minimumcoveringby
cliques, andmaximumindependent setofachordal graph. SIAMJournal onComputing,
1(2):180–187,1972.
[132] Alan George and Joseph W-H Liu. Computer Solution of Large Sparse Positive Definite
Systems. PrenticeHall,1981.
[133] E. N. Gilbert and E.F. Moore. Variable-length binary encodings. Bell System Technical
Journal,38(4):933–967,1959.
[134] Michel X. Goemans and David P. Williamson. Improved approximation algorithms for
maximumcutandsatisfiabilityproblemsusingsemidefiniteprogramming. Journal ofthe
ACM,42(6):1115–1145,1995.
[135] MichelX.GoemansandDavidP.Williamson. Theprimal-dualmethodforapproximation
algorithmsanditsapplicationtonetworkdesignproblems. InDoritS.Hochbaum, editor,
ApproximationAlgorithmsforNP-HardProblems,pages144–191.PWSPublishingCom-
pany,1997.
[136] Andrew V.Goldberg. EfficientGraphAlgorithmsforSequential andParallelComputers.
PhDthesis,DepartmentofElectricalEngineeringandComputerScience,MIT,1987.
[137] AndrewV.Goldberg. Scalingalgorithmsfortheshortestpathsproblem. SIAMJournalon
Computing,24(3):494–504,1995.
[138] Andrew V.GoldbergandSatishRao. Beyondtheflowdecomposition barrier. Journal of
theACM,45(5):783–797,1998.
[139] AndrewV.Goldberg,E´vaTardos,andRobertE.Tarjan. Networkflowalgorithms. InBern-
hard Korte, La´szlo´ Lova´sz, HansJu¨rgenPro¨mel, and Alexander Schrijver, editors, Paths,
Flows,andVLSI-Layout,pages101–164.Springer,1990.
[140] AndrewV.GoldbergandRobertE.Tarjan. Anewapproachtothemaximumflowproblem.
JournaloftheACM,35(4):921–940,1988.
[141] D.GoldfarbandM.J.Todd. Linearprogramming. InG.L.Nemhauser,A.H.G.Rinnooy-
Kan,andM.J.Todd,editors,HandbookinOperationsResearchandManagementScience,
Vol.1,Optimization,pages73–170.ElsevierSciencePublishers,1989.
[142] Shafi Goldwasser and Silvio Micali. Probabilistic encryption. Journal of Computer and
SystemSciences,28(2):270–299,1984.
Bibliography 1239
[143] ShafiGoldwasser,SilvioMicali,andRonaldL.Rivest. Adigitalsignatureschemesecure
against adaptive chosen-message attacks. SIAM Journal on Computing, 17(2):281–308,
1988.
[144] GeneH.GolubandCharlesF.VanLoan. MatrixComputations. TheJohnsHopkinsUni-
versityPress,thirdedition,1996.
[145] G.H.Gonnet. HandbookofAlgorithmsandDataStructures. Addison-Wesley,1984.
[146] Rafael C. Gonzalez and Richard E.Woods. Digital Image Processing. Addison-Wesley,
1992.
[147] MichaelT.GoodrichandRobertoTamassia. DataStructuresandAlgorithmsinJava. John
Wiley&Sons,1998.
[148] MichaelT.GoodrichandRobertoTamassia. AlgorithmDesign:Foundations,Analysis,and
InternetExamples. JohnWiley&Sons,2001.
[149] RonaldL.Graham. Bounds forcertainmultiprocessor anomalies. BellSystem Technical
Journal,45(9):1563–1581,1966.
[150] RonaldL.Graham. Anefficientalgorithmfordeterminingtheconvexhullofafiniteplanar
set. InformationProcessingLetters,1(4):132–133,1972.
[151] RonaldL.GrahamandPavolHell. Onthehistoryoftheminimumspanningtreeproblem.
AnnalsoftheHistoryofComputing,7(1):43–57,1985.
[152] Ronald L. Graham, Donald E. Knuth, and Oren Patashnik. Concrete Mathematics.
Addison-Wesley,secondedition,1994.
[153] DavidGries. TheScienceofProgramming. Springer,1981.
[154] M.Gro¨tschel,La´szlo´ Lova´sz,andAlexanderSchrijver. GeometricAlgorithmsandCombi-
natorialOptimization. Springer,1988.
[155] Leo J. Guibas and Robert Sedgewick. A dichromatic framework for balanced trees. In
Proceedings of the 19th Annual Symposium on Foundations of Computer Science, pages
8–21,1978.
[156] DanGusfield. AlgorithmsonStrings,Trees,andSequences: ComputerScienceandCom-
putationalBiology. CambridgeUniversityPress,1997.
[157] H.HalberstamandR.E.Ingram,editors. TheMathematicalPapersofSirWilliamRowan
Hamilton,volumeIII(Algebra). CambridgeUniversityPress,1967.
[158] YijieHan. Improvedfastintegersortinginlinearspace. InProceedingsofthe12thACM-
SIAMSymposiumonDiscreteAlgorithms,pages793–796,2001.
[159] YijieHan. AnO.n3.loglogn=logn/5=4/timealgorithmforallpairsshortestpath. Algo-
rithmica,51(4):428–434,2008.
[160] FrankHarary. GraphTheory. Addison-Wesley,1969.
[161] GregoryC.HarfstandEdwardM.Reingold. Apotential-basedamortizedanalysisofthe
union-finddatastructure. SIGACTNews,31(3):86–95,2000.
[162] J.HartmanisandR.E.Stearns. Onthecomputationalcomplexityofalgorithms. Transac-
tionsoftheAmericanMathematicalSociety,117:285–306,May1965.
1240 Bibliography
[163] MichaelT.Heideman,DonH.Johnson,andC.SidneyBurrus. Gaussandthehistoryofthe
FastFourierTransform. IEEEASSPMagazine,1(4):14–21,1984.
[164] Monika R.Henzinger andValerieKing. Fullydynamicbiconnectivity andtransitiveclo-
sure. InProceedingsofthe36thAnnualSymposiumonFoundationsofComputerScience,
pages664–672,1995.
[165] MonikaR.HenzingerandValerieKing. Randomizedfullydynamicgraphalgorithmswith
polylogarithmictimeperoperation. JournaloftheACM,46(4):502–516,1999.
[166] MonikaR.Henzinger,SatishRao,andHaroldN.Gabow. Computingvertexconnectivity:
Newboundsfromoldtechniques. JournalofAlgorithms,34(2):222–250,2000.
[167] NicholasJ.Higham. Exploitingfastmatrixmultiplicationwithinthelevel3BLAS. ACM
TransactionsonMathematicalSoftware,16(4):352–368,1990.
[168] W.DanielHillisandJr.GuyL.Steele. Dataparallelalgorithms. Communications ofthe
ACM,29(12):1170–1183,1986.
[169] C.A.R.Hoare. Algorithm63(PARTITION)andalgorithm65(FIND). Communications
oftheACM,4(7):321–322,1961.
[170] C.A.R.Hoare. Quicksort. ComputerJournal,5(1):10–15,1962.
[171] DoritS.Hochbaum. Efficientboundsforthestableset,vertexcoverandsetpackingprob-
lems. DiscreteAppliedMathematics,6(3):243–254,1983.
[172] DoritS.Hochbaum,editor. ApproximationAlgorithmsforNP-HardProblems. PWSPub-
lishingCompany,1997.
[173] W.Hoeffding. Onthedistributionofthenumberofsuccessesinindependenttrials. Annals
ofMathematicalStatistics,27(3):713–721,1956.
[174] MichaHofri. ProbabilisticAnalysisofAlgorithms. Springer,1987.
[175] MichaHofri. AnalysisofAlgorithms. OxfordUniversityPress,1995.
[176] John E. Hopcroft and Richard M. Karp. An n5=2 algorithm for maximum matchings in
bipartitegraphs. SIAMJournalonComputing,2(4):225–231,1973.
[177] JohnE.Hopcroft,RajeevMotwani,andJeffreyD.Ullman. IntroductiontoAutomataThe-
ory,Languages,andComputation. AddisonWesley,thirdedition,2006.
[178] JohnE.HopcroftandRobertE.Tarjan. Efficientalgorithmsforgraphmanipulation. Com-
municationsoftheACM,16(6):372–378,1973.
[179] JohnE.HopcroftandJeffreyD.Ullman. Setmergingalgorithms. SIAMJournalonCom-
puting,2(4):294–303,1973.
[180] JohnE.HopcroftandJeffreyD.Ullman. IntroductiontoAutomataTheory,Languages,and
Computation. Addison-Wesley,1979.
[181] EllisHorowitz,SartajSahni, andSanguthevar Rajasekaran. ComputerAlgorithms. Com-
puterSciencePress,1998.
[182] T.C.HuandM.T.Shing. Computationofmatrixchainproducts.PartI. SIAMJournalon
Computing,11(2):362–373,1982.
[183] T.C.HuandM.T.Shing. Computationofmatrixchainproducts.PartII. SIAMJournalon
Computing,13(2):228–251,1984.
Bibliography 1241
[184] T.C.HuandA.C.Tucker. Optimalcomputersearchtreesandvariable-lengthalphabetic
codes. SIAMJournalonAppliedMathematics,21(4):514–532,1971.
[185] DavidA.Huffman. A method for theconstruction ofminimum-redundancy codes. Pro-
ceedingsoftheIRE,40(9):1098–1101,1952.
[186] StevenHuss-Lederman,ElaineM.Jacobson,JeremyR.Johnson,AnnaTsao,andThomas
Turnbull. ImplementationofStrassen’salgorithmformatrixmultiplication. InProceedings
ofthe1996ACM/IEEEConferenceonSupercomputing,article32,1996.
[187] OscarH.IbarraandChulE.Kim. Fastapproximationalgorithmsfortheknapsackandsum
ofsubsetproblems. JournaloftheACM,22(4):463–468,1975.
[188] E. J. Isaac and R. C. Singleton. Sorting by address calculation. Journal of the ACM,
3(3):169–174,1956.
[189] R.A.Jarvis. Ontheidentificationoftheconvexhullofafinitesetofpointsintheplane.
InformationProcessingLetters,2(1):18–21,1973.
[190] DavidS.Johnson. Approximationalgorithmsforcombinatorialproblems. JournalofCom-
puterandSystemSciences,9(3):256–278,1974.
[191] DavidS.Johnson. TheNP-completenesscolumn: Anongoingguide—Thetaleofthesec-
ondprover. JournalofAlgorithms,13(3):502–524,1992.
[192] DonaldB.Johnson. Efficientalgorithmsforshortestpathsinsparsenetworks. Journalof
theACM,24(1):1–13,1977.
[193] RichardJohnsonbaughandMarcusSchaefer. Algorithms. PearsonPrenticeHall,2004.
[194] A. Karatsuba and Yu. Ofman. Multiplication of multidigit numbers on automata. Soviet
Physics—Doklady,7(7):595–596,1963. TranslationofanarticleinDokladyAkademiiNauk
SSSR,145(2),1962.
[195] DavidR.Karger,PhilipN.Klein,andRobertE.Tarjan. Arandomizedlinear-timealgorithm
tofindminimumspanningtrees. JournaloftheACM,42(2):321–328,1995.
[196] David R. Karger, Daphne Koller, and Steven J. Phillips. Finding the hidden path: Time
boundsforall-pairsshortestpaths. SIAMJournalonComputing,22(6):1199–1217,1993.
[197] HowardKarloff. LinearProgramming. Birkha¨user,1991.
[198] N.Karmarkar. Anewpolynomial-timealgorithmforlinearprogramming. Combinatorica,
4(4):373–395,1984.
[199] RichardM.Karp. Reducibilityamongcombinatorialproblems. InRaymondE.Millerand
JamesW.Thatcher,editors,ComplexityofComputerComputations,pages85–103.Plenum
Press,1972.
[200] RichardM.Karp. Anintroductiontorandomizedalgorithms. DiscreteAppliedMathemat-
ics,34(1–3):165–201,1991.
[201] RichardM.KarpandMichaelO.Rabin. Efficientrandomizedpattern-matchingalgorithms.
IBMJournalofResearchandDevelopment,31(2):249–260,1987.
[202] A.V. Karzanov. Determiningthemaximal flowinanetwork bythemethod ofpreflows.
SovietMathematicsDoklady,15(2):434–437,1974.
1242 Bibliography
[203] Valerie King. A simpler minimum spanning tree verification algorithm. Algorithmica,
18(2):263–270,1997.
[204] ValerieKing,SatishRao,andRobertE.Tarjan. Afasterdeterministicmaximumflowalgo-
rithm. JournalofAlgorithms,17(3):447–474,1994.
[205] Jeffrey H. Kingston. Algorithms and Data Structures: Design, Correctness, Analysis.
Addison-Wesley,secondedition,1997.
[206] D.G.KirkpatrickandR.Seidel. Theultimateplanarconvexhullalgorithm? SIAMJournal
onComputing,15(2):287–299,1986.
[207] PhilipN.KleinandNeal E.Young. Approximation algorithmsfor NP-hardoptimization
problems. InCRCHandbookonAlgorithms,pages34-1–34-19.CRCPress,1999.
[208] JonKleinbergandE´vaTardos. AlgorithmDesign. Addison-Wesley,2006.
[209] Donald E.Knuth. Fundamental Algorithms, volume1ofTheArtof ComputerProgram-
ming. Addison-Wesley,1968. Thirdedition,1997.
[210] DonaldE.Knuth. SeminumericalAlgorithms,volume2ofTheArtofComputerProgram-
ming. Addison-Wesley,1969. Thirdedition,1997.
[211] DonaldE.Knuth. SortingandSearching,volume3ofTheArtofComputerProgramming.
Addison-Wesley,1973. Secondedition,1998.
[212] DonaldE.Knuth. Optimumbinarysearchtrees. ActaInformatica,1(1):14–25,1971.
[213] DonaldE.Knuth. Bigomicronandbigomegaandbigtheta. SIGACTNews,8(2):18–23,
1976.
[214] Donald E. Knuth, James H. Morris, Jr., and Vaughan R. Pratt. Fast pattern matching in
strings. SIAMJournalonComputing,6(2):323–350,1977.
[215] J.Komlo´s. Linearverificationforspanningtrees. Combinatorica,5(1):57–65,1985.
[216] BernhardKorteandLa´szlo´ Lova´sz. Mathematicalstructuresunderlyinggreedyalgorithms.
InF.Gecseg,editor,FundamentalsofComputationTheory,volume117ofLectureNotesin
ComputerScience,pages205–209.Springer,1981.
[217] Bernhard Korte and La´szlo´ Lova´sz. Structural properties of greedoids. Combinatorica,
3(3–4):359–374,1983.
[218] Bernhard Korte and La´szlo´ Lova´sz. Greedoids—A structural framework for the greedy
algorithm. InW.Pulleybank,editor,ProgressinCombinatorialOptimization,pages221–
243.AcademicPress,1984.
[219] BernhardKorteandLa´szlo´Lova´sz. Greedoidsandlinearobjectivefunctions. SIAMJournal
onAlgebraicandDiscreteMethods,5(2):229–238,1984.
[220] DexterC.Kozen. TheDesignandAnalysisofAlgorithms. Springer,1992.
[221] DavidW.Krumme,GeorgeCybenko,andK.N.Venkataraman. Gossipinginminimaltime.
SIAMJournalonComputing,21(1):111–139,1992.
[222] JosephB.Kruskal,Jr. Ontheshortestspanningsubtreeofagraphandthetravelingsalesman
problem. ProceedingsoftheAmericanMathematicalSociety,7(1):48–50,1956.
[223] LeslieLamport. Howtomakeamultiprocessorcomputerthatcorrectlyexecutesmultipro-
cessprograms. IEEETransactionsonComputers,C-28(9):690–691,1979.
Bibliography 1243
[224] EugeneL.Lawler. CombinatorialOptimization: NetworksandMatroids. Holt,Rinehart,
andWinston,1976.
[225] EugeneL.Lawler,J.K.Lenstra,A.H.G.RinnooyKan, andD.B.Shmoys, editors. The
TravelingSalesmanProblem. JohnWiley&Sons,1985.
[226] C. Y. Lee. An algorithm for path connection and its applications. IRE Transactions on
ElectronicComputers,EC-10(3):346–365,1961.
[227] TomLeighton. Tightboundsonthecomplexityofparallelsorting. IEEETransactionson
Computers,C-34(4):344–354,1985.
[228] TomLeighton. Notesonbettermastertheoremsfordivide-and-conquerrecurrences. Class
notes.Availableathttp://citeseer.ist.psu.edu/252350.html,October1996.
[229] TomLeightonandSatishRao. Multicommoditymax-flowmin-cuttheoremsandtheiruse
indesigningapproximationalgorithms. JournaloftheACM,46(6):787–832,1999.
[230] Daan Leijen and Judd Hall. Optimize managed code for multi-core machines. MSDN
Magazine,October2007.
[231] DebraA.LelewerandDanielS.Hirschberg. Datacompression. ACMComputingSurveys,
19(3):261–296,1987.
[232] A.K.Lenstra,H.W.Lenstra,Jr.,M.S.Manasse,andJ.M.Pollard. Thenumberfieldsieve.
InA.K.LenstraandH.W.Lenstra,Jr.,editors,TheDevelopmentoftheNumberFieldSieve,
volume1554ofLectureNotesinMathematics,pages11–42.Springer,1993.
[233] H. W. Lenstra, Jr. Factoring integers with elliptic curves. Annals of Mathematics,
126(3):649–673,1987.
[234] L.A.Levin. Universalsortingproblems. ProblemyPeredachi Informatsii,9(3):265–266,
1973. InRussian.
[235] Anany Levitin. Introduction to the Design & Analysis of Algorithms. Addison-Wesley,
2007.
[236] Harry R. Lewisand Christos H. Papadimitriou. Elements of the Theory of Computation.
PrenticeHall,secondedition,1998.
[237] C.L.Liu. IntroductiontoCombinatorialMathematics. McGraw-Hill,1968.
[238] La´szlo´ Lova´sz. Ontheratioofoptimalintegralandfractionalcovers. DiscreteMathemat-
ics,13(4):383–390,1975.
[239] La´szlo´ Lova´szandMichaelD.Plummer. MatchingTheory,volume121ofAnnalsofDis-
creteMathematics. NorthHolland,1986.
[240] Bruce M. Maggs and Serge A. Plotkin. Minimum-cost spanning tree as a path-finding
problem. InformationProcessingLetters,26(6):291–293,1988.
[241] MichaelMain. DataStructuresandOtherObjectsUsingJava. Addison-Wesley,1999.
[242] UdiManber. IntroductiontoAlgorithms:ACreativeApproach. Addison-Wesley,1989.
[243] Conrado Mart´ınez and Salvador Roura. Randomized binary search trees. Journal of the
ACM,45(2):288–323,1998.
[244] WilliamJ.MasekandMichael S.Paterson. Afasteralgorithmcomputing stringeditdis-
tances. JournalofComputerandSystemSciences,20(1):18–31,1980.
1244 Bibliography
[245] H.A.Maurer,Th.Ottmann,andH.-W.Six. Implementingdictionariesusingbinarytreesof
verysmallheight. InformationProcessingLetters,5(1):11–14,1976.
[246] ErnstW.Mayr,HansJu¨rgenPro¨mel,andAngelikaSteger,editors. LecturesonProofVerifi-
cationandApproximationAlgorithms,volume1367ofLectureNotesinComputerScience.
Springer,1998.
[247] C. C. McGeoch. All pairs shortest paths and the essential subgraph. Algorithmica,
13(5):426–441,1995.
[248] M. D. McIlroy. A killer adversary for quicksort. Software—Practice and Experience,
29(4):341–344,1999.
[249] Kurt Mehlhorn. Sorting and Searching, volume 1 of Data Structures and Algorithms.
Springer,1984.
[250] KurtMehlhorn. GraphAlgorithmsandNP-Completeness,volume2ofDataStructuresand
Algorithms. Springer,1984.
[251] Kurt Mehlhorn. Multidimensional Searching and Computational Geometry, volume 3 of
DataStructuresandAlgorithms. Springer,1984.
[252] KurtMehlhornandStefanNa¨her. BoundedordereddictionariesinO.loglogN/timeand
O.n/space. InformationProcessingLetters,35(4):183–189,1990.
[253] Kurt Mehlhorn and Stefan Na¨her. LEDA: A Platform for Combinatorial and Geometric
Computing. CambridgeUniversityPress,1999.
[254] Alfred J. Menezes, Paul C. van Oorschot, and Scott A. Vanstone. Handbook of Applied
Cryptography. CRCPress,1997.
[255] Gary L. Miller. Riemann’s hypothesis and tests for primality. Journal of Computer and
SystemSciences,13(3):300–317,1976.
[256] JohnC.Mitchell. FoundationsforProgrammingLanguages. TheMITPress,1996.
[257] JosephS.B.Mitchell. Guillotinesubdivisionsapproximatepolygonalsubdivisions: Asim-
plepolynomial-timeapproximationschemeforgeometricTSP,k-MST,andrelatedprob-
lems. SIAMJournalonComputing,28(4):1298–1309,1999.
[258] LouisMonier. AlgorithmesdeFactorisationD’Entiers. PhDthesis,L’Universite´Paris-Sud,
1980.
[259] Louis Monier. Evaluation and comparison of two efficient probabilistic primality testing
algorithms. TheoreticalComputerScience,12(1):97–108,1980.
[260] EdwardF.Moore. Theshortestpaththroughamaze. InProceedingsoftheInternational
SymposiumontheTheoryofSwitching,pages285–292.HarvardUniversityPress,1959.
[261] RajeevMotwani, Joseph(Seffi)Naor, andPrabhakarRaghavan. Randomizedapproxima-
tionalgorithmsincombinatorialoptimization. InDoritHochbaum, editor,Approximation
AlgorithmsforNP-HardProblems,chapter11,pages447–481.PWSPublishingCompany,
1997.
[262] RajeevMotwaniandPrabhakarRaghavan. RandomizedAlgorithms. CambridgeUniversity
Press,1995.
[263] J.I.MunroandV.Raman. Faststablein-placesortingwithO.n/datamoves. Algorithmica,
16(2):151–160,1996.
Bibliography 1245
[264] J.NievergeltandE.M.Reingold. Binarysearchtreesofboundedbalance. SIAMJournal
onComputing,2(1):33–43,1973.
[265] IvanNivenand Herbert S.Zuckerman. AnIntroduction totheTheoryof Numbers. John
Wiley&Sons,fourthedition,1980.
[266] Alan V. Oppenheim and Ronald W. Schafer, with John R. Buck. Discrete-Time Signal
Processing. PrenticeHall,secondedition,1998.
[267] Alan V. Oppenheim and Alan S. Willsky, with S. Hamid Nawab. Signals and Systems.
PrenticeHall,secondedition,1997.
[268] JamesB.Orlin. Apolynomial timeprimal networksimplexalgorithmforminimumcost
flows. MathematicalProgramming,78(1):109–129,1997.
[269] Joseph O’Rourke. Computational Geometry in C. Cambridge University Press, second
edition,1998.
[270] ChristosH.Papadimitriou. ComputationalComplexity. Addison-Wesley,1994.
[271] ChristosH.PapadimitriouandKennethSteiglitz. CombinatorialOptimization:Algorithms
andComplexity. PrenticeHall,1982.
[272] MichaelS.Paterson. Progressinselection. InProceedingsoftheFifthScandinavianWork-
shoponAlgorithmTheory,pages368–379,1996.
[273] MihaiPaˇtras¸cuandMikkelThorup. Time-spacetrade-offsforpredecessorsearch. InPro-
ceedings of the 38th Annual ACM Symposium on Theory of Computing, pages 232–240,
2006.
[274] MihaiPaˇtras¸cuandMikkelThorup. Randomizationdoesnothelpsearchingpredecessors.
InProceedingsofthe18thACM-SIAMSymposiumonDiscreteAlgorithms,pages555–564,
2007.
[275] PavelA.Pevzner. ComputationalMolecularBiology: AnAlgorithmicApproach. TheMIT
Press,2000.
[276] StevenPhillipsandJefferyWestbrook. Onlineloadbalancingandnetworkflow. InPro-
ceedings of the 25th Annual ACM Symposium on Theory of Computing, pages 402–411,
1993.
[277] J.M.Pollard. AMonteCarlomethodforfactorization. BIT,15(3):331–334,1975.
[278] J.M.Pollard. Factoringwithcubicintegers. InA.K.LenstraandH.W.Lenstra,Jr.,editors,
TheDevelopmentoftheNumberFieldSieve,volume1554ofLectureNotesinMathematics,
pages4–10.Springer,1993.
[279] Carl Pomerance. On the distribution of pseudoprimes. Mathematics of Computation,
37(156):587–593,1981.
[280] CarlPomerance,editor. ProceedingsoftheAMSSymposiainAppliedMathematics: Com-
putationalNumberTheoryandCryptography. AmericanMathematicalSociety,1990.
[281] WilliamK.Pratt. DigitalImageProcessing. JohnWiley&Sons,fourthedition,2007.
[282] FrancoP.PreparataandMichaelIanShamos. ComputationalGeometry: AnIntroduction.
Springer,1985.
1246 Bibliography
[283] WilliamH.Press,SaulA.Teukolsky,WilliamT.Vetterling,andBrianP.Flannery. Numer-
icalRecipesinC++:TheArtofScientificComputing. CambridgeUniversityPress,second
edition,2002.
[284] WilliamH.Press,SaulA.Teukolsky,WilliamT.Vetterling,andBrianP.Flannery. Numer-
icalRecipes: TheArtofScientificComputing. CambridgeUniversityPress,thirdedition,
2007.
[285] R.C.Prim. Shortestconnectionnetworksandsomegeneralizations. BellSystemTechnical
Journal,36(6):1389–1401,1957.
[286] WilliamPugh. Skiplists:Aprobabilisticalternativetobalancedtrees. Communicationsof
theACM,33(6):668–676,1990.
[287] PaulW.Purdom, Jr.andCynthiaA.Brown. TheAnalysisofAlgorithms. Holt, Rinehart,
andWinston,1985.
[288] Michael O.Rabin. Probabilisticalgorithms. InJ.F.Traub, editor, AlgorithmsandCom-
plexity:NewDirectionsandRecentResults,pages21–39.AcademicPress,1976.
[289] MichaelO.Rabin. Probabilisticalgorithmfortestingprimality. JournalofNumberTheory,
12(1):128–138,1980.
[290] P.RaghavanandC.D.Thompson. Randomizedrounding: Atechniqueforprovablygood
algorithmsandalgorithmicproofs. Combinatorica,7(4):365–374,1987.
[291] RajeevRaman. Recentresultsonthesingle-sourceshortestpathsproblem. SIGACTNews,
28(2):81–87,1997.
[292] JamesReinders. IntelThreadingBuildingBlocks:OutfittingC++forMulti-coreProcessor
Parallelism. O’ReillyMedia,Inc.,2007.
[293] EdwardM.Reingold,Ju¨rgNievergelt,andNarsinghDeo. CombinatorialAlgorithms:The-
oryandPractice. PrenticeHall,1977.
[294] EdwardM.Reingold,KennethJ.Urban,andDavidGries. K-M-Pstringmatchingrevisited.
InformationProcessingLetters,64(5):217–223,1997.
[295] Hans Riesel. Prime Numbers and Computer Methods for Factorization, volume 126 of
ProgressinMathematics. Birkha¨user,secondedition,1994.
[296] RonaldL.Rivest,AdiShamir,andLeonardM.Adleman. Amethodforobtainingdigital
signatures and public-key cryptosystems. Communications of the ACM, 21(2):120–126,
1978. SeealsoU.S.Patent4,405,829.
[297] Herbert Robbins. A remark on Stirling’s formula. American Mathematical Monthly,
62(1):26–29,1955.
[298] D.J.Rosenkrantz,R.E.Stearns,andP.M.Lewis. Ananalysisofseveralheuristicsforthe
travelingsalesmanproblem. SIAMJournalonComputing,6(3):563–581,1977.
[299] Salvador Roura. An improved master theorem for divide-and-conquer recurrences. In
Proceedings of Automata, Languages and Programming, 24th International Colloquium,
ICALP’97,volume1256ofLectureNotesinComputerScience,pages449–459.Springer,
1997.
[300] Y.A.Rozanov. ProbabilityTheory:AConciseCourse. Dover,1969.
Bibliography 1247
[301] S. Sahni and T. Gonzalez. P-complete approximation problems. Journal of the ACM,
23(3):555–565,1976.
[302] A.Scho¨nhage,M.Paterson,andN.Pippenger. Findingthemedian. JournalofComputer
andSystemSciences,13(2):184–199,1976.
[303] Alexander Schrijver. Theory of Linear and Integer Programming. John Wiley & Sons,
1986.
[304] AlexanderSchrijver. Pathsandflows—Ahistoricalsurvey. CWIQuarterly,6(3):169–183,
1993.
[305] Robert Sedgewick. Implementing quicksort programs. Communications of the ACM,
21(10):847–857,1978.
[306] RobertSedgewick. Algorithms. Addison-Wesley,secondedition,1988.
[307] Robert Sedgewick and Philippe Flajolet. An Introduction to the Analysis of Algorithms.
Addison-Wesley,1996.
[308] RaimundSeidel. Ontheall-pairs-shortest-pathprobleminunweightedundirectedgraphs.
JournalofComputerandSystemSciences,51(3):400–403,1995.
[309] RaimundSeidelandC.R.Aragon. Randomizedsearchtrees. Algorithmica,16(4–5):464–
497,1996.
[310] Joa˜oSetubalandJoa˜oMeidanis. IntroductiontoComputationalMolecularBiology. PWS
PublishingCompany,1997.
[311] CliffordA. Shaffer. APractical Introduction toDataStructures and Algorithm Analysis.
PrenticeHall,secondedition,2001.
[312] JeffreyShallit. OriginsoftheanalysisoftheEuclideanalgorithm. HistoriaMathematica,
21(4):401–419,1994.
[313] MichaelI.ShamosandDanHoey. Geometricintersectionproblems. InProceedingsofthe
17thAnnualSymposiumonFoundationsofComputerScience,pages208–215,1976.
[314] M.Sharir. Astrong-connectivityalgorithmanditsapplicationsindataflowanalysis. Com-
putersandMathematicswithApplications,7(1):67–72,1981.
[315] DavidB.Shmoys. Computingnear-optimalsolutionstocombinatorialoptimizationprob-
lems. InWilliamCook, La´szlo´ Lova´sz, and Paul Seymour, editors, Combinatorial Opti-
mization,volume20ofDIMACSSeriesinDiscreteMathematicsandTheoreticalComputer
Science.AmericanMathematicalSociety,1995.
[316] Avi Shoshan and Uri Zwick. All pairs shortest paths in undirected graphs with integer
weights. InProceedingsofthe40thAnnualSymposiumonFoundationsofComputerSci-
ence,pages605–614,1999.
[317] MichaelSipser. IntroductiontotheTheoryofComputation. ThomsonCourseTechnology,
secondedition,2006.
[318] StevenS.Skiena. TheAlgorithmDesignManual. Springer,secondedition,1998.
[319] Daniel D. Sleator and Robert E. Tarjan. A data structure for dynamic trees. Journal of
ComputerandSystemSciences,26(3):362–391,1983.
1248 Bibliography
[320] DanielD.SleatorandRobertE.Tarjan. Self-adjustingbinarysearchtrees. Journalofthe
ACM,32(3):652–686,1985.
[321] JoelSpencer. TenLecturesontheProbabilisticMethod,volume64ofCBMS-NSFRegional
ConferenceSeriesinAppliedMathematics. SocietyforIndustrialandAppliedMathematics,
1993.
[322] DanielA.SpielmanandShang-HuaTeng. Smoothedanalysisofalgorithms: Whythesim-
plexalgorithmusuallytakespolynomialtime. JournaloftheACM,51(3):385–463,2004.
[323] GilbertStrang. IntroductiontoAppliedMathematics. Wellesley-CambridgePress,1986.
[324] GilbertStrang. LinearAlgebraandItsApplications. ThomsonBrooks/Cole,fourthedition,
2006.
[325] VolkerStrassen. Gaussianeliminationisnotoptimal. NumerischeMathematik,14(3):354–
356,1969.
[326] T.G.Szymanski. Aspecialcaseofthemaximalcommonsubsequenceproblem. Technical
ReportTR-170,ComputerScienceLaboratory,PrincetonUniversity,1975.
[327] RobertE.Tarjan. Depthfirstsearchandlineargraphalgorithms. SIAMJournalonCom-
puting,1(2):146–160,1972.
[328] Robert E.Tarjan. Efficiencyof agoodbutnot linearsetunion algorithm. Journal ofthe
ACM,22(2):215–225,1975.
[329] RobertE.Tarjan. Aclassofalgorithmswhichrequirenonlineartimetomaintaindisjoint
sets. JournalofComputerandSystemSciences,18(2):110–127,1979.
[330] Robert E. Tarjan. Data Structures and Network Algorithms. Society for Industrial and
AppliedMathematics,1983.
[331] Robert E.Tarjan. Amortizedcomputational complexity. SIAMJournal onAlgebraicand
DiscreteMethods,6(2):306–318,1985.
[332] RobertE.Tarjan. Classnotes:Disjointsetunion. COS423,PrincetonUniversity,1999.
[333] RobertE.TarjanandJanvanLeeuwen. Worst-caseanalysisofsetunionalgorithms. Jour-
naloftheACM,31(2):245–281,1984.
[334] George B. Thomas, Jr., Maurice D. Weir, Joel Hass, and Frank R. Giordano. Thomas’
Calculus. Addison-Wesley,eleventhedition,2005.
[335] Mikkel Thorup. Fasterdeterministic sortingand priorityqueues in linear space. InPro-
ceedingsofthe9thACM-SIAMSymposiumonDiscreteAlgorithms,pages550–555,1998.
[336] Mikkel Thorup. Undirected single-source shortest paths with positive integer weights in
lineartime. JournaloftheACM,46(3):362–394,1999.
[337] Mikkel Thorup. On RAM priority queues. SIAM Journal on Computing, 30(1):86–109,
2000.
[338] Richard Tolimieri, Myoung An, and ChaoLu. Mathematics of Multidimensional Fourier
TransformAlgorithms. Springer,secondedition,1997.
[339] P.vanEmdeBoas. Preservingorderinaforestinlessthanlogarithmictime. InProceedings
ofthe16thAnnualSymposiumonFoundationsofComputerScience,pages75–84,1975.
Bibliography 1249
[340] P. van Emde Boas. Preserving order in a forest in less than logarithmic time and linear
space. InformationProcessingLetters,6(3):80–82,1977.
[341] P. van Emde Boas, R. Kaas, and E. Zijlstra. Design and implementation of an efficient
priorityqueue. MathematicalSystemsTheory,10(1):99–127,1976.
[342] Jan van Leeuwen, editor. Handbook of Theoretical Computer Science, Volume A: Algo-
rithmsandComplexity. ElsevierSciencePublishersandtheMITPress,1990.
[343] CharlesVanLoan. ComputationalFrameworksfortheFastFourierTransform. Societyfor
IndustrialandAppliedMathematics,1992.
[344] RobertJ.Vanderbei. LinearProgramming:FoundationsandExtensions. KluwerAcademic
Publishers,1996.
[345] VijayV.Vazirani. ApproximationAlgorithms. Springer,2001.
[346] RakeshM.Verma. Generaltechniquesforanalyzingrecursivealgorithmswithapplications.
SIAMJournalonComputing,26(2):568–581,1997.
[347] Hao Wang and Bill Lin. Pipelined van Emde Boas tree: Algorithms, analysis, and ap-
plications. In 26th IEEEInternational Conference on Computer Communications, pages
2471–2475,2007.
[348] Antony F. Ware. Fastapproximate Fourier transforms for irregularly spaced data. SIAM
Review,40(4):838–856,1998.
[349] StephenWarshall. Atheoremonbooleanmatrices. JournaloftheACM,9(1):11–12,1962.
[350] Michael S. Waterman. Introduction to Computational Biology, Maps, Sequences and
Genomes. Chapman&Hall,1995.
[351] Mark Allen Weiss. Data Structures and Problem Solving Using C++. Addison-Wesley,
secondedition,2000.
[352] Mark Allen Weiss. Data Structures and Problem Solving Using Java. Addison-Wesley,
thirdedition,2006.
[353] MarkAllenWeiss. DataStructuresandAlgorithmAnalysisinC++. Addison-Wesley,third
edition,2007.
[354] Mark Allen Weiss. Data Structures and Algorithm Analysis in Java. Addison-Wesley,
secondedition,2007.
[355] Hassler Whitney. On the abstract properties of linear dependence. American Journal of
Mathematics,57(3):509–533,1935.
[356] HerbertS.Wilf. AlgorithmsandComplexity. AKPeters,secondedition,2002.
[357] J.W.J.Williams. Algorithm232(HEAPSORT). CommunicationsoftheACM,7(6):347–
348,1964.
[358] ShmuelWinograd. Onthealgebraiccomplexityoffunctions. InActesduCongre`sInterna-
tionaldesMathe´maticiens,volume3,pages283–288,1970.
[359] AndrewC.-C.Yao. Alowerboundtofindingconvexhulls. JournaloftheACM,28(4):780–
787,1981.
[360] CheeYap. Arealelementaryapproachtothemasterrecurrenceandgeneralizations. Un-
publishedmanuscript.Availableathttp://cs.nyu.edu/yap/papers/,July2008.
1250 Bibliography
[361] YinyuYe. InteriorPointAlgorithms:TheoryandAnalysis. JohnWiley&Sons,1997.
[362] DanielZwillinger,editor. CRCStandardMathematicalTablesandFormulae. Chapman&
Hall/CRCPress,31stedition,2003.
Index
Thisindexusesthefollowingconventions. Numbersarealphabetized asifspelled
out;forexample, “2-3-4 tree”isindexed asifitwere“two-three-four tree.” When
anentry referstoaplaceother thanthemaintext, thepagenumber isfollowed by
atag: ex.forexercise,pr.forproblem,fig.forfigure,andn.forfootnote. Atagged
page number often indicates the first page of anexercise or problem, which isnot
necessarily thepageonwhichthereference actuallyappears.
˛.n/,574 (setdifference),1159
(cid:0)
(goldenratio),59,108pr.
jj
(conjugateofthegoldenratio),59 (flowvalue),710
y
.n/(Euler’sphifunction),943 (lengthofastring),986
.n/-approximationalgorithm,1106,1123 (setcardinality),1161
o-notation,50–51,64

O-notation,45fig.,47–48,64 (Cartesianproduct),1162
O -notation,62pr. (crossproduct),1016
0
O-notation,62pr.
hi
!-notation,51 (sequence),1166
-notation,45fig.,48–49,64 (standardencoding),1057
e n
1-notation,62pr. k (choose),1185
-notation,62pr. (euclideannorm),1222
(cid:0)kk
‚-notation,44–47,45fig.,64 Š(factorial),57
‚-notation,62pr. (ceiling),54
e de
(set),1158 (floor),54
fg (setmember),1158 pb #c (lowersquareroot),546
e2
(notasetmember),1158 p" (uppersquareroot),546
62 (sum),1145
; (emptylanguage),1058 (product),1148
P
(emptyset),1158 (adjacencyrelation),1169
(subset),1159
Q!;(reachabilityrelation),1170
 (propersubset),1159 (AND),697,1071
 (suchthat),1159 ^ (NOT),1071
W (setintersection),1159 : (OR),697,1071
\ (setunion),1159 _ (groupoperator),939
[ ˚ (convolutionoperator),901
˝
1252 Index
(closureoperator),1058 ADD-SUBARRAY,805pr.
(dividesrelation),927 adjacency-listrepresentation,590
j
−(does-not-dividerelation),927 replacedbyahashtable,593ex.
(equivalentmodulon),54,1165ex. adjacency-matrixrepresentation,591

(notequivalentmodulon),54 adjacencyrelation( ),1169
6 !
Œan(equivalenceclassmodulon),928 adjacentvertices,1169
n(additionmodulon),940 admissibleedge,749
C
n(multiplicationmodulon),940 admissiblenetwork,749–750
 .a/(Legendresymbol),982pr.
adversary,190
p
"(emptystring),986,1058 aggregateanalysis,452–456
<(prefixrelation),986 forbinarycounters,454–455
=(suffixrelation),986 forbreadth-firstsearch,597
<x (aboverelation),1022 fordepth-firstsearch,606
//(commentsymbol),21 forDijkstra’salgorithm,661
(much-greater-thanrelation),574 fordisjoint-setdatastructures,566–567,
(much-less-thanrelation),783 568ex.
P(polynomial-timereducibilityrelation), fordynamictables,465
1067,1077ex. forFibonacciheaps,518,522ex.
forGraham’sscan,1036
AA-tree,338 fortheKnuth-Morris-Prattalgorithm,1006
abeliangroup,940 forPrim’salgorithm,636
ABOVE,1024 forrod-cutting,367
aboverelation(<x),1022 forshortestpathsinadag,655
absentchild,1178 forstackoperations,452–454
absolutelyconvergentseries,1146 aggregateflow,863
absorptionlawsforsets,1160 Akra-Bazzimethodforsolvingarecurrence,
abstractproblem,1054 112–113
acceptablepairofintegers,972 algorithm,5
acceptance correctnessof,6
byanalgorithm,1058 originofword,42
byafiniteautomaton,996 runningtimeof,25
acceptingstate,995 asatechnology,13
accountingmethod,456–459 Alice,959
forbinarycounters,458 ALLOCATE-NODE,492
fordynamictables,465–466 ALLOCATE-OBJECT,244
forstackoperations,457–458,458ex. allocationofobjects,243–244
Ackermann’sfunction,585 all-pairsshortestpaths,644,684–707
activity-selectionproblem,415–422,450 indynamicgraphs,707
acyclicgraph,1170 in-densegraphs,706pr.
relationtomatroids,448pr. Floyd-Warshallalgorithmfor,693–697,706
addinstruction,23 Johnson’salgorithmfor,700–706
addition bymatrixmultiplication,686–693,706–707
ofbinaryintegers,22ex. byrepeatedsquaring,689–691
ofmatrices,1220 alphabet,995,1057
modulon( n),940 ˛.n/,574
C
ofpolynomials,898 amortizedanalysis,451–478
additivegroupmodulon,940 accountingmethodof,456–459
addressing,open,seeopen-addresshashtable aggregateanalysis,367,452–456
Index 1253
forbit-reversalpermutation,472pr. forsetcover,1117–1122,1139
forbreadth-firstsearch,597 forsubsetsum,1128–1134,1139
fordepth-firstsearch,606 fortraveling-salesmanproblem,1111–1117,
forDijkstra’salgorithm,661 1139
fordisjoint-setdatastructures,566–567, forvertexcover,1108–1111,1139
568ex.,572ex.,575–581,581–582ex. forweightedsetcover,1135pr.
fordynamictables,463–471 for0-1knapsackproblem,1137pr.,1139
forFibonacciheaps,509–512,517–518, approximationerror,836
520–522,522ex. approximationratio,1106,1123
forthegenericpush-relabelalgorithm,746 approximationscheme,1107
forGraham’sscan,1036 APPROX-MIN-WEIGHT-VC,1126
fortheKnuth-Morris-Prattalgorithm,1006 APPROX-SUBSET-SUM,1131
formakingbinarysearchdynamic,473pr. APPROX-TSP-TOUR,1112
potentialmethodof,459–463 APPROX-VERTEX-COVER,1109
forrestructuringred-blacktrees,474pr. arbitrage,679pr.
forself-organizinglistswithmove-to-front, arc,seeedge
476pr. argumentofafunction,1166–1167
forshortestpathsinadag,655 arithmeticinstructions,23
forstacksonsecondarystorage,502pr. arithmetic,modular,54,939–946
forweight-balancedtrees,473pr. arithmeticseries,1146
amortizedcost arithmeticwithinfinities,650
intheaccountingmethod,456 arm,485
inaggregateanalysis,452 array,21
inthepotentialmethod,459 Monge,110pr.
ancestor,1176 passingasaparameter,21
leastcommon,584pr. articulationpoint,621pr.
ANDfunction( ),697,1071 assignment
^
ANDgate,1070 multiple,21
and,inpseudocode,22 satisfying,1072,1079
antiparalleledges,711–712 truth,1072,1079
antisymmetricrelation,1164 associativelawsforsets,1160
ANY-SEGMENTS-INTERSECT,1025 associativeoperation,939
approximation asymptoticallylarger,52
byleastsquares,835–839 asymptoticallynonnegative,45
ofsummationbyintegrals,1154–1156 asymptoticallypositive,45
approximationalgorithm,10,1105–1140 asymptoticallysmaller,52
forbinpacking,1134pr. asymptoticallytightbound,45
forMAX-CNFsatisfiability,1127ex. asymptoticefficiency,43
formaximumclique,1111ex.,1134pr. asymptoticlowerbound,48
formaximummatching,1135pr. asymptoticnotation,43–53,62pr.
formaximumspanningtree,1137pr. andgraphalgorithms,588
formaximum-weightcut,1127ex. andlinearityofsummations,1146
forMAX-3-CNFsatisfiability,1123–1124, asymptoticupperbound,47
1139 attributeofanobject,21
forminimum-weightvertexcover, augmentationofaflow,716
1124–1127,1139 augmentingdatastructures,339–355
forparallelmachinescheduling,1136pr. augmentingpath,719–720,763pr.
randomized,1123 authentication,284pr.,960–961,964
1254 Index
automaton best-caserunningtime,29ex.,49
finite,995 BFS,595
string-matching,996–1002 BIASED-RANDOM,117ex.
auxiliaryhashfunction,272 biconnectedcomponent,621pr.
auxiliarylinearprogram,886 big-ohnotation,45fig.,47–48,64
average-caserunningtime,28,116 big-omeganotation,45fig.,48–49,64
AVL-INSERT,333pr. bijectivefunction,1167
AVLtree,333pr.,337 binarycharactercode,428
axioms,forprobability,1190 binarycounter
analyzedbyaccountingmethod,458
babyface,602ex. analyzedbyaggregateanalysis,454–455
backedge,609,613 analyzedbypotentialmethod,461–462
backsubstitution,817 bit-reversed,472pr.
BAD-SET-COVER-INSTANCE,1122ex. binaryentropyfunction,1187
BALANCE,333pr. binarygcdalgorithm,981pr.
balancedsearchtree binaryheap,seeheap
AA-trees,338 binaryrelation,1163
AVLtrees,333pr.,337 binarysearch,39ex.
B-trees,484–504 withfastinsertion,473pr.
k-neighbortrees,338 ininsertionsort,39ex.
red-blacktrees,308–338 inmultithreadedmerging,799–800
scapegoattrees,338 insearchingB-trees,499ex.
splaytrees,338,482 BINARY-SEARCH,799
treaps,333pr.,338 binarysearchtree,286–307
2-3-4trees,489,503pr. AA-trees,338
2-3trees,337,504 AVLtrees,333pr.,337
weight-balancedtrees,338,473pr. deletionfrom,295–298,299ex.
ballsandbins,133–134,1215pr. withequalkeys,303pr.
base-apseudoprime,967 insertioninto,294–295
basecase,65,84 k-neighbortrees,338
base,inDNA,391 maximumkeyof,291
basicfeasiblesolution,866 minimumkeyof,291
basicsolution,866 optimal,397–404,413
basicvariable,855 predecessorin,291–292
basisfunction,835 querying,289–294
Bayes’stheorem,1194 randomlybuilt,299–303,304pr.
BELLMAN-FORD,651 right-convertingof,314ex.
Bellman-Fordalgorithm,651–655,682 scapegoattrees,338
forall-pairsshortestpaths,684 searching,289–291
inJohnson’salgorithm,702–704 forsorting,299ex.
andobjectivefunctions,670ex. splaytrees,338
tosolvesystemsofdifferenceconstraints, successorin,291–292
668 andtreaps,333pr.
Yen’simprovementto,678pr. weight-balancedtrees,338
BELOW,1024 seealsored-blacktree
Bernoullitrial,1201 binary-search-treeproperty,287
andballsandbins,133–134 intreaps,333pr.
andstreaks,135–139 vs.min-heapproperty,289ex.
Index 1255
binarytree,1177 bottleneckspanningtree,640pr.
full,1178 bottlenecktraveling-salesmanproblem,
numberofdifferentones,306pr. 1117ex.
representationof,246 bottomofastack,233
superimposeduponabitvector,533–534 BOTTOM-UP-CUT-ROD,366
seealsobinarysearchtree bottom-upmethod,fordynamicprogramming,
binomialcoefficient,1186–1187 365
binomialdistribution,1203–1206 bound
andballsandbins,133 asymptoticallytight,45
maximumvalueof,1207ex. asymptoticlower,48
tailsof,1208–1215 asymptoticupper,47
binomialexpansion,1186 onbinomialcoefficients,1186–1187
binomialheap,527pr. onbinomialdistributions,1206
binomialtree,527pr. polylogarithmic,57
binpacking,1134pr. onthetailsofabinomialdistribution,
bipartitegraph,1172 1208–1215
correspondingflownetworkof,732 seealsolowerbounds
d-regular,736ex. boundarycondition,inarecurrence,67,84
andhypergraphs,1173ex. boundaryofapolygon,1020ex.
bipartitematching,530,732–736,747ex.,766 boundingasummation,1149–1156
Hopcroft-Karpalgorithmfor,763pr. box,nesting,678pr.
birthdayparadox,130–133,142ex. B -tree,488
C
bisectionofatree,1181pr. branchingfactor,inB-trees,487
bitoniceuclideantraveling-salesmanproblem, branchinstructions,23
405pr. breadth-firstsearch,594–602,623
bitonicsequence,682pr. inmaximumflow,727–730,766
bitonictour,405pr. andshortestpaths,597–600,644
bitoperation,927 similaritytoDijkstra’salgorithm,662,
inEuclid’salgorithm,981pr. 663ex.
bit-reversalpermutation,472pr.,918 breadth-firsttree,594,600
BIT-REVERSE-COPY,918 bridge,621pr.
bit-reversedbinarycounter,472pr. B -tree,489n.

BIT-REVERSED-INCREMENT,472pr. B-tree,484–504
bitvector,255ex.,532–536 comparedwithred-blacktrees,484,490
black-height,309 creating,492
blackvertex,594,603 deletionfrom,499–502
blockingflow,765 fullnodein,489
blockstructureinpseudocode,20 heightof,489–490
Bob,959 insertioninto,493–497
Boole’sinequality,1195ex. minimumdegreeof,489
booleancombinationalcircuit,1071 minimumkeyof,497ex.
booleancombinationalelement,1070 propertiesof,488–491
booleanconnective,1079 searching,491–492
booleanformula,1049,1066ex.,1079, splittinganodein,493–495
1086ex. 2-3-4trees,489
booleanfunction,1187ex. B-TREE-CREATE,492
booleanmatrixmultiplication,832ex. B-TREE-DELETE,499
Boru°vka’salgorithm,641 B-TREE-INSERT,495
1256 Index
B-TREE-INSERT-NONFULL,496 CHAINED-HASH-SEARCH,258
B-TREE-SEARCH,492,499ex. chaining,257–260,283pr.
B-TREE-SPLIT-CHILD,494 chainofaconvexhull,1038
BUBBLESORT,40pr. changingakey,inaFibonacciheap,529pr.
bucket,200 changingvariables,inthesubstitutionmethod,
bucketsort,200–204 86–87
BUCKET-SORT,201 charactercode,428
BUILD-MAX-HEAP,157 chess-playingprogram,790–791
BUILD-MAX-HEAP0,167pr. child
BUILD-MIN-HEAP,159 inabinarytree,1178
butterflyoperation,915 inamultithreadedcomputation,776
by,inpseudocode,21 inarootedtree,1176
childlistinaFibonacciheap,507
cache,24,449pr. Chineseremaindertheorem,950–954,983
cachehit,449pr. chipmultiprocessor,772
cachemiss,449pr. chirptransform,914ex.
n
cacheobliviousness,504 choose ,1185
k
caching,off-line,449pr. chord,345ex.
(cid:0) 
call Cilk,774,812
inamultithreadedcomputation,776 Cilk++,774,812
ofasubroutine,23,25n. ciphertext,960
byvalue,21 circuit
calledge,778 booleancombinational,1071
cancellationlemma,907 depthof,919
cancellationofflow,717 forfastFouriertransform,919–920
canonicalformfortaskscheduling,444 CIRCUIT-SAT,1072
capacity circuitsatisfiability,1070–1077
ofacut,721 circular,doublylinkedlistwithasentinel,239
ofanedge,709 circularlinkedlist,236
residual,716,719 seealsolinkedlist
ofavertex,714ex. class
capacityconstraint,709–710 complexity,1059
cardinalityofaset( ),1161 equivalence,1164
jj
Carmichaelnumber,968,975ex. classificationofedges
Cartesianproduct( ),1162 inbreadth-firstsearch,621pr.

Cartesiansum,906ex. indepth-firstsearch,609–610,611ex.
cascadingcut,520 inamultithreadeddag,778–779
CASCADING-CUT,519 clause,1081–1082
Catalannumbers,306pr.,372 cleanarea,208pr.
ceilingfunction( ),54 clique,1086–1089,1105
de
inmastertheorem,103–106 approximationalgorithmfor,1111ex.,
ceilinginstruction,23 1134pr.
certainevent,1190 CLIQUE,1087
certificate closedinterval,348
inacryptosystem,964 closedsemiring,707
forverificationalgorithms,1063 closestpair,finding,1039–1044,1047
CHAINED-HASH-DELETE,258 closest-pointheuristic,1117ex.
CHAINED-HASH-INSERT,258
Index 1257
closure commonmultiple,939ex.
groupproperty,939 commonsubexpression,915
ofalanguage,1058 commonsubsequence,7,391
operator( ),1058 longest,7,390–397,413

transitive,seetransitiveclosure commutativelawsforsets,1159
cluster commutativeoperation,940
inabitvectorwithasuperimposedtreeof COMPACTIFY-LIST,245ex.
constantheight,534 compactlist,250pr.
forparallelcomputing,772 COMPACT-LIST-SEARCH,250pr.
inprotovanEmdeBoasstructures,538 COMPACT-LIST-SEARCH0,251pr.
invanEmdeBoastrees,546 comparablelinesegments,1022
clustering,272 COMPARE-EXCHANGE,208pr.
CNF(conjunctivenormalform),1049,1082 compare-exchangeoperation,208pr.
CNFsatisfiability,1127ex. comparisonsort,191
coarseningleavesofrecursion andbinarysearchtrees,289ex.
inmergesort,39pr. randomized,205pr.
whenrecursivelyspawning,787 andselection,222
code,428–429 compatibleactivities,415
Huffman,428–437,450 compatiblematrices,371,1221
codeword,429 competitiveanalysis,476pr.
codomain,1166 complement
coefficient ofanevent,1190
binomial,1186 ofagraph,1090
ofapolynomial,55,898 ofalanguage,1058
inslackform,856 Schur,820,834
coefficientrepresentation,900 ofaset,1160
andfastmultiplication,903–905 complementaryslackness,894pr.
cofactor,1224 completegraph,1172
coinchanging,446pr. completek-arytree,1179
colinearity,1016 seealsoheap
collision,257 completenessofalanguage,1077ex.
resolutionbychaining,257–260 completestep,782
resolutionbyopenaddressing,269–277 completiontime,447pr.,1136pr.
collision-resistanthashfunction,964 complexityclass,1059
coloring,1103pr.,1180pr. co-NP,1064
color,ofared-black-treenode,308 NP,1049,1064
column-majororder,208pr. NPC,1050,1069
columnrank,1223 P,1049,1055
columnsort,208pr. complexitymeasure,1059
columnvector,1218 complexnumbers
combination,1185 invertingmatricesof,832ex.
combinationalcircuit,1071 multiplicationof,83ex.
combinationalelement,1070 complexrootofunity,906
combinestep,individe-and-conquer,30,65 interpolationat,912–913
comment,inpseudocode(//),21 component
commodity,862 biconnected,621pr.
commondivisor,929 connected,1170
greatest,seegreatestcommondivisor stronglyconnected,1171
1258 Index
componentgraph,617 violationof,865
compositenumber,928 constraintgraph,666–668
witnessto,968 contain,inapath,1170
composition,ofmultithreadedcomputations, continuationedge,778
784fig. continuousuniformprobabilitydistribution,
computationaldepth,812 1192
computationalgeometry,1014–1047 contraction
computationalproblem,5–6 ofadynamictable,467–471
computationdag,777 ofamatroid,442
computation,multithreaded,777 ofanundirectedgraphbyanedge,1172
COMPUTE-PREFIX-FUNCTION,1006 controlinstructions,23
COMPUTE-TRANSITION-FUNCTION,1001 convergenceproperty,650,672–673
concatenation convergentseries,1146
oflanguages,1058 convertingbinarytodecimal,933ex.
ofstrings,986 convexcombinationofpoints,1015
concreteproblem,1055 convexfunction,1199
concurrencykeywords,774,776,785 convexhull,8,1029–1039,1046pr.
concurrencyplatform,773 convexlayers,1044pr.
conditionalbranchinstruction,23 convexpolygon,1020ex.
conditionalindependence,1195ex. convexset,714ex.
conditionalprobability,1192,1194 convolution( ),901
˝
configuration,1074 convolutiontheorem,913
conjugateofthegoldenratio( ),59 copyinstruction,23
y
conjugatetranspose,832ex. correctnessofanalgorithm,6
conjunctivenormalform,1049,1082 correspondingflownetworkforbipartite
connectedcomponent,1170–1171 matching,732
identifiedusingdepth-firstsearch,612ex. countablyinfiniteset,1161
identifiedusingdisjoint-setdatastructures, counter,seebinarycounter
562–564 counting,1183–1189
CONNECTED-COMPONENTS,563 probabilistic,143pr.
connectedgraph,1170 countingsort,194–197
connective,1079 inradixsort,198
co-NP(complexityclass),1064 COUNTING-SORT,195
conquerstep,individe-and-conquer,30,65 couponcollector’sproblem,134
conservationofflow,709–710 cover
consistency path,761pr.
ofliterals,1088 byasubset,1118
sequential,779,812 vertex,1089,1108,1124–1127,1139
CONSOLIDATE,516 covertical,1024
consolidatingaFibonacci-heaprootlist, CREATE-NEW-RS-VEB-TREE,557pr.
513–517 credit,456
constraint,851 criticaledge,729
difference,665 criticalpath
equality,670ex.,852–853 ofadag,657
inequality,852–853 ofamultithreadedcomputation,779
linear,846 crossacut,626
nonnegativity,851,853 crossedge,609
tight,865 crossproduct( ),1016

Index 1259
cryptosystem,958–965,983 hashtables,256–261
cubicspline,840pr. heaps,151–169
currencyexchange,390ex.,679pr. intervaltrees,348–354
curvefitting,835–839 k-neighbortrees,338
cut linkedlists,236–241
capacityof,721 mergeableheap,505
cascading,520 order-statistictrees,339–345
ofaflownetwork,720–724 persistent,331pr.,482
minimum,721,731ex. potentialof,459
netflowacross,720 priorityqueues,162–166
ofanundirectedgraph,626 protovanEmdeBoasstructures,538–545
weightof,1127ex. queues,232,234–235
CUT,519 radixtrees,304pr.
CUT-ROD,363 red-blacktrees,308–338
cutting,inaFibonacciheap,519 relaxedheaps,530
cycleofagraph,1170 rootedtrees,246–249
hamiltonian,1049,1061 scapegoattrees,338
minimummean-weight,680pr. onsecondarystorage,484–487
negative-weight,seenegative-weightcycle skiplists,338
andshortestpaths,646–647 splaytrees,338,482
cyclicgroup,955 stacks,232–233
cyclicrotation,1012ex. treaps,333pr.,338
cycling,ofsimplexalgorithm,875 2-3-4heaps,529pr.
2-3-4trees,489,503pr.
dag,seedirectedacyclicgraph 2-3trees,337,504
DAG-SHORTEST-PATHS,655 vanEmdeBoastrees,531–560
d-aryheap,167pr. weight-balancedtrees,338
inshortest-pathsalgorithms,706pr. datatype,23
data-movementinstructions,23 deadline,444
data-parallelmodel,811 deallocationofobjects,243–244
datastructure,9,229–355,481–585 decisionbyanalgorithm,1058–1059
AA-trees,338 decisionproblem,1051,1054
augmentationof,339–355 andoptimizationproblems,1051
AVLtrees,333pr.,337 decisiontree,192–193
binarysearchtrees,286–307 DECREASE-KEY,162,505
binomialheaps,527pr. decreasingakey
bitvectors,255ex.,532–536 inFibonacciheaps,519–522
B-trees,484–504 in2-3-4heaps,529pr.
deques,236ex. DECREMENT,456ex.
dictionaries,229 degeneracy,874
direct-addresstables,254–255 degree
fordisjointsets,561–585 ofabinomial-treeroot,527pr.
fordynamicgraphs,483 maximum,ofaFibonacciheap,509,
dynamicsets,229–231 523–526
dynamictrees,482 minimum,ofaB-tree,489
exponentialsearchtrees,212,483 ofanode,1177
Fibonacciheaps,505–530 ofapolynomial,55,898
fusiontrees,212,483 ofavertex,1169
1260 Index
degree-bound,898 infindingarticulationpoints,bridges,and
DELETE,230,505 biconnectedcomponents,621pr.
DELETE-LARGER-HALF,463ex. infindingstronglyconnectedcomponents,
deletion 615–621,623
frombinarysearchtrees,295–298,299ex. intopologicalsorting,612–615
fromabitvectorwithasuperimposedbinary depth-firsttree,603
tree,534 deque,236ex.
fromabitvectorwithasuperimposedtreeof DEQUEUE,235
constantheight,535 derivativeofaseries,1147
fromB-trees,499–502 descendant,1176
fromchainedhashtables,258 destinationvertex,644
fromdirect-addresstables,254 det,seedeterminant
fromdynamictables,467–471 determinacyrace,788
fromFibonacciheaps,522,526pr. determinant,1224–1225
fromheaps,166ex. andmatrixmultiplication,832ex.
fromintervaltrees,349 deterministicalgorithm,123
fromlinkedlists,238 multithreaded,787
fromopen-addresshashtables,271 DETERMINISTIC-SEARCH,143pr.
fromorder-statistictrees,343–344 DFS,604
fromprotovanEmdeBoasstructures,544 DFS-VISIT,604
fromqueues,234 DFT(discreteFouriertransform),9,909
fromred-blacktrees,323–330 diagonalmatrix,1218
fromstacks,232 LUPdecompositionof,827ex.
fromsweep-linestatuses,1024 diameterofatree,602ex.
from2-3-4heaps,529pr. dictionary,229
fromvanEmdeBoastrees,554–556 differenceconstraints,664–670
DeMorgan’slaws differenceequation,seerecurrence
forpropositionallogic,1083 differenceofsets( ),1159
(cid:0)
forsets,1160,1162ex. symmetric,763pr.
densegraph,589 differentiationofaseries,1147
-dense,706pr. digitalsignature,960
density digraph,seedirectedgraph
ofprimenumbers,965–966 DIJKSTRA,658
ofarod,370ex. Dijkstra’salgorithm,658–664,682
dependence forall-pairsshortestpaths,684,704
andindicatorrandomvariables,119 implementedwithaFibonacciheap,662
linear,1223 implementedwithamin-heap,662
seealsoindependence withintegeredgeweights,664ex.
depth inJohnson’salgorithm,702
average,ofanodeinarandomlybuiltbinary similaritytobreadth-firstsearch,662,
searchtree,304pr. 663ex.
ofacircuit,919 similaritytoPrim’salgorithm,634,662
ofanodeinarootedtree,1177 DIRECT-ADDRESS-DELETE,254
ofquicksortrecursiontree,178ex. directaddressing,254–255,532–536
ofastack,188pr. DIRECT-ADDRESS-INSERT,254
depth-determinationproblem,583pr. DIRECT-ADDRESS-SEARCH,254
depth-firstforest,603 direct-addresstable,254–255
depth-firstsearch,603–612,623 directedacyclicgraph(dag),1172
Index 1261
andbackedges,613 linked-listimplementationof,564–568
andcomponentgraphs,617 inoff-lineleastcommonancestors,584pr.
andhamiltonianpaths,1066ex. inoff-lineminimum,582pr.
longestsimplepathin,404pr. intaskscheduling,448pr.
forrepresentingamultithreaded disjoint-setforest,568–572
computation,777 analysisof,575–581,581ex.
single-sourceshortest-pathsalgorithmfor, rankpropertiesof,575,581ex.
655–658 seealsodisjoint-setdatastructure
topologicalsortof,612–615,623 disjointsets,1161
directedgraph,1168 disjunctivenormalform,1083
all-pairsshortestpathsin,684–707 disk,1028ex.
constraintgraph,666 diskdrive,485–487
Eulertourof,623pr.,1048 seealsosecondarystorage
hamiltoniancycleof,1049 DISK-READ,487
andlongestpaths,1048 DISK-WRITE,487
pathcoverof,761pr. distance
PERTchart,657,657ex. edit,406pr.
semiconnected,621ex. euclidean,1039
shortestpathin,643 Lm,1044ex.
single-sourceshortestpathsin,643–683 Manhattan,225pr.,1044ex.
singlyconnected,612ex. ofashortestpath,597
squareof,593ex. distributedmemory,772
transitiveclosureof,697 distribution
transposeof,592ex. binomial,1203–1206
universalsinkin,593ex. continuousuniform,1192
seealsodirectedacyclicgraph,graph, discrete,1191
network geometric,1202–1203
directedsegment,1015–1017 ofinputs,116,122
directedversionofanundirectedgraph,1172 ofprimenumbers,965
DIRECTION,1018 probability,1190
dirtyarea,208pr. sparse-hulled,1046pr.
DISCHARGE,751 uniform,1191
dischargeofanoverflowingvertex,751 distributivelawsforsets,1160
discoveredvertex,594,603 divergentseries,1146
discoverytime,indepth-firstsearch,605 divide-and-conquermethod,30–35,65
discreteFouriertransform,9,909 analysisof,34–35
discretelogarithm,955 forbinarysearch,39ex.
discretelogarithmtheorem,955 forconversionofbinarytodecimal,933ex.
discreteprobabilitydistribution,1191 forfastFouriertransform,909–912
discreterandomvariable,1196–1201 forfindingtheclosestpairofpoints,
disjoint-setdatastructure,561–585 1040–1043
analysisof,575–581,581ex. forfindingtheconvexhull,1030
inconnectedcomponents,562–564 formatrixinversion,829–831
indepthdetermination,583pr. formatrixmultiplication,76–83,792–797
disjoint-set-forestimplementationof, formaximum-subarrayproblem,68–75
568–572 formergesort,30–37,797–805
inKruskal’salgorithm,631 formultiplication,920pr.
linear-timespecialcaseof,585
1262 Index
formultithreadedmatrixmultiplication, comparedwithgreedyalgorithms,381,
792–797 390ex.,418,423–427
formultithreadedmergesort,797–805 foreditdistance,406pr.
forquicksort,170–190 elementsof,378–390
relationtodynamicprogramming,359 forFloyd-Warshallalgorithm,693–697
forselection,215–224 forinventoryplanning,411pr.
solvingrecurrencesfor,83–106,112–113 forlongestcommonsubsequence,390–397
forStrassen’salgorithm,79–83 forlongestpalindromesubsequence,405pr.
divideinstruction,23 forlongestsimplepathinaweighted
dividesrelation(),927 directedacyclicgraph,404pr.
j
dividestep,individe-and-conquer,30,65 formatrix-chainmultiplication,370–378
divisionmethod,263,268–269ex. andmemoization,387–389
divisiontheorem,928 foroptimalbinarysearchtrees,397–404
divisor,927–928 optimalsubstructurein,379–384
common,929 overlappingsubproblemsin,384–386
seealsogreatestcommondivisor forprintingneatly,405pr.
DNA,6–7,390–391,406pr. reconstructinganoptimalsolutionin,387
DNF(disjunctivenormalform),1083 relationtodivide-and-conquer,359
does-not-dividerelation(−),927 forrod-cutting,360–370
domain,1166 forseamcarving,409pr.
dominatesrelation,1045pr. forsigningfreeagents,411pr.
doublehashing,272–274,277ex. top-downwithmemoization,365
doublylinkedlist,236 fortransitiveclosure,697–699
seealsolinkedlist forViterbialgorithm,408pr.
downto,inpseudocode,21 for0-1knapsackproblem,427ex.
d-regulargraph,736ex. dynamicset,229–231
duality,879–886,895pr. seealsodatastructure
weak,880–881,886ex. dynamictable,463–471
duallinearprogram,879 analyzedbyaccountingmethod,465–466
dummykey,397 analyzedbyaggregateanalysis,465
dynamicgraph,562n. analyzedbypotentialmethod,466–471
all-pairsshortestpathsalgorithmsfor,707 loadfactorof,463
datastructuresfor,483 dynamictree,482
minimum-spanning-treealgorithmfor,
637ex. e,55
transitiveclosureof,705pr.,707 EŒ(expectedvalue),1197
dynamicmultithreadedalgorithm,see early-firstform,444
multithreadedalgorithm earlytask,444
dynamicmultithreading,773 edge,1168
dynamicorderstatistics,339–345 admissible,749
dynamic-programmingmethod,359–413 antiparallel,711–712
foractivityselection,421ex. attributesof,592
forall-pairsshortestpaths,686–697 back,609
forbitoniceuclideantraveling-salesman bridge,621pr.
problem,405pr. call,778
bottom-up,365 capacityof,709
forbreakingastring,410pr. classificationinbreadth-firstsearch,621pr.
classificationindepth-firstsearch,609–610
Index 1263
continuation,778 violationof,865
critical,729 equation
cross,609 andasymptoticnotation,49–50
forward,609 normal,837
inadmissible,749 recurrence,seerecurrence
light,626 equivalenceclass,1164
negative-weight,645–646 modulon(Œan),928
residual,716 equivalence,modular( ),54,1165ex.

return,779 equivalencerelation,1164
safe,626 andmodularequivalence,1165ex.
saturated,739 equivalentlinearprograms,852
spawn,778 error,inpseudocode,22
tree,601,603,609 escapeproblem,760pr.
weightof,591 EUCLID,935
edgeconnectivity,731ex. Euclid’salgorithm,933–939,981pr.,983
edgeset,1168 euclideandistance,1039
editdistance,406pr. euclideannorm( ),1222
kk
Edmonds-Karpalgorithm,727–730 Euler’sconstant,943
elementaryevent,1189 Euler’sphifunction,943
elementaryinsertion,465 Euler’stheorem,954,975ex.
elementofaset( ),1158 Eulertour,623pr.,1048
2
ellipsoidalgorithm,850,897 andhamiltoniancycles,1048
elliptic-curvefactorizationmethod,984 evaluationofapolynomial,41pr.,900,905ex.
elseif,inpseudocode,20n. derivativesof,922pr.
else,inpseudocode,20 atmultiplepoints,923pr.
emptylanguage( ),1058 event,1190
;
emptyset( ),1158 eventpoint,1023
;
emptysetlaws,1159 event-pointschedule,1023
emptystack,233 EXACT-SUBSET-SUM,1129
emptystring("),986,1058 excessflow,736
emptytree,1178 exchangeproperty,437
encodingofprobleminstances,1055–1057 exclusionandinclusion,1163ex.
endpoint executeasubroutine,25n.
ofaninterval,348 expansionofadynamictable,464–467
ofalinesegment,1015 expectation,seeexpectedvalue
ENQUEUE,235 expectedrunningtime,28,117
enteringavertex,1169 expectedvalue,1197–1199
enteringvariable,867 ofabinomialdistribution,1204
entropyfunction,1187 ofageometricdistribution,1202
-densegraph,706pr. ofanindicatorrandomvariable,118
-universalhashfunction,269ex. exploredvertex,605
equality exponentialfunction,55–56
offunctions,1166 exponentialheight,300
linear,845 exponentialsearchtree,212,483
ofsets,1158 exponentialseries,1147
equalityconstraint,670ex.,852 exponentiationinstruction,24
andinequalityconstraints,853 exponentiation,modular,956
tight,865 EXTENDED-BOTTOM-UP-CUT-ROD,369
1264 Index
EXTENDED-EUCLID,937 FIB-HEAP-LINK,516
EXTEND-SHORTEST-PATHS,688 FIB-HEAP-PRUNE,529pr.
extensionofaset,438 FIB-HEAP-UNION,512
exteriorofapolygon,1020ex. Fibonacciheap,505–530
externalnode,1176 changingakeyin,529pr.
externalpathlength,1180ex. comparedwithbinaryheaps,506–507
extractingthemaximumkey creating,510
fromd-aryheaps,167pr. decreasingakeyin,519–522
frommax-heaps,163 deletionfrom,522,526pr.
extractingtheminimumkey inDijkstra’salgorithm,662
fromFibonacciheaps,512–518 extractingtheminimumkeyfrom,512–518
from2-3-4heaps,529pr. insertioninto,510–511
fromYoungtableaus,167pr. inJohnson’salgorithm,704
EXTRACT-MAX,162–163 maximumdegreeof,509,523–526
EXTRACT-MIN,162,505 minimumkeyof,511
potentialfunctionfor,509
factor,928 inPrim’salgorithm,636
twiddle,912 pruning,529pr.
factorialfunction(Š),57–58 runningtimesofoperationson,506fig.
factorization,975–980,984 uniting,511–512
unique,931 Fibonaccinumbers,59–60,108pr.,523
failure,inaBernoullitrial,1201 computationof,774–780,981pr.
faircoin,1191 FIFO(first-in,first-out),232
fan-out,1071 seealsoqueue
Farkas’slemma,895pr. final-statefunction,996
farthest-pairproblem,1030 finalstrand,779
FASTER-ALL-PAIRS-SHORTEST-PATHS,691, FIND-DEPTH,583pr.
692ex. FIND-MAX-CROSSING-SUBARRAY,71
fastFouriertransform(FFT),898–925 FIND-MAXIMUM-SUBARRAY,72
circuitfor,919–920 findpath,569
iterativeimplementationof,915–918 FIND-SET,562
multidimensional,921pr. disjoint-set-forestimplementationof,571,
multithreadedalgorithmfor,804ex. 585
recursiveimplementationof,909–912 linked-listimplementationof,564
usingmodulararithmetic,923pr. finishedvertex,603
feasibilityproblem,665,894pr. finishingtime,indepth-firstsearch,605
feasiblelinearprogram,851 andstronglyconnectedcomponents,618
feasibleregion,847 finishtime,inactivityselection,415
feasiblesolution,665,846,851 finiteautomaton,995
Fermat’stheorem,954 forstringmatching,996–1002
FFT,seefastFouriertransform FINITE-AUTOMATON-MATCHER,999
FFTW,924 finitegroup,940
FIB,775 finitesequence,1166
FIB-HEAP-CHANGE-KEY,529pr. finiteset,1161
FIB-HEAP-DECREASE-KEY,519 first-fitheuristic,1134pr.
FIB-HEAP-DELETE,522 first-in,first-out,232
FIB-HEAP-EXTRACT-MIN,513 seealsoqueue
FIB-HEAP-INSERT,510 fixed-lengthcode,429
Index 1265
floating-pointdatatype,23 fullwalkofatree,1114
floorfunction( ),54 fullyparenthesizedmatrix-chainproduct,370
bc
inmastertheorem,103–106 fullypolynomial-timeapproximationscheme,
floorinstruction,23 1107
flow,709–714 forsubsetsum,1128–1134,1139
aggregate,863 function,1166–1168
augmentationof,716 Ackermann’s,585
blocking,765 basis,835
cancellationof,717 convex,1199
excess,736 final-state,996
integer-valued,733 hash,seehashfunction
net,acrossacut,720 linear,26,845
valueof,710 objective,664,847,851
flowconservation,709–710 potential,459
flownetwork,709–714 prefix,1003–1004
correspondingtoabipartitegraph,732 quadratic,27
cutof,720–724 reduction,1067
withmultiplesourcesandsinks,712 suffix,996
FLOYD-WARSHALL,695 transition,995,1001–1002,1012ex.
FLOYD-WARSHALL0,699ex. functionaliteration,58
Floyd-Warshallalgorithm,693–697, fundamentaltheoremoflinearprogramming,
699–700ex.,706 892
multithreaded,797ex. furthest-in-futurestrategy,449pr.
FORD-FULKERSON,724 fusiontree,212,483
Ford-Fulkersonmethod,714–731,765 fuzzysorting,189pr.
FORD-FULKERSON-METHOD,715
forest,1172–1173 Gabow’sscalingalgorithmforsingle-source
depth-first,603 shortestpaths,679pr.
disjoint-set,568–572 gapcharacter,989ex.,1002ex.
for,inpseudocode,20–21 gapheuristic,760ex.,766
andloopinvariants,19n. garbagecollection,151,243
formalpowerseries,108pr. gate,1070
formulasatisfiability,1079–1081,1105 Gaussianelimination,819,842
forwardedge,609 gcd,seegreatestcommondivisor
forwardsubstitution,816–817 generalnumber-fieldsieve,984
Fouriertransform,seediscreteFourier generatingfunction,108pr.
transform,fastFouriertransform generator
fractionalknapsackproblem,426,428ex. ofasubgroup,944
freeagent,411pr. ofZ n,955
freeingofobjects,243–244 GENERIC-MST,626
freelist,243 GENERIC-PUSH-RELABEL,741
FREE-OBJECT,244 genericpush-relabelalgorithm,740–748
freetree,1172–1176 geometricdistribution,1202–1203
frequencydomain,898 andballsandbins,134
fullbinarytree,1178,1180ex. geometricseries,1147
relationtooptimalcode,430 geometry,computational,1014–1047
fullnode,489 GF.2/,1227pr.
fullrank,1223 giftwrapping,1037,1047
1266 Index
globalvariable,21 GREEDY-ACTIVITY-SELECTOR,421
Goldberg’salgorithm,seepush-relabel greedyalgorithm,414–450
algorithm foractivityselection,415–422
goldenratio( ),59,108pr. forcoinchanging,446pr.
gossiping,478 comparedwithdynamicprogramming,381,
GRAFT,583pr. 390ex.,418,423–427
Graham’sscan,1030–1036,1047 Dijkstra’salgorithm,658–664
GRAHAM-SCAN,1031 elementsof,423–428
graph,1168–1173 forfractionalknapsackproblem,426
adjacency-listrepresentationof,590 greedy-choicepropertyin,424–425
adjacency-matrixrepresentationof,591 forHuffmancode,428–437
algorithmsfor,587–766 Kruskal’salgorithm,631–633
andasymptoticnotation,588 andmatroids,437–443
attributesof,588,592 forminimumspanningtree,631–638
breadth-firstsearchof,594–602,623 formultithreadedscheduling,781–783
coloringof,1103pr. foroff-linecaching,449pr.
complementof,1090 optimalsubstructurein,425
component,617 Prim’salgorithm,634–636
constraint,666–668 forsetcover,1117–1122,1139
dense,589 fortaskscheduling,443–446,447–448pr.
depth-firstsearchof,603–612,623 onaweightedmatroid,439–442
dynamic,562n. forweightedsetcover,1135pr.
-dense,706pr. greedy-choiceproperty,424–425
hamiltonian,1061 ofactivityselection,417–418
incidencematrixof,448pr.,593ex. ofHuffmancodes,433–434
interval,422ex. ofaweightedmatroid,441
nonhamiltonian,1061 greedyscheduler,782
shortestpathin,597 GREEDY-SET-COVER,1119
singlyconnected,612ex. grid,760pr.
sparse,589 group,939–946
static,562n. cyclic,955
subproblem,367–368 operator( ),939
˚
tourof,1096 guessingthesolution,inthesubstitution
weighted,591 method,84–85
seealsodirectedacyclicgraph,directed
graph,flownetwork,undirectedgraph, half3-CNFsatisfiability,1101ex.
tree half-openinterval,348
graphicmatroid,437–438,642 Hall’stheorem,735ex.
GRAPH-ISOMORPHISM,1065ex. haltingproblem,1048
grayvertex,594,603 halvinglemma,908
greatestcommondivisor(gcd),929–930, HAM-CYCLE,1062
933ex. hamiltoniancycle,1049,1061,1091–1096,
binarygcdalgorithmfor,981pr. 1105
Euclid’salgorithmfor,933–939,981pr.,983 hamiltoniangraph,1061
withmorethantwoarguments,939ex. hamiltonianpath,1066ex.,1101ex.
recursiontheoremfor,934 HAM-PATH,1066ex.
greedoid,450 handle,163,507
GREEDY,440 handshakinglemma,1172ex.
Index 1267
harmonicnumber,1147,1153–1154 max-heap,152
harmonicseries,1147,1153–1154 maximumkeyof,163
HASH-DELETE,277ex. mergeable,seemergeableheap
hashfunction,256,262–269 min-heap,153
auxiliary,272 inPrim’salgorithm,636
collision-resistant,964 asapriorityqueue,162–166
divisionmethodfor,263,268–269ex. relaxed,530
-universal,269ex. runningtimesofoperationson,506fig.
multiplicationmethodfor,263–264 andtreaps,333pr.
universal,265–268 2-3-4,529pr.
hashing,253–285 HEAP-DECREASE-KEY,165ex.
withchaining,257–260,283pr. HEAP-DELETE,166ex.
double,272–274,277ex. HEAP-EXTRACT-MAX,163
k-universal,284pr. HEAP-EXTRACT-MIN,165ex.
inmemoization,365,387 HEAP-INCREASE-KEY,164
withopenaddressing,269–277 HEAP-MAXIMUM,163
perfect,277–282,285 HEAP-MINIMUM,165ex.
toreplaceadjacencylists,593ex. heapproperty,152
universal,265–268 maintenanceof,154–156
HASH-INSERT,270,277ex. vs.binary-search-treeproperty,289ex.
HASH-SEARCH,271,277ex. heapsort,151–169
hashtable,256–261 HEAPSORT,160
dynamic,471ex. heel,602ex.
secondary,278 height
seealsohashing ofabinomialtree,527pr.
hashvalue,256 black-,309
hat-checkproblem,122ex. ofaB-tree,489–490
head ofad-aryheap,167pr.
inadiskdrive,485 ofadecisiontree,193
ofalinkedlist,236 exponential,300
ofaqueue,234 ofaheap,153
heap,151–169 ofanodeinaheap,153,159ex.
analyzedbypotentialmethod,462ex. ofanodeinatree,1177
binomial,527pr. ofared-blacktree,309
building,156–159,166pr. ofatree,1177
comparedwithFibonacciheaps,506–507 height-balancedtree,333pr.
d-ary,167pr.,706pr. heightfunction,inpush-relabelalgorithms,738
deletionfrom,166ex. hereditaryfamilyofsubsets,437
inDijkstra’salgorithm,662 Hermitianmatrix,832ex.
extractingthemaximumkeyfrom,163 highendpointofaninterval,348
Fibonacci,seeFibonacciheap highfunction,537,546
asgarbage-collectedstorage,151 HIRE-ASSISTANT,115
heightof,153 hiringproblem,114–115,123–124,145
inHuffman’salgorithm,433 on-line,139–141
toimplementamergeableheap,506 probabilisticanalysisof,120–121
increasingakeyin,163–164 hit
insertioninto,164 cache,449pr.
inJohnson’salgorithm,704 spurious,991
1268 Index
HOARE-PARTITION,185pr. indicatorrandomvariable,118–121
HOPCROFT-KARP,764pr. inanalysisofexpectedheightofarandomly
Hopcroft-Karpbipartitematchingalgorithm, builtbinarysearchtree,300–303
763pr. inanalysisofinsertingintoatreap,333pr.
horizontalray,1021ex. inanalysisofstreaks,138–139
Horner’srule,41pr.,900 inanalysisofthebirthdayparadox,132–133
intheRabin-Karpalgorithm,990 inapproximationalgorithmfor
HUFFMAN,431 MAX-3-CNFsatisfiability,1124
Huffmancode,428–437,450 inboundingtherighttailofthebinomial
hull,convex,8,1029–1039,1046pr. distribution,1212–1213
HumanGenomeProject,6 inbucketsortanalysis,202–204
hyperedge,1172 expectedvalueof,118
hypergraph,1172 inhashinganalysis,259–260
andbipartitegraphs,1173ex. inhiring-problemanalysis,120–121
andlinearityofexpectation,119
idealparallelcomputer,779 inquicksortanalysis,182–184,187pr.
idempotencylawsforsets,1159 inrandomized-selectionanalysis,217–219,
identity,939 226pr.
identitymatrix,1218 inuniversal-hashinganalysis,265–266
if,inpseudocode,20 inducedsubgraph,1171
image,1167 inequalityconstraint,852
imagecompression,409pr.,413 andequalityconstraints,853
inadmissibleedge,749 inequality,linear,846
incidence,1169 infeasiblelinearprogram,851
incidencematrix infeasiblesolution,851
anddifferenceconstraints,666 infinitesequence,1166
ofadirectedgraph,448pr.,593ex. infiniteset,1161
ofanundirectedgraph,448pr. infinitesum,1145
inclusionandexclusion,1163ex. infinity,arithmeticwith,650
incompletestep,782 INITIALIZE-PREFLOW,740
INCREASE-KEY,162 INITIALIZE-SIMPLEX,871,887
increasingakey,inamax-heap,163–164 INITIALIZE-SINGLE-SOURCE,648
INCREMENT,454 initialstrand,779
incrementaldesignmethod,29 injectivefunction,1167
forfindingtheconvexhull,1030 innerproduct,1222
in-degree,1169 inordertreewalk,287,293ex.,342
indentationinpseudocode,20 INORDER-TREE-WALK,288
independence in-placesorting,17,148,206pr.
ofevents,1192–1193,1195ex. input
ofrandomvariables,1197 toanalgorithm,5
ofsubproblemsindynamicprogramming, toacombinationalcircuit,1071
383–384 distributionof,116,122
independentfamilyofsubsets,437 toalogicgate,1070
independentset,1101pr. sizeof,25
oftasks,444 inputalphabet,995
independentstrands,789 INSERT,162,230,463ex.,505
indexfunction,537,546 insertion
indexofanelementofZ n,955 intobinarysearchtrees,294–295
Index 1269
intoabitvectorwithasuperimposedbinary interior-pointmethod,850,897
tree,534 intermediatevertex,693
intoabitvectorwithasuperimposedtreeof internalnode,1176
constantheight,534 internalpathlength,1180ex.
intoB-trees,493–497 interpolationbyacubicspline,840pr.
intochainedhashtables,258 interpolationbyapolynomial,901,906ex.
intod-aryheaps,167pr. atcomplexrootsofunity,912–913
intodirect-addresstables,254 intersection
intodynamictables,464–467 ofchords,345ex.
elementary,465 determining,forasetoflinesegments,
intoFibonacciheaps,510–511 1021–1029,1047
intoheaps,164 determining,fortwolinesegments,
intointervaltrees,349 1017–1019
intolinkedlists,237–238 oflanguages,1058
intoopen-addresshashtables,270 ofsets( ),1159
\
intoorder-statistictrees,343 interval,348
intoprotovanEmdeBoasstructures,544 fuzzysortingof,189pr.
intoqueues,234 INTERVAL-DELETE,349
intored-blacktrees,315–323 intervalgraph,422ex.
intostacks,232 INTERVAL-INSERT,349
intosweep-linestatuses,1024 INTERVAL-SEARCH,349,351
intotreaps,333pr. INTERVAL-SEARCH-EXACTLY,354ex.
into2-3-4heaps,529pr. intervaltree,348–354
intovanEmdeBoastrees,552–554 intervaltrichotomy,348
intoYoungtableaus,167pr. intractability,1048
insertionsort,12,16–20,25–27 invalidshift,985
inbucketsort,201–204 inventoryplanning,411pr.
comparedwithmergesort,14ex. inverse
comparedwithquicksort,178ex. ofabijectivefunction,1167
decisiontreefor,192fig. inagroup,940
inmergesort,39pr. ofamatrix,827–831,842,1223,1225ex.
inquicksort,185ex. multiplicative,modulon,949
usingbinarysearch,39ex. inversion
INSERTION-SORT,18,26,208pr. inaself-organizinglist,476pr.
instance inasequence,41pr.,122ex.,345ex.
ofanabstractproblem,1051,1054 inverter,1070
ofaproblem,5 invertiblematrix,1223
instructionsoftheRAMmodel,23 isolatedvertex,1169
integerdatatype,23 isomorphicgraphs,1171
integerlinearprogramming,850,895pr., iteratedfunction,63pr.
1101ex. iteratedlogarithmfunction,58–59
integers(Z),1158 ITERATIVE-FFT,917
integer-valuedflow,733 ITERATIVE-TREE-SEARCH,291
integralitytheorem,734 iterfunction,577
integral,toapproximatesummations,
1154–1156 Jarvis’smarch,1037–1038,1047
integrationofaseries,1147 Jensen’sinequality,1199
interiorofapolygon,1020ex. JOHNSON,704
1270 Index
Johnson’salgorithm,700–706 Lagrange’stheorem,944
joining Lame´’stheorem,936
ofred-blacktrees,332pr. language,1057
of2-3-4trees,503pr. completenessof,1077ex.
jointprobabilitydensityfunction,1197 provingNP-completenessof,1078–1079
Josephuspermutation,355pr. verificationof,1063
last-in,first-out,232
Karmarkar’salgorithm,897 seealsostack
Karp’sminimummean-weightcyclealgorithm, latetask,444
680pr. layers
k-arytree,1179 convex,1044pr.
k-CNF,1049 maximal,1045pr.
k-coloring,1103pr.,1180pr. LCA,584pr.
k-combination,1185 lcm(leastcommonmultiple),939ex.
k-conjunctivenormalform,1049 LCS,7,390–397,413
kernelofapolygon,1038ex. LCS-LENGTH,394
key,16,147,162,229 leadingsubmatrix,833,839ex.
dummy,397 leaf,1176
interpretedasanaturalnumber,263 leastcommonancestor,584pr.
median,ofaB-treenode,493 leastcommonmultiple,939ex.
public,959,962 least-squaresapproximation,835–839
secret,959,962 leavingavertex,1169
static,277 leavingvariable,867
keywords,inpseudocode,20–22 LEFT,152
multithreaded,774,776–777,785–786 leftchild,1178
“killeradversary”forquicksort,190 left-child,right-siblingrepresentation,246,
Kirchhoff’scurrentlaw,708 249ex.
Kleenestar( ),1058 LEFT-ROTATE,313,353ex.
KMPalgorithm,1002–1013 leftrotation,312
KMP-MATCHER,1005 leftspine,333pr.
knapsackproblem leftsubtree,1178
fractional,426,428ex.
Legendresymbol.a/,982pr.
p
0-1,425,427ex.,1137pr.,1139 length
k-neighbortree,338 ofapath,1170
knot,ofaspline,840pr. ofasequence,1166
Knuth-Morris-Prattalgorithm,1002–1013 ofaspine,333pr.
k-permutation,126,1184 ofastring,986,1184
Kraftinequality,1180ex. level
Kruskal’salgorithm,631–633,642 ofafunction,573
withintegeredgeweights,637ex. ofatree,1177
k-sorted,207pr. levelfunction,576
k-string,1184 lexicographicallylessthan,304pr.
k-subset,1161 lexicographicsorting,304pr.
k-substring,1184 lg(binarylogarithm),56
kthpower,933ex. lg(iteratedlogarithmfunction),58–59
k-universalhashing,284pr. lgk (exponentiationoflogarithms),56
lglg(compositionoflogarithms),56
Lagrange’sformula,902 LIFO(last-in,first-out),232
Index 1271
seealsostack determiningwhetheranyintersect,
lightedge,626 1021–1029,1047
linearconstraint,846 determiningwhethertwointersect,
lineardependence,1223 1017–1019
linearequality,845 link
linearequations ofbinomialtrees,527pr.
solvingmodular,946–950 ofFibonacci-heaproots,513
solvingsystemsof,813–827 oftreesinadisjoint-setforest,570–571
solvingtridiagonalsystemsof,840pr. LINK,571
linearfunction,26,845 linkedlist,236–241
linearindependence,1223 compact,245ex.,250pr.
linearinequality,846 deletionfrom,238
linear-inequalityfeasibilityproblem,894pr. toimplementdisjointsets,564–568
linearityofexpectation,1198 insertioninto,237–238
andindicatorrandomvariables,119 neighborlist,750
linearityofsummations,1146 searching,237,268ex.
linearorder,1165 self-organizing,476pr.
linearpermutation,1229pr. list,seelinkedlist
linearprobing,272 LIST-DELETE,238
linearprogramming,7,843–897 LIST-DELETE0,238
algorithmsfor,850 LIST-INSERT,238
applicationsof,849 LIST-INSERT0,240
dualityin,879–886 LIST-SEARCH,237
ellipsoidalgorithmfor,850,897 LIST-SEARCH0,239
findinganinitialsolutionin,886–891 literal,1082
fundamentaltheoremof,892 little-ohnotation,50–51,64
interior-pointmethodsfor,850,897 little-omeganotation,51
Karmarkar’salgorithmfor,897 Lm-distance,1044ex.
andmaximumflow,860–861 ln(naturallogarithm),56
andminimum-costcirculation,896pr. loadfactor
andminimum-costflow,861–862 ofadynamictable,463
andminimum-costmulticommodityflow, ofahashtable,258
864ex. loadinstruction,23
andmulticommodityflow,862–863 localvariable,21
simplexalgorithmfor,864–879,896 logarithmfunction(log),56–57
andsingle-pairshortestpath,859–860 discrete,955
andsingle-sourceshortestpaths,664–670, iterated(lg),58–59
863ex. logicalparallelism,777
slackformfor,854–857 logicgate,1070
standardformfor,850–854 longestcommonsubsequence,7,390–397,413
seealsointegerlinearprogramming,0-1 longestpalindromesubsequence,405pr.
integerprogramming LONGEST-PATH,1060ex.
linear-programmingrelaxation,1125 LONGEST-PATH-LENGTH,1060ex.
linearsearch,22ex. longestsimplecycle,1101ex.
linearspeedup,780 longestsimplepath,1048
linesegment,1015 inanunweightedgraph,382
comparable,1022 inaweighteddirectedacyclicgraph,404pr.
determiningturnof,1017 LOOKUP-CHAIN,388
1272 Index
loop,inpseudocode,20 formedianfinding,227
parallel,785–787 formerging,208pr.
loopinvariant,18–19 forminimum-weightvertexcover,
forbreadth-firstsearch,595 1124–1126
forbuildingaheap,157 formultithreadedcomputations,780
forconsolidatingtherootlistofaFibonacci andpotentialfunctions,478
heap,517 forpriority-queueoperations,531
fordeterminingtherankofanelementinan andrecurrences,67
order-statistictree,342 forsimultaneousminimumandmaximum,
forDijkstra’salgorithm,660 215ex.
andforloops,19n. forsizeofanoptimalvertexcover,1110,
forthegenericminimum-spanning-tree 1135pr.
method,625 forsorting,191–194,205pr.,211,531
forthegenericpush-relabelalgorithm,743 forstreaks,136–138,142ex.
forGraham’sscan,1034 onsummations,1152,1154
forheapsort,160ex. lowermedian,213
forHorner’srule,41pr. lowersquareroot p# ,546
forincreasingakeyinaheap,166ex. lower-triangularmatrix,1219,1222ex.,
(cid:0) 
initializationof,19 1225ex.
forinsertionsort,18 lowfunction,537,546
maintenanceof,19 LUdecomposition,806pr.,819–822
formerging,32 LU-DECOMPOSITION,821
formodularexponentiation,957 LUPdecomposition,806pr.,815
originof,42 computationof,823–825
forpartitioning,171 ofadiagonalmatrix,827ex.
forPrim’salgorithm,636 inmatrixinversion,828
fortheRabin-Karpalgorithm,993 andmatrixmultiplication,832ex.
forrandomlypermutinganarray,127, ofapermutationmatrix,827ex.
128ex. useof,815–819
forred-blacktreeinsertion,318 LUP-DECOMPOSITION,824
fortherelabel-to-frontalgorithm,755 LUP-SOLVE,817
forsearchinganintervaltree,352
forthesimplexalgorithm,872 mainmemory,484
forstring-matchingautomata,998,1000 MAKE-HEAP,505
andtermination,19 MAKE-SET,561
lowendpointofaninterval,348 disjoint-set-forestimplementationof,571
lowerbounds linked-listimplementationof,564
onapproximations,1140 makespan,1136pr.
asymptotic,48 MAKE-TREE,583pr.
foraveragesorting,207pr. Manhattandistance,225pr.,1044ex.
onbinomialcoefficients,1186 markednode,508,519–520
forcompartingwaterjugs,206pr. Markov’sinequality,1201ex.
forconvexhull,1038ex.,1047 mastermethodforsolvingarecurrence,93–97
fordisjoint-setdatastructures,585 mastertheorem,94
forfindingtheminimum,214 proofof,97–106
forfindingthepredecessor,560 matchedvertex,732
forlengthofanoptimaltraveling-salesman matching
tour,1112–1115 bipartite,732,763pr.
Index 1273
maximal,1110,1135pr. multithreadedalgorithmfor,792–797,
maximum,1135pr. 806pr.
andmaximumflow,732–736,747ex. Pan’smethodfor,82ex.
perfect,735ex. Strassen’salgorithmfor,79–83,111–112
ofstrings,985–1013 MATRIX-MULTIPLY,371
weightedbipartite,530 matrix-vectormultiplication,multithreaded,
matricmatroid,437 785–787,792ex.
matrix,1217–1229 withrace,789–790
additionof,1220 matroid,437–443,448pr.,450,642
adjacency,591 fortaskscheduling,443–446
conjugatetransposeof,832ex. MAT-VEC,785
determinantof,1224–1225 MAT-VEC-MAIN-LOOP,786
diagonal,1218 MAT-VEC-WRONG,790
Hermitian,832ex. MAX-CNFsatisfiability,1127ex.
identity,1218 MAX-CUTproblem,1127ex.
incidence,448pr.,593ex. MAX-FLOW-BY-SCALING,763pr.
inversionof,806pr.,827–831,842 max-flowmin-cuttheorem,723
lower-triangular,1219,1222ex.,1225ex. max-heap,152
multiplicationof,seematrixmultiplication building,156–159
negativeof,1220 d-ary,167pr.
permutation,1220,1222ex. deletionfrom,166ex.
predecessor,685 extractingthemaximumkeyfrom,163
productof,withavector,785–787,789–790, inheapsort,159–162
792ex. increasingakeyin,163–164
pseudoinverseof,837 insertioninto,164
scalarmultipleof,1220 maximumkeyof,163
subtractionof,1221 asamax-priorityqueue,162–166
symmetric,1220 mergeable,250n.,481n.,505n.
symmetricpositive-definite,832–835,842 MAX-HEAPIFY,154
Toeplitz,921pr. MAX-HEAP-INSERT,164
transposeof,797ex.,1217 buildingaheapwith,166pr.
transposeof,multithreaded,792ex. max-heapproperty,152
tridiagonal,1219 maintenanceof,154–156
unitlower-triangular,1219 maximalelement,ofapartiallyorderedset,
unitupper-triangular,1219 1165
upper-triangular,1219,1225ex. maximallayers,1045pr.
Vandermonde,902,1226pr. maximalmatching,1110,1135pr.
matrix-chainmultiplication,370–378 maximalpoint,1045pr.
MATRIX-CHAIN-MULTIPLY maximalsubset,inamatroid,438
MATRIX-CHAIN-ORDER,375 maximizationlinearprogram,846
matrixmultiplication,75–83,1221 andminimizationlinearprograms,852
forall-pairsshortestpaths,686–693, maximum,213
706–707 inbinarysearchtrees,291
boolean,832ex. ofabinomialdistribution,1207ex.
andcomputingthedeterminant,832ex. inabitvectorwithasuperimposedbinary
divide-and-conquermethodfor,76–83 tree,533
andLUPdecomposition,832ex. inabitvectorwithasuperimposedtreeof
andmatrixinversion,828–831,842 constantheight,535
1274 Index
finding,214–215 memoryhierarchy,24
inheaps,163 MERGE,31
inorder-statistictrees,347ex. mergeableheap,481,505
inprotovanEmdeBoasstructures,544ex. binomialheaps,527pr.
inred-blacktrees,311 linked-listimplementationof,250pr.
invanEmdeBoastrees,550 relaxedheaps,530
MAXIMUM,162–163,230 runningtimesofoperationson,506fig.
maximumbipartitematching,732–736, 2-3-4heaps,529pr.
747ex.,766 seealsoFibonacciheap
Hopcroft-Karpalgorithmfor,763pr. mergeablemax-heap,250n.,481n.,505n.
maximumdegree,inaFibonacciheap,509, mergeablemin-heap,250n.,481n.,505
523–526 MERGE-LISTS,1129
maximumflow,708–766 mergesort,12,30–37
Edmonds-Karpalgorithmfor,727–730 comparedwithinsertionsort,14ex.
Ford-Fulkersonmethodfor,714–731,765 multithreadedalgorithmfor,797–805,812
asalinearprogram,860–861 useofinsertionsortin,39pr.
andmaximumbipartitematching,732–736, MERGE-SORT,34
747ex. MERGE-SORT0,797
push-relabelalgorithmsfor,736–760,765 merging
relabel-to-frontalgorithmfor,748–760 ofksortedlists,166ex.
scalingalgorithmfor,762pr.,765 lowerboundsfor,208pr.
updating,762pr. multithreadedalgorithmfor,798–801
maximummatching,1135pr. oftwosortedarrays,30
maximumspanningtree,1137pr. MILLER-RABIN,970
maximum-subarrayproblem,68–75,111 Miller-Rabinprimalitytest,968–975,983
max-priorityqueue,162 MIN-GAP,354ex.
MAX-3-CNFsatisfiability,1123–1124,1139 min-heap,153
MAYBE-MST-A,641pr. analyzedbypotentialmethod,462ex.
MAYBE-MST-B,641pr. building,156–159
MAYBE-MST-C,641pr. d-ary,706pr.
mean,seeexpectedvalue inDijkstra’salgorithm,662
meanweightofacycle,680pr. inHuffman’salgorithm,433
median,213–227 inJohnson’salgorithm,704
multithreadedalgorithmfor,805ex. mergeable,250n.,481n.,505
ofsortedlists,223ex. asamin-priorityqueue,165ex.
oftwosortedlists,804ex. inPrim’salgorithm,636
weighted,225pr. MIN-HEAPIFY,156ex.
mediankey,ofaB-treenode,493 MIN-HEAP-INSERT,165ex.
median-of-3method,188pr. min-heapordering,507
memberofaset( ),1158 min-heapproperty,153,507
2
membership maintenanceof,156ex.
inprotovanEmdeBoasstructures,540–541 intreaps,333pr.
inVanEmdeBoastrees,550 vs.binary-search-treeproperty,289ex.
memoization,365,387–389 minimizationlinearprogram,846
MEMOIZED-CUT-ROD,365 andmaximizationlinearprograms,852
MEMOIZED-CUT-ROD-AUX,366 minimum,213
MEMOIZED-MATRIX-CHAIN,388 inbinarysearchtrees,291
memory,484
Index 1275
inabitvectorwithasuperimposedbinary modularequivalence,54,1165ex.
tree,533 modularexponentiation,956
inabitvectorwithasuperimposedtreeof MODULAR-EXPONENTIATION,957
constantheight,535 modularlinearequations,946–950
inB-trees,497ex. MODULAR-LINEAR-EQUATION-SOLVER,
inFibonacciheaps,511 949
finding,214–215 modulo,54,928
off-line,582pr. Mongearray,110pr.
inorder-statistictrees,347ex. monotonesequence,168
inprotovanEmdeBoasstructures,541–542 monotonicallydecreasing,53
inred-blacktrees,311 monotonicallyincreasing,53
in2-3-4heaps,529pr. MontyHallproblem,1195ex.
invanEmdeBoastrees,550 move-to-frontheuristic,476pr.,478
MINIMUM,162,214,230,505 MST-KRUSKAL,631
minimum-costcirculation,896pr. MST-PRIM,634
minimum-costflow,861–862 MST-REDUCE,639pr.
minimum-costmulticommodityflow,864ex. much-greater-than( ),574
minimum-costspanningtree,seeminimum much-less-than( ),783
spanningtree multicommodityflow,862–863
minimumcut,721,731ex. minimum-cost,864ex.
minimumdegree,ofaB-tree,489 multicorecomputer,772
minimummean-weightcycle,680pr. multidimensionalfastFouriertransform,
minimumnode,ofaFibonacciheap,508 921pr.
minimumpathcover,761pr. multigraph,1172
minimumspanningtree,624–642 convertingtoequivalentundirectedgraph,
inapproximationalgorithmfor 593ex.
traveling-salesmanproblem,1112 multiple,927
Boru°vka’salgorithmfor,641 ofanelementmodulon,946–950
ondynamicgraphs,637ex. leastcommon,939ex.
genericmethodfor,625–630 scalar,1220
Kruskal’salgorithmfor,631–633 multipleassignment,21
Prim’salgorithmfor,634–636 multiplesourcesandsinks,712
relationtomatroids,437,439–440 multiplication
second-best,638pr. ofcomplexnumbers,83ex.
minimum-weightspanningtree,seeminimum divide-and-conquermethodfor,920pr.
spanningtree ofmatrices,seematrixmultiplication
minimum-weightvertexcover,1124–1127, ofamatrixchain,370–378
1139 matrix-vector,multithreaded,785–787,
minorofamatrix,1224 789–790,792ex.
min-priorityqueue,162 modulon(n),940

inconstructingHuffmancodes,431 ofpolynomials,899
inDijkstra’salgorithm,661 multiplicationmethod,263–264
inPrim’salgorithm,634,636 multiplicativegroupmodulon,941
miss,449pr. multiplicativeinverse,modulon,949
missingchild,1178 multiplyinstruction,23
mod,54,928 MULTIPOP,453
modifyingoperation,230 multiprocessor,772
modulararithmetic,54,923pr.,939–946 MULTIPUSH,456ex.
1276 Index
multiset,1158n. netflowacrossacut,720
multithreadedalgorithm,10,772–812 network
forcomputingFibonaccinumbers,774–780 admissible,749–750
forfastFouriertransform,804ex. flow,seeflownetwork
Floyd-Warshallalgorithm,797ex. residual,715–719
forLUdecomposition,806pr. forsorting,811
forLUPdecomposition,806pr. NEXT-TO-TOP,1031
formatrixinversion,806pr. NIL,21
formatrixmultiplication,792–797,806pr. node,1176
formatrixtranspose,792ex.,797ex. seealsovertex
formatrix-vectorproduct,785–787, nonbasicvariable,855
789–790,792ex. nondeterministicmultithreadedalgorithm,787
formedian,805ex. nondeterministicpolynomialtime,1064n.
formergesorting,797–805,812 seealsoNP
formerging,798–801 nonhamiltoniangraph,1061
fororderstatistics,805ex. noninstance,1056n.
forpartitioning,804ex. noninvertiblematrix,1223
forprefixcomputation,807pr. nonnegativityconstraint,851,853
forquicksort,811pr. nonoverlappablestringpattern,1002ex.
forreduction,807pr. nonsaturatingpush,739,745
forasimplestencilcalculation,809pr. nonsingularmatrix,1223
forsolvingsystemsoflinearequations, nontrivialpower,933ex.
806pr. nontrivialsquarerootof1,modulon,956
Strassen’salgorithm,795–796 no-pathproperty,650,672
multithreadedcomposition,784fig. normalequation,837
multithreadedcomputation,777 normofavector,1222
multithreadedscheduling,781–783 NOTfunction( ),1071
:
mutuallyexclusiveevents,1190 notasetmember( ),1158
62
mutuallyindependentevents,1193 notequivalent( ),54
6
NOTgate,1070
N(setofnaturalnumbers),1158 NP(complexityclass),1049,1064,1066ex.,
naivealgorithm,forstringmatching,988–990 1105
NAIVE-STRING-MATCHER,988 NPC(complexityclass),1050,1069
naturalcubicspline,840pr. NP-complete,1050,1069
naturalnumbers(N),1158 NP-completeness,9–10,1048–1105
keysinterpretedas,263 ofthecircuit-satisfiabilityproblem,
negativeofamatrix,1220 1070–1077
negative-weightcycle ofthecliqueproblem,1086–1089,1105
anddifferenceconstraints,667 ofdeterminingwhetherabooleanformulais
andrelaxation,677ex. atautology,1086ex.
andshortestpaths,645,653–654,692ex., oftheformula-satisfiabilityproblem,
700ex. 1079–1081,1105
negative-weightedges,645–646 ofthegraph-coloringproblem,1103pr.
neighbor,1172 ofthehalf3-CNFsatisfiabilityproblem,
neighborhood,735ex. 1101ex.
neighborlist,750 ofthehamiltonian-cycleproblem,
nestedparallelism,776,805pr. 1091–1096,1105
nestingboxes,678pr. ofthehamiltonian-pathproblem,1101ex.
Index 1277
oftheindependent-setproblem,1101pr. one-passmethod,585
ofintegerlinearprogramming,1101ex. one-to-onecorrespondence,1167
ofthelongest-simple-cycleproblem, one-to-onefunction,1167
1101ex. on-lineconvex-hullproblem,1039ex.
proving,ofalanguage,1078–1079 on-linehiringproblem,139–141
ofschedulingwithprofitsanddeadlines, ON-LINE-MAXIMUM,140
1104pr. on-linemultithreadedscheduler,781
oftheset-coveringproblem,1122ex. ON-SEGMENT,1018
oftheset-partitionproblem,1101ex. ontofunction,1167
ofthesubgraph-isomorphismproblem, open-addresshashtable,269–277
1100ex. withdoublehashing,272–274,277ex.
ofthesubset-sumproblem,1097–1100 withlinearprobing,272
ofthe3-CNF-satisfiabilityproblem, withquadraticprobing,272,283pr.
1082–1085,1105 openinterval,348
ofthetraveling-salesmanproblem, OpenMP,774
1096–1097 optimalbinarysearchtree,397–404,413
ofthevertex-coverproblem,1089–1091, OPTIMAL-BST,402
1105 optimalobjectivevalue,851
of0-1integerprogramming,1100ex. optimalsolution,851
NP-hard,1069 optimalsubset,ofamatroid,439
n-set,1161 optimalsubstructure
n-tuple,1162 ofactivityselection,416
nullevent,1190 ofbinarysearchtrees,399–400
nulltree,1178 indynamicprogramming,379–384
nullvector,1224 ofthefractionalknapsackproblem,426
number-fieldsieve,984 ingreedyalgorithms,425
numericalstability,813,815,842 ofHuffmancodes,435
n-vector,1218 oflongestcommonsubsequences,392–393
ofmatrix-chainmultiplication,373
o-notation,50–51,64 ofrod-cutting,362
O-notation,45fig.,47–48,64 ofshortestpaths,644–645,687,693–694
O -notation,62pr. ofunweightedshortestpaths,382
0
O-notation,62pr. ofweightedmatroids,442
object,21 ofthe0-1knapsackproblem,426
allocationandfreeingof,243–244 optimalvertexcover,1108
e
arrayimplementationof,241–246 optimizationproblem,359,1050,1054
passingasparameter,21 approximationalgorithmsfor,10,
objectivefunction,664,847,851 1106–1140
objectivevalue,847,851 anddecisionproblems,1051
obliviouscompare-exchangealgorithm,208pr. ORfunction( ),697,1071
_
occurrenceofapattern,985 order
OFF-LINE-MINIMUM,583pr. ofagroup,945
off-lineproblem linear,1165
caching,449pr. partial,1165
leastcommonancestors,584pr. total,1165
minimum,582pr. orderedpair,1161
Omega-notation,45fig.,48–49,64 orderedtree,1177
1-approximationalgorithm,1107 orderofgrowth,28
1278 Index
orderstatistics,213–227 ofamultithreadedcomputation,780
dynamic,339–345 nested,776
multithreadedalgorithmfor,805ex. ofarandomizedmultithreadedalgorithm,
order-statistictree,339–345 811pr.
querying,347ex. parallelloop,785–787,805pr.
ORgate,1070 parallel-machine-schedulingproblem,1136pr.
origin,1015 parallelprefix,807pr.
or,inpseudocode,22 parallelrandom-accessmachine,811
orthonormal,842 parallelslackness,781
OS-KEY-RANK,344ex. ruleofthumb,783
OS-RANK,342 parallel,strandsbeinglogicallyin,778
OS-SELECT,341 parameter,21
out-degree,1169 costsofpassing,107pr.
outerproduct,1222 parent
output inabreadth-firsttree,594
ofanalgorithm,5 inamultithreadedcomputation,776
ofacombinationalcircuit,1071 inarootedtree,1176
ofalogicgate,1070 PARENT,152
overdeterminedsystemoflinearequations,814 parenthesisstructureofdepth-firstsearch,606
overflow parenthesistheorem,606
ofaqueue,235 parenthesizationofamatrix-chainproduct,370
ofastack,233 parsetree,1082
overflowingvertex,736 partiallyorderedset,1165
dischargeof,751 partialorder,1165
overlappingintervals,348 PARTITION,171
findingall,354ex. PARTITION0,186pr.
pointofmaximumoverlap,354pr. partitionfunction,361n.
overlappingrectangles,354ex. partitioning,171–173
overlappingsubproblems,384–386 aroundmedianof3elements,185ex.
overlapping-suffixlemma,987 Hoare’smethodfor,185pr.
multithreadedalgorithmfor,804ex.
P(complexityclass),1049,1055,1059, randomized,179
1061ex.,1105 partitionofaset,1161,1164
packagewrapping,1037,1047 Pascal’striangle,1188ex.
pageonadisk,486,499ex.,502pr. path,1170
pair,ordered,1161 augmenting,719–720,763pr.
pairwisedisjointsets,1161 critical,657
pairwiseindependence,1193 find,569
pairwiserelativelyprime,931 hamiltonian,1066ex.
palindrome,405pr. longest,382,1048
Pan’smethodformatrixmultiplication,82ex. shortest,seeshortestpaths
parallelalgorithm,10,772 simple,1170
seealsomultithreadedalgorithm weightof,643
parallelcomputer,772 PATH,1051,1058
ideal,779 pathcompression,569
parallelfor,inpseudocode,785–786 pathcover,761pr.
parallelism pathlength,ofatree,304pr.,1180ex.
logical,777 path-relaxationproperty,650,673
Index 1279
pattern,instringmatching,985 star-shaped,1038ex.
nonoverlappable,1002ex. polylogarithmicallybounded,57
patternmatching,seestringmatching polynomial,55,898
penalty,444 additionof,898
perfecthashing,277–282,285 asymptoticbehaviorof,61pr.
perfectlinearspeedup,780 coefficientrepresentationof,900
perfectmatching,735ex. derivativesof,922pr.
permutation,1167 evaluationof,41pr.,900,905ex.,923pr.
bit-reversal,472pr.,918 interpolationby,901,906ex.
Josephus,355pr. multiplicationof,899,903–905,920pr.
k-permutation,126,1184 point-valuerepresentationof,901
linear,1229pr. polynomial-growthcondition,113
inplace,126 polynomiallybounded,55
random,124–128 polynomiallyrelated,1056
ofaset,1184 polynomial-timeacceptance,1058
uniformrandom,116,125 polynomial-timealgorithm,927,1048
permutationmatrix,1220,1222ex.,1226ex. polynomial-timeapproximationscheme,1107
LUPdecompositionof,827ex. formaximumclique,1134pr.
PERMUTE-BY-CYCLIC,129ex. polynomial-timecomputability,1056
PERMUTE-BY-SORTING,125 polynomial-timedecision,1059
PERMUTE-WITH-ALL,129ex. polynomial-timereducibility( P),1067,
PERMUTE-WITHOUT-IDENTITY,128ex. 1077ex.
persistentdatastructure,331pr.,482 polynomial-timesolvability,1055
PERSISTENT-TREE-INSERT,331pr. polynomial-timeverification,1061–1066
PERTchart,657,657ex. POP,233,452
P-FIB,776 popfromarun-timestack,188pr.
phase,oftherelabel-to-frontalgorithm,758 positionaltree,1178
phifunction( .n/),943 positive-definitematrix,1225
PISANO-DELETE,526pr. post-officelocationproblem,225pr.
pivot postordertreewalk,287
inlinearprogramming,867,869–870, potentialfunction,459
878ex. forlowerbounds,478
inLUdecomposition,821 potentialmethod,459–463
inquicksort,171 forbinarycounters,461–462
PIVOT,869 fordisjoint-setdatastructures,575–581,
platter,485 582ex.
P-MATRIX-MULTIPLY-RECURSIVE,794 fordynamictables,466–471
P-MERGE,800 forFibonacciheaps,509–512,517–518,
P-MERGE-SORT,803 520–522
pointer,21 forthegenericpush-relabelalgorithm,746
arrayimplementationof,241–246 formin-heaps,462ex.
trailing,295 forrestructuringred-blacktrees,474pr.
point-valuerepresentation,901 forself-organizinglistswithmove-to-front,
polarangle,1020ex. 476pr.
Pollard’srhoheuristic,976–980,980ex.,984 forstackoperations,460–461
POLLARD-RHO,976 potential,ofadatastructure,459
polygon,1020ex. power
kernelof,1038ex. ofanelement,modulon,954–958
1280 Index
kth,933ex. similaritytoDijkstra’salgorithm,634,662
nontrivial,933ex. forsparsegraphs,638pr.
powerseries,108pr. primalitytesting,965–975,983
powerset,1161 Miller-Rabintest,968–975,983
Pr (probabilitydistribution),1190 pseudoprimalitytesting,966–968
fg
PRAM,811 primallinearprogram,880
predecessor primaryclustering,272
inbinarysearchtrees,291–292 primarymemory,484
inabitvectorwithasuperimposedbinary primedistributionfunction,965
tree,534 primenumber,928
inabitvectorwithasuperimposedtreeof densityof,965–966
constantheight,535 primenumbertheorem,965
inbreadth-firsttrees,594 primitiverootofZ n,955
inB-trees,497ex. principalrootofunity,907
inlinkedlists,236 principleofinclusionandexclusion,1163ex.
inorder-statistictrees,347ex. PRINT-ALL-PAIRS-SHORTEST-PATH,685
inprotovanEmdeBoasstructures,544ex. PRINT-CUT-ROD-SOLUTION,369
inred-blacktrees,311 PRINT-INTERSECTING-SEGMENTS,1028ex.
inshortest-pathstrees,647 PRINT-LCS,395
inVanEmdeBoastrees,551–552 PRINT-OPTIMAL-PARENS,377
PREDECESSOR,230 PRINT-PATH,601
predecessormatrix,685 PRINT-SET,572ex.
predecessorsubgraph priorityqueue,162–166
inall-pairsshortestpaths,685 inconstructingHuffmancodes,431
inbreadth-firstsearch,600 inDijkstra’salgorithm,661
indepth-firstsearch,603 heapimplementationof,162–166
insingle-sourceshortestpaths,647 lowerboundsfor,531
predecessor-subgraphproperty,650,676 max-priorityqueue,162
preemption,447pr. min-priorityqueue,162,165ex.
prefix withmonotoneextractions,168
ofasequence,392 inPrim’salgorithm,634,636
ofastring(<),986 protovanEmdeBoasstructure
prefixcode,429 implementationof,538–545
prefixcomputation,807pr. vanEmdeBoastreeimplementationof,
prefixfunction,1003–1004 531–560
prefix-functioniterationlemma,1007 seealsobinarysearchtree,binomialheap,
preflow,736,765 Fibonacciheap
preimageofamatrix,1228pr. probabilisticallycheckableproof,1105,1140
preorder,total,1165 probabilisticanalysis,115–116,130–142
preordertreewalk,287 ofapproximationalgorithmfor
presorting,1043 MAX-3-CNFsatisfiability,1124
Prim’salgorithm,634–636,642 andaverageinputs,28
withanadjacencymatrix,637ex. ofaveragenodedepthinarandomlybuilt
inapproximationalgorithmfor binarysearchtree,304pr.
traveling-salesmanproblem,1112 ofballsandbins,133–134
implementedwithaFibonacciheap,636 ofbirthdayparadox,130–133
implementedwithamin-heap,636 ofbucketsort,201–204,204ex.
withintegeredgeweights,637ex. ofcollisions,261ex.,282ex.
Index 1281
ofconvexhulloverasparse-hulled procedure,6,16–17
distribution,1046pr. product. /,1148
offilecomparison,995ex. Cartesian,1162
Q
offuzzysortingofintervals,189pr. cross,1016
ofhashingwithchaining,258–260 inner,1222
ofheightofarandomlybuiltbinarysearch ofmatrices,1221,1226ex.
tree,299–303 outer,1222
ofhiringproblem,120–121,139–141 ofpolynomials,899
ofinsertionintoabinarysearchtreewith ruleof,1184
equalkeys,303pr. scalarflow,714ex.
oflongest-probeboundforhashing,282pr. professionalwrestler,602ex.
oflowerboundforsorting,205pr. programcounter,1073
ofMiller-Rabinprimalitytest,971–975 programming,seedynamicprogramming,
andmultithreadedalgorithms,811pr. linearprogramming
ofon-linehiringproblem,139–141 properancestor,1176
ofopen-addresshashing,274–276,277ex. properdescendant,1176
ofpartitioning,179ex.,185ex.,187–188pr. propersubgroup,944
ofperfecthashing,279–282 propersubset( ),1159

ofPollard’srhoheuristic,977–980 protovanEmdeBoasstructure,538–545
ofprobabilisticcounting,143pr. clusterin,538
ofquicksort,181–184,187–188pr.,303ex. comparedwithvanEmdeBoastrees,547
ofRabin-Karpalgorithm,994 deletionfrom,544
andrandomizedalgorithms,123–124 insertioninto,544
ofrandomizedselection,217–219,226pr. maximumin,544ex.
ofsearchingacompactlist,250pr. membershipin,540–541
ofslot-sizeboundforchaining,283pr. minimumin,541–542
ofsortingpointsbydistancefromorigin, predecessorin,544ex.
204ex. successorin,543–544
ofstreaks,135–139 summaryin,540
ofuniversalhashing,265–268 PROTO-VEB-INSERT,544
probabilisticcounting,143pr. PROTO-VEB-MEMBER,541
probability,1189–1196 PROTO-VEB-MINIMUM,542
probabilitydensityfunction,1196 proto-vEBstructure,seeprotovanEmdeBoas
probabilitydistribution,1190 structure
probabilitydistributionfunction,204ex. PROTO-VEB-SUCCESSOR,543
probesequence,270 prune-and-searchmethod,1030
probing,270,282pr. pruningaFibonacciheap,529pr.
seealsolinearprobing,quadraticprobing, P-SCAN-1,808pr.
doublehashing P-SCAN-2,808pr.
problem P-SCAN-3,809pr.
abstract,1054 P-SCAN-DOWN,809pr.
computational,5–6 P-SCAN-UP,809pr.
concrete,1055 pseudocode,16,20–22
decision,1051,1054 pseudoinverse,837
intractable,1048 pseudoprime,966–968
optimization,359,1050,1054 PSEUDOPRIME,967
solutionto,6,1054–1055 pseudorandom-numbergenerator,117
tractable,1048 P-SQUARE-MATRIX-MULTIPLY,793
1282 Index
P-TRANSPOSE,792ex. QUICKSORT,171
publickey,959,962 QUICKSORT0,186pr.
public-keycryptosystem,958–965,983 quotient,928
PUSH
push-relabeloperation,739 R(setofrealnumbers),1158
stackoperation,233,452 Rabin-Karpalgorithm,990–995,1013
pushontoarun-timestack,188pr. RABIN-KARP-MATCHER,993
pushoperation(inpush-relabelalgorithms), race,787–790
738–739 RACE-EXAMPLE,788
nonsaturating,739,745 radixsort,197–200
saturating,739,745 comparedwithquicksort,199
push-relabelalgorithm,736–760,765 RADIX-SORT,198
basicoperationsin,738–740 radixtree,304pr.
bydischarginganoverflowingvertexof RAM,23–24
maximumheight,760ex. RANDOM,117
tofindamaximumbipartitematching, random-accessmachine,23–24
747ex. parallel,811
gapheuristicfor,760ex.,766 randomizedalgorithm,116–117,122–130
genericalgorithm,740–748 andaverageinputs,28
withaqueueofoverflowingvertices,759ex. comparisonsort,205pr.
relabel-to-frontalgorithm,748–760 forfuzzysortingofintervals,189pr.
forhiringproblem,123–124
quadraticfunction,27 forinsertionintoabinarysearchtreewith
quadraticprobing,272,283pr. equalkeys,303pr.
quadraticresidue,982pr. forMAX-3-CNFsatisfiability,1123–1124,
quantile,223ex. 1139
query,230 Miller-Rabinprimalitytest,968–975,983
queue,232,234–235 multithreaded,811pr.
inbreadth-firstsearch,595 forpartitioning,179,185ex.,187–188pr.
implementedbystacks,236ex. forpermutinganarray,124–128
linked-listimplementationof,240ex. Pollard’srhoheuristic,976–980,980ex.,
priority,seepriorityqueue 984
inpush-relabelalgorithms,759ex. andprobabilisticanalysis,123–124
quicksort,170–190 quicksort,179–180,185ex.,187–188pr.
analysisof,174–185 randomizedrounding,1139
average-caseanalysisof,181–184 forsearchingacompactlist,250pr.
comparedwithinsertionsort,178ex. forselection,215–220
comparedwithradixsort,199 universalhashing,265–268
withequalelementvalues,186pr. worst-caseperformanceof,180ex.
goodworst-caseimplementationof,223ex. RANDOMIZED-HIRE-ASSISTANT,124
“killeradversary”for,190 RANDOMIZED-PARTITION,179
withmedian-of-3method,188pr. RANDOMIZED-QUICKSORT,179,303ex.
multithreadedalgorithmfor,811pr. relationtorandomlybuiltbinarysearch
randomizedversionof,179–180,187pr. trees,304pr.
stackdepthof,188pr. randomizedrounding,1139
tail-recursiveversionof,188pr. RANDOMIZED-SELECT,216
useofinsertionsortin,185ex. RANDOMIZE-IN-PLACE,126
worst-caseanalysisof,180–181
Index 1283
randomlybuiltbinarysearchtree,299–303, RECURSIVE-FFT,911
304pr. RECURSIVE-MATRIX-CHAIN,385
random-numbergenerator,117 red-blacktree,308–338
randompermutation,124–128 augmentationof,346–347
uniform,116,125 comparedwithB-trees,484,490
RANDOM-SAMPLE,130ex. deletionfrom,323–330
randomsampling,129ex.,179 indeterminingwhetheranylinesegments
RANDOM-SEARCH,143pr. intersect,1024
randomvariable,1196–1201 forenumeratingkeysinarange,348ex.
indicator,seeindicatorrandomvariable heightof,309
range,1167 insertioninto,315–323
ofamatrix,1228pr. joiningof,332pr.
rank maximumkeyof,311
column,1223 minimumkeyof,311
full,1223 predecessorin,311
ofamatrix,1223,1226ex. propertiesof,308–312
ofanodeinadisjoint-setforest,569,575, relaxed,311ex.
581ex. restructuring,474pr.
ofanumberinanorderedset,300,339 rotationin,312–314
inorder-statistictrees,341–343,344–345ex. searchingin,311
row,1223 successorin,311
rateofgrowth,28 seealsointervaltree,order-statistictree
ray,1021ex. REDUCE,807pr.
RB-DELETE,324 reduced-spacevanEmdeBoastree,557pr.
RB-DELETE-FIXUP,326 reducibility,1067–1068
RB-ENUMERATE,348ex. reductionalgorithm,1052,1067
RB-INSERT,315 reductionfunction,1067
RB-INSERT-FIXUP,316 reduction,ofanarray,807pr.
RB-JOIN,332pr. reflexiverelation,1163
RB-TRANSPLANT,323 reflexivityofasymptoticnotation,51
reachabilityinagraph(;),1170 region,feasible,847
realnumbers(R),1158 regularitycondition,95
reconstructinganoptimalsolution,indynamic rejection
programming,387 byanalgorithm,1058
record,147 byafiniteautomaton,996
rectangle,354ex. RELABEL,740
recurrence,34,65–67,83–113 relabeledvertex,740
solutionbyAkra-Bazzimethod,112–113 relabeloperation,inpush-relabelalgorithms,
solutionbymastermethod,93–97 740,745
solutionbyrecursion-treemethod,88–93 RELABEL-TO-FRONT,755
solutionbysubstitutionmethod,83–88 relabel-to-frontalgorithm,748–760
recurrenceequation,seerecurrence phaseof,758
recursion,30 relation,1163–1166
recursiontree,37,88–93 relativelyprime,931
inproofofmastertheorem,98–100 RELAX,649
andthesubstitutionmethod,91–92 relaxation
RECURSIVE-ACTIVITY-SELECTOR,419 ofanedge,648–650
recursivecase,65 linearprogramming,1125
1284 Index
relaxedheap,530 row-majororder,394
relaxedred-blacktree,311ex. rowrank,1223
releasetime,447pr. rowvector,1218
remainder,54,928 RSApublic-keycryptosystem,958–965,983
remainderinstruction,23 RS-vEBtree,557pr.
repeatedsquaring ruleofproduct,1184
forall-pairsshortestpaths,689–691 ruleofsum,1183
forraisinganumbertoapower,956 runningtime,25
repeat,inpseudocode,20 average-case,28,116
repetitionfactor,ofastring,1012pr. best-case,29ex.,49
REPETITION-MATCHER,1013pr. expected,28,117
representativeofaset,561 ofagraphalgorithm,588
RESET,459ex. andmultithreadedcomputation,779–780
residualcapacity,716,719 orderofgrowth,28
residualedge,716 rateofgrowth,28
residualnetwork,715–719 worst-case,27,49
residue,54,928,982pr.
respectingasetofedges,626 sabermetrics,412n.
returnedge,779 safeedge,626
return,inpseudocode,22 SAME-COMPONENT,563
returninstruction,23 samplespace,1189
reweighting sampling,129ex.,179
inall-pairsshortestpaths,700–702 SAT,1079
insingle-sourceshortestpaths,679pr. satellitedata,147,229
rhoheuristic,976–980,980ex.,984 satisfiability,1072,1079–1081,1105,
.n/-approximationalgorithm,1106,1123 1123–1124,1127ex.,1139
RIGHT,152 satisfiableformula,1049,1079
rightchild,1178 satisfyingassignment,1072,1079
right-conversion,314ex. saturatededge,739
righthorizontalray,1021ex. saturatingpush,739,745
RIGHT-ROTATE,313 scalarflowproduct,714ex.
rightrotation,312 scalarmultiple,1220
rightspine,333pr. scaling
rightsubtree,1178 inmaximumflow,762pr.,765
rod-cutting,360–370,390ex. insingle-sourceshortestpaths,679pr.
root scan,807pr.
ofatree,1176 SCAN,807pr.
ofunity,906–907 scapegoattree,338
ofZ n,955 schedule,444,1136pr.
rootedtree,1176 event-point,1023
representationof,246–249 scheduler,formultithreadedcomputations,
rootlist,ofaFibonacciheap,509 777,781–783,812
rotation centralized,782
cyclic,1012ex. greedy,782
inared-blacktree,312–314 work-stealingalgorithmfor,812
rotationalsweep,1030–1038 scheduling,443–446,447pr.,450,1104pr.,
rounding,1126 1136pr.
randomized,1139 Schurcomplement,820,834
Index 1285
Schurcomplementlemma,834 sequence( )
hi
SCRAMBLE-SEARCH,143pr. bitonic,682pr.
seamcarving,409pr.,413 finite,1166
SEARCH,230 infinite,1166
searching,22ex. inversionin,41pr.,122ex.,345ex.
binarysearch,39ex.,799–800 probe,270
inbinarysearchtrees,289–291 sequentialconsistency,779,812
inB-trees,491–492 serialalgorithmversusparallelalgorithm,772
inchainedhashtables,258 serialization,ofamultithreadedalgorithm,
incompactlists,250pr. 774,776
indirect-addresstables,254 series,108pr.,1146–1148
foranexactinterval,354ex. strandsbeinglogicallyin,778
inintervaltrees,350–353 set( ),1158–1163
fg
linearsearch,22ex. cardinality( ),1161
jj
inlinkedlists,237 convex,714ex.
inopen-addresshashtables,270–271 difference( ),1159
(cid:0)
inprotovanEmdeBoasstructures,540–541 independent,1101pr.
inred-blacktrees,311 intersection( ),1159
\
inanunsortedarray,143pr. member( ),1158
2
inVanEmdeBoastrees,550 notamember( ),1158
62
searchtree,seebalancedsearchtree,binary union( ),1159
[
searchtree,B-tree,exponentialsearch set-coveringproblem,1117–1122,1139
tree,intervaltree,optimalbinarysearch weighted,1135pr.
tree,order-statistictree,red-blacktree, set-partitionproblem,1101ex.
splaytree,2-3tree,2-3-4tree shadowofapoint,1038ex.
secondaryclustering,272 sharedmemory,772
secondaryhashtable,278 Shell’ssort,42
secondarystorage shift,instringmatching,985
searchtreefor,484–504 shiftinstruction,24
stackson,502pr. short-circuitingoperator,22
second-bestminimumspanningtree,638pr. SHORTEST-PATH,1050
secretkey,959,962 shortestpaths,7,643–707
segment,seedirectedsegment,linesegment all-pairs,644,684–707
SEGMENTS-INTERSECT,1018 Bellman-Fordalgorithmfor,651–655
SELECT,220 withbitonicpaths,682pr.
selection,213 andbreadth-firstsearch,597–600,644
ofactivities,415–422,450 convergencepropertyof,650,672–673
andcomparisonsorts,222 anddifferenceconstraints,664–670
inexpectedlineartime,215–220 Dijkstra’salgorithmfor,658–664
multithreaded,805ex. inadirectedacyclicgraph,655–658
inorder-statistictrees,340–341 in-densegraphs,706pr.
inworst-caselineartime,220–224 estimateof,648
selectionsort,29ex. Floyd-Warshallalgorithmfor,693–697,
selectorvertex,1093 700ex.,706
self-loop,1168 Gabow’sscalingalgorithmfor,679pr.
self-organizinglist,476pr.,478 Johnson’salgorithmfor,700–706
semiconnectedgraph,621ex. asalinearprogram,859–860
sentinel,31,238–240,309 andlongestpaths,1048
1286 Index
bymatrixmultiplication,686–693,706–707 singlylinkedlist,236
andnegative-weightcycles,645,653–654, seealsolinkedlist
692ex.,700ex. singularmatrix,1223
withnegative-weightedges,645–646 singularvaluedecomposition,842
no-pathpropertyof,650,672 sinkvertex,593ex.,709,712
optimalsubstructureof,644–645,687, size
693–694 ofanalgorithm’sinput,25,926–927,
path-relaxationpropertyof,650,673 1055–1057
predecessor-subgraphpropertyof,650,676 ofabinomialtree,527pr.
problemvariants,644 ofabooleancombinationalcircuit,1072
andrelaxation,648–650 ofaclique,1086
byrepeatedsquaring,689–691 ofaset,1161
single-destination,644 ofasubtreeinaFibonacciheap,524
single-pair,381,644 ofavertexcover,1089,1108
single-source,643–683 skiplist,338
treeof,647–648,673–676 slack,855
triangleinequalityof,650,671 slackform,846,854–857
inanunweightedgraph,381,597 uniquenessof,876
upper-boundpropertyof,650,671–672 slackness
inaweightedgraph,643 complementary,894pr.
sibling,1176 parallel,781
sideofapolygon,1020ex. slackvariable,855
signature,960 slot
simplecycle,1170 ofadirect-accesstable,254
simplegraph,1170 ofahashtable,256
simplepath,1170 SLOW-ALL-PAIRS-SHORTEST-PATHS,689
longest,382,1048 smoothedanalysis,897
simplepolygon,1020ex. ?Socrates,790
simplestencilcalculation,809pr. solution
simpleuniformhashing,259 toanabstractproblem,1054
simplex,848 basic,866
SIMPLEX,871 toacomputationalproblem,6
simplexalgorithm,848,864–879,896–897 toaconcreteproblem,1055
single-destinationshortestpaths,644 feasible,665,846,851
single-pairshortestpath,381,644 infeasible,851
asalinearprogram,859–860 optimal,851
single-sourceshortestpaths,643–683 toasystemoflinearequations,814
Bellman-Fordalgorithmfor,651–655 sortedlinkedlist,236
withbitonicpaths,682pr. seealsolinkedlist
anddifferenceconstraints,664–670 sorting,5,16–20,30–37,147–212,797–805
Dijkstra’salgorithmfor,658–664 bubblesort,40pr.
inadirectedacyclicgraph,655–658 bucketsort,200–204
in-densegraphs,706pr. columnsort,208pr.
Gabow’sscalingalgorithmfor,679pr. comparisonsort,191
asalinearprogram,863ex. countingsort,194–197
andlongestpaths,1048 fuzzy,189pr.
singleton,1161 heapsort,151–169
singlyconnectedgraph,612ex. insertionsort,12,16–20
Index 1287
k-sorting,207pr. spurioushit,991
lexicographic,304pr. squarematrix,1218
inlineartime,194–204,206pr. SQUARE-MATRIX-MULTIPLY,75,689
lowerboundsfor,191–194,211,531 SQUARE-MATRIX-MULTIPLY-RECURSIVE,
mergesort,12,30–37,797–805 77
byobliviouscompare-exchangealgorithms, squareofadirectedgraph,593ex.
208pr. squareroot,moduloaprime,982pr.
inplace,17,148,206pr. squaring,repeated
ofpointsbypolarangle,1020ex. forall-pairsshortestpaths,689–691
probabilisticlowerboundfor,205pr. forraisinganumbertoapower,956
quicksort,170–190 stability
radixsort,197–200 numerical,813,815,842
selectionsort,29ex. ofsortingalgorithms,196,200ex.
Shell’ssort,42 stack,232–233
stable,196 inGraham’sscan,1030
tableofrunningtimes,149 implementedbyqueues,236ex.
topological,8,612–615,623 linked-listimplementationof,240ex.
usingabinarysearchtree,299ex. operationsanalyzedbyaccountingmethod,
withvariable-lengthitems,206pr. 457–458
0-1sortinglemma,208pr. operationsanalyzedbyaggregateanalysis,
sortingnetwork,811 452–454
sourcevertex,594,644,709,712 operationsanalyzedbypotentialmethod,
spanlaw,780 460–461
spanningtree,439,624 forprocedureexecution,188pr.
bottleneck,640pr. onsecondarystorage,502pr.
maximum,1137pr. STACK-EMPTY,233
verificationof,642 standarddeviation,1200
seealsominimumspanningtree standardencoding( ),1057
hi
span,ofamultithreadedcomputation,779 standardform,846,850–854
sparsegraph,589 star-shapedpolygon,1038ex.
all-pairsshortestpathsfor,700–705 startstate,995
andPrim’salgorithm,638pr. starttime,415
sparse-hulleddistribution,1046pr. stateofafiniteautomaton,995
spawn,inpseudocode,776–777 staticgraph,562n.
spawnedge,778 staticsetofkeys,277
speedup,780 staticthreading,773
ofarandomizedmultithreadedalgorithm, stencil,809pr.
811pr. stencilcalculation,809pr.
spindle,485 Stirling’sapproximation,57
spine storagemanagement,151,243–244,245ex.,
ofastring-matchingautomaton,997fig. 261ex.
ofatreap,333pr. storeinstruction,23
splaytree,338,482 straddle,1017
spline,840pr. strand,777
splitting final,779
ofB-treenodes,493–495 independent,789
of2-3-4trees,503pr. initial,779
splittingsummations,1152–1154 logicallyinparallel,778
1288 Index
logicallyinseries,778 success,inaBernoullitrial,1201
Strassen’salgorithm,79–83,111–112 successor
multithreaded,795–796 inbinarysearchtrees,291–292
streaks,135–139 inabitvectorwithasuperimposedbinary
strictlydecreasing,53 tree,533
strictlyincreasing,53 inabitvectorwithasuperimposedtreeof
string,985,1184 constantheight,535
stringmatching,985–1013 findingith,ofanodeinanorder-statistic
basedonrepetitionfactors,1012pr. tree,344ex.
byfiniteautomata,995–1002 inlinkedlists,236
withgapcharacters,989ex.,1002ex. inorder-statistictrees,347ex.
Knuth-Morris-Prattalgorithmfor, inprotovanEmdeBoasstructures,543–544
1002–1013 inred-blacktrees,311
naivealgorithmfor,988–990 inVanEmdeBoastrees,550–551
Rabin-Karpalgorithmfor,990–995,1013 SUCCESSOR,230
string-matchingautomaton,996–1002, suchthat(),1159
1002ex.
suffix(=),W
986
stronglyconnectedcomponent,1171 suffixfunction,996
decompositioninto,615–621,623 suffix-functioninequality,999
STRONGLY-CONNECTED-COMPONENTS,617 suffix-functionrecursionlemma,1000
stronglyconnectedgraph,1171 sum. /,1145
subgraph,1171 Cartesian,906ex.
P
predecessor,seepredecessorsubgraph infinite,1145
subgraph-isomorphismproblem,1100ex. ofmatrices,1220
subgroup,943–946 ofpolynomials,898
subpath,1170 ruleof,1183
subproblemgraph,367–368 telescoping,1148
subroutine SUM-ARRAYS,805pr.
calling,21,23,25n. SUM-ARRAYS0,805pr.
executing,25n. summary
subsequence,391 inabitvectorwithasuperimposedtreeof
subset( ),1159,1161 constantheight,534

hereditaryfamilyof,437 inprotovanEmdeBoasstructures,540
independentfamilyof,437 invanEmdeBoastrees,546
SUBSET-SUM,1097 summation,1145–1157
subset-sumproblem inasymptoticnotation,49–50,1146
approximationalgorithmfor,1128–1134, bounding,1149–1156
1139 formulasandpropertiesof,1145–1149
NP-completenessof,1097–1100 linearityof,1146
withunarytarget,1101ex. summationlemma,908
substitutionmethod,83–88 supercomputer,772
andrecursiontrees,91–92 superpolynomialtime,1048
substring,1184 supersink,712
subtractinstruction,23 supersource,712
subtractionofmatrices,1221 surjection,1167
subtree,1176 SVD,842
maintainingsizesof,inorder-statistictrees, sweeping,1021–1029,1045pr.
343–344 rotational,1030–1038
Index 1289
sweepline,1022 tightconstraint,865
sweep-linestatus,1023–1024 time,seerunningtime
symboltable,253,262,265 timedomain,898
symmetricdifference,763pr. time-memorytrade-off,365
symmetricmatrix,1220,1222ex.,1226ex. timestamp,603,611ex.
symmetricpositive-definitematrix,832–835, Toeplitzmatrix,921pr.
842 to,inpseudocode,20
symmetricrelation,1163 TOP,1031
symmetryof‚-notation,52 top-downmethod,fordynamicprogramming,
sync,inpseudocode,776–777 365
systemofdifferenceconstraints,664–670 topofastack,232
systemoflinearequations,806pr.,813–827, topologicalsort,8,612–615,623
840pr. incomputingsingle-sourceshortestpathsin
adag,655
TABLE-DELETE,468 TOPOLOGICAL-SORT,613
TABLE-INSERT,464 totalorder,1165
tail totalpathlength,304pr.
ofabinomialdistribution,1208–1215 totalpreorder,1165
ofalinkedlist,236 totalrelation,1165
ofaqueue,234 tour
tailrecursion,188pr.,419 bitonic,405pr.
TAIL-RECURSIVE-QUICKSORT,188pr. Euler,623pr.,1048
target,1097 ofagraph,1096
Tarjan’soff-lineleast-common-ancestors track,486
algorithm,584pr. tractability,1048
task,443 trailingpointer,295
TaskParallelLibrary,774 transitionfunction,995,1001–1002,1012ex.
taskscheduling,443–446,448pr.,450 transitiveclosure,697–699
tautology,1066ex.,1086ex. andbooleanmatrixmultiplication,832ex.
Taylorseries,306pr. ofdynamicgraphs,705pr.,707
telescopingseries,1148 TRANSITIVE-CLOSURE,698
telescopingsum,1148 transitiverelation,1163
testing transitivityofasymptoticnotation,51
ofprimality,965–975,983 TRANSPLANT,296,323
ofpseudoprimality,966–968 transpose
text,instringmatching,985 conjugate,832ex.
thenclause,20n. ofadirectedgraph,592ex.
Theta-notation,44–47,64 ofamatrix,1217
thread,773 ofamatrix,multithreaded,792ex.
ThreadingBuildingBlocks,774 transposesymmetryofasymptoticnotation,52
3-CNF,1082 traveling-salesmanproblem
3-CNF-SAT,1082 approximationalgorithmfor,1111–1117,
3-CNFsatisfiability,1082–1085,1105 1139
approximationalgorithmfor,1123–1124, bitoniceuclidean,405pr.
1139 bottleneck,1117ex.
and2-CNFsatisfiability,1049 NP-completenessof,1096–1097
3-COLOR,1103pr. withthetriangleinequality,1112–1115
3-conjunctivenormalform,1082 withoutthetriangleinequality,1115–1116
1290 Index
traversalofatree,287,293ex.,342,1114 TREE-PREDECESSOR,292
treap,333pr.,338 TREE-SEARCH,290
TREAP-INSERT,333pr. TREE-SUCCESSOR,292
tree,1173–1180 treewalk,287,293ex.,342,1114
AA-trees,338 trial,Bernoulli,1201
AVL,333pr.,337 trialdivision,966
binary,seebinarytree triangleinequality,1112
binomial,527pr. forshortestpaths,650,671
bisectionof,1181pr. triangularmatrix,1219,1222ex.,1225ex.
breadth-first,594,600 trichotomy,interval,348
B-trees,484–504 trichotomypropertyofrealnumbers,52
decision,192–193 tridiagonallinearsystems,840pr.
depth-first,603 tridiagonalmatrix,1219
diameterof,602ex. trie(radixtree),304pr.
dynamic,482 y-fast,558pr.
free,1172–1176 TRIM,1130
fullwalkof,1114 trimmingalist,1130
fusion,212,483 trivialdivisor,928
heap,151–169 truthassignment,1072,1079
height-balanced,333pr. truthtable,1070
heightof,1177 TSP,1096
interval,348–354 tuple,1162
k-neighbor,338 twiddlefactor,912
minimumspanning,seeminimumspanning 2-CNF-SAT,1086ex.
tree 2-CNFsatisfiability,1086ex.
optimalbinarysearch,397–404,413 and3-CNFsatisfiability,1049
order-statistic,339–345 two-passmethod,571
parse,1082 2-3-4heap,529pr.
recursion,37,88–93 2-3-4tree,489
red-black,seered-blacktree joining,503pr.
rooted,246–249,1176 splitting,503pr.
scapegoat,338 2-3tree,337,504
search,seesearchtree
shortest-paths,647–648,673–676 unary,1056
spanning,seeminimumspanningtree, unboundedlinearprogram,851
spanningtree unconditionalbranchinstruction,23
splay,338,482 uncountableset,1161
treap,333pr.,338 underdeterminedsystemoflinearequations,
2-3,337,504 814
2-3-4,489,503pr. underflow
vanEmdeBoas,531–560 ofaqueue,234
walkof,287,293ex.,342,1114 ofastack,233
weight-balancedtrees,338 undirectedgraph,1168
TREE-DELETE,298,299ex.,323–324 articulationpointof,621pr.
treeedge,601,603,609 biconnectedcomponentof,621pr.
TREE-INSERT,294,315 bridgeof,621pr.
TREE-MAXIMUM,291 cliquein,1086
TREE-MINIMUM,291 coloringof,1103pr.,1180pr.
Index 1291
computingaminimumspanningtreein, upper-boundproperty,650,671–672
624–642 uppermedian,213
convertingto,fromamultigraph,593ex. uppersquareroot p" ,546
d-regular,736ex. upper-triangularmatrix,1219,1225ex.
(cid:0) 
grid,760pr.
hamiltonian,1061 validshift,985
independentsetof,1101pr. value
matchingof,732 ofaflow,710
nonhamiltonian,1061 ofafunction,1166
vertexcoverof,1089,1108 objective,847,851
seealsograph valueoverreplacementplayer,411pr.
undirectedversionofadirectedgraph,1172 Vandermondematrix,902,1226pr.
uniformhashing,271 vanEmdeBoastree,531–560
uniformprobabilitydistribution,1191–1192 clusterin,546
uniformrandompermutation,116,125 comparedwithprotovanEmdeBoas
union structures,547
ofdynamicsets,seeuniting deletionfrom,554–556
oflanguages,1058 insertioninto,552–554
ofsets( ),1159 maximumin,550
[
UNION,505,562 membershipin,550
disjoint-set-forestimplementationof,571 minimumin,550
linked-listimplementationof,565–567, predecessorin,551–552
568ex. withreducedspace,557pr.
unionbyrank,569 successorin,550–551
uniquefactorizationofintegers,931 summaryin,546
unit(1),928 VarŒ(variance),1199
uniting variable
ofFibonacciheaps,511–512 basic,855
ofheaps,506 entering,867
oflinkedlists,241ex. leaving,867
of2-3-4heaps,529pr. nonbasic,855
unitlower-triangularmatrix,1219 inpseudocode,21
unit-timetask,443 random,1196–1201
unitupper-triangularmatrix,1219 slack,855
unitvector,1218 seealsoindicatorrandomvariable
universalcollectionofhashfunctions,265 variable-lengthcode,429
universalhashing,265–268 variance,1199
universalsink,593ex. ofabinomialdistribution,1205
universe,1160 ofageometricdistribution,1203
ofkeysinvanEmdeBoastrees,532 VEB-EMPTY-TREE-INSERT,553
universesize,532 vEBtree,seevanEmdeBoastree
unmatchedvertex,732 VEB-TREE-DELETE,554
unsortedlinkedlist,236 VEB-TREE-INSERT,553
seealsolinkedlist VEB-TREE-MAXIMUM,550
until,inpseudocode,20 VEB-TREE-MEMBER,550
unweightedlongestsimplepaths,382 VEB-TREE-MINIMUM,550
unweightedshortestpaths,381 VEB-TREE-PREDECESSOR,552
upperbound,47 VEB-TREE-SUCCESSOR,551
1292 Index
vector,1218,1222–1224 inaweightedmatroid,439
convolutionof,901 while,inpseudocode,20
crossproductof,1016 white-paththeorem,608
orthonormal,842 whitevertex,594,603
intheplane,1015 widget,1092
Venndiagram,1160 wire,1071
verification,1061–1066 WITNESS,969
ofspanningtrees,642 witness,tothecompositenessofanumber,968
verificationalgorithm,1063 worklaw,780
vertex work,ofamultithreadedcomputation,779
articulationpoint,621pr. work-stealingschedulingalgorithm,812
attributesof,592 worst-caserunningtime,27,49
capacityof,714ex.
inagraph,1168 Yen’simprovementtotheBellman-Ford
intermediate,693 algorithm,678pr.
isolated,1169 y-fasttrie,558pr.
overflowing,736 Youngtableau,167pr.
ofapolygon,1020ex.
relabeled,740 Z(setofintegers),1158
selector,1093 Z n(equivalenceclassesmodulon),928
vertexcover,1089,1108,1124–1127,1139 Z n(elementsofmultiplicativegroup
VERTEX-COVER,1090 modulon),941
vertex-coverproblem Z Cn (nonzeroelementsofZ n),967
approximationalgorithmfor,1108–1111, zeromatrix,1218
1139 zeroofapolynomialmoduloaprime,950ex.
NP-completenessof,1089–1091,1105 0-1integerprogramming,1100ex.,1125
vertexset,1168 0-1knapsackproblem,425,427ex.,1137pr.,
violation,ofanequalityconstraint,865 1139
virtualmemory,24 0-1sortinglemma,208pr.
Viterbialgorithm,408pr. zonk,1195ex.
VORP,411pr.
walkofatree,287,293ex.,342,1114
weakduality,880–881,886ex.,895pr.
weight
ofacut,1127ex.
ofanedge,591
mean,680pr.
ofapath,643
weight-balancedtree,338,473pr.
weightedbipartitematching,530
weightedmatroid,439–442
weightedmedian,225pr.
weightedset-coveringproblem,1135pr.
weighted-unionheuristic,566
weightedvertexcover,1124–1127,1139
weightfunction
foragraph,591

